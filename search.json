[{"title":"Java的IO模型","path":"/2024/02/05/nio/02【Java的IO模型】/","content":"文章目录 二、Java的IO模型 2.1 Java的IO模型支持 2.2 BIO（blocking I&#x2F;O）模型 2.3 NIO（non-blocking I&#x2F;O）模型 2.3.1 Buffer（缓冲区） 2.3.2 Channel（通道） 2.3.3 Selector（选择器） 2.3 AIO模型 2.4 JavaIO模型小结 二、Java的IO模型2.1 Java的IO模型支持Java共支持3种网络编程模型&#x2F;IO模式，分别是： BIO（同步阻塞IO） NIO（同步非阻塞IO） AIO（异步非阻塞IO） 我们可以根据不同的业务场景来决定选择不同I&#x2F;O处理模型； 2.2 BIO（blocking I&#x2F;O）模型 BIO（blocking I/O）：也叫同步阻塞IO，在JDK1.4之前，我们建立网络连接的时候采用的是 BIO 模式。 阻塞 IO（BIO）是最传统的一种 IO 模型，即在读写数据过程中会发生阻塞现象，直至有可供读取的数据或者数据能够写入。 在BIO模式中，服务器会为每个客户端的请求创建一个对应的线程来处理，由该线程单独负责处理一个客户请求，如果这个连接不做任何事情会造成不必要的线程开销，为此我们可以通过线程池机制改善性能； 虽然使用线程池可以一定程度上改善BIO的性能，但依旧无法冲本质上解决BIO同步阻塞的的问题；如果连接大多是长连接，则会导致连接无法释放，新的请求将无法得到处理，另外，BIO这种一个请求对应一个线程的方式在应对高并发的情况下，服务器必须也要创建同等量的线程来处理客户端的请求，这样对系统的消耗是非常大的； BIO优点： 实现简单，IO模式适用于连接数目比较小且固定的架构，是JDK1.4以前的唯一选择； BIO缺点： 1）每个请求都需要创建独立的线程来处理，当连接数较大时，需要创建大量的线程来处理 2）一个线程只能处理一个请求，连接建立后，如果当前线程暂时没有数据可读，那么该线程就一直阻塞在读操作，并不能处理其他事情，造成性能浪费； 2.3 NIO（non-blocking I&#x2F;O）模型NIO是从JDK1.4版本开始引入的一个新的IO API，NIO支持面向缓冲区的、基于通道的IO操作。NIO将以更加高效的方式进行文件的读写操作。 BIO是同步阻塞IO，同步：即在同一时间点只能同时处理一个客户端连接，阻塞：即当调用方法获取数据时，如果没有可用的数据将会阻塞当前线程； NIO则是同步非阻塞IO，NIO中有三大组件，分别是：Channel（通道），Buffer（缓冲区），Selector（选择器）；当有客户端连接时，服务器可以获取与该客户端的连接Channel（通道），所有的通道都会被注册到Selector（选择器）上，当Channel上有读写数据时将会被Selector侦测到，服务器只需要派发一个线程来处理Selector上的事件即可；当前Channel如果没有读写数据时，Selector并不会一直阻塞的等待Channel的数据返回，而是轮询式的侦测所有的Channel，这是NIO非阻塞的核心； 另外，客户端的连接都变成了Channel，这些Channel都注册到了Selector中，服务器再也不需要为每一个连接来创建一个独立的线程为之服务了；这也是NIO能够应对高并发的核心之一； NIO（non-blocking IO）：也叫同步非阻塞IO，由于BIO的各种弊端，JDK1.4从开始提供了一系列改进的输入&#x2F;输出的新特性，被统称为 NIO(即 New IO)，是同步非阻塞的。NIO相关类都被放在java.nio包及子包下，并且对原java.io包中的很多类进行改写。 在NIO模型中，每个请求都会有一个与服务器做数据交互的通道（Channel），所有的通道都被注册到一个选择器中（selector），当需要与服务器做数据交互时，数据通过管道写入到一个缓冲区（Buffer）中，服务器通过往缓冲区中读取数据，如果当前通道没有数据时，就什么都不会获取，而不是保持线程阻塞，直至数据变的可以读取之前，该线程可以继续做其他的事情。 在Java NIO有三大核心部分：Buffer（缓冲区）、Channel（通道）、Selector（选择器） ； 2.3.1 Buffer（缓冲区）Buffer本质上就是一块存储数据的内存，我们可以在这一块内存中进行读写操作，这与我们之前的数组非常类似。与数组不同的是，这块内存被封装成Buffer对象，并根据不同的数据类型提供有不同的Buffer子类。Java对Buffer提供了更多的API，使得Buffer功能更加强大； Java中常见的Buffer如下： CharBuffer DoubleBuffer IntBuffer LongBuffer ByteBuffer ShortBuffer FloatBuffer Tips：以上Buffer都继承与Buffer抽象类，StringBuffer和以上的Buffer并不是同一类的，没有继承与NIO包下的Buffer接口； 2.3.2 Channel（通道）Java NIO的通道类似流，都是用于传输数据的。但通过又与流有些不同；流的数据走向是单向的，分为输入流（只能读取数据），输出流（只能写出数据），但NIO中的通道不一样，通道既可以写数据到Buffer，又可以从Buffer中读取数据； 另外流的操作对象是数组，而通道的操作对象是Buffer； Java中常见的Channel如下： FileChannel：用于文件 I&#x2F;O 编程 SocketChannel、ServerSocketChannel：用于 TCP I&#x2F;O 编程 DatagramChannel：用于 UDP I&#x2F;O 编程 2.3.3 Selector（选择器）Selector选择器，也叫多路复用器；NIO中实现非阻塞 I&#x2F;O 的核心对象就是 Selector。当一个连接创建后，不需要创建一个线程来处理这个来连接，这个连接（管道）会被注册到选择器上，选择器可以检查一个或多个 NIO 通道，并确定哪些通道已经准备好进行读取或写入。这样，一个单独的线程可以管理多个channel，系统不必创建大量的线程，也不必维护这些线程，从而大大减小了系统的开销。 2.3 AIO模型 AIO ：AIO也叫异步非阻塞，JDK1.7之后的新特性，AIO引入异步通道的概念，采用了 Proactor 模式，简化了程序编写，有效的请求才启动线程，它的特点是先由操作系统完成后才通知服务端程序启动线程去处理，一般适用于连接数较多且连接时间较长的应用。 与NIO模型不同，读写操作为例，只需直接调用read和write的API即可，这方法都是异步的对于读操作:当有流可读是，系统会将可读的流传入到read方法的缓冲区，并通知应用程序读写都是异步的，完成之后会主动调用回调函数 AIO需要操作系统的支持，在Linux内核2.6版本之后增加了对真正异步IO的实现。Java从JDK1.7之后支持AIO，JDK1.7新增一些与文件&#x2F;网络IO相关的一些API，称之为NIO2.0或者称之为AIO（Asynchronous IO）。AIO最大的特征提供了异步功能，对于socket网络通信和文件IO都是起作用的。 目前 AIO 还没有广泛应用，Netty也是基于NIO，而不是AIO。 2.4 JavaIO模型小结 BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解； NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持； AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如文件服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 仅限学习、交流使用！ QQ群名片 本文转自 https://blog.csdn.net/Bb15070047748/article/details/125438283，如有侵权，请联系删除。","tags":["nio"]},{"title":"五种IO模型：操作系统五种IO模型大全","path":"/2024/02/05/nio/五种IO模型：操作系统五种IO模型大全/","content":"文章目录 五种IO模型：操作系统五种IO模型大全 一、IO模型简介 1.1 操作系统的内存简介 1.1.1 操作系统的应用与内核 1.1.2 内核空间与用户空间 1.1.3 CPU指令等级 1.1.4 进程的内核态和用户态 1.2 IO的分类 1.2.1 网络IO和磁盘IO 1.2.1 同步IO和异步IO 1.2.2 阻塞IO和非阻塞IO 1.3 操作系统的五种IO模型 1.3.1 阻塞IO模型 1.3.2 非阻塞IO模型 1.3.3 复用IO模型 1）select(时间复杂度O(n)) 2）poll(时间复杂度O(n)) 3）epoll (时间复杂度O(1)) 4）IO复用模型小结 1.3.4 信号驱动IO模型 1.3.5 异步IO模型 五种IO模型：操作系统五种IO模型大全一、IO模型简介1.1 操作系统的内存简介1.1.1 操作系统的应用与内核现代计算机是由硬件和操作系统组成，我们的应用程序要操作硬件（如往磁盘上写数据），就需要先与内核交互，然后再由内核与硬件交互； 操作系统可以划分为：内核与应用两部分； 内核提供进程管理、内存管理、网络等底层功能，封装了与硬件交互的接口，通过系统调用提供给上层应用使用。 1.1.2 内核空间与用户空间现在操作系统都是采用虚拟地址空间，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间（内核空间），也有访问底层硬件设备的所有权限。 为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。内核空间是操作系统内核访问的区域，独立于普通的应用程序，是受保护的内存空间。用户空间是普通应用程序可访问的内存区域。 针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 用户态的程序不能随意操作内核地址空间，即使用户的程序崩溃了，内核也不受影响。这样对操作系统具有一定的安全保护作用。 1.1.3 CPU指令等级其实早期操作系统是不区分内核空间和用户空间的，但是应用程序能访问任意内存空间，如果程序不稳定常常把系统搞崩溃，比如清除操作系统的内存数据。后来觉得让应用程序随便访问内存太危险了，就按照CPU 指令的重要程度对指令进行了分级； CPU指令分为四个级别：Ring0~Ring3，linux 只使用了 Ring0 和 Ring3 两个运行级别，进程运行Ring3级别的指令时运行在用户态，指令只访问用户空间，而运行在 Ring0级别时被称为运行在内核态，可以访问任意内存空间。 1.1.4 进程的内核态和用户态当进程运行在内核空间时，它就处于内核态；当进程运行在用户空间时，它就处于用户态。 那什么时候运行再内核空间什么时候运行再用户空间呢？ 当我们需要进行IO操作时，如读写硬盘文件、读写网卡数据等，进程需要切换到内核态，否则无法进行这样的操作，无论是从内核态切换到用户态，还是从用户态切换到内核态，都需要进行一次上下文的切换。一般情况下，应用不能直接操作内核空间的数据，需要把内核态的数据拷贝到用户空间才能操作。 比如我们 Java 中需要新建一个线程，调用 start() 方法时，基于Hotspot Linux 的JVM 源码实现，最终是调pthread_create系统方法来创建的线程，这里会从用户态切换到内核态完成系统资源的分配，线程的创建。 当一个任务（进程）执行系统调用而陷入内核代码中执行时，称进程处于内核运行态（内核态） Tips：除了系统调用可以实现用户态到内核态的切换，软中断和硬中断也会切换用户态和内核态。 在内核态下：进程运行在内核地址空间中，此时 CPU 可以执行任何指令。运行的代码也不受任何的限制，可以自由地访问任何有效地址，也可以直接进行端口的访问。 在用户态下：进程运行在用户地址空间中，被执行的代码要受到 CPU 的很多检查，比如：进程只能访问映射其地址空间的页表项中规定的在用户态下可访问页面的虚拟地址。 1.2 IO的分类通常用户进程中的一个完整IO分为两阶段：用户进程空间&lt;- -&gt;内核空间、**内核空间&lt;- -&gt;设备空间(磁盘、网络等)**。 1.2.1 网络IO和磁盘IOIO从读取数据的来源分为内存IO、 网络IO和磁盘IO三种，通常我们说的IO指的是后两者(因为内存IO的读写速度比网络IO和磁盘IO快的多)。 I&#x2F;O按照设备来分的话，分为两种：一种是网络I&#x2F;O，也就是通过网络进行数据的拉取和输出。一种是磁盘I&#x2F;O，主要是对磁盘进行读写工作。 网络IO：等待网络数据到达网卡→把网卡中的数据读取到内核缓冲区，然后从内核缓冲区复制数据到进程空间。 磁盘IO：把数据从磁盘中读取到内核缓冲区，然后从内核缓冲区复制数据到进程空间。 Tips：由于CPU和内存的速度远远高于外部设备（网卡，磁盘等）的速度，所以在IO编程中，存在速度严重不匹配的问题。 1.2.1 同步IO和异步IO 同步：A调用B，B的处理是同步的，在处理完之前他不会通知A，只有处理完之后才会明确的通知A。B在没有处理完A的请求时不能处理其他请求； 异步：A调用B，B的处理是异步的，B在接到请求后先告诉A我已经接到请求了，然后异步去处理，处理完之后通过回调等方式再通知A。B在处理A请求的同时，也可以接着处理其他人发送过来的请求； 同步和异步最大的区别就是被调用方的执行方式和返回时机。同步指的是被调用方做完事情之后再返回，异步指的是被调用方先返回，然后再做事情，做完之后再想办法通知调用方。 1.2.2 阻塞IO和非阻塞IO 阻塞：A调用B，A一直等着B的返回，别的事情什么也不干。 非阻塞：A调用B，A不用一直等着B的返回，先去忙别的事情了。 阻塞和非阻塞最大的区别就是在被调用方返回结果之前的这段时间内，调用方是否一直等待。阻塞指的是调用方一直等待别的事情什么都不做。非阻塞指的是调用方先去忙别的事情。 Tips：同步和异步强调的是被调用方（B–操作系统），阻塞和非阻塞强调的是调用方（A–应用程序）； 1.3 操作系统的五种IO模型1.3.1 阻塞IO模型阻塞IO就是当应用A发起读取数据申请时，在内核数据没有准备好之前，应用A会一直处于等待数据状态，直到内核把数据准备好了交给应用A才结束。 Tips：我们之前所学过的所有的套接字，默认都是阻塞方式。 优点：开发相对简单，在阻塞期间，用户线程被挂起，挂起期间不会占用CPU资源； 缺点： 1）连接利用率不高，内核如果没有响应数据，则该连接一直处于阻塞状态，占用连接资源 2）一个线程维护一个IO资源，当用大量并发请求时，需要创建等价的线程来处理请求，不适合用于高并发场景； 1.3.2 非阻塞IO模型非阻塞IO就是当应用A发起读取数据申请时，如果内核数据没有准备好会即刻告诉应用A（返回错误码等），不会让A在这里等待。一旦内核中的数据准备好了，并且又再次收到了A的请求，那么它马上就将数据拷贝到了用户线程，然后返回。 优点：每次发起IO调用去内核获取数据时，在内核等待数据的过程中可以立即返回，用户线程不会被阻塞，实时性较好； 缺点： 1）当用户线程A没有获取到数据时，不断轮询内核，查看是否有新的数据，占用大量CPU时间，效率不高； 2）和阻塞IO一样，一个线程维护一个IO资源，当用大量并发请求时，需要创建等价的线程来处理请求，不适合用于高并发场景； 1.3.3 复用IO模型如果在并发的环境下，可能会N个人向应用B发送消息，这种情况下我们的应用就必须创建多个线程去接收N个人发送过来的请求，每个请求都是一个独立的线程来处理；在并发量呈线性增长时，我们需要创建的线程数也随之而然的激增； 这种情况下应用B就需要创建N个线程去读取数据，同时又因为应用线程是不知道什么时候会有数据读取，为了保证消息能及时读取到，那么这些线程自己必须不断的向内核发送请求来读取数据（非阻塞式）； 这么多的线程不断请求数据，先不说服务器能不能扛得住这么多线程，就算扛得住那么很明显这种方式是不是太浪费资源了，线程是我们操作系统的宝贵资源，大量的线程用来去读取数据了，那么就意味着能做其它事情的线程就会少。 后来，有人就提出了一个思路，能不能提供一种方式，可以由一个线程监控多个网络请求（linux系统把所有网络请求以一个fd来标识，我们后面将称为fd即文件描述符），这样就可以只需要一个或几个线程就可以完成数据状态询问的操作，当有数据准备就绪之后再分配对应的线程去读取数据，这么做就可以节省出大量的线程资源出来，这个就是IO复用模型的思路。 IO复用模型的思路就是系统提供了一种函数（select&#x2F;poll&#x2F;epoll）可以同时监控多个fd的操作，有了这个函数后，应用线程通过调用select函数就可以同时监控多个fd，如果select监听的fd都没有可读数据，select调用进程会被阻塞；而只要有任何一个fd准备就绪了，select函数就会返回可读状态，这时询问线程再去通知处理数据的线程，对应的线程此时再发起请求去读取内核中准备好的数据； Tips：在IO复用模型下，允许单线程内处理多个IO请求； Linux中IO复用的实现方式主要有select，poll和epoll 1）select(时间复杂度O(n)) select：线性轮询扫描所有的fd，不管他们是否活跃，监听的IO最大连接数不能多于FD_ SIZE（32位操作系统1024，64位操作系统2048）。 Tips：select方式仅仅知道有I&#x2F;O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），用户线程只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。 2）poll(时间复杂度O(n)) **poll**：原理和select相似，poll底层需要分配一个pollfd结构数组，维护在内核中，它没有数量限制，但IO数量大，扫描线性性能下降。 3）epoll (时间复杂度O(1)) epoll ：用于代替poll和select，没有大小限制。epoll采用事件驱动代替了轮询，epoll会把哪个流发生了怎样的I&#x2F;O事件通知用户线程，所以我们说epoll实际上是事件驱动（每个事件关联上fd）的，此时用户线程对这些流的操作都是有意义的。（复杂度降低到了O(1)），另外epoll模型采用mmap内存映射实现内核与用户空间的消息传递，减少用户态和内核态数据传输的开销，epoll模型在Linux2.6后内核支持。 select，poll，epoll都是IO多路复用的机制。I&#x2F;O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符准备就绪，能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I&#x2F;O，因为他们都需要在读写事件就绪后自己负责进行读写(一个个的处理)，也就是说这个读写过程是阻塞的，而异步I&#x2F;O则无需自己负责进行读写，异步I&#x2F;O的实现会负责把数据从内核拷贝到用户空间。 Tips：epoll跟select都能提供多路I&#x2F;O复用的解决方案。在现在的Linux内核里有都能够支持，其中epoll是Linux所特有，而select则应该是POSIX所规定，一般操作系统均有实现 4）IO复用模型小结 关于IO复用模型，下面这个例子可以很好的说明IO复用模型的原理： 某教室有10名学生和1名老师，这些学生上课会不停的提问，所以一个老师处理不了这么多的问题。那么学校为每个学生都配一名老师，也就是这个教室目前有10名老师。此后，只要有新的转校生，那么就会为这个学生专门分配一个老师，因为转校生也喜欢提问题。如果把以上例子中的学生比作客户端，那么老师就是负责进行数据交换的服务端。则该例子可以比作是多进程的方式。 后来有一天，来了一位具有超能力的老师，这位老师回答问题非常迅速，并且可以应对所有的问题。而这位老师采用的方式是学生提问前必须先举手，确认举手学生后在回答问题。则现在的情况就是IO复用。 IO复用模型的优点：系统不必创建和维护大量的线程，只使用一个或几个线程来监听select选择器的操作，而一个选择器可同时处理成千上万个连接，大大减少了系统的开销； IO复用模型的缺点：select本质上还是同步阻塞模式； 总结： 复用IO的基本思路就是通过select或poll、epoll来监控多fd ，来达到不必为每个fd创建一个对应的监控线程，从而减少线程资源创建的目的。复用IO模型的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。 1.3.4 信号驱动IO模型当进程发起一个IO操作，系统调用sigaction执行一个信号处理函数，该函数向内核注册一个信号处理函数（回调函数），然后进程返回，并且不阻塞当前进程；当内核数据准备好时，内核使用信号（SIGIO）通知应用线程调用recvfrom来读取数据（运行回调函数）。 信号驱动IO它也可以看成是一种异步非阻塞IO 我们说信号驱动IO模型是一种异步非阻塞IO模型，指的是用户线程去内核空间请求数据时，直接注册一个信号处理函数，然后用户线程返回（异步），而内核空间接收到请求后，开始处理（此时并不会阻塞，内核空间可以同时接收多个请求，注册多个信号处理函数）； 但是，等到内核空间读取到数据之后，应用线程需要将数据从内核空间拷贝到用户空间，**此时是用户线程是阻塞的；**也就是说：应用程序将数据从内核态拷贝到用户态的过程是阻塞等待的，这是和异步IO的本质区别； 1.3.5 异步IO模型在前面几种IO模型中，应用线程要获取数据总是先发送请求到内核，然后进行如下处理： 1）阻塞IO：应用线程等待内核响应数据，期间什么都不能做 2）非阻塞IO：应用线程立即响应，可以去处理其他事情，但需要不断轮询内核去获取数据 3）复用IO：采用IO复用机制，请求都先交给select函数，由应用线程调用select函数来轮询所有的请求，当有请求需要获取数据时，应用线程再去内核获取数据； 4）信号驱动IO：系统注册一个信号处理函数（回调函数），然后应用线程返回（不阻塞）；当内核中准备好数据后，应用线程需要把内核中的数据拷贝到用户空间，此时用户线程是阻塞的； 在以上4种IO模型中，每次要去读取数据时都是事先发送请求询问内核是否有可读数据，然后再发起真正的读取数据请求； 在异步IO模型中，应用只需要向内核发送一个请求，告诉内核它要读取数据后即刻返回；内核收到请求后会建立一个信号联系，当数据准备就绪，内核会主动把数据从内核复制到用户空间（而信号驱动是告诉应用程序何时可以开始拷贝数据），异步IO模型真正的做到了完完全全的非阻塞； Tips：异步IO模型和前面模型最大的区别是：前4个都是阻塞的，需要自己把用户准备好的数据，去内核拷贝到用户空间。而全异步不同，用户线程完全不需要关心实际的整个IO操作是如何进行的，只需要先发起一个请求，当接收内核返回的成功信号时表示IO操作已经完成，可以直接去使用数据，它是最理想的模型。 仅限学习、交流使用！ QQ群名片 本文转自 https://blog.csdn.net/Bb15070047748/article/details/124699009，如有侵权，请联系删除。","tags":["nio"]},{"title":"05 NIO核心组件之Channel","path":"/2024/02/05/nio/05【NIO核心组件之Channel】/","content":"文章目录 五、NIO核心组件之Channel 5.1 FileChannel 5.1.1 FileChannel的基本使用 5.1.2 FileChannel的常用方法 1）truncate 2）size 3）position 4）force 5）transferFrom 6）transferTo 5.1.3 聚集和分散 5.2 DatagramChannel 5.2.1 DatagramChannel简介 5.2.2 DatagramChannel的获取 5.2.3 DatagramChannel的常用方法 1）发送和接收 2）读取和写入 5.3 SocketChannel与ServerSocketChannel 5.3.1 Channel的简介 5.3.2 Channel的获取 5.3.3 Channel的常用方法 1）基本读写 2）拷贝图片案例 3）在线聊天案例 五、NIO核心组件之ChannelJava NIO的通道类似流，都是用于传输数据的。但通过又与流有些不同；流的数据走向是单向的，分为输入流（只能读取数据），输出流（只能写出数据），但NIO中的通道不一样，通道既可以写数据到Buffer，又可以从Buffer中读取数据； 另外流的操作对象是数组，而通道的操作对象是Buffer； Java中常见的Channel如下： FileChannel：用于文件 I&#x2F;O 编程 SocketChannel、ServerSocketChannel：用于 TCP I&#x2F;O 编程 DatagramChannel：用于 UDP I&#x2F;O 编程 5.1 FileChannelFileChannel是用于文件I&#x2F;O编程的管道类；通过FileInputStream和FileOutputStream可以获取一个关联文件的Channel，即FileChannel； public FileChannel getChannel()：获取该流的Channel； 示例代码： 12345// 通过输入流获取Channel,该Channel只能读取数据FileChannel inChannel = new FileInputStream(&quot;&quot;).getChannel();// 通过输出流获取Channel,该Channel只能写出数据FileChannel outChannel = new FileOutputStream(&quot;&quot;).getChannel(); 我们可以读取Channel中的数据，也可以往Channel中写数据，Channel是可读可写的；但FileChannel最终需要将数据写入到对应的输入&#x2F;输出流，因为流是有顺序的，输入流只能读取数据，而输出流只能写出数据，因此使用FileInputStream&#x2F;FileOutputStream获取到的FileChannel只能读或写； 5.1.1 FileChannel的基本使用 int read(ByteBuffer dst)：从Channel中读取数据，写入到Buffer中，返回读取到的有效字节个数，读取到末尾返回-1 int write(ByteBuffer src)：从Buffer中读取数据，写入到Channel中，返回写出的有效字节个数，如果没有数据写出返回0 案例1：通过Channel输出数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.dfbz.channel.fileChannel;import org.junit.Test;import java.io.FileInputStream;import java.io.FileOutputStream;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo01_FileChannel的基本使用 &#123; @Test public void writer() throws Exception &#123; // 1. 创建一个输出流 FileOutputStream fos = new FileOutputStream(&quot;001.txt&quot;); // 2. 通过输出流获取Channel FileChannel channel = fos.getChannel(); // 3. 创建一个Buffer ByteBuffer buffer = ByteBuffer.allocate(1024); buffer.put(&quot;hello&quot;.getBytes()); // 4. 切换读模式(limit=position=5;position=0) buffer.flip(); // 5. 将Buffer的数据读出来,写入到Channel中 int len = channel.write(buffer); // System.out.println(&quot;写出的有效字节个数: &quot; + len); // 5 len = channel.write(buffer); System.out.println(&quot;写出的有效字节个数: &quot; + len); // 0 channel.close(); fos.close(); &#125;&#125; 注意：channel.write()方法是将数据从Buffer中读取出来，然后写入到Channel中，这对Buffer本质上是一次读操作，我们对Buffer的任何读写操作都会造成Buffer中的position位移； 再次测试： 1234567891011121314151617181920212223242526272829303132@Testpublic void writer_02() throws Exception &#123; // 1. 创建一个输出流 FileOutputStream fos = new FileOutputStream(&quot;001.txt&quot;); // 2. 通过输出流获取Channel FileChannel channel = fos.getChannel(); // 3. 创建一个Buffer ByteBuffer buffer = ByteBuffer.allocate(1024); System.err.println(buffer); // position=0,limit=capacity=1024 buffer.put(&quot;hello&quot;.getBytes()); System.err.println(buffer); // position=5,limit=capacity=1024 // 4. 切换读模式(limit=position=5;position=0) buffer.flip(); System.err.println(buffer); // position=0,limit=5,capacity=1024 // 5. 将Buffer的数据读出来(也会造成position的位移),写入到Channel中 int len = channel.write(buffer); // System.out.println(&quot;写出的有效字节个数: &quot; + len); // 5 len = channel.write(buffer); System.out.println(&quot;写出的有效字节个数: &quot; + len); // 0 System.err.println(buffer); // position=5,limit=5,capacity=1024 channel.close(); fos.close();&#125; 执行程序，查看控制台： 案例2：通过Channel读取数据 123456789101112131415161718192021222324252627282930@Testpublic void reader() throws Exception &#123; // 1. 创建一个输入流 FileInputStream fis = new FileInputStream(&quot;001.txt&quot;); // 2. 通过输入流获取Channel FileChannel channel = fis.getChannel(); // 3. 创建一个Buffer ByteBuffer buffer = ByteBuffer.allocate(1024); // 4. 从Channel中读取数据,写入到Buffer中 int len = channel.read(buffer); System.out.println(&quot;读取到的有效字节个数: &quot; + len); // 5 len = channel.read(buffer); System.out.println(&quot;读取到的有效字节个数: &quot; + len); // -1 // limit=position,position=0 buffer.flip(); byte[] data = new byte[buffer.limit()]; buffer.get(data); // 从Buffer中读取数据 System.out.println(new String(data, 0, data.length)); channel.close(); fis.close();&#125; 注意：和write方法一样，channel调用read方法是将数据从Channel读取出来，往Buffer中写入，这对Buffer来说本质上是一种写的操作，我们对Buffer的任何读写操作都会造成Buffer中的position位移； 再次测试： 12345678910111213141516171819202122232425262728293031@Testpublic void reader_01() throws Exception &#123; // 1. 创建一个输入流 FileInputStream fis = new FileInputStream(&quot;001.txt&quot;); // 文件内容: hello // 2. 通过输入流获取Channel FileChannel channel = fis.getChannel(); // 3. 创建一个Buffer ByteBuffer buffer = ByteBuffer.allocate(1024); System.err.println(buffer); // position=0,limit=capacity=1024 // 4. 从Channel中读取数据,写入到Buffer中(会造成position的位移) channel.read(buffer); System.err.println(buffer); // position=5,limit=capacity=1024 // limit=position=5,position=0,capacity=1024 buffer.flip(); System.err.println(buffer); // position=0,limit=5,capacity=1024 byte[] data = new byte[buffer.limit()]; // 从Buffer中读取数据 buffer.get(data); System.err.println(buffer); // position=5,limit=5,capacity=1024 System.out.println(new String(data, 0, data.length)); channel.close(); fis.close();&#125; 执行程序，查看控制台： 案例3-使用FileChannel拷贝文件： 1234567891011121314151617181920212223242526272829@Testpublic void copy() throws Exception &#123; FileInputStream fis = new FileInputStream(&quot;100.png&quot;); FileOutputStream fos = new FileOutputStream(&quot;200.png&quot;); FileChannel inChannel = fis.getChannel(); FileChannel outChannel = fos.getChannel(); ByteBuffer buffer = ByteBuffer.allocate(1024); // 读取Channel中的数据,写入到Buffer中 while (inChannel.read(buffer) != -1) &#123; // limit=position,position=0 buffer.flip(); // 从Buffer中读取数据,写入Channel中 outChannel.write(buffer); // limit=capacity,position=0 buffer.clear(); &#125; inChannel.close(); outChannel.close(); fis.close(); fos.close();&#125; 5.1.2 FileChannel的常用方法 FileChannel truncate(long s)：将此通道的文件截取为给定大小 long size()：返回此通道的文件的当前大小 long position()：返回该Channel目前所在的文件位置； FileChannel position(long p)：设置该Channel目前所在的文件位置； public void force(boolean metaData)：将当前Channel中的数据强制写入到磁盘中 public long transferFrom(ReadableByteChannel src, long position, long count)：从src的position位置开始读取，读取count个字节到当前Channel中 public long transferTo(long position, long count,WritableByteChannel target)：从当前Channel的position位置开始读取，读取count个字节到target中 long write(ByteBuffer[] srcs) 将ByteBuffer[]到中的数据全部写入（聚集）到 Channel long read(ByteBuffer[] dsts) 将Channel到中的数据读取出来，然后全部写入（分散）到ByteBuffer[] 1）truncate FileChannel truncate(long s)：将此通道的文件截取为给定大小； 可以使用FileChannel.truncate()方法截取一个文件。截取文件时，文件将中指定长度后面的部分将被删除。 123456789101112131415161718192021222324@Testpublic void truncate() throws Exception &#123; FileInputStream stream = new FileInputStream(&quot;001.txt&quot;); // 文件内容: hello// FileOutputStream stream = new FileOutputStream(&quot;001.txt&quot;); // 文件内容: hello // 创建流(可读可写)// RandomAccessFile stream = new RandomAccessFile(&quot;001.txt&quot;, &quot;rw&quot;); // 文件内容: hello // 获取Channel FileChannel channel = stream.getChannel(); // 将Channel的文件截取为2 channel.truncate(2); // 使用FileInputStream出现: java.nio.channels.NonWritableChannelException ByteBuffer buffer = ByteBuffer.allocate(10); channel.read(buffer); // 使用FileOutputStream出现: java.nio.channels.NonReadableChannelException System.out.println(new String(buffer.array(), 0, buffer.array().length)); // he stream.close(); channel.close();&#125; Tips：FileInputStream只能读取数据，因此使用FileInputStream获取的FileChannel也只能读取数据；同理FileOutputStream只能写出数据，使用FileOutputStream获取FileChannel也只能写出数据；因此上述案例中使用RandomAccessFile，即可读又可写； 2）sizeFileChannel实例的size()方法将返回该实例所关联文件的大小。 long size()：返回此通道的文件的当前大小 12345678@Testpublic void size() throws Exception &#123; FileInputStream fis = new FileInputStream(&quot;001.txt&quot;); FileChannel channel = fis.getChannel(); System.out.println(channel.size()); // 5&#125; 3）position long position()：返回该Channel目前所在的文件位置； 有时可能需要在FileChannel的某个特定位置进行数据的读&#x2F;写操作。可以通过调用position()方法获取FileChannel的当前位置。也可以通过调用position(long pos)方法设置FileChannel的当前位置。 测试position： 1234567891011121314151617181920212223242526272829303132333435363738394041package com.dfbz.channel;import org.junit.Test;import java.io.FileInputStream;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo02_常用方法 &#123; @Test public void position() throws Exception &#123; FileInputStream fis = new FileInputStream(&quot;001.txt&quot;); FileChannel channel = fis.getChannel(); // 获取此Channel的读写位置(默认为0) System.out.println(channel.position()); // 0 ByteBuffer buffer = ByteBuffer.allocate(1024); // 从Channel中读取数据到buffer中,读取了5个有效字节 channel.read(buffer); System.out.println(&quot;buffer中的内容: &quot; + new String(buffer.array(), 0, buffer.array().length)); System.out.println(&quot;---------------------&quot;); // limit=capacity,position=0 buffer.clear(); System.out.println(channel.position()); // 5 // 将channel的读取位置设置为2 channel.position(2); channel.read(buffer); System.out.println(&quot;buffer中的内容: &quot; + new String(buffer.array(), 0, buffer.array().length)); &#125;&#125; Tips：如果将位置设置在文件结束符之后，然后试图从文件通道中读取数据，读方法将返回-1 —— 文件结束标志。 如果将位置设置在文件结束符之后，然后向通道中写数据，文件将撑大到当前位置并写入数据。这可能导致“文件空洞”，磁盘上物理文件中写入的数据间有空隙。 测试position-2： 123456789101112131415161718@Testpublic void position2() throws Exception &#123; // 创建流(可读可写) RandomAccessFile raf = new RandomAccessFile(&quot;001.txt&quot;, &quot;rw&quot;); // 文件内容: hello // 获取Channel FileChannel channel = raf.getChannel(); // 将position设置为size+10(空隙) channel.position(channel.size()+10); // 往Channel写出数据 channel.write(ByteBuffer.wrap(&quot;abc&quot;.getBytes())); raf.close(); channel.close();&#125; 001.txt文件内容如下： 4）force public void force(boolean metaData)：将当前Channel中的数据强制写入到磁盘中 FileChannel.force()方法将通道里尚未写入磁盘的数据强制写到磁盘上。出于性能方面的考虑，操作系统会将数据缓存在内存中，所以无法保证写入到FileChannel里的数据一定会即时写到磁盘上。要保证这一点，需要调用force()方法。 另外，force()方法有一个boolean类型的参数，指明是否同时将文件元数据（权限信息等）写到磁盘上 测试代码： 123456789101112131415161718192021@Testpublic void force() throws Exception &#123; // 创建流(可读可写) RandomAccessFile raf = new RandomAccessFile(&quot;001.txt&quot;, &quot;rw&quot;); // 文件内容: hello // 获取Channel FileChannel channel = raf.getChannel(); channel.write(ByteBuffer.wrap(&quot;Hello Everyone!&quot;.getBytes())); /* 将内存中的数据强制写入到磁盘中 true: 将文件元信息(权限信息等)写入磁盘 false: 不写入文件元信息到磁盘 */ channel.force(true); raf.close(); channel.close();&#125; 5）transferFrom public long transferFrom(ReadableByteChannel src, long position, long count)：从src的position位置开始读取，读取count个字节到当前Channel中 FileChannel的transferFrom()方法可以将数据从源通道传输到FileChannel中 123456789101112131415@Testpublic void transferFrom() throws Exception &#123; RandomAccessFile fromFile = new RandomAccessFile(&quot;001.txt&quot;, &quot;rw&quot;); FileChannel fromChannel = fromFile.getChannel(); RandomAccessFile toFile = new RandomAccessFile(&quot;002.txt&quot;, &quot;rw&quot;); FileChannel toChannel = toFile.getChannel(); long position = 0; long count = fromChannel.size(); // 从fromChannel的position位置开始读取,读取count个字节到toChannel中 toChannel.transferFrom(fromChannel, position, count);&#125; 6）transferTo public long transferTo(long position, long count,WritableByteChannel target)：从当前Channel的position位置开始读取，读取count个字节到target中 transferTo()方法将数据从FileChannel传输到其他的channel中 123456789101112131415@Testpublic void transferTo() throws Exception &#123; RandomAccessFile fromFile = new RandomAccessFile(&quot;001.txt&quot;, &quot;rw&quot;); FileChannel fromChannel = fromFile.getChannel(); RandomAccessFile toFile = new RandomAccessFile(&quot;002.txt&quot;, &quot;rw&quot;); FileChannel toChannel = toFile.getChannel(); long position = 0; long count = fromChannel.size(); // 从fromChannel的position位置开始读取,读取count到toChannel中 fromChannel.transferTo(position, count, toChannel);&#125; 5.1.3 聚集和分散 聚集（gather）：写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather）”后发送到Channel。 分散（scatter）：从Channel中读取是指在读操作时将读取的数据写入多个buffer中。因此，Channel将从Channel中读取的数据“分散（scatter）”到多个Buffer中。 scatter &#x2F; gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。 long write(ByteBuffer[] srcs) 将ByteBuffer[]到中的数据全部写入（聚集）到 Channel long read(ByteBuffer[] dsts) 将Channel到中的数据读取出来，然后全部写入（分散）到ByteBuffer[] Gathering Writes（聚集）：是指数据从多个buffer写入到同一个channel。如下图描述： Scattering Reads（分散）：是指数据从一个channel读取到多个buffer中。如下图描述： 测试代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.dfbz.channel;import org.junit.Test;import java.io.IOException;import java.io.RandomAccessFile;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo03_聚集和分散 &#123; // 聚集 @Test public void gather() throws IOException &#123; RandomAccessFile raf = new RandomAccessFile(&quot;004.txt&quot;, &quot;rw&quot;); FileChannel channel = raf.getChannel(); ByteBuffer header = ByteBuffer.wrap(&quot;&#123;&#x27;content-Type&#x27;:&#x27;application/json&#x27;&#125;&quot;.getBytes()); ByteBuffer body = ByteBuffer.wrap(&quot;&#123;&#x27;username&#x27;:&#x27;admin&#x27;,&#x27;password&#x27;:&#x27;123&#x27;&#125;&quot;.getBytes()); ByteBuffer[] bufferArray = &#123;header, body&#125;; // 将多个Buffer聚集到一起写入到channel中 channel.write(bufferArray); channel.close(); raf.close(); &#125; // 分散 @Test public void test2() throws IOException &#123; RandomAccessFile raf = new RandomAccessFile(&quot;004.txt&quot;, &quot;rw&quot;); FileChannel channel = raf.getChannel(); ByteBuffer header = ByteBuffer.allocate(&quot;&#123;&#x27;content-Type&#x27;:&#x27;application/json&#x27;&#125;&quot;.getBytes().length); ByteBuffer body = ByteBuffer.allocate(&quot;&#123;&#x27;username&#x27;:&#x27;admin&#x27;,&#x27;password&#x27;:&#x27;123&#x27;&#125;&quot;.getBytes().length); ByteBuffer[] bufferArray = &#123;header, body&#125;; // 将channel中的数据分散读取,然后逐个写入到每个Buffer中 channel.read(bufferArray); for (ByteBuffer buffer : bufferArray) &#123; System.out.println(new String(buffer.array(),0,buffer.array().length)); &#125; channel.close(); raf.close(); &#125;&#125; 5.2 DatagramChannel5.2.1 DatagramChannel简介DatagramChannel是用于UDP编程的Channel；获取到了DatagramChannel之后，可以使用Channel直接发送Buffer数据；因为UDP是无连接的网络协议，因此使用DatagramChannel发送的Buffer数据在发送时都会被封装成UDP报文，并且存在UDP协议的特性； 发送和接收报文： 在使用DatagramChannel发送数据时，必须通过InetSocketAddress类来指定接收端的地址和端口；而在接收数据时，接收端的Channel必须先通过InetSocketAddress类绑定一个地址和端口； 读取和写入数据： DatagramChannel不仅可以发送报文和接收报文，还可以读取DatagreamChannel中的数据，或往DatagreamChannel中写入数据；与发送和接收不同，在使用DatagramChannel发送或接收时，DatagramChannel充当一个接收器&#x2F;发送器的角色，自己本身并不存储那些数据；而是将数据接收到一个Buffer中，而在使用DatagramChannel读取&#x2F;写入数据时，数据可以从Buffer中读取到Channel中，也可以从Channel写出到Buffer； 5.2.2 DatagramChannel的获取在Java NIO中，我们可以通过DatagreamChannel来直接打开一个Channel，也可以DatagreamSocket可以获取到一个属于该Socket的Channel，但该Socket必须是由Channel获取的Socket；这两种方式获取到的Channel是同一个； DatagreamChannel方法： public static DatagramChannel open() ：打开一个基于UDP协议的Channel管道； public DatagramSocket socket()：通过Channel获取一个Socket； DatagreamSocket方法： public DatagramChannel getChannel()：获取该Socket的Channel管道； 测试代码： 123456789101112131415161718192021222324252627282930313233343536373839package com.dfbz.channel.datagramChannel;import org.junit.Test;import java.net.DatagramSocket;import java.nio.channels.DatagramChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo01_DatagramChannel的获取 &#123; @Test public void test() throws Exception &#123; // 通过DatagramChannel打开一个Channel DatagramChannel channel = DatagramChannel.open(); // 通过channel也可以获取一个socket DatagramSocket socket = channel.socket(); // 通过socket也可以获取Channel DatagramChannel channel2 = socket.getChannel(); System.out.println(channel2.getClass()); // class sun.nio.ch.DatagramChannelImpl System.out.println(channel == channel2); // true &#125; @Test public void test2() throws Exception &#123; DatagramSocket socket = new DatagramSocket(9999); // 不能通过DatagramSocket来获取channel DatagramChannel channel = socket.getChannel(); System.out.println(channel); // null &#125;&#125; 5.2.3 DatagramChannel的常用方法 public DatagramChannel bind(SocketAddress local)：绑定当前客户端的地址和端口，其他Channel向当前Channel发送数据时指定的地址。在接收UDP报文时，必须先绑定； public SocketAddress receive(ByteBuffer dst)：接收UDP报文，将接收到的UDP报文赋值给dst； public int send(ByteBuffer src, SocketAddress target)：发送一个UDP报文；并指定报文要发送的地址和端口 public DatagramChannel connect(SocketAddress remote)：用于连接其他的DatagramChannel ，DatagramChannel之间建立连接后可以相互读取&#x2F;写入数据； Tips：由于UDP是无连接的，使用connect方法连接到特定地址并不会像TCP通道那样创建一个真正的连接。而是锁住DatagramChannel ，让其只能从特定地址收发数据。因此即使是连接的地址不存在，也不会报错； 1）发送和接收测试代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.dfbz.channel.datagramChannel;import org.junit.Test;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.DatagramChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo02_发送和接收 &#123; /** * 发送端 * * @throws Exception */ @Test public void sender() throws Exception &#123; // 通过DatagramChannel打开一个Channel DatagramChannel channel = DatagramChannel.open(); // 往该Channel写入数据 channel.send(ByteBuffer.wrap(&quot;hello&quot;.getBytes()), new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); channel.close(); &#125; /** * 接收端 * * @throws Exception */ @Test public void receive() throws Exception &#123; // 通过DatagramChannel打开一个Channel DatagramChannel channel = DatagramChannel.open(); // 绑定地址(用于接收该地址发送过来的数据) channel.bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); ByteBuffer buffer = ByteBuffer.allocate(1024); // 使用Buffer接收报文 channel.receive(buffer); System.out.println(new String(buffer.array(), 0, buffer.array().length)); &#125;&#125; 2）读取和写入 读取端： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.dfbz.channel.datagramChannel;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.DatagramChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo03_读取端 &#123; public static void main(String[] args) throws Exception &#123; // 获取Channel DatagramChannel channel = DatagramChannel.open(); /* 绑定一个地址 1) 用于接收该地址发送的UDP报文 2) 用于其他DatagramChannel与当前Channel建立连接(逻辑连接),待会可以使用Channel从127.0.0.1的9999端口读取数据 */ channel.bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); /* 连接一个地址: 1) 建立一个UDP逻辑连接,如果需要读取127.0.0.2主机的数据则必须建立逻辑连接 2) 建立好逻辑连接后,可以使用channel向127.0.0.2主机写入数据 */ channel.connect(new InetSocketAddress(&quot;127.0.0.2&quot;, 9999)); System.out.println(&quot;开始接收数据: &quot;); System.out.println(&quot;----------------&quot;); // 创建Buffer,用读取Channel中的数据 ByteBuffer readBuffer = ByteBuffer.allocate(1024); // position=0,limit=1024,capacity=1024 while (true) &#123; // 将数据从Channel中读取出来,写入到readBuffer中 channel.read(readBuffer); // 预备下次一次从Channel读出数据写入到Buffer中[position=0,limit=capacity] readBuffer.clear(); System.out.println(&quot;接收到来自【&quot; + channel.getRemoteAddress() + &quot;】的数据: &quot; + new String(readBuffer.array(), 0, readBuffer.array().length)); &#125; &#125;&#125; 写入端： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.dfbz.channel.datagramChannel;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.DatagramChannel;import java.util.Scanner;/** * @author lscl * @version 1.0 * @intro: */public class Demo04_写入端 &#123; public static void main(String[] args) throws Exception &#123; // 获取Channel DatagramChannel channel = DatagramChannel.open(); /* 绑定一个地址 1) 用于接收该地址发送的UDP报文 2) 用于其他DatagramChannel与当前Channel建立连接(逻辑连接),待会可以使用Channel从127.0.0.2的9999端口读取数据 */ channel.bind(new InetSocketAddress(&quot;127.0.0.2&quot;, 9999)); // 连接一个地址: 与指定的地址建立逻辑连接,用于向这个地址发送数据,待会可以使用Channel向写入数据到127.0.0.1的9999端口 channel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); // 创建Buffer ByteBuffer writeBuffer = ByteBuffer.allocate(1024); // position=0,limit=1024,capacity=1024 // 获取一个扫描器 Scanner scanner = new Scanner(System.in); while (true) &#123; System.out.println(&quot;请输入数据: &quot;); // 接收控制台输入的数据 String str = scanner.nextLine(); // 将控制台输入的数据添加到buffer中 writeBuffer.put(str.getBytes()); // buffer的position会进行位移 // limit=position,position=0 writeBuffer.flip(); // 从Buffer中读取数据出来,往Channel中写入数据 // buffer的position会进行位移 channel.write(writeBuffer); // position=0,limit=capacity(预备下一次写入) writeBuffer.clear(); System.out.println(&quot;使用Channel向【&quot; + channel.getRemoteAddress() + &quot;】发送了数据: &quot; + str); &#125; &#125;&#125; 使用一个Channel进行读取和写入： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.dfbz.channel.datagramChannel;import org.junit.Test;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.DatagramChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo05_读取和写入 &#123; /** * 读取/写入数据 * * @throws Exception */ @Test public void readerAndWriter() throws Exception &#123; // 获取Channel DatagramChannel channel = DatagramChannel.open(); /* 绑定一个端口(本机) 1) 接收127.0.0.1主机9999端口发送的UDP报文 2) 用于其他DatagramChannel与当前Channel建立连接(逻辑连接),待会可以使用Channel从127.0.0.1的9999端口读取数据 */ channel.bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); /* 连接一个地址: 1) 建立一个UDP逻辑连接,如果需要读取127.0.0.2主机的数据则必须建立逻辑连接 2) 建立好逻辑连接后,可以使用channel向127.0.0.2主机写入数据 */ channel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); // 创建Buffer,用于往Channel写入数据 ByteBuffer writeBuffer = ByteBuffer.wrap(&quot;hello&quot;.getBytes()); // position=0,limit=5,capacity=5 // 创建Buffer,用读取Channel中的数据 ByteBuffer readBuffer = ByteBuffer.allocate(1024); // position=0,limit=1024,capacity=1024 while (true) &#123; // 将数据从writeBuffer中读取出来,写入到Channel中 channel.write(writeBuffer); // writeBuffer:[position=5,limit=5,capacity=5] // 切换写模式读模式,writeBuffer:[position=0,limit=5,capacity=5] writeBuffer.flip(); // 将数据从Channel中读取出来,写入到readBuffer中 channel.read(readBuffer); // readBuffer:[position=5,limit=1024,capacity=1024] // 切换写模式,readBuffer:[position=0,limit=1024,capacity=1024] readBuffer.clear(); System.out.println(&quot;Channel的数据: &quot; + new String(readBuffer.array(), 0, readBuffer.array().length)); Thread.sleep(1000); &#125; &#125;&#125; 5.3 SocketChannel与ServerSocketChannel5.3.1 Channel的简介BIO中Socket编程的两个核心类分别为：Socket（代表客户端）和ServerSocket（代表服务器端），通过ServerSocket的accept可以接收一个客户端的Socket； 在NIO中，提供有SocketChannel和ServerSocketChannel，分别代表客户端和服务端；底层依旧采用TCP协议进行数据的网络传输，同时这些Channel还支持非阻塞方式运行，这一点与原生的Socket&#x2F;ServerSocket有很大的不同；例如ServerSocketChannel在接收一个客户端时，如果还未有客户端来连接服务端，那么accept会返回null，而不是将当前线程阻塞； 5.3.2 Channel的获取通过ServerSocketChannel也可以来获取一个SocketChannel；也可以和DatagramChannel一样，通过open方法来打开一个管道；并且通过Socket可以获取SocketChannel，通过ServerSocket可以获取ServerSocketChannel； 和DatagramChannel一样，虽然通过Socket可以获取Channel，但该Socket必须是由Channel获取的Socket；因为原生的Socket的getChannel()方法永远返回的是null； 测试代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.dfbz.channel.socketChannel;import org.junit.Test;import java.net.ServerSocket;import java.net.Socket;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo01_Channel的获取 &#123; @Test public void test1() throws Exception &#123; // 获取socketChannel SocketChannel socketChannel = SocketChannel.open(); // 通过channel获取socket Socket socket = socketChannel.socket(); // 通过socket也可以获取对应的Channel System.out.println(socket.getChannel() == socketChannel); // true System.out.println(socket.getClass()); // class sun.nio.ch.SocketAdaptor(并不是一个原生的Socket对象) // serverSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 通过channel获取socket ServerSocket serverSocket = serverSocketChannel.socket(); // 通过socket也可以获取对应的Channel System.out.println(serverSocket.getChannel() == serverSocketChannel); // true System.out.println(serverSocket.getClass()); // class sun.nio.ch.ServerSocketAdaptor(并不是一个原生的ServerSocket对象) &#125; @Test public void test2() throws Exception &#123; // 原生的socket并不能获取Channel Socket socket = new Socket(); SocketChannel socketChannel = socket.getChannel(); System.out.println(socketChannel); // null // 原生的ServerSocket并不能获取Channel ServerSocket serverSocket = new ServerSocket(); ServerSocketChannel serverSocketChannel = serverSocket.getChannel(); System.out.println(serverSocketChannel); // null &#125; @Test public void test3() throws Exception &#123; // serverSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 接收到一个socketChannel(客户端) SocketChannel socketChannel = serverSocketChannel.accept(); &#125;&#125; 5.3.3 Channel的常用方法 ServerSocketChannel： public static ServerSocketChannel open()：获取一个ServerSocketChannel public ServerSocket socket()：通过当前Channel获取Socket public ServerSocketChannel bind(SocketAddress local)：绑定一个地址，用于客户端（SocketChannel）来连接 public SocketChannel accept()：接收一个客户端（SocketChannel）；默认情况下，如果没有客户端来连接，那么accept会使得当前线程一直处于等待状态； public SelectableChannel configureBlocking(boolean block)：将当前Channel设置为非阻塞模式；默认为true（阻塞模式） public boolean isBlocking()：判断当前Channel是否为非阻塞模式；默认true（阻塞模式） SocketChannel： public static SocketChannel open() ：获取一个SocketChannel public Socket socket()：通过当前Channel获取Socket； public SocketChannel bind(SocketAddress local)：绑定一个地址，默认是本机地址，SocketChannel的该方法没有意义，因为SocketChannel是一个客户端，用于连接ServerSocketChannel，通过ServerSocketChannel可以获取客户端的地址，默认情况下SocketChannel为本机地址，并随机分配一个端口； public boolean connect(SocketAddress remote)：用于连接服务端 public int read(ByteBuffer dst)：读取Channel中的数据到Buffer中 public int write(ByteBuffer src)：将Buffer中的数据写入到Channel中； public SelectableChannel configureBlocking(boolean block)：将当前Channel设置为非阻塞模式；默认为true（阻塞模式） public boolean isBlocking()：判断当前Channel是否为非阻塞模式；默认true（阻塞模式） 1）基本读写 服务端： 12345678910111213141516171819202122232425262728293031323334353637383940package com.dfbz.channel.socketChannel和serverSocketChannel;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo02_基本演示_服务端 &#123; public static void main(String[] args) throws Exception&#123; // 创建一个服务器 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); System.out.println(&quot;等待客户端连接....&quot;); // 绑定一个地址 serverSocketChannel.bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); /* 接收一个客户端(如果没有客户端来连接则会造成阻塞) 接收到了与客户端交互的Channel后,通过SocketChannel既可以向客户端写出数据,又可以读取来自客户端发送的数据 */ SocketChannel socketChannel = serverSocketChannel.accept(); System.out.println(&quot;客户端【&quot; + socketChannel.getRemoteAddress() + &quot;】连接成功成功....&quot;); // 准备一个Buffer ByteBuffer buffer = ByteBuffer.wrap(&quot;hello&quot;.getBytes()); // 往客户端写入数据 socketChannel.write(buffer); socketChannel.close(); serverSocketChannel.close(); &#125;&#125; 客户端： 123456789101112131415161718192021222324252627282930package com.dfbz.channel.socketChannel和serverSocketChannel;import org.junit.Test;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SocketChannel;/** * @author lscl * @version 1.0 * @intro: */public class Demo03_基本演示_客户端 &#123; @Test public void server() throws Exception &#123; SocketChannel socketChannel = SocketChannel.open(); // 连接到一个地址(如果该地址不存在,则默认情况下会阻塞,等到一定时间后抛出异常) socketChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); // 准备一个Buffer用于接收服务端的数据 ByteBuffer buffer = ByteBuffer.allocate(1024); // 读取服务端的数据,返回数据的长度 int len = socketChannel.read(buffer); System.out.println(&quot;数据长度: &quot; + len); System.out.println(new String(buffer.array(), 0, buffer.array().length)); &#125;&#125; 2）拷贝图片案例3）在线聊天案例仅限学习、交流使用！ QQ群名片 本文转自 https://blog.csdn.net/Bb15070047748/article/details/125438312，如有侵权，请联系删除。","tags":["nio"]},{"title":"线程池","path":"/2023/08/05/线程池/","content":"一、创建@Async线程池创建一个配置类 配置线程池参数注入 spring 使用时加入线程池名称 123456789101112131415161718@Configurationpublic class AsyncConfig &#123; @Bean(name = &quot;taskExecutor&quot;) public TaskExecutor taskExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(10); executor.setMaxPoolSize(100); executor.setQueueCapacity(200); executor.setThreadNamePrefix(&quot;MyExecutor-&quot;); executor.initialize(); return executor; &#125;&#125;@Async(&quot;taskExecutor&quot;)public void doSomething() &#123; // do something here&#125; 二、Executors类创建四种常用线程池参考 https://blog.csdn.net/ThinkWon/article/details/102541990 Java里面线程池的顶级接口是Executor，Executor并不是一个线程 池，而只是一个执行线程的工具。真正的线程池接口是ExecutorService 比较重要的几个类： 类&#x2F;接口 描述 ExecutorService 真正的线程池接口 ScheduledExecutorService 能和Timer&#x2F;TimerTask类似，解决那些需要任务重复执行的问题 ThreadPoolExecutor ExecutorService的默认实现 ScheduledThreadPoolExecutor 继承ThreadPoolExecutor的ScheduledExecutorService接口实现，周期性任务调度的类实现 Java通过Executors工厂类提供四种线程池，分别为： 类名 描述 场景 源码 newCachedThreadPool 可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，否则新建线程。（线程最大并发数不可控制） 用于并发执行大量短期的小任务，或者是负载较轻的服务器。 newFixedThreadPool 固定大小的线程池，可控制线程最大并发数，超出的线程会在队列中等待。 用于负载比较重的服务器，为了资源的合理利用，需要限制当前线程数量。 newScheduledThreadPool 定时线程池，支持定时及周期性任务执行。 用于需要多个后台线程执行周期任务，同时需要限制线程数量的场景。 newSingleThreadExecutor 单线程的线程池，保证任务按照指定顺序(FIFO, LIFO, 优先级)执行 用于串行执行任务的场景，每个任务必须按顺序执行，不需要并发执行。 Executors和ThreaPoolExecutor创建线程池的区别Executors 各个方法的弊端：newFixedThreadPool 和 newSingleThreadExecutor:主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至 OOM。newCachedThreadPool 和 newScheduledThreadPool:主要问题是线程数最大数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至 OOM。ThreaPoolExecutor:创建线程池方式只有一种，就是走它的构造函数，参数自己指定,灵活 两种提交任务的方法execute提交不需要返回值的任务 12345678ExecutorService executor = Executors.newCachedThreadPool();executor.execute(new Runnable() &#123; @Override public void run() &#123; //do something &#125;&#125;); execute(执行)() 的参数是一个 Runnable，也没有返回值。因此提交后无法判断该任务是否被线程池执行成功。 submit提交需要返回值的任务 1234&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result);Future&lt;?&gt; submit(Runnable task); submit(提交)() 有三种重载，参数可以是 Callable 也可以是 Runnable。同时它会返回一个 Funture 对象，通过它我们可以判断任务是否执行成功。获得执行结果调用 Future(未来).get() 方法，这个方法会阻塞当前线程直到任务完成。 提交一个 Callable 任务时，需要使用 FutureTask 包一层： 12345678910111213141516FutureTask futureTask = new FutureTask(new Callable&lt;String&gt;() &#123; //创建 Callable 任务 @Override public String call() throws Exception &#123; String result = &quot;&quot;; //do something return result; &#125;&#125;);Future&lt;?&gt; submit = executor.submit(futureTask); //提交到线程池try &#123; Object result = submit.get(); //获取结果&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125; catch (ExecutionException e) &#123; e.printStackTrace();&#125;","tags":["Java","线程池"]},{"title":"Spring 整合 Redis Stream 做消息队列","path":"/2023/08/05/Spring-整合-Redis-Stream-做消息队列/","content":"参考 https://www.jianshu.com/p/b95a265f838a为什么选redis stream1 不希望引入其它中间件2 业务规模目前不大 ，可以试水3 redis stream 比kafka还快？ 核心逻辑1.自定义注解 12345678@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface RedisStreamMqListen &#123;String value(); Class type();&#125; 2.监听器注册和发送方法封装 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283@Slf4jpublic class RedisStreamMqServiceImpl implements RedisStreamMqService &#123;private final long dataCenterId = getDataCenterId();private final RedisTemplate&lt;String, String&gt; redisTemplate;/** * 最大长度 */long maxLen;private String group;public RedisStreamMqServiceImpl(RedisTemplate&lt;String, String&gt; redisTemplate, String group, Long maxLen) &#123; this.redisTemplate = redisTemplate; this.group = group; this.maxLen = maxLen;&#125;private static Long getDataCenterId() &#123; try &#123; String hostName = Inet4Address.getLocalHost().getHostName(); int[] ints = StringUtils.toCodePoints(hostName); int sums = 0; for (int b : ints) &#123; sums += b; &#125; return (long) (sums % 32); &#125; catch (UnknownHostException e) &#123; // 如果获取失败，则使用随机数备用 return RandomUtils.nextLong(0, 31); &#125;&#125;public void listener(String event, Class type, StreamListener streamListener) &#123; createGroup(event); startSubscription(event, type, streamListener);&#125;public &lt;V&gt; void coverSend(String event, V val) &#123; ObjectRecord&lt;String, V&gt; record = StreamRecords.newRecord().ofObject(val).withId(RecordId.autoGenerate()) .withStreamKey(event); redisTemplate.opsForStream().add(record); redisTemplate.opsForStream().trim(event, maxLen, true); log.info(&quot;event &#123;&#125; send content &#123;&#125;&quot;, event, StringUtils.abbreviate(val.toString().trim(), 100));&#125;private void startSubscription(String event, Class type, StreamListener streamListener) &#123; RedisConnectionFactory redisConnectionFactory = redisTemplate.getConnectionFactory(); StreamMessageListenerContainer.StreamMessageListenerContainerOptions options = StreamMessageListenerContainer.StreamMessageListenerContainerOptions .builder().batchSize(5) // 设置批次大小为 5 .pollTimeout(Duration.ofSeconds(1)).targetType(type).build(); StreamMessageListenerContainer listenerContainer = StreamMessageListenerContainer.create(redisConnectionFactory, options); // redisTemplate 会自动加上前缀,所有在监听的时候也要加上 event = &quot;frl-stream:&quot; + event; listenerContainer.receiveAutoAck(Consumer.from(group, group + dataCenterId), StreamOffset.create(event, ReadOffset.lastConsumed()), streamListener); listenerContainer.start();&#125;private void createGroup(String event) &#123; try &#123; redisTemplate.opsForStream().createGroup(event, group); &#125; catch (RedisSystemException e) &#123; if (e.getRootCause() instanceof RedisBusyException) &#123; log.info(&quot;STREAM - Redis group already exists, skipping Redis group creation: &#123;&#125;&quot;, group); &#125; else if (e.getRootCause() instanceof RedisCommandExecutionException) &#123; log.info(&quot;STREAM - Stream does not yet exist, creating empty stream: &#123;&#125;&quot;, event); boolean streamExists = redisTemplate.hasKey(event); if (!streamExists) &#123; redisTemplate.opsForStream().add(event, Collections.singletonMap(&quot;&quot;, &quot;&quot;)); &#125; redisTemplate.opsForStream().createGroup(event, group); &#125; else &#123; throw e; &#125; &#125;&#125;&#125; 其中 核心逻辑为监听方法 1234567891011121314151617181920212223242526272829303132333435363738394041public void listener(String event, Class type, StreamListener streamListener) &#123;createGroup(event);startSubscription(event, type, streamListener);&#125;private void createGroup(String event) &#123;try &#123;redisTemplate.opsForStream().createGroup(event, group);&#125; catch (RedisSystemException e) &#123;if (e.getRootCause() instanceof RedisBusyException) &#123;log.info(&quot;STREAM - Redis group already exists, skipping Redis group creation: &#123;&#125;&quot;, group);&#125; else if (e.getRootCause() instanceof RedisCommandExecutionException) &#123;log.info(&quot;STREAM - Stream does not yet exist, creating empty stream: &#123;&#125;&quot;, event);boolean streamExists = redisTemplate.hasKey(event);if (!streamExists) &#123;redisTemplate.opsForStream().add(event, Collections.singletonMap(&quot;&quot;, &quot;&quot;));&#125;redisTemplate.opsForStream().createGroup(event, group);&#125; else &#123;throw e;&#125;&#125;&#125;private void startSubscription(String event, Class type, StreamListener streamListener) &#123;RedisConnectionFactory redisConnectionFactory = redisTemplate.getConnectionFactory(); StreamMessageListenerContainer.StreamMessageListenerContainerOptions options = StreamMessageListenerContainer.StreamMessageListenerContainerOptions .builder().batchSize(5) // 设置批次大小为 5 .pollTimeout(Duration.ofSeconds(1)).targetType(type).build(); StreamMessageListenerContainer listenerContainer = StreamMessageListenerContainer.create(redisConnectionFactory, options); // redisTemplate 会自动加上前缀,所有在监听的时候也要加上 event = &quot;frl-stream:&quot; + event; listenerContainer.receiveAutoAck(Consumer.from(group, group + dataCenterId), StreamOffset.create(event, ReadOffset.lastConsumed()), streamListener); listenerContainer.start();&#125; 核心逻辑 通过 listenerContainer.receiveAutoAck(…) 注册了一个消息监听器，该监听器实现了 StreamListener 接口，定义了 onMessage 方法用于处理接收到的消息。 调用 listenerContainer.start() 启动消息监听。 当 Redis Stream 中有消息到达时，StreamMessageListenerContainer 会调用注册的 StreamListener 的 onMessage 方法，并将消息作为参数传递给这个方法。 在 onMessage 方法中，编写逻辑来处理接收到的消息 这里 event 做了特殊操作，是因为在redisTemplate中设置了 key 的前缀，是用redisTemplate创建的队列，而这里是通过listenerContainer添加监听，所有为了匹配上 key 添加这个操作 项目启动后自动对注解添加监听操作 public class ListenAnnotation implements ApplicationListener&lt;ContextRefreshedEvent&gt; &#123; private static final Logger log = LoggerFactory.getLogger(ListenAnnotation.class); final RedisStreamMqService redisStreamMqService; public ListenAnnotation(RedisStreamMqService redisStreamMqService) &#123; this.redisStreamMqService = redisStreamMqService; &#125; @Override public void onApplicationEvent(ContextRefreshedEvent event) &#123; if (event.getApplicationContext().getParent() == null) &#123; Map&lt;String, Object&gt; beans = event.getApplicationContext().getBeansWithAnnotation(RedisStreamMqListen.class); for (Object bean : beans.values()) &#123; RedisStreamMqListen ca = bean.getClass().getAnnotation(RedisStreamMqListen.class); redisStreamMqService.listener(ca.value(), ca.type(), (StreamListener) bean); log.info(&quot;event &#123;&#125; start listen&quot;, ca.value()); &#125; &#125; &#125; &#125; 在应用启动过程中，ListenAnnotation会监听 ContextRefreshedEvent 事件，然后扫描应用上下文中的所有 Bean，查找标记了 @RedisStreamMqListen 注解的 Bean，然后将它们注册到RedisStreamMqStartService 中进行消息监听。event.getApplicationContext().getParent() &#x3D;&#x3D; null判断是否为根上下文，确保注册方法只执行一次3.实现监听类,使用 @Component @Slf4j @RedisStreamMqListen(value = &quot;briefReportListener&quot;, type = AnalyzeToolDTO.class) public class BriefReportListener implements StreamListener&lt;String, ObjectRecord&lt;String, AnalyzeToolDTO&gt;&gt; &#123; @Resource private BriefReportServiceImpl briefReportService; @SneakyThrows @Override public void onMessage(ObjectRecord&lt;String, AnalyzeToolDTO&gt; message) &#123; ... &#125; }","tags":["Spring","Redis"]},{"title":"oauth","path":"/2023/01/08/oauth/","content":"2认证和授权的概念​\t认证即解决 我是谁 ​\t授权即解决 我能做什么 ​ 1234567891011@EnableWebSecurity(debug = true)public class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; //检查请求是否认证 http.authorizeRequests(req -&gt; req.antMatchers(&quot;/api/**&quot;).authenticated()); //检查请求是否有权限 http.authorizeRequests(req -&gt; req.antMatchers(&quot;/api/**&quot;).hasRole(&quot;ADMIN&quot;)); &#125;&#125; 过滤器和过滤器链​ 任何Spring Web 应用本质上都是一个servlet，Security Filter 在HTTP请求到达Controller之前会过滤每一个请求。 使用过滤器链的好处： 职责单一、将负责逻辑简单化。 HTTP 请求的结构 12POST localhost:8080/api/user/hello2?name=安慰Authorization: Basic user 15afce2c-a151-4507-800a-9805ca227506 HTTP 响应和 HTTP Basic Auth 客户端请求服务端，服务端返回未授权，同时headle 里返回指定的认证方式，客户端收到响应后让用户填写信息发送服务端，服务端认证通过后返回通过状态码 安全配置security 标准传统写法 and() 会返回一个HttpSecurity 继续配置 也可以使用函数式写法 定制登录页​ 使用传统模板引擎 加入thymeleaf webjars 依赖 12345678dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-security&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; implementation &#x27;org.projectlombok:lombok&#x27; implementation(&#x27;org.springframework.boot:spring-boot-starter-thymeleaf&#x27;) implementation(&#x27;org.webjars:bootstrap:4.5.3&#x27;) implementation(&#x27;org.webjars:webjars-locator-core&#x27;)&#125; ​ 配置SecurityConfig 1234567891011121314151617181920212223242526272829303132@EnableWebSecurity(debug = true)@Slf4jpublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http //配置需要拦截的请求 req.anyRequest() 如何页面都需要认证 .authorizeRequests(req -&gt; req.antMatchers(&quot;/api/**&quot;).authenticated()) //配置登录 使用全部过滤需要加.permitAll() .formLogin(from -&gt; from.loginPage(&quot;/login&quot;)) .csrf(Customizer.withDefaults()) ; &#125; /** * 静态资源不拦截 不启动安全拦截 * * @param web * @throws Exception */ @Override public void configure(WebSecurity web) throws Exception &#123; web.ignoring().antMatchers(&quot;/public/**&quot;, &quot;/css/**&quot;, &quot;/js/**&quot;, &quot;/images/**&quot;) //常用静态资源地址 .requestMatchers(PathRequest.toStaticResources().atCommonLocations()); &#125;&#125; ​ 配置mvcConfig 123456789101112131415161718@Configurationpublic class WebMvcConfig implements WebMvcConfigurer &#123; @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(&quot;/webjars/**&quot;) .addResourceLocations(&quot;/webjars/**&quot;) .resourceChain(false); // 不缓存 registry.setOrder(1) ; &#125; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry .addViewController(&quot;/login&quot;)//url .setViewName(&quot;login&quot;);//视图html名称 &#125;&#125; 即指定了自己的登录页面 登录成功及失败的处理 登录成功后返回json 由前端判断路由 123456789101112131415161718192021222324.formLogin(from -&gt; from.loginPage(&quot;/login&quot;) //传入AuthenticationSuccessHandler 类中的onAuthenticationSuccess方法，可以用函数式接口 //认证成功后的处理 .successHandler(getAuthenticationSuccessHandler()) //认证失败的处理器 .failureHandler(getAuthenticationFailureHandler()) .permitAll()) private static AuthenticationSuccessHandler getAuthenticationSuccessHandler() &#123; return (req, resp, auth) -&gt; &#123; resp.setStatus(HttpStatus.OK.value()); resp.setContentType(&quot;application/json;charset=utf-8&quot;); resp.getWriter().write( new ObjectMapper().writeValueAsString(auth)); &#125;; &#125; private static AuthenticationFailureHandler getAuthenticationFailureHandler() &#123; return (req, resp, e) -&gt; &#123; resp.setStatus(HttpStatus.UNAUTHORIZED.value()); resp.setContentType(&quot;application/json;charset=utf-8&quot;); resp.getWriter().write(new ObjectMapper().writeValueAsString(e.getMessage())); &#125;; &#125; spring 自带的登录成功处理器 -传统项目用到 前后端分离的不适合 12345678910111213141516171819202122232425262728293031323334/** * Calls the parent class &#123;@code handle()&#125; method to forward or redirect to the target * URL, and then calls &#123;@code clearAuthenticationAttributes()&#125; to remove any leftover * session data. */ public void onAuthenticationSuccess(HttpServletRequest request, HttpServletResponse response, Authentication authentication) throws IOException, ServletException &#123; handle(request, response, authentication); //清理存在session 中的信息 删除在身份验证过程中可能存储在会话中的临时身份验证相关数据。 clearAuthenticationAttributes(request); &#125; /** * Invokes the configured &#123;@code RedirectStrategy&#125; with the URL returned by the * &#123;@code determineTargetUrl&#125; method. * &lt;p&gt; * The redirect will not be performed if the response has already been committed. */ protected void handle(HttpServletRequest request, HttpServletResponse response, Authentication authentication) throws IOException, ServletException &#123; //记录访问的目标url String targetUrl = determineTargetUrl(request, response, authentication); if (response.isCommitted()) &#123; logger.debug(&quot;Response has already been committed. Unable to redirect to &quot; + targetUrl); return; &#125; //认证通过后重定向到目标url redirectStrategy.sendRedirect(request, response, targetUrl); &#125; 自定义 FilterSpring 默认用来处理登录的过滤器 UsernamePasswordAuthenticationFilter 12345678910111213141516171819202122232425262728public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; if (postOnly &amp;&amp; !request.getMethod().equals(&quot;POST&quot;)) &#123; throw new AuthenticationServiceException( &quot;Authentication method not supported: &quot; + request.getMethod()); &#125; String username = obtainUsername(request); String password = obtainPassword(request); if (username == null) &#123; username = &quot;&quot;; &#125; if (password == null) &#123; password = &quot;&quot;; &#125; username = username.trim(); //构造一个安全对象 UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken( username, password); // Allow subclasses to set the &quot;details&quot; property setDetails(request, authRequest); //认证处理最终机制 return this.getAuthenticationManager().authenticate(authRequest); &#125; 自己新建一个模仿默认过滤器写一个RestAuthenticationFilter 1234567891011121314151617181920212223242526272829//@RequiredArgsConstructor//将flnal 修饰的私有成员变量放到构造函数中public class RestAuthenticationFilter extends UsernamePasswordAuthenticationFilter &#123; private ObjectMapper objectMapper; public RestAuthenticationFilter(ObjectMapper objectMapper) &#123; this.objectMapper = objectMapper; &#125; @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; UsernamePasswordAuthenticationToken authRequest; try &#123; ServletInputStream inputStream = request.getInputStream(); JsonNode jsonNode = objectMapper.readTree(inputStream); String username = jsonNode.get(&quot;username&quot;).textValue(); String password = jsonNode.get(&quot;password&quot;).textValue(); authRequest= new UsernamePasswordAuthenticationToken(username, password); setDetails(request, authRequest); &#125; catch (IOException e) &#123; e.printStackTrace(); throw new BadCredentialsException(&quot;Invalid username or password&quot;); &#125; return this.getAuthenticationManager().authenticate(authRequest); &#125;&#125; 然后在securityConfig中把自定义filter 加入过滤器链中 选择addFilterAt 替代默认过滤器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Override protected void configure(HttpSecurity http) throws Exception &#123; http //配置需要拦截的请求 .authorizeRequests(req -&gt; req //不需要拦截的路径 .antMatchers(&quot;/authorize/**&quot;).permitAll() //配置需要权限的路径 .antMatchers(&quot;/admin/**&quot;).hasRole(&quot;ADMIN&quot;) .antMatchers(&quot;/api/**&quot;).hasRole(&quot;USER&quot;) .anyRequest().authenticated()) //将自定义过滤器加入 .addFilterAt(restAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class) /*将原来的登录配置去掉*/ //配置登录// .formLogin(from -&gt; from.loginPage(&quot;/login&quot;)// //传入AuthenticationSuccessHandler 类中的onAuthenticationSuccess方法，可以用函数式接口// //认证成功后的处理// .successHandler(getAuthenticationSuccessHandler())// //认证失败的处理器// .failureHandler(getAuthenticationFailureHandler())// .permitAll())// .logout(logout -&gt; logout.logoutUrl(&quot;/perform_logout&quot;)// .logoutSuccessHandler((request, response, authentication) -&gt; &#123;// response.setStatus(HttpStatus.OK.value());// response.setContentType(&quot;application/json;charset=UTF-8&quot;);// response.getWriter().write(new ObjectMapper().writeValueAsString(&quot;注销成功&quot;));// &#125;)// .permitAll())// .csrf(csrf-&gt;csrf.disable()) ; &#125;/*配置自定义过滤器设置*/private RestAuthenticationFilter restAuthenticationFilter () throws Exception &#123; RestAuthenticationFilter filter = new RestAuthenticationFilter(objectMapper); //设置认证成功的处理方法 filter.setAuthenticationSuccessHandler(getAuthenticationSuccessHandler()); filter.setAuthenticationFailureHandler(getAuthenticationFailureHandler()); //设置AuthenticationManager 分类的方法 filter.setAuthenticationManager(authenticationManager()); //设置请求路径 filter.setFilterProcessesUrl(&quot;/authorize/login&quot;); return filter; &#125; 思路：先去看spring默认的过滤器怎么写的，然后模仿他。 33-1 密码进化史 哈希可用彩虹表 比对 得出原文 加盐：每次加密会随机生成一个盐值，存放在系统中，使用盐值+原文 哈希加密。 加大了密码复杂度后，响应时间会加长。 3-2 密码编码器常用的有 BCryptPasswordEncoder 使用多编码格式时，在存储密文时会在前面加上他的加密方式标识，设置时的key。{bcrypt}*** 密码升级的思路： 用户登录时拿到 明文密码，认证通过后 使用新的加密方式得到密文 替换 原有密文 3-3 验证注解和自定义验证注解 JSR-380验证框架implementation(‘org.springframework.boot:spring-boot-starter-validation’) implementation(‘org.springframework.boot:spring-boot-starter-validation’) 3-4密码的验证规则和自定义注解和验证器 密码校验创建一个校验注解 123456789101112131415161718192021222324252627package com.li.springsecurity.annotation;import com.li.springsecurity.validation.PasswordConstraintValidator;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import javax.validation.Constraint;import javax.validation.Payload;@Target(&#123;ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER, ElementType.ANNOTATION_TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = &#123;PasswordConstraintValidator.class&#125;)@Documentedpublic @interface ValidPassword &#123; String message() default &quot;Invalid password&quot;; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; 创建一个类实现密码验证ConstraintValidator接口 ： 接口使用了泛型，需要指定两个参数，第一个自定义注解类，第二个为需要校验的数据类型。 实现接口后要override两个方法，分别为initialize方法和isValid方法。其中initialize为初始化方法，可以在里面做一些初始化操作，isValid方法就是最终需要的校验方法。可以在该方法中实现具体的校验步骤。 本方法使用了passay 提供的校验方式。 123456789101112131415161718192021222324252627public class PasswordConstraintValidator implements ConstraintValidator&lt;ValidPassword,String&gt; &#123; @Override public boolean isValid(String password, ConstraintValidatorContext constraintValidatorContext) &#123; val validator = new PasswordValidator(Arrays.asList( new LengthRule(8, 30), //至少一个大写字母 new CharacterRule(EnglishCharacterData.UpperCase, 1), //至少一个小写字母 new CharacterRule(EnglishCharacterData.LowerCase, 1), //至少一个数字字符 new CharacterRule(EnglishCharacterData.Special, 1), //至少一个特殊字符 new CharacterRule(EnglishCharacterData.Digit, 1), new WhitespaceRule() )); //验证密码 val result = validator.validate(new PasswordData(password)); return result.isValid(); &#125; @Override public void initialize(ValidPassword constraintAnnotation) &#123; ConstraintValidator.super.initialize(constraintAnnotation); &#125;&#125; ​\t使用时加上该注解即可 123@NotNull @ValidPassword private String password; 多字段联合校验同样先创建一个注解 作用于类上 123456789101112@Target(&#123;ElementType.TYPE, ElementType.METHOD, ElementType.PARAMETER, ElementType.ANNOTATION_TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = &#123;PasswordMatchValidator.class&#125;)@Documentedpublic @interface PasswordMatch &#123; String message() default &quot;password not matched&quot;; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; 创建一个验证类 ​ 这里需要实现的泛型写需要校验的类 ​ 在isValid中进行多字段校验 123456789101112public class PasswordMatchValidator implements ConstraintValidator&lt;PasswordMatch, UserDTO&gt; &#123; @Override public void initialize(PasswordMatch constraintAnnotation) &#123; ConstraintValidator.super.initialize(constraintAnnotation); &#125; @Override public boolean isValid(UserDTO userDTO, ConstraintValidatorContext constraintValidatorContext) &#123; //两次密码是否一致 val result = userDTO.getPassword().equals(userDTO.getMatchingPassword()); return result; &#125; 3-5 passay 异常消息的国际化​\t在之前的异常返回信息中都是英文 用passay的中英文字典 处理 在WebMvcConfig中配置messageResolver 在passay PasswordValidator() 构造器中引入 springMessageResolver消息解析器。 将原有错误信息禁用，使用新的错误信息 配置 添加中文和英文的国际化消息内容 -https://blog.csdn.net/weixin_38657051/article/details/115221338 valid 消息国际化 在mvcconfig中再加一个 bean ​ 3-6 异常统一处理主要用到了 problem 的功能 引入 problem 依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.zalando&lt;/groupId&gt; &lt;artifactId&gt;problem-spring-web&lt;/artifactId&gt; &lt;version&gt;0.26.1&lt;/version&gt;&lt;/dependency&gt; 创建ExcepptionHandler 1234567891011@ControllerAdvice public class ExceptionHandler implements ProblemHandling &#123; /** * 是否将堆栈中的错误信息返回 */ @Override public boolean isCausalChainsEnabled() &#123; return true; &#125; &#125; 创建SecurityExceptionHandler 123public class SecurityExceptionHandler implements SecurityAdviceTrait&#123;&#125; 配置异常替换 https://blog.csdn.net/weixin_38657051/article/details/115221684 3-7 多种安全配置共存 既能实现form表单登录，同时也支持api的rest请求的登录。 再写一个针对传统页面登录的配置 12345678910111213141516171819202122232425@Configuration@Slf4j@Order(100)public class LoginSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests(req -&gt; req.anyRequest().authenticated())//授权请求控制 .formLogin(form -&gt; form.loginPage(&quot;/login&quot;) .usernameParameter(&quot;username1&quot;) .defaultSuccessUrl(&quot;/index&quot;) .permitAll()) .logout(logout -&gt; logout.logoutUrl(&quot;/perform_logout&quot;)) .rememberMe(rememberMe-&gt; rememberMe.tokenValiditySeconds(60*60)); &#125; @Override public void configure(WebSecurity web) throws Exception &#123; web.ignoring().mvcMatchers(&quot;/webjars/**&quot;, &quot;/public/**&quot;) //常用静态资源地址 .requestMatchers(PathRequest.toStaticResources().atCommonLocations()); &#125;&#125; 写法和普通配置一致，然后在其中配置针对表单登录显示的处理，多配置类共存的情况下，需要设置@Order注解，用来顺序加载Bean。否则会冲突。 4 4-1 核心组件 - SecurityContext SecurityContextHolder Authentication securityContext 存储的 为 Authentication 接口 Principal ： 个人信息 Object 可以放任何信息 不一定是一个人 可以是程序 credentials ： 可以存储各种密码信息 如 人脸识别 数据信息 包含各种token 实践一下 写一个获取用户权限信息的接口 由于get不能携带body，开启httpBasic 使其能携带认证头 测试一下 在Spring中 被Spring管理的 可以直接注入用户信息 同样拿到了用户信息 4-2 UserDetails、UserDetailsService和jdbcAuthenticationUserDetails 是通常意义上的用户 UserDetailsService 用于调取用户信息的服务 把用户信息提取出来 形成UserDetails UserDetails 是一个接口 高可拓展性 想要定制化 可以实现这些接口 真正的认证服务在 AuthenticationManager 中配置 只有一个方法，加载用户信息 添加数据库依赖 配置 使用h2内存数据库 可以设置h2 模式mysql 123456789101112131415161718192021222324&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; datasource: driver-class-name: org.h2.Driver password: &quot;&quot; url: jdbc:h2:mem:test;MODE=MySQL;DATABASE_TO_LOWER=TRUE username: sa h2: console: enabled: true path: /h2-console settings: trace: false web-allow-others: false 添加内建数据库支持 配置好后生成了设置的两个用户 4-3 定制化数据库设置表查询语句 4-4 深度定制化上 - 实现 UserDetails 和 GrantedAuthority 创建user类 和 role 类 实现UserDetails 并实现其方法 12345678910111213141516171819202122232425262728293031323334353637383940414243@Entity@Table(name = &quot;users&quot;)public class User implements UserDetails, Serializable &#123; private static final long serialVersionUID = 1L; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(length = 50,unique = true,nullable = false) private String username; @Column(name =&quot;password_hash&quot;,length = 80,nullable = false) private String password; @Column(length = 255,unique = true,nullable = false) private String email; @Column(length = 50) private String realName; @Column(name = &quot;enabled&quot;,nullable = false) private boolean enabled; @Column(name = &quot;account_non_expired&quot;,nullable = false) private boolean accountNonExpired; @Column(name = &quot;account_non_locked&quot;,nullable = false) private boolean accountNonLocked; @Column(name = &quot;credentials_non_expired&quot;,nullable = false) private boolean credentialsNonExpired; @ManyToMany @Fetch(FetchMode.JOIN) @JoinTable(name = &quot;user_role&quot; ,joinColumns =&#123; @JoinColumn(name = &quot;user_id&quot;,referencedColumnName = &quot;id&quot;)&#125; ,inverseJoinColumns =&#123; @JoinColumn(name = &quot;role_id&quot;,referencedColumnName = &quot;id&quot;)&#125;) private Set&lt;Role&gt; authorities; Role 实现 GrantedAuthority 接口 1234567891011121314151617181920212223242526@Data@Entity@Table(name = &quot;roles&quot;)public class Role implements GrantedAuthority , Serializable &#123; private static final long serialVersionUID = 1L; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(name = &quot;role_name&quot;,length = 50,unique = true,nullable = false) private String authority; @ManyToMany(mappedBy = &quot;authorities&quot;) private Collection&lt;User&gt; users; public Collection&lt;User&gt; getUsers() &#123; return users; &#125; public void setUsers(Collection&lt;User&gt; users) &#123; this.users = users; &#125;&#125; 在security config 中 配置自己的用户、权限 查找sql 1234567891011121314151617181920@Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.jdbcAuthentication() //设置默认的schema .withDefaultSchema() //设置数据源 .dataSource(dataSource) .usersByUsernameQuery(&quot;select username,password,enabled from users where username=?&quot;) .authoritiesByUsernameQuery(&quot;select username,authority from authorities where username=?&quot;) .withUser(&quot;user&quot;) .password(&quot;12345678&quot;) .roles(&quot;USER&quot;, &quot;ADMIN&quot;) .and() .withUser(&quot;old_user&quot;) .password(&quot;123456&quot;) .roles(&quot;USER&quot;) ; &#125; 4-5 深度定制化下 - UserDetailsService 和 UserDetailsPasswordService创建 UserDetailsServiceImpl 实现 UserDetaiilsService 123456789101112@Service@RequiredArgsConstructorpublic class UserDetailsServiceImpl implements UserDetailsService &#123; private final UserRepo userRepo; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; return userRepo.findByUsername(username) .orElseThrow(() -&gt; new UsernameNotFoundException(&quot;User not found&quot;)); &#125;&#125; 创建UserDetailsPasswordServiceImpl 实现 UserDetailsPasswordService 12345678910111213141516@Service@RequiredArgsConstructorpublic class UserDetailsPasswordServiceImpl implements UserDetailsPasswordService &#123; private final UserRepo userRepo; //用于密码升级 @Override public UserDetails updatePassword(UserDetails user, String newPassword) &#123; return userRepo.findByUsername(user.getUsername()) .map(u -&gt; (UserDetails)userRepo.save(u.withPassword(newPassword)) ) .orElse(user); &#125;&#125; dao 层代码 12345678910111213@Repositorypublic interface UserRepo extends JpaRepository&lt;User,Long&gt;&#123; Optional&lt;User&gt; findByUsername(String username);&#125;@Repositorypublic interface RoleRepo extends JpaRepository&lt;Role, Long&gt; &#123;&#125; 用 Optional作为返回值 能够方便的判空并进行处理 实体类使用@ With 能够修改属性的值 并返回一个新的对象https://www.jianshu.com/p/6660142f70c7 4-6 环境和环境变量 环境变量可以把敏感信息 隔离出来，不写在yml中 开发与运维隔离 不同环境可以设置不同的yml 并用 123spring:\tprofiles: active: dev 标注这个文件是哪个环境 在application.yml 或者环境 中指定使用哪个配置 4-7 自动化测试 12345678910111213141516171819202122232425@SpringBootTest public class SecurityRestAPIIntTests &#123; @Autowired private WebApplicationContext context; private MockMvc mockMvc; @BeforeEach public void setup() &#123; mockMvc = MockMvcBuilders.webAppContextSetup(context) //应用安全配置 .apply(springSecurity()) .build(); &#125; //提供一个虚拟用户 @WithMockUser(username = &quot;user&quot;, password = &quot;password&quot;, roles = &quot;USER&quot;) @Test public void givenNoToken_whenGetSecureRequest_thenUnauthorized() throws Exception &#123; mockMvc.perform(get(&quot;/api/gr&quot;)) .andExpect(status().isOk()); &#125; &#125; 5 深入了解springSecurity 认证过程 5-1 认证流程和源码解析 AuthenticationManager中可以有多个AuthenticationProvider ，Provider 是认证机制，支持多种方式 如 数据库密码严重 通过http去其他服务认证 只要一个 provider 认证成功 会将用户信息拿到 构建一个UserDetails https://blog.51cto.com/u_15072920/4180591 密码升级 5-2 LDAP 配置和多 AuthenticationProvider 共存日常接触场景不多 用户信息不止存储于sql数据库中 security认证流程很复杂 是为了适配多种认证方式 https://www.cnblogs.com/wilburxu/p/9174353.html ldap是树状结构 配置： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-ldap&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.ldap&lt;/groupId&gt; &lt;artifactId&gt;spring-ldap-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-ldap&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.unboundid&lt;/groupId&gt; &lt;artifactId&gt;unboundid-ldapsdk&lt;/artifactId&gt;&lt;/dependency&gt; 12345678spring:\tldap: base: dc=imooc,dc=com embedded: base-dn: dc=imooc,dc=com ldif: classpath:test-ldap-server.ldif port: 8389 urls: ldap://localhost:8389/ 创建LDAPUser模型 实现UserDetails 1234567891011121314151617181920212223242526272829303132333435363738394041@Builder @Data @AllArgsConstructor @NoArgsConstructor @Entry(objectClasses = &#123;&quot;inetOrgPerson&quot;, &quot;organizationalPerson&quot;, &quot;person&quot;, &quot;top&quot;&#125;) public final class LDAPUser implements UserDetails &#123; @Id @JsonIgnore private Name id; @Attribute(name = &quot;uid&quot;) private String username; @Attribute(name = &quot;userPassword&quot;) private String password; @Override public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() &#123; return Collections.singletonList(new SimpleGrantedAuthority(&quot;ROLE_USER&quot;)); &#125; @Override public boolean isAccountNonExpired() &#123; return true; &#125; @Override public boolean isAccountNonLocked() &#123; return true; &#125; @Override public boolean isCredentialsNonExpired() &#123; return true; &#125; @Override public boolean isEnabled() &#123; return true; &#125; &#125; ORM层 12345678910111213141516package com.imooc.uaa.security.auth.ldap;import org.springframework.data.ldap.repository.LdapRepository;import org.springframework.stereotype.Repository;import java.util.List;import java.util.Optional;@Repositorypublic interface LDAPUserRepo extends LdapRepository&lt;LDAPUser&gt; &#123; Optional&lt;LDAPUser&gt; findByUsername(String username); Optional&lt;LDAPUser&gt; findByUsernameAndPassword(String username, String password); List&lt;LDAPUser&gt; findByUsernameLikeIgnoreCase(String username);&#125; Provider 12345678910111213141516171819202122232425262728package com.imooc.uaa.security.auth.ldap;import org.springframework.security.authentication.BadCredentialsException;import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;import org.springframework.security.authentication.dao.AbstractUserDetailsAuthenticationProvider;import org.springframework.security.core.AuthenticationException;import org.springframework.security.core.userdetails.UserDetails;import lombok.RequiredArgsConstructor;/**● 展示和 DaoAuthenticationProvider 一起工作的场景 ● 使用 UsernamePasswordAuthenticationToken*/@RequiredArgsConstructorpublic class LDAPMultiAuthenticationProvider extends AbstractUserDetailsAuthenticationProvider &#123;private final LDAPUserRepo ldapUserRepo;@Override protected void additionalAuthenticationChecks(UserDetails userDetails, UsernamePasswordAuthenticationToken authentication) throws AuthenticationException &#123;&#125;@Override protected UserDetails retrieveUser(String username, UsernamePasswordAuthenticationToken authentication) throws AuthenticationException &#123;return ldapUserRepo.findByUsernameAndPassword(username, authentication.getCredentials().toString()).orElseThrow(() -&gt; new BadCredentialsException(&quot;[LDAP] 用户名或密码错误&quot;));&#125; &#125; 将LDAP Provider 加入到Security 配置中 12345678910@Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; // 配置 LdapAuthenticationProvider auth.authenticationProvider(new LDAPMultiAuthenticationProvider(ldapUserRepo)); // 配置 DaoAuthenticationProvider auth .userDetailsService(userDetailsServiceImpl) // 配置 AuthenticationManager 使用 userService .passwordEncoder(passwordEncoder()) // 配置 AuthenticationManager 使用 userService .userDetailsPasswordManager(userDetailsPasswordServiceImpl); // 配置密码自动升级服务 &#125; 建立LADP 模型 ORM层 再配置一个LDAP的Provider 在Provider中配置认证用户的方式。将其加入到Security中。 ProviderManager会扫描到这个Provider 只要所有Provider中有一个认证通过 即认证通过。 相似的，可以用此方法配置其他数据源的认证方式。 5-3 JWT 的概念和创建以及解析 自包含：自己可以验证自己，不需要再去数据库查询 jwt 的承载数据部分是公开的，不能放隐私信息 https://jwt.io/ 12345678910111213141516171819&lt;jjwt.version&gt;0.11.1&lt;/jjwt.version&gt;&lt;!-- JWT 依赖开始 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt-api&lt;/artifactId&gt; &lt;version&gt;$&#123;jjwt.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt-impl&lt;/artifactId&gt; &lt;version&gt;$&#123;jjwt.version&#125;&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt-jackson&lt;/artifactId&gt; &lt;version&gt;$&#123;jjwt.version&#125;&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 5-4 访问令牌和刷新令牌 对于公开的令牌 缩短其有效期 还可以可以综合其方式 如高频访问的ip等手段来防止盗取 刷新令牌 是不能用于访问的，只有一个唯一的用途，获得访问令牌。 前端存放的话 cookie最安全 直接由服务端设置 服务端设置时还可以把域名设置上，只允许这个域名访问。 从yml中配置参数 创建一个Properties类 prefix对应 yml 123456789101112131415161718192021222324252627282930313233public String createAccessToken(UserDetails userDetails) &#123; return createJWTToken(userDetails, appProperties.getJwt().getAccessTokenExpireTime());&#125;public String createRefreshToken(UserDetails userDetails) &#123; return createJWTToken(userDetails, appProperties.getJwt().getRefreshTokenExpireTime(), refreshKey);&#125;public String createJWTToken(UserDetails userDetails, long timeToExpire) &#123; return createJWTToken(userDetails, timeToExpire, key);&#125;/*** 根据用户信息生成一个 JWT** @param userDetails 用户信息* @param timeToExpire 毫秒单位的失效时间* @param signKey 签名使用的 key* @return JWT*/public String createJWTToken(UserDetails userDetails, long timeToExpire, Key signKey) &#123; return Jwts .builder() .setId(&quot;imooc&quot;) .setSubject(userDetails.getUsername()) .claim(&quot;authorities&quot;, userDetails.getAuthorities().stream() .map(GrantedAuthority::getAuthority) .collect(Collectors.toList())) .setIssuedAt(new Date(System.currentTimeMillis())) .setExpiration(new Date(System.currentTimeMillis() + timeToExpire)) .signWith(signKey, SignatureAlgorithm.HS512).compact();&#125; 5-5 创建JwtFilter编写filter 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Slf4j@RequiredArgsConstructor@Componentpublic class JwtFilter extends OncePerRequestFilter &#123; private final AppProperties appProperties; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; //1. 检查 JWT Token 是否在 HTTP 报头中 if(checkJwtToken(request)) &#123; //2. 有的话，解析 JWT Token validateToken(request) .filter(claims -&gt; claims.get(&quot;authorities&quot;) != null) .ifPresentOrElse( this::setupSpringAuthentication, // 有值 SecurityContextHolder::clearContext //为空 ); &#125; //3. 无论如何都要继续过滤器链 filterChain.doFilter(request, response); &#125; private void setupSpringAuthentication(Claims claims) &#123; val rawList = CollectionUtil.convertObjectToList(claims.get(&quot;authorities&quot;)); val authorities = rawList.stream() .map(String::valueOf) .map(SimpleGrantedAuthority::new) .collect(toList()); val authentication = new UsernamePasswordAuthenticationToken(claims.getSubject(), null, authorities); //将对象设置进 Security 中 SecurityContextHolder.getContext().setAuthentication(authentication); &#125; private Optional&lt;Claims&gt; validateToken(HttpServletRequest req) &#123; // 从 HTTP 报头中取出 JWT Token String jwtToken = req.getHeader(appProperties.getJwt().getHeader()).replace(appProperties.getJwt().getPrefix(), &quot;&quot;); try &#123; return Optional.of(Jwts.parserBuilder().setSigningKey(JwtUtil.key).build().parseClaimsJws(jwtToken).getBody()); &#125; catch (ExpiredJwtException | SignatureException | MalformedJwtException | UnsupportedJwtException | IllegalArgumentException e) &#123; return Optional.empty(); &#125; &#125; /** * 检查 JWT Token 是否在 HTTP 报头中 * * @param req HTTP 请求 * @return 是否有 JWT Token */ private boolean checkJwtToken(HttpServletRequest req) &#123; String authenticationHeader = req.getHeader(appProperties.getJwt().getHeader()); return authenticationHeader != null &amp;&amp; authenticationHeader.startsWith(appProperties.getJwt().getPrefix()); &#125;&#125; 在config中配置jwtFliter 将其加入到UsernamePasswordAu.. 前面 5-6 实现登录接口和刷新令牌接口重新生成token 利用了 token自包含特性 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@PostMapping(&quot;/token&quot;) public Auth login(@Valid @RequestBody LoginDto loginDTO) &#123; return userService.login(loginDTO.getUsername(), loginDTO.getPassword()); &#125; /** * 用户登录 * * @param username 用户名 * @param password 密码 * @return JWT */ public Auth login(String username, String password) &#123; return userRepo.findOptionalByUsername(username) .filter(user -&gt; passwordEncoder.matches(password, user.getPassword())) .map(user -&gt; new Auth(jwtUtil.createAccessToken(user), jwtUtil.createRefreshToken(user))) .orElseThrow(() -&gt; new AccessDeniedException(&quot;用户名密码错误&quot;)); &#125; @PostMapping(&quot;/token/refresh&quot;) public Auth refreshToken(@RequestHeader(name = &quot;Authorization&quot;) String authorization, @RequestParam String refreshToken) &#123; val PREFIX = &quot;Bearer &quot;; val accessToken = authorization.replace(PREFIX, &quot;&quot;); // 1. 验证refreshToken是否有效 // 2. 验证accessToken是否有效 if (jwtUtil.validateRefreshToken(refreshToken) &amp;&amp; jwtUtil.validateWithoutExpiration(accessToken)) &#123; return new Auth(jwtUtil.buildAccessTokenWithRefreshToken(refreshToken), refreshToken); &#125; throw new AccessDeniedException(&quot;Bad Credentials&quot;); &#125; public boolean validateRefreshToken(String jwtToken) &#123; return validateToken(jwtToken, refreshKey); &#125;public boolean validateWithoutExpiration(String jwtToken) &#123; try &#123; Jwts.parserBuilder().setSigningKey(JwtUtil.key).build().parseClaimsJws(jwtToken); return true; &#125; catch (ExpiredJwtException | SignatureException | MalformedJwtException | UnsupportedJwtException | IllegalArgumentException e) &#123; if (e instanceof ExpiredJwtException) &#123; return true; &#125; &#125; return false; &#125; 5-8 完成注册接口 1234567891011121314151617181920212223242526272829303132333435@PostMapping(&quot;/register&quot;) public void register(@Valid @RequestBody UserDto userDto, Locale locale) &#123; // 1. 验证用户名是否存在 if (userService.isUsernameExisted(userDto.getUsername())) &#123; throw new DuplicateProblem(&quot;Exception.duplicate.username&quot;, messageSource, locale); &#125; // 2. 验证邮箱是否存在 if (userService.isEmailExisted(userDto.getEmail())) &#123; throw new DuplicateProblem(&quot;Exception.duplicate.email&quot;, messageSource, locale); &#125; // 3. 验证手机号是否存在 if (userService.isMobileExisted(userDto.getMobile())) &#123; throw new DuplicateProblem(&quot;Exception.duplicate.mobile&quot;, messageSource, locale); &#125; val user = User.builder() .username(userDto.getUsername()) .name(userDto.getName()) .email(userDto.getEmail()) .mobile(userDto.getMobile()) .password(userDto.getPassword()) .build(); userService.register(user); &#125;\t@Transactional public User register(User user) &#123; return roleRepo.findOptionalByAuthority(ROLE_USER) .map(role -&gt; &#123; val userToSave = user .withAuthorities(Set.of(role)) .withPassword(passwordEncoder.encode(user.getPassword())); return userRepo.save(userToSave); &#125;) .orElseThrow(); &#125; 6 6-1 多因子认证和TOTP 可以用多种因素来验证登录 如 指纹、位置。。 redis 主要做缓存 本质就是生成一个在一定时间内不会变的密码，在有效期内输入验证正确即可通过，超过有效期 不能通过 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * 用于一次性验证码 */@Slf4j@Component@RequiredArgsConstructorpublic class TotpUtil &#123; //密码有效期，单位秒 private static final long TIME_STEP = 60 * 5L; //密码长度 private static final int PASSWORD_LENGTH = 6; private KeyGenerator keyGenerator; //key 存储在用户中 private TimeBasedOneTimePasswordGenerator totp; /* * 初始化代码块，Java 8 开始支持。这种初始化代码块的执行在构造函数之前 * 准确说应该是 Java 编译器会把代码块拷贝到构造函数的最开始。 */ &#123; try &#123; totp = new TimeBasedOneTimePasswordGenerator(Duration.ofSeconds(TIME_STEP), PASSWORD_LENGTH); // 生成一个 key keyGenerator = KeyGenerator.getInstance(totp.getAlgorithm()); // SHA-1 and SHA-256 需要 64 字节 (512 位) 的 key; SHA512 需要 128 字节 (1024 位) 的 key keyGenerator.init(512); &#125; catch (NoSuchAlgorithmException e) &#123; log.error(&quot;没有找到算法 &#123;&#125;&quot;, e.getLocalizedMessage()); &#125; &#125; /** * @param time 用于生成 TOTP 的时间 * @return 一次性验证码 * @throws InvalidKeyException 非法 Key 抛出异常 */ public String createTotp(final Key key, final Instant time) throws InvalidKeyException &#123; // 格式化字符串，前面补 0 val format = &quot;%0&quot; + PASSWORD_LENGTH + &quot;d&quot;; return String.format(format, totp.generateOneTimePassword(key, time)); &#125; public Optional&lt;String&gt; createTotp(final String strKey) &#123; try &#123; return Optional.of(createTotp(decodeKeyFromString(strKey), Instant.now())); &#125; catch (InvalidKeyException e) &#123; return Optional.empty(); &#125; &#125; /** * 验证 TOTP * * @param code 要验证的 TOTP * @return 是否一致 * @throws InvalidKeyException 非法 Key 抛出异常 */ public boolean validateTotp(final Key key, final String code) throws InvalidKeyException &#123; val now = Instant.now(); //再创建一个密码，验证是否一致 超过有效期，验证失败 return createTotp(key, now).equals(code); &#125; public Key generateKey() &#123; return keyGenerator.generateKey(); &#125; public String encodeKeyToString(Key key) &#123; return Base64.getEncoder().encodeToString(key.getEncoded()); &#125; public String encodeKeyToString() &#123; return encodeKeyToString(generateKey()); &#125; public Key decodeKeyFromString(String strKey) &#123; return new SecretKeySpec(Base64.getDecoder().decode(strKey), totp.getAlgorithm()); &#125; public long getTimeStepInLong() &#123; return TIME_STEP; &#125; public Duration getTimeStep() &#123; return totp.getTimeStep(); &#125;&#125; 6-2-3 实战发送TOTP阿里云短信 key Id Secret ：认证对 签名需要审核 LeanCloud 和阿里云类似 电子邮件 主要是两种 api smtp 因为有多种发消息的方式，可以先定义一个接口 来限制的方法 123public interface SmsService &#123; void send(String mobile, String msg);&#125; @ConditionalOnProperty 这个配置指 可以在yaml中配置使用 当yml中 mooc.sms-provider.name &#x3D; ali 时会启用这个实现类 没有则不生效 1234567891011121314151617181920212223242526272829303132333435@Slf4j @RequiredArgsConstructor //这个配置指 可以在yaml中配置使用 当yml中 mooc.sms-provider.name = ali 时会启用这个实现类 @ConditionalOnProperty(prefix = &quot;mooc.sms-provider&quot;, name = &quot;name&quot;, havingValue = &quot;ali&quot;) @Service public class SmsServiceAliSmsImpl implements SmsService &#123; private final IAcsClient client; private final AppProperties appProperties; @Override public void send(String mobile, String msg) &#123; //就是构建一个请求 只不过帮我们屏蔽了很多认证的方法 val request = new CommonRequest(); request.setSysMethod(MethodType.POST); request.setSysDomain(appProperties.getSmsProvider().getApiUrl()); request.setSysAction(&quot;SendSms&quot;); request.setSysVersion(&quot;2017-05-25&quot;); request.putQueryParameter(&quot;RegionId&quot;, &quot;cn-hangzhou&quot;); request.putQueryParameter(&quot;PhoneNumbers&quot;, mobile); request.putQueryParameter(&quot;SignName&quot;, &quot;登录验证&quot;); request.putQueryParameter(&quot;TemplateCode&quot;, &quot;SMS_1610048&quot;); request.putQueryParameter(&quot;TemplateParam&quot;, &quot;&#123;\\&quot;code\\&quot;:\\&quot;&quot; + msg + &quot;\\&quot;,\\&quot;product\\&quot;:\\&quot;慕课网实战Spring Security\\&quot;&#125;&quot;); try &#123; val response = client.getCommonResponse(request); log.info(&quot;短信发送结果 &#123;&#125;&quot;, response.getData()); &#125; catch (ServerException e) &#123; log.error(&quot;发送短信时产生服务端异常 &#123;&#125;&quot;, e.getLocalizedMessage()); &#125; catch (ClientException e) &#123; log.error(&quot;发送短信时产生客户端异常 &#123;&#125;&quot;, e.getLocalizedMessage()); &#125; &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Validated @Configuration @ConfigurationProperties(prefix = &quot;mooc&quot;) public class AppProperties &#123; @Getter @Setter @Valid private Jwt jwt = new Jwt(); @Getter @Setter @Valid private SmsProvider smsProvider = new SmsProvider(); @Getter @Setter @Valid private EmailProvider emailProvider = new EmailProvider(); @Getter @Setter @Valid private LeanCloud leanCloud = new LeanCloud(); @Getter @Setter @Valid private Ali ali = new Ali(); @Getter @Setter public static class Jwt &#123; private String header = &quot;Authorization&quot;; // HTTP 报头的认证字段的 key private String prefix = &quot;Bearer &quot;; // HTTP 报头的认证字段的值的前缀 @Min(5000L) private long accessTokenExpireTime = 60 * 1000L; // Access Token 过期时间 @Min(3600000L) private long refreshTokenExpireTime = 30 * 24 * 3600 * 1000L; // Refresh Token 过期时间 private String key; private String refreshKey; &#125; @Getter @Setter public static class LeanCloud &#123; private String appId; private String appKey; &#125; @Getter @Setter public static class Ali &#123; private String apiKey; private String apiSecret; &#125; @Getter @Setter public static class SmsProvider &#123; private String name; private String apiUrl; &#125; @Getter @Setter public static class EmailProvider &#123; private String name; private String apiKey; &#125; &#125; 123456789101112131415161718192021222324@RequiredArgsConstructor @Slf4j @Service @ConditionalOnProperty(prefix = &quot;mooc.sms-provider&quot;, name = &quot;name&quot;, havingValue = &quot;lean-cloud&quot;) public class SmsServiceLeanCloudSmsImpl implements SmsService &#123; @Override public void send(String mobile, String msg) &#123; val option = new AVSMSOption(); option.setTtl(10); option.setApplicationName(&quot;慕课网实战Spring Security&quot;); option.setOperation(&quot;两步验证&quot;); option.setTemplateName(&quot;登录验证&quot;); option.setSignatureName(&quot;慕课网&quot;); option.setType(AVSMS.TYPE.TEXT_SMS); option.setEnvMap(Map.of(&quot;smsCode&quot;, msg)); AVSMS.requestSMSCodeInBackground(mobile, option) .take(1) .subscribe( (res) -&gt; log.info(&quot;短信发送成功 &#123;&#125;&quot;, res), (err) -&gt; log.error(&quot;发送短信时产生服务端异常 &#123;&#125;&quot;, err.getLocalizedMessage()) ); &#125; &#125; 123456789101112131415161718@RequiredArgsConstructor @Configuration public class LeanCloudConfig &#123; private final AppProperties appProperties; private final Environment env; @PostConstruct()//初始化前执行 public void initialize() &#123; if (env.acceptsProfiles(Profiles.of(&quot;prod&quot;))) &#123; AVOSCloud.setLogLevel(AVLogger.Level.ERROR); &#125; else &#123; AVOSCloud.setLogLevel(AVLogger.Level.DEBUG); &#125; //初始化 AVOSCloud.initialize(appProperties.getLeanCloud().getAppId(), appProperties.getLeanCloud().getAppKey()); &#125; &#125; 1234# 使用环境变量形式来配置 ali: api-key: $&#123;ALI_API_KEY&#125; api-secret: $&#123;ALI_API_SECRET&#125; 6-4 邮件发送123public interface EmailService &#123; void send(String email, String msg);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Slf4j @ConditionalOnProperty(prefix = &quot;mooc.email-provider&quot;, name = &quot;name&quot;, havingValue = &quot;api&quot;) @RequiredArgsConstructor @Service public class EmailServiceApiImpl implements EmailService &#123; private final SendGrid sendGrid; @Override public void send(String email, String msg) &#123; val from = new Email(&quot;service@imooc.com&quot;); val subject = &quot;慕课网实战Spring Security 登录验证码&quot;; val to = new Email(email); val content = new Content(&quot;text/plain&quot;, &quot;验证码为:&quot; + msg); val mail = new Mail(from, subject, to, content); val request = new Request(); try &#123; request.setMethod(Method.POST); request.setEndpoint(&quot;mail/send&quot;); request.setBody(mail.build()); Response response = sendGrid.api(request); if (response.getStatusCode() == 202) &#123; log.info(&quot;邮件发送成功&quot;); &#125; else &#123; log.error(response.getBody()); &#125; &#125; catch (IOException e) &#123; log.error(&quot;请求发生异常 &#123;&#125;&quot;, e.getLocalizedMessage()); &#125; &#125; &#125;@ConditionalOnProperty(prefix = &quot;mooc.email-provider&quot;, name = &quot;name&quot;, havingValue = &quot;smtp&quot;) @RequiredArgsConstructor @Service public class EmailServiceSmtpImpl implements EmailService &#123; private final JavaMailSender emailSender; @Override public void send(String email, String msg) &#123; val message = new SimpleMailMessage(); message.setTo(email); message.setFrom(&quot;service@imooc.com&quot;); message.setSubject(&quot;慕课网实战Spring Security 登录验证码&quot;); message.setText(&quot;验证码为:&quot; + msg); emailSender.send(message); &#125; &#125; 12345678910111213@RequiredArgsConstructor @Configuration public class EmailConfig &#123; private final AppProperties appProperties; //如果有apikey才启动 @ConditionalOnProperty(prefix = &quot;mooc.email-provider&quot;, name = &quot;api-key&quot;) @Bean public SendGrid sendGrid() &#123; return new SendGrid(appProperties.getEmailProvider().getApiKey()); &#125; &#125; 6-5 用户登录逻辑 可以让用户选择是否需要二次认证，也可以管理员设置 当useringMfa 为true时 启用 1.表加字段 2. userService 创建用户时 创建一个mfa key 存入user表中 3. 登录接口 返回mfa 标识和 用户信息key123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@PostMapping(&quot;/token&quot;) public ResponseEntity&lt;?&gt; login(@Valid @RequestBody LoginDto loginDTO) &#123; return userService.findOptionalByUsernameAndPassword(loginDTO.getUsername(), loginDTO.getPassword()) .map(user -&gt; &#123; userService.upgradePasswordEncodingIfNeeded(user, loginDTO.getPassword()); if (!user.isEnabled()) &#123; throw new UserNotEnabledProblem(); &#125; if (!user.isAccountNonLocked()) &#123; throw new UserAccountLockedProblem(); &#125; if (!user.isAccountNonExpired()) &#123; throw new UserAccountExpiredProblem(); &#125; // 不使用多因子认证 if (!user.isUsingMfa()) &#123; return ResponseEntity.ok().body(userService.login(user)); &#125; // 使用多因子认证 //将用户信息缓存起来 val mfaId = userCacheService.cacheUser(user); return ResponseEntity .status(HttpStatus.UNAUTHORIZED) //在请求头中加上 mfa标识 realm id 用于 二次认证时找到这个用户 .header(&quot;X-Authenticate&quot;, &quot;mfa&quot;, &quot;realm=&quot; + mfaId) .build(); &#125;) .orElseThrow(BadCredentialProblem::new); &#125;//校验用户 public Optional&lt;User&gt; findOptionalByUsernameAndPassword(String username, String password) &#123; return findOptionalByUsername(username) .filter(user -&gt; passwordEncoder.matches(password, user.getPassword())); &#125;//密码升级 public void upgradePasswordEncodingIfNeeded(User user, String rawPassword) &#123; if (passwordEncoder.upgradeEncoding(user.getPassword())) &#123; userRepo.save(user.withPassword(passwordEncoder.encode(rawPassword))); &#125; &#125;\t// 用户缓存 private final TotpUtil totpUtil; public String cacheUser(User user) &#123; val mfaId = cryptoUtil.randomAlphanumeric(12); log.debug(&quot;生成 mfaId: &#123;&#125;&quot;, mfaId); RMapCache&lt;String, User&gt; cache = redisson.getMapCache(Constants.CACHE_MFA); if (!cache.containsKey(mfaId)) &#123; cache.put(mfaId, user, totpUtil.getTimeStepInLong(), TimeUnit.SECONDS); &#125; return mfaId; &#125; 4. 选择方式 发送校验码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//生成一个totp@PutMapping(&quot;/totp&quot;) public void sendTotp(@Valid @RequestBody SendTotpDto sendTotpDto) &#123; userCacheService.retrieveUser(sendTotpDto.getMfaId()) //把流 拍扁 返回opt..&lt;&gt; //pair 一次返回两个字 减少包装对象 .flatMap(user -&gt; userService.createTotp(user).map(code -&gt; Pair.of(user, code))) .ifPresentOrElse(pair -&gt; &#123; log.debug(&quot;totp: &#123;&#125;&quot;, pair.getSecond()); if (sendTotpDto.getMfaType() == MfaType.SMS) &#123; smsService.send(pair.getFirst().getMobile(), pair.getSecond()); &#125; else &#123; emailService.send(pair.getFirst().getEmail(), pair.getSecond()); &#125; &#125;, () -&gt; &#123; throw new InvalidTotpProblem(); &#125;); &#125;//验证totp@PostMapping(&quot;/totp&quot;) public Auth verifyTotp(@Valid @RequestBody TotpVerificationDto totpVerificationDto) &#123; return userCacheService.verifyTotp(totpVerificationDto.getMfaId(), totpVerificationDto.getCode()) .map(User::getUsername) .flatMap(userService::findOptionalByUsername) .map(userService::loginWithTotp) .orElseThrow(InvalidTotpProblem::new); &#125; public Optional&lt;User&gt; verifyTotp(String mfaId, String code) &#123; log.debug(&quot;输入参数 mfaId: &#123;&#125;, code: &#123;&#125;&quot;, mfaId, code); RMapCache&lt;String, User&gt; cache = redisson.getMapCache(Constants.CACHE_MFA); if (!cache.containsKey(mfaId) || cache.get(mfaId) == null) &#123; return Optional.empty(); &#125; val cachedUser = cache.get(mfaId); log.debug(&quot;找到用户 &#123;&#125;&quot;, cachedUser); try &#123; val isValid = totpUtil.validateTotp(totpUtil.decodeKeyFromString(cachedUser.getMfaKey()), code); log.debug(&quot;code &#123;&#125; 的验证结果为 &#123;&#125;&quot;, code, isValid); if (!isValid) &#123; return Optional.empty(); &#125; cache.remove(mfaId); log.debug(&quot;移除 mfaId: &#123;&#125;&quot;, mfaId); return Optional.of(cachedUser); &#125; catch (InvalidKeyException e) &#123; log.error(&quot;Key is invalid &#123;&#125;&quot;, e.getLocalizedMessage()); &#125; return Optional.empty(); &#125; 6-8 前端集成1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;uaa-ui&lt;/artifactId&gt; &lt;parent&gt; &lt;groupId&gt;com.imooc&lt;/groupId&gt; &lt;artifactId&gt;mono-uaa&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.github.eirslett&lt;/groupId&gt; &lt;artifactId&gt;frontend-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;frontend-maven-plugin.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;!-- 安装 node.js和npm --&gt; &lt;execution&gt; &lt;id&gt;install node and npm&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;install-node-and-npm&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;nodeVersion&gt;v12.12.0&lt;/nodeVersion&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;!-- 安装项目依赖 --&gt; &lt;execution&gt; &lt;id&gt;npm install&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;npm&lt;/goal&gt; &lt;/goals&gt; &lt;!-- 可选步骤，因为默认的阶段就是&quot;generate-resources&quot; --&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;configuration&gt; &lt;arguments&gt;install&lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;!-- 编译构建前端文件 --&gt; &lt;execution&gt; &lt;id&gt;npm run build&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;npm&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;arguments&gt;run build&lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;!-- 运行单元测试 --&gt; &lt;!-- &lt;execution&gt; &lt;id&gt;npm run test:unit&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;npm&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;configuration&gt; &lt;arguments&gt;run test:unit&lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; --&gt; &lt;!-- 运行集成测试 --&gt; &lt;!-- &lt;execution&gt; &lt;id&gt;npm run test:e2e&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;npm&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;configuration&gt; &lt;arguments&gt;run test:e2e&lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; --&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 6-9 跨域处理两种方法 避免跨域 前后端放一起。 适用于传统单体项目 前端处理： 设置代理 进行转发 端口映射 后端开启支持 mvc中配置 security 中配置 12345678910111213141516171819202122@Bean CorsConfigurationSource corsConfigurationSource() &#123; CorsConfiguration configuration = new CorsConfiguration(); // 允许跨域访问的主机 if (environment.acceptsProfiles(Profiles.of(&quot;dev&quot;))) &#123; configuration.setAllowedOrigins(Collections.singletonList(&quot;http://localhost:4001&quot;)); &#125; else &#123; configuration.setAllowedOrigins(Collections.singletonList(&quot;https://uaa.imooc.com&quot;)); &#125; //允许的请求方式 configuration.setAllowedMethods(Arrays.asList(&quot;GET&quot;, &quot;POST&quot;, &quot;PUT&quot;, &quot;DELETE&quot;, &quot;OPTIONS&quot;)); configuration.setAllowedHeaders(Collections.singletonList(&quot;*&quot;)); //开放返回响应请求头 如 用于 totp时返回的请求头 configuration.addExposedHeader(&quot;X-Authenticate&quot;); // 设置 配置bean UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); //把这个配置应用于那些url上 所有的 source.registerCorsConfiguration(&quot;/**&quot;, configuration); return source; &#125;//在安全配置中加入 7 7-1 授权的概念和安全表达式的作用 7-3 方法级注释先在类上开启方法级安全配置 执行方法之前授权 @PreAuthorize 执行方法之前过滤 @PreFilter 执行方法之前授权 @PostAuthorize 执行方法之前过滤 @PostFilter web 请求是成功的 url层 方法后的安全注解： 12345@PostAuthorize(&quot;authentication.name.equals(returnObject.username)&quot;)@GetMapping(&quot;/users/by-email/&#123;email&#125;&quot;)public User getUserByEmail(@PathVariable String email) &#123; return userService.findOptionalByEmail(email).orElseThrow();&#125; 7-3 RBAC 和 角色分级 角色是权限的集合 角色分层体系 一个角色可以包含多个子角色","tags":["认证","springsecurity"]},{"title":"福瑞莱","path":"/2022/11/30/py学习/","content":"​","categories":["公司"]},{"title":"mysql表名列名转小写","path":"/2022/11/12/mysql表名列名转小写/","content":"记录一下，根据工作中项目交付要求，要将MySQL数据库中的表名和字段名中做一个规范，其中就有将表名和字段名统一做小写处理。 废话不多说，直接上MySQL脚本： 批量修改数据库下的表名（大写改小写）： 1234567SELECT concat( &#x27;rename table &#x27; , TABLE_NAME , &#x27; to &#x27; , LOWER(TABLE_NAME) ,&#x27; ;&#x27; ) AS &#x27;修改脚本sql&#x27;FROM information_schema.TABLES t WHERE TABLE_SCHEMA = &#x27;数据库名&#x27;; 批量修改列名（大写改小写）： 123456789101112131415161718192021222324252627282930SELECT\tconcat( &#x27;alter table &#x27;, TABLE_NAME, &#x27; change column &#x27;, COLUMN_NAME, &#x27; &#x27;, LOWER( COLUMN_NAME ), &#x27; &#x27;, COLUMN_TYPE, &#x27; comment \\&#x27;&#x27;, TRIM( REPLACE ( REPLACE ( REPLACE ( REPLACE ( COLUMN_COMMENT, &#x27;,&#x27;, &#x27;:&#x27; ), &#x27;&quot;&#x27;, &#x27;&#x27; ), CHAR ( 10 ), &#x27;&#x27; ), CHAR ( 13 ), &#x27;&#x27; )), &#x27;\\&#x27;&#x27;, &#x27; &#x27;,\tIF ( COLUMN_DEFAULT IS NULL, &#x27;&#x27;, concat( &#x27; default \\&#x27;&#x27;, TRIM( COLUMN_DEFAULT ), &#x27;\\&#x27;&#x27; )), &#x27;;&#x27; ) AS &#x27;修改脚本sql&#x27; FROM\tinformation_schema.COLUMNS t WHERE\tTABLE_SCHEMA = &#x27;数据库名&#x27;; 如果是小写改大写，只需要将LOWER 修改为 UCASE即可。 运行脚本之后会在下面生成修改脚本的SQL，复制出来运行即可完成修改。如下图所示：","tags":["小技巧"]},{"title":"docker学习","path":"/2022/11/12/docker学习/","content":"Docker安装：https://blog.csdn.net/qq_43418737/article/details/125707321 常用命令帮助启动类命令 123456789101112131415· 启动docker： systemctl start docker· 停止docker： systemctl stop docker· 重启docker： systemctl restart docker· 查看docker状态： systemctl status docker· 开机启动： systemctl enable docker· 查看docker概要信息： docker info· 查看docker总体帮助文档： docker --help· 查看docker命令帮助文档： docker 具体命令 --help 镜像命令· docker images · 列出本地主机上的镜像 各个选项说明: 12345REPOSITORY：表示镜像的仓库源TAG：镜像的标签版本号IMAGE ID：镜像IDCREATED：镜像创建时间SIZE：镜像大小 同一仓库源可以有多个 TAG版本，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像 · 下载镜像 1234567891011· docker pull 镜像名字[:TAG]· docker pull 镜像名字· 没有TAG就是最新版· 等价于· docker pull 镜像名字:latest· docker pull ubuntu · docker system df 查看镜像&#x2F;容器&#x2F;数据卷所占的空间 · docker rmi 某个XXX镜像名字ID · 删除镜像 面试题：谈谈docker虚悬镜像是什么？· 仓库名、标签都是的镜像，俗称虚悬镜像dangling image 容器命令安装可视化面板 https://blog.csdn.net/weixin_46152207/article/details/125936769 设置挂载目录启动Jenkins 12docker run -d -v /data/jenkins/jenkins_home:/var/jenkins_home -p 9010:8080 -p 50000:50000 --restart=on-failure jenkins/jenkins:lts-jdk11 docker 启动nacos 1docker run --name nacos -d -p 8848:8848 -p 9848:9848 -p 9849:9849 --privileged=true --restart=always -e JVM_XMS=256m -e JVM_XMX=256m -e MODE=standalone -e PREFER_HOST_MODE=hostname -e SPRING_DATASOURCE_PLATFORM=mysql -e MYSQL_SERVICE_HOST=1.15.220.36 -e MYSQL_SERVICE_PORT=3306 -e MYSQL_SERVICE_DB_NAME=nacos_config -e MYSQL_SERVICE_USER=root -e MYSQL_SERVICE_PASSWORD=123456 -v /root/apply/docker/apply/nacos/logs:/home/nacos/logs -v /root/apply/docker/apply/nacos/init.d/custom.properties:/etc/nacos/init.d/custom.properties -v /root/apply/docker/apply/nacos/data:/home/nacos/data nacos/nacos-server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354docker 启动容器docker run \\容器名称叫nacos -d后台运行--name nacos -d acos默认端口8848 映射到外部端口8848-p 8848:8848 aocs 应该是2.0版本以后就需要一下的两个端口 所以也需要开放-p 9848:9848 -p 9849:9849 --privileged=true \\docker重启时 nacos也一并重启--restart=always \\-e 配置 启动参数配置 jvm-e JVM_XMS=256m -e JVM_XMX=256m \\单机模式-e MODE=standalone -e PREFER_HOST_MODE=hostname \\数据库是mysql 配置持久化 不使用nacos自带的数据库-e SPRING_DATASOURCE_PLATFORM=mysql \\写自己的数据库地址-e MYSQL_SERVICE_HOST=###### \\数据库端口号-e MYSQL_SERVICE_PORT=3306 \\mysql的数据库名称-e MYSQL_SERVICE_DB_NAME=nacos \\mysql的账号密码-e MYSQL_SERVICE_USER=root -e MYSQL_SERVICE_PASSWORD=root \\-v 映射docker内部的文件到docker外部 我这里将nacos的日志 数据 以及配置文件 映射出来映射日志-v /root/apply/docker/apply/nacos/logs:/home/nacos/logs \\映射配置文件 (应该没用了 因为前面已经配置参数了)-v /root/apply/docker/apply/nacos/init.d/custom.properties:/etc/nacos/init.d/custom.properties \\映射nacos的本地数据 也没啥用因为使用了mysql-v /root/apply/docker/apply/nacos/data:/home/nacos/data \\启动镜像名称nacos/nacos-server"},{"title":"activiti学习","path":"/2022/11/12/activiti学习/","content":"​ 视频地址：https://www.bilibili.com/video/BV1H54y167gf/?p=2 p1-p3.工作流 Activiti7 概述工作流(Workflow)，就是通过计算机对业务流程自动化执行管理。它主要解决的是“使在多个参与者之间按照某种预定义的规则自动进行传递文档、信息或任务的过程，从而实现某个预期的业务目标，或者促使此目标的实现”。 Activiti是一个工作流引擎， activiti可以将业务系统中复杂的业务流程抽取出来，使用专门的建模语言BPMN2.0进行定义，业务流程按照预先定义的流程进行执行，实现了系统的流程由activiti进行管理，减少业务系统由于流程变更进行系统升级改造的工作量，从而提高系统的健壮性，同时也减少了系统开发维护成本。 官方网站：https://www.activiti.org/ BPM（Business Process Management），即业务流程管理，是一种规范化的构造端到端的业务流程 BPMN（Business Process Model AndNotation）- 业务流程模型和符号 是由BPMI（BusinessProcess Management Initiative）开发的一套标准的业务流程建模符号，使用BPMN提供的符号可以创建业务流程。 审批流程： Bpmn图形其实是通过xml表示业务流程，上边的.bpmn文件使用文本编辑器打开： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;definitions xmlns=&quot;http://www.omg.org/spec/BPMN/20100524/MODEL&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:activiti=&quot;http://activiti.org/bpmn&quot; xmlns:bpmndi=&quot;http://www.omg.org/spec/BPMN/20100524/DI&quot; xmlns:omgdc=&quot;http://www.omg.org/spec/DD/20100524/DC&quot; xmlns:omgdi=&quot;http://www.omg.org/spec/DD/20100524/DI&quot; typeLanguage=&quot;http://www.w3.org/2001/XMLSchema&quot; expressionLanguage=&quot;http://www.w3.org/1999/XPath&quot; targetNamespace=&quot;http://www.activiti.org/test&quot;&gt; &lt;process id=&quot;myProcess&quot; name=&quot;My process&quot; isExecutable=&quot;true&quot;&gt; &lt;startEvent id=&quot;startevent1&quot; name=&quot;Start&quot;&gt;&lt;/startEvent&gt; &lt;userTask id=&quot;usertask1&quot; name=&quot;创建请假单&quot;&gt;&lt;/userTask&gt; &lt;sequenceFlow id=&quot;flow1&quot; sourceRef=&quot;startevent1&quot; targetRef=&quot;usertask1&quot;&gt;&lt;/sequenceFlow&gt; &lt;userTask id=&quot;usertask2&quot; name=&quot;部门经理审核&quot;&gt;&lt;/userTask&gt; &lt;sequenceFlow id=&quot;flow2&quot; sourceRef=&quot;usertask1&quot; targetRef=&quot;usertask2&quot;&gt;&lt;/sequenceFlow&gt; &lt;userTask id=&quot;usertask3&quot; name=&quot;人事复核&quot;&gt;&lt;/userTask&gt; &lt;sequenceFlow id=&quot;flow3&quot; sourceRef=&quot;usertask2&quot; targetRef=&quot;usertask3&quot;&gt;&lt;/sequenceFlow&gt; &lt;endEvent id=&quot;endevent1&quot; name=&quot;End&quot;&gt;&lt;/endEvent&gt; &lt;sequenceFlow id=&quot;flow4&quot; sourceRef=&quot;usertask3&quot; targetRef=&quot;endevent1&quot;&gt;&lt;/sequenceFlow&gt; &lt;/process&gt; &lt;bpmndi:BPMNDiagram id=&quot;BPMNDiagram_myProcess&quot;&gt; &lt;bpmndi:BPMNPlane bpmnElement=&quot;myProcess&quot; id=&quot;BPMNPlane_myProcess&quot;&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;startevent1&quot; id=&quot;BPMNShape_startevent1&quot;&gt; &lt;omgdc:Bounds height=&quot;35.0&quot; width=&quot;35.0&quot; x=&quot;130.0&quot; y=&quot;160.0&quot;&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;usertask1&quot; id=&quot;BPMNShape_usertask1&quot;&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;105.0&quot; x=&quot;210.0&quot; y=&quot;150.0&quot;&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;usertask2&quot; id=&quot;BPMNShape_usertask2&quot;&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;105.0&quot; x=&quot;360.0&quot; y=&quot;150.0&quot;&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;usertask3&quot; id=&quot;BPMNShape_usertask3&quot;&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;105.0&quot; x=&quot;510.0&quot; y=&quot;150.0&quot;&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;endevent1&quot; id=&quot;BPMNShape_endevent1&quot;&gt; &lt;omgdc:Bounds height=&quot;35.0&quot; width=&quot;35.0&quot; x=&quot;660.0&quot; y=&quot;160.0&quot;&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNEdge bpmnElement=&quot;flow1&quot; id=&quot;BPMNEdge_flow1&quot;&gt; &lt;omgdi:waypoint x=&quot;165.0&quot; y=&quot;177.0&quot;&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x=&quot;210.0&quot; y=&quot;177.0&quot;&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement=&quot;flow2&quot; id=&quot;BPMNEdge_flow2&quot;&gt; &lt;omgdi:waypoint x=&quot;315.0&quot; y=&quot;177.0&quot;&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x=&quot;360.0&quot; y=&quot;177.0&quot;&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement=&quot;flow3&quot; id=&quot;BPMNEdge_flow3&quot;&gt; &lt;omgdi:waypoint x=&quot;465.0&quot; y=&quot;177.0&quot;&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x=&quot;510.0&quot; y=&quot;177.0&quot;&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement=&quot;flow4&quot; id=&quot;BPMNEdge_flow4&quot;&gt; &lt;omgdi:waypoint x=&quot;615.0&quot; y=&quot;177.0&quot;&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x=&quot;660.0&quot; y=&quot;177.0&quot;&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;/bpmndi:BPMNPlane&gt; &lt;/bpmndi:BPMNDiagram&gt;&lt;/definitions&gt; p4-p10.使用步骤 部署activiti 定义流程部署 启动流程实例 用户查询待办任务（由activiti接口完成） 用户完成任务（调用activiti接口） 导入依赖： 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-dependencies&lt;/artifactId&gt; &lt;version&gt;7.0.0.Beta1&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 安装流程设计器 Activiti 在运行时需要数据库的支持，使用25张表，把流程定义节点内容读取到数据库表中，以供后续使用。 Activiti 支持的数据库activiti 支持的数据库和最低版本如下： 数据库类型 版本 JDBC连接示例 说明 h2 1.3.168 jdbc:h2:tcp:&#x2F;&#x2F;localhost&#x2F;activiti 默认配置的数据库 mysql 5.1.21 jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;activiti?autoReconnect&#x3D;true 使用 mysql-connector-java 驱动测试 oracle 11.2.0.1.0 jdbc:oracle:thin:@localhost:1521:xe postgres 8.1 jdbc:postgresql:&#x2F;&#x2F;localhost:5432&#x2F;activiti db2 DB2 10.1 using db2jcc4 jdbc:db2:&#x2F;&#x2F;localhost:50000&#x2F;activiti mssql 2008 using sqljdbc4 jdbc:sqlserver:&#x2F;&#x2F;localhost:1433&#x2F;activiti 在Mysql生成表创建数据库创建 mysql 数据库 activiti （名字任意）： CREATE DATABASE activiti DEFAULT CHARACTER SET utf8; 使用java代码生成表创建 java 工程使用idea 创建 java 的maven工程，取名：activiti01。 加入 maven 依赖的坐标（jar 包）首先需要在 java 工程中加入 ProcessEngine 所需要的 jar 包，包括： activiti-engine-7.0.0.beta1.jar activiti 依赖的 jar 包： mybatis、 alf4j、 log4j 等 activiti 依赖的 spring 包 mysql数据库驱动 第三方数据连接池 dbcp 单元测试 Junit-4.12.jar 我们使用 maven 来实现项目的构建，所以应当导入这些 jar 所对应的坐标到 pom.xml 文件中。 完整的依赖内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;properties&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;activiti.version&gt;7.0.0.Beta1&lt;/activiti.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-engine&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-spring&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- bpmn 模型处理 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-bpmn-model&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- bpmn 转换 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-bpmn-converter&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- bpmn json数据转换 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-json-converter&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- bpmn 布局 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-bpmn-layout&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- activiti 云支持 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti.cloud&lt;/groupId&gt; &lt;artifactId&gt;activiti-cloud-services-api&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.28&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.5&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 链接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;commons-dbcp&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log start --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 添加log4j日志配置我们使用log4j日志包，可以对日志进行配置 在resources 下创建log4j.properties 123456789101112131415# Set root category priority to INFO and its only appender to CONSOLE.#log4j.rootCategory=INFO, CONSOLE debug info warn error fatallog4j.rootCategory=debug, CONSOLE, LOGFILE# Set the enterprise logger category to FATAL and its only appender to CONSOLE.log4j.logger.org.apache.axis.enterprise=FATAL, CONSOLE# CONSOLE is set to be a ConsoleAppender using a PatternLayout.log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppenderlog4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayoutlog4j.appender.CONSOLE.layout.ConversionPattern=%d&#123;ISO8601&#125; %-6r[%15.15t] %-5p %30.30c %x - %m # LOGFILE is set to be a File appender using a PatternLayout.log4j.appender.LOGFILE=org.apache.log4j.FileAppenderlog4j.appender.LOGFILE.File=f:\\act\\activiti.loglog4j.appender.LOGFILE.Append=truelog4j.appender.LOGFILE.layout=org.apache.log4j.PatternLayoutlog4j.appender.LOGFILE.layout.ConversionPattern=%d&#123;ISO8601&#125; %-6r[%15.15t] %-5p %30.30c %x - %m 添加activiti配置文件我们使用activiti提供的默认方式来创建mysql的表。 默认方式的要求是在 resources 下创建 activiti.cfg.xml 文件，注意：默认方式目录和文件名不能修改，因为activiti的源码中已经设置，到固定的目录读取固定文件名的文件。 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsdhttp://www.springframework.org/schema/contexhttp://www.springframework.org/schema/context/spring-context.xsdhttp://www.springframework.org/schema/txhttp://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt; &lt;!-- 默认id对应的值 为processEngineConfiguration --&gt; &lt;!-- processEngine Activiti的流程引擎 --&gt; &lt;bean id=&quot;processEngineConfiguration&quot; class=&quot;org.activiti.engine.impl.cfg.StandaloneProcessEngineConfiguration&quot;&gt; &lt;property name=&quot;jdbcDriver&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;/&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://1.15.220.36:3306/activiti?useUnicode=true&quot;/&gt; &lt;property name=&quot;jdbcUsername&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;jdbcPassword&quot; value=&quot;123456&quot;/&gt; &lt;!-- activiti数据库表处理策略 --&gt; &lt;property name=&quot;databaseSchemaUpdate&quot; value=&quot;true&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 使用连接池 123456789101112131415161718192021222324252627&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsdhttp://www.springframework.org/schema/contexhttp://www.springframework.org/schema/context/spring-context.xsdhttp://www.springframework.org/schema/txhttp://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://1.15.220.36:3306/activiti?useUnicode=true&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;/&gt; &lt;property name=&quot;maxActive&quot; value=&quot;3&quot;/&gt; &lt;property name=&quot;maxIdle&quot; value=&quot;1&quot;/&gt; &lt;/bean&gt; &lt;!--在默认方式下 bean的id 固定为 processEngineConfiguration--&gt; &lt;bean id=&quot;processEngineConfiguration&quot; class=&quot;org.activiti.engine.impl.cfg.StandaloneProcessEngineConfiguration&quot;&gt; &lt;!--引入上面配置好的 链接池--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;!--actviti数据库表在生成时的策略 true - 如果数据库中已经存在相应的表，那么直接使用，如果不存在，那么会创建--&gt; &lt;property name=&quot;databaseSchemaUpdate&quot; value=&quot;true&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; java类编写程序生成表创建一个测试类，调用activiti的工具类，生成acitivti需要的数据库表。 直接使用activiti提供的工具类ProcessEngines，会默认读取classpath下的activiti.cfg.xml文件，读取其中的数据库配置，创建 ProcessEngine，在创建ProcessEngine 时会自动创建表。 代码如下： 123456789101112131415161718package com.itheima.activiti01.test;import org.activiti.engine.ProcessEngine;import org.activiti.engine.ProcessEngineConfiguration;import org.junit.Test;public class TestDemo &#123; /** * 生成 activiti的数据库表 */ @Test public void testCreateDbTable() &#123; //使用classpath下的activiti.cfg.xml中的配置创建processEngine ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); System.out.println(processEngine); &#125;&#125; 说明：1、运行以上程序段即可完成 activiti 表创建，通过改变 activiti.cfg.xml 中databaseSchemaUpdate 参数的值执行不同的数据表处理策略。2 、 上 边 的 方法 getDefaultProcessEngine方法在执行时，从activiti.cfg.xml 中找固定的名称 processEngineConfiguration 。 在测试程序执行过程中，idea的控制台会输出日志，说明程序正在创建数据表，类似如下,注意红线内容： 执行完成后我们查看数据库， 创建了 25 张表，结果如下： 到这，我们就完成activiti运行需要的数据库和表的创建。 p11 .activiti的表结构表的命名规则和作用Activiti 的表都以 ACT_ 开头。 第二部分是表示表的用途的两个字母标识。 用途也和服务的 API 对应。ACT_RE ：’RE’表示 repository。 这个前缀的表包含了流程定义和流程静态资源 （图片，规则，等等）。ACT_RU：’RU’表示 runtime。 这些运行时的表，包含流程实例，任务，变量，异步任务，等运行中的数据。 Activiti 只在流程实例执行过程中保存这些数据， 在流程结束时就会删除这些记录。 这样运行时表可以一直很小速度很快。ACT_HI：’HI’表示 history。 这些表包含历史数据，比如历史流程实例， 变量，任务等等。 ACT_GE ： GE 表示 general。 通用数据， 用于不同场景下 Activiti数据表介绍 表分类 表名 解释 一般数据 [ACT_GE_BYTEARRAY] 通用的流程定义和流程资源 [ACT_GE_PROPERTY] 系统相关属性 流程历史记录 [ACT_HI_ACTINST] 历史的流程实例 [ACT_HI_ATTACHMENT] 历史的流程附件 [ACT_HI_COMMENT] 历史的说明性信息 [ACT_HI_DETAIL] 历史的流程运行中的细节信息 [ACT_HI_IDENTITYLINK] 历史的流程运行过程中用户关系 [ACT_HI_PROCINST] 历史的流程实例 [ACT_HI_TASKINST] 历史的任务实例 [ACT_HI_VARINST] 历史的流程运行中的变量信息 流程定义表 [ACT_RE_DEPLOYMENT] 部署单元信息 [ACT_RE_MODEL] 模型信息 [ACT_RE_PROCDEF] 已部署的流程定义 运行实例表 [ACT_RU_EVENT_SUBSCR] 运行时事件 [ACT_RU_EXECUTION] 运行时流程执行实例 [ACT_RU_IDENTITYLINK] 运行时用户关系信息，存储任务节点与参与者的相关信息 [ACT_RU_JOB] 运行时作业 [ACT_RU_TASK] 运行时任务 [ACT_RU_VARIABLE] 运行时变量表 p12-p14.Activiti体系架构类关系图 Activiti7 IdentityService，FormService两个Serivce都已经删除了。 前两个字母对应相关表 SpringProcessEngineConfiguration配置通过org.activiti.spring.SpringProcessEngineConfiguration 与Spring整合。 创建spring与activiti的整合配置文件： activity-spring.cfg.xml（名称可修改） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.1.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.1.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.1.xsd &quot;&gt; &lt;!-- 工作流引擎配置bean --&gt; &lt;bean id=&quot;processEngineConfiguration&quot; class=&quot;org.activiti.spring.SpringProcessEngineConfiguration&quot;&gt; &lt;!-- 数据源 --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;!-- 使用spring事务管理器 --&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; /&gt; &lt;!-- 数据库策略 --&gt; &lt;property name=&quot;databaseSchemaUpdate&quot; value=&quot;drop-create&quot; /&gt; &lt;!-- activiti的定时任务关闭 --&gt; &lt;property name=&quot;jobExecutorActivate&quot; value=&quot;false&quot; /&gt; &lt;/bean&gt; &lt;!-- 流程引擎 --&gt; &lt;bean id=&quot;processEngine&quot; class=&quot;org.activiti.spring.ProcessEngineFactoryBean&quot;&gt; &lt;property name=&quot;processEngineConfiguration&quot; ref=&quot;processEngineConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 资源服务service --&gt; &lt;bean id=&quot;repositoryService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getRepositoryService&quot; /&gt; &lt;!-- 流程运行service --&gt; &lt;bean id=&quot;runtimeService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getRuntimeService&quot; /&gt; &lt;!-- 任务管理service --&gt; &lt;bean id=&quot;taskService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getTaskService&quot; /&gt; &lt;!-- 历史管理service --&gt; &lt;bean id=&quot;historyService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getHistoryService&quot; /&gt; &lt;!-- 用户管理service --&gt; &lt;bean id=&quot;identityService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getIdentityService&quot; /&gt; &lt;!-- 引擎管理service --&gt; &lt;bean id=&quot;managementService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getManagementService&quot; /&gt; &lt;!-- 数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/activiti&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;mysql&quot; /&gt; &lt;property name=&quot;maxActive&quot; value=&quot;3&quot; /&gt; &lt;property name=&quot;maxIdle&quot; value=&quot;1&quot; /&gt; &lt;/bean&gt; &lt;!-- 事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;/bean&gt; &lt;!-- 通知 --&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt;&lt;/tx:attributes&gt; &lt;!-- 传播行为 --&gt; &lt;tx:method name=&quot;save*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;insert*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;delete*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;update*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;find*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; /&gt; &lt;tx:method name=&quot;get*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; /&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 切面，根据具体项目修改切点配置 --&gt; &lt;aop:config proxy-target-class=&quot;true&quot;&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut=&quot;execution(* com.itheima.ihrm.service.impl.*.(..))&quot;* /&gt; &lt;/aop:config&gt;&lt;/beans&gt; 创建processEngineConfiguration1ProcessEngineConfiguration configuration = ProcessEngineConfiguration.createProcessEngineConfigurationFromResource(&quot;activiti.cfg.xml&quot;) ​ 上边的代码要求activiti.cfg.xml中必须有一个processEngineConfiguration的bean 也可以使用下边的方法，更改bean 的名字： 1ProcessEngineConfiguration.createProcessEngineConfigurationFromResource(String resource, String beanName); 工作流引擎创建工作流引擎（ProcessEngine），相当于一个门面接口，通过ProcessEngineConfiguration创建processEngine，通过ProcessEngine创建各个service接口。 默认创建方式将activiti.cfg.xml文件名及路径固定，且activiti.cfg.xml文件中有 processEngineConfiguration的配置， 可以使用如下代码创建processEngine: 123//直接使用工具类 ProcessEngines，使用classpath下的activiti.cfg.xml中的配置创建processEngineProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();System.out.println(processEngine); 一般创建方式1234//先构建ProcessEngineConfigurationProcessEngineConfiguration configuration = ProcessEngineConfiguration.createProcessEngineConfigurationFromResource(&quot;activiti.cfg.xml&quot;);//通过ProcessEngineConfiguration创建ProcessEngine，此时会创建数据库ProcessEngine processEngine = configuration.buildProcessEngine(); Servcie服务接口Service是工作流引擎提供用于进行工作流部署、执行、管理的服务接口，我们使用这些接口可以就是操作服务对应的数据表 Service创建方式通过ProcessEngine创建Service 方式如下： 123RuntimeService runtimeService = processEngine.getRuntimeService();RepositoryService repositoryService = processEngine.getRepositoryService();TaskService taskService = processEngine.getTaskService(); Service总览 service名称 service作用 RepositoryService activiti的资源管理类 RuntimeService activiti的流程运行管理类 TaskService activiti的任务管理类 HistoryService activiti的历史管理类 ManagerService activiti的引擎管理类 简单介绍： RepositoryService 是activiti的资源管理类，提供了管理和控制流程发布包和流程定义的操作。使用工作流建模工具设计的业务流程图需要使用此service将流程定义文件的内容部署到计算机。 RuntimeServiceActiviti的流程运行管理类。可以从这个服务类中获取很多关于流程执行相关的信息 TaskServiceActiviti的任务管理类。可以从这个类中获取任务的信息。 HistoryServiceActiviti的历史管理类，可以查询历史信息，执行流程时，引擎会保存很多数据（根据配置），比如流程实例启动时间，任务的参与者， 完成任务的时间，每个流程实例的执行路径，等等。 这个服务主要通过查询功能来获得这些数据。 ManagementServiceActiviti的引擎管理类，提供了对 Activiti 流程引擎的管理和维护功能，这些功能不在工作流驱动的应用程序中使用，主要用于 Activiti 系统的日常维护。 p14.Activiti入门在本章内容中，我们来创建一个Activiti工作流，并启动这个流程。 创建Activiti工作流主要包含以下几步： 1、定义流程，按照BPMN的规范，使用流程定义工具，用流程符号把整个流程描述出来 2、部署流程，把画好的流程定义文件，加载到数据库中，生成表的数据 3、启动流程，使用java代码来操作数据库表中的内容 p15.流程符号BPMN 2.0是业务流程建模符号2.0的缩写。 它由Business Process Management Initiative这个非营利协会创建并不断发展。作为一种标识，BPMN 2.0是使用一些符号来明确业务流程设计流程图的一整套符号规范，它能增进业务建模时的沟通效率。 目前BPMN2.0是最新的版本，它用于在BPM上下文中进行布局和可视化的沟通。 接下来我们先来了解在流程设计中常见的 符号。 BPMN2.0的基本符合主要包含： 事件 Event 活动 Activity活动是工作或任务的一个通用术语。一个活动可以是一个任务，还可以是一个当前流程的子处理流程； 其次，你还可以为活动指定不同的类型。常见活动如下： 网关 GateWay网关用来处理决策，有几种常用网关需要了解： 排他网关 (x)——只有一条路径会被选择。流程执行到该网关时，按照输出流的顺序逐个计算，当条件的计算结果为true时，继续执行当前网关的输出流； ​ 如果多条线路计算结果都是 true，则会执行第一个值为 true 的线路。如果所有网关计算结果没有true，则引擎会抛出异常。 ​ 排他网关需要和条件顺序流结合使用，default 属性指定默认顺序流，当所有的条件不满足时会执行默认顺序流。 并行网关 (+)——所有路径会被同时选择 ​ 拆分 —— 并行执行所有输出顺序流，为每一条顺序流创建一个并行执行线路。 ​ 合并 —— 所有从并行网关拆分并执行完成的线路均在此等候，直到所有的线路都执行完成才继续向下执行。 包容网关 (+)—— 可以同时执行多条线路，也可以在网关上设置条件 ​ 拆分 —— 计算每条线路上的表达式，当表达式计算结果为true时，创建一个并行线路并继续执行 ​ 合并 —— 所有从并行网关拆分并执行完成的线路均在此等候，直到所有的线路都执行完成才继续向下执行。 事件网关 (+)—— 专门为中间捕获事件设置的，允许设置多个输出流指向多个不同的中间捕获事件。当流程执行到事件网关后，流程处于等待状态，需要等待抛出事件才能将等待状态转换为活动状态。 流向 Flow流是连接两个流程节点的连线。常见的流向包含以下几种： P20-p21.流程定义部署概述将上面在设计器中定义的流程部署到activiti数据库中，就是流程定义部署。 通过调用activiti的api将流程定义的bpmn和png两个文件一个一个添加部署到activiti中，也可以将两个文件打成zip包进行部署。 单个文件部署方式分别将bpmn文件和png图片文件部署。 123456789101112131415161718192021222324252627282930package com.itheima.test;import org.activiti.engine.ProcessEngine;import org.activiti.engine.ProcessEngines;import org.activiti.engine.RepositoryService;import org.activiti.engine.repository.Deployment;import org.junit.Test;public class ActivitiDemo &#123; /** * 部署流程定义 */ @Test public void testDeployment()&#123;// 1、创建ProcessEngine ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();// 2、得到RepositoryService实例 RepositoryService repositoryService = processEngine.getRepositoryService();// 3、使用RepositoryService进行部署 Deployment deployment = repositoryService.createDeployment() .addClasspathResource(&quot;bpmn/evection.bpmn&quot;) // 添加bpmn资源 .addClasspathResource(&quot;bpmn/evection.png&quot;) // 添加png资源 .name(&quot;出差申请流程&quot;) .deploy();// 4、输出部署信息 System.out.println(&quot;流程部署id：&quot; + deployment.getId()); System.out.println(&quot;流程部署名称：&quot; + deployment.getName()); &#125;&#125; 执行此操作后activiti会将上边代码中指定的bpm文件和图片文件保存在activiti数据库。 压缩包部署方式将evection.bpmn和evection.png压缩成zip包。 1234567891011121314151617181920@Test\tpublic void deployProcessByZip() &#123; // 定义zip输入流 InputStream inputStream = this .getClass() .getClassLoader() .getResourceAsStream( &quot;bpmn/evection.zip&quot;); ZipInputStream zipInputStream = new ZipInputStream(inputStream); // 获取repositoryService RepositoryService repositoryService = processEngine .getRepositoryService(); // 流程部署 Deployment deployment = repositoryService.createDeployment() .addZipInputStream(zipInputStream) .deploy(); System.out.println(&quot;流程部署id：&quot; + deployment.getId()); System.out.println(&quot;流程部署名称：&quot; + deployment.getName());\t&#125; 执行此操作后activiti会将上边代码中指定的bpm文件和图片文件保存在activiti数据库。 操作数据表流程定义部署后操作activiti的3张表如下： act_re_deployment 流程定义部署表，每部署一次增加一条记录 act_re_procdef 流程定义表，部署每个新的流程定义都会在这张表中增加一条记录 act_ge_bytearray 流程资源表 接下来我们来看看，写入了什么数据： 1SELECT * FROM act_re_deployment #流程定义部署表，记录流程部署信息 结果： 1SELECT * FROM act_re_procdef #流程定义表，记录流程定义信息 结果： 注意，KEY 这个字段是用来唯一识别不同流程的关键字 1SELECT * FROM act_ge_bytearray #资源表 结果： 注意： act_re_deployment和act_re_procdef一对多关系，一次部署在流程部署表生成一条记录，但一次部署可以部署多个流程定义，每个流程定义在流程定义表生成一条记录。每一个流程定义在act_ge_bytearray会存在两个资源记录，bpmn和png。 建议：一次部署一个流程，这样部署表和流程定义表是一对一有关系，方便读取流程部署及流程定义信息。 p22- .启动流程实例流程定义部署在activiti后就可以通过工作流管理业务流程了，也就是说上边部署的出差申请流程可以使用了。 针对该流程，启动一个流程表示发起一个新的出差申请单，这就相当于java类与java对象的关系，类定义好后需要new创建一个对象使用，当然可以new多个对象。对于请出差申请流程，张三发起一个出差申请单需要启动一个流程实例，出差申请单发起一个出差单也需要启动一个流程实例。 代码如下： 123456789101112131415161718 /** * 启动流程实例 */ @Test public void testStartProcess()&#123;// 1、创建ProcessEngine ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();// 2、获取RunTimeService RuntimeService runtimeService = processEngine.getRuntimeService();// 3、根据流程定义Id启动流程 ProcessInstance processInstance = runtimeService .startProcessInstanceByKey(&quot;myEvection&quot;);// 输出内容 System.out.println(&quot;流程定义id：&quot; + processInstance.getProcessDefinitionId()); System.out.println(&quot;流程实例id：&quot; + processInstance.getId()); System.out.println(&quot;当前活动Id：&quot; + processInstance.getActivityId()); &#125; 输出内容如下： 操作数据表 act_hi_actinst 流程实例执行历史 act_hi_identitylink 流程的参与用户历史信息 act_hi_procinst 流程实例历史信息 act_hi_taskinst 流程任务历史信息 act_ru_execution 流程执行信息 act_ru_identitylink 流程的参与用户信息 act_ru_task 任务信息"},{"title":"git","path":"/2022/11/08/git/","content":"​ 安装与配置 一般选择默认即可 设置默认编辑器，一般选择vim 基本上 git安装全选默认配置就好","tags":["工具"],"categories":["工具"]},{"title":"SpringCloud学习","path":"/2022/11/01/SpringCloud学习/","content":"Boot和Cloud版本选型版本对应地址：https://spring.io/projects/spring-cloud 详细信息：https://start.spring.io/actuator/info 本次版本选型 Cloud组件 父工程创建 约定&gt;配置&gt;编码 ：先选定好框架和配置再开始编码 总父工程 各个模块 工程创建后 设置字符编码 开启注解支持 选择java编译版本 文件类型过滤 Pom依赖管理需要注意boot版本和cloud版本对应,下面是课程源码复制过来的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495!-- 统一管理jar包版本 --&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;lombok.version&gt;1.18.12&lt;/lombok.version&gt; &lt;mysql.version&gt;5.1.47&lt;/mysql.version&gt; &lt;druid.version&gt;1.1.16&lt;/druid.version&gt; &lt;mybatis.spring.boot.version&gt;1.3.0&lt;/mybatis.spring.boot.version&gt; &lt;/properties&gt; &lt;!-- 子模块继承之后，提供作用：锁定版本+子modlue不用写groupId和version --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!--spring boot 2.2.2--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!--spring cloud Hoxton.SR1--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!--spring cloud alibaba 2.1.0.RELEASE--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;$&#123;druid.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.spring.boot.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;$&#123;lombok.version&#125;&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;addResources&gt;true&lt;/addResources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;","tags":["SpringCloud"]},{"title":"javase","path":"/2022/10/31/javase复习/","content":"​ 单例模式 饿汉式：直接创建对象，不存在线程安全问题 直接实例化饿汉式（简洁直观） 枚举类（最简洁） 静态代码块（适合复杂实例化） 用于在配置文件中读取配置 初始化对象 懒汉式：延迟创建对象 线程不安全 线程安全（ 双重检查 ） 静态内部类 ：内部类不会随着外部类创建和初始化而初始化 类初始化过程 一个类要创建实列，需要先加载并初始化该类 main方法所在的类需要先加载和初始化 一个子类要初始化需要先初始化父类 一个类初始化就是执行()方法 ()方法由静态类变量显示赋值代码和静态代码块组成 类变量显示赋值代码和静态代码块代码从上到下顺序执行 ()方法只执行一次 实列初始化过程","tags":["javas"],"categories":["java"]},{"title":"redis学习小记","path":"/2022/07/09/redis学习小记/","content":"基础基本数据类型 String 字符串 Hash 哈希表 List 有序集合 可重复 Set 无序集合 不重复 SortedSet 可排序集合 不重复 通用命令 keys 查询key keys * 查询所有key 模糊查询占用资源 堵塞进程 生产不使用 del 删除一个或多个key mest 批量添加k -v exists 判断一个key 是否存在 expire 给一个key设置有效期 ttl 查询一个key的有效期 String 类型1234567value 是字符串 根据字符串的格式不同 又可以分为3类：String：普通字符串int：整形 可以做自增自减操作float：浮点 可以做自增自减操作如果是数字类型 会转为二进制进行存储 节省空间字符串只能转为字节码最大512m key的结构12可以通过多个单词形成层级结构,多个单词之间用：隔开 如 项目：业务：用户 Hash类型123hash类型也叫散列，value是一个无序字典，类似java hashMapString结构是将对象序列化为json字符串存储，但需要对其某个字段修改时很不方便Hash结构可以将对象中每个字段独立存储，可针对单个字段crud List类型123456789与jav中linkedList类似，可以看作一个双向链表 支持正反向检索有序元素可重复插入删除快 查询速度一般lpush key element 向左侧插入lpop 从左侧取出rpush key element 向右侧插入rpop blpop key timeout 指定一个时间内阻塞从左边取出 Set类型12345678910111213与java中HashSet类似 可以看作一个value为null的HashMap无序不可重复查找快支持 交集 并集 差集 等功能sadd key member... 向set中添加元素srem key member... 移除set中元素scard key 返回set中元素个数sismember key member 判断set中是否存在一个元素sinter key1 key2 ... 求交集sdiff key1 key2... 求交集sunion key1 key2 求并集 SortedSet类型12345678910是一个可排序的set集合 功能上和java中TreeSet类似，但是底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性（分数），可以基于score属性对元素排序 底层是一个跳表（SkipList）加Hash表实现。可排序不可重复查询速度快一般用来实现排行榜功能排名默认升序 命令后加REV 反转 降序zscore key member 获取指定元素score值zrank key member 获取指定元素排名zrange key min max 获取指定名次内元素 Java使用Redis123456https://redis.io/clientsJedis: Jedis实列线程不安全，多线程操作需要基于连接池使用Lettuce： 基于Netty实现 支持同步,异步和响应式编程 线程安全 支持redis 哨兵模式，集群模式和管道模式Redisson: 基于redis实现的分布式，在分布式环境下推荐使用springData Redis 兼容了 Jedis和Lettuce redis 序列化1234567afterPropertiesSet()&#123;\tif (defaultSerializer == null) &#123; defaultSerializer = new JdkSerializationRedisSerializer( classLoader != null ? classLoader : this.getClass().getClassLoader()); &#125;&#125;如果没有指定序列化方式 默认使用jdk的二进制序列化 （可读性差） redis序列化修改123456789101112131415161718@Bean public RedisTemplate&lt;String,Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory)&#123; //1 创建对象 RedisTemplate&lt;String , Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); //2 设置链接工厂 redisTemplate.setConnectionFactory(redisConnectionFactory); //3 参加json序列化对象 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); //4 设置key序列化格式为字符串 redisTemplate.setKeySerializer(RedisSerializer.string()); redisTemplate.setHashKeySerializer(RedisSerializer.string()); //5 设置value序列化方式为json redisTemplate.setValueSerializer(jsonRedisSerializer); redisTemplate.setHashValueSerializer(jsonRedisSerializer); return redisTemplate; &#125;","tags":["中间件","redis"],"categories":["中间件"]},{"title":"Frame","path":"/wiki/javaNode/Frame.html","content":"Maven基本介绍Mvn介绍Maven：本质是一个项目管理工具，将项目开发和管理过程抽象成一个项目对象模型（POM） POM：Project Object Model 项目对象模型。Maven 是用 Java 语言编写的，管理的东西以面向对象的形式进行设计，最终把一个项目看成一个对象，这个对象叫做 POM pom.xml：Maven 需要一个 pom.xml 文件，Maven 通过加载这个配置文件可以知道项目的相关信息，这个文件代表就一个项目。如果做 8 个项目，对应的是 8 个 pom.xml 文件 依赖管理：Maven 对项目所有依赖资源的一种管理，它和项目之间是一种双向关系，即做项目时可以管理所需要的其他资源，当其他项目需要依赖我们项目时，Maven 也会把我们的项目当作一种资源去进行管理。 管理资源的存储位置：本地仓库，私服，中央仓库 基本作用： 项目构建：提供标准的，跨平台的自动化构建项目的方式 依赖管理：方便快捷的管理项目依赖的资源（jar 包），避免资源间的版本冲突等问题 统一开发结构：提供标准的，统一的项目开发结构 各目录存放资源类型说明： src&#x2F;main&#x2F;java：项目 java 源码 src&#x2F;main&#x2F;resources：项目的相关配置文件（比如 mybatis 配置，xml 映射配置，自定义配置文件等） src&#x2F;main&#x2F;webapp：web 资源（比如 html、css、js 等） src&#x2F;test&#x2F;java：测试代码 src&#x2F;test&#x2F;resources：测试相关配置文件 src&#x2F;pom.xml：项目 pom 文件 参考视频：https://www.bilibili.com/video/BV1Ah411S7ZE 基础概念仓库：用于存储资源，主要是各种 jar 包。有本地仓库，私服，中央仓库，私服和中央仓库都是远程仓库 中央仓库：Maven 团队自身维护的仓库，属于开源的 私服：各公司&#x2F;部门等小范围内存储资源的仓库，私服也可以从中央仓库获取资源，作用： 保存具有版权的资源，包含购买或自主研发的 jar 一定范围内共享资源，能做到仅对内不对外开放 本地仓库：开发者自己电脑上存储资源的仓库，也可从远程仓库获取资源 坐标：Maven 中的坐标用于描述仓库中资源的位置 作用：使用唯一标识，唯一性定义资源位置，通过该标识可以将资源的识别与下载工作交由机器完成 https://mvnrepository.com：查询 maven 某一个资源的坐标，输入资源名称进行检索 依赖设置： groupId：定义当前资源隶属组织名称（通常是域名反写，如：org.mybatis） artifactId：定义当前资源的名称（通常是项目或模块名称，如：crm、sms） version：定义当前资源的版本号 packaging：定义资源的打包方式，取值一般有如下三种 jar：该资源打成 jar 包，默认是 jar war：该资源打成 war 包 pom：该资源是一个父资源（表明使用 Maven 分模块管理），打包时只生成一个 pom.xml 不生成 jar 或其他包结构 环境搭建环境配置Maven 的官网：http://maven.apache.org/ 下载安装：Maven 是一个绿色软件，解压即安装 目录结构： bin：可执行程序目录 boot：Maven 自身的启动加载器 conf：Maven 配置文件的存放目录 lib：Maven运行所需库的存放目录 配置 MAVEN_HOME： Path 下配置：%MAVEN_HOME%\\bin 环境变量配置好之后需要测试环境配置结果，在 DOS 命令窗口下输入以下命令查看输出：mvn -v 仓库配置默认情况 Maven 本地仓库在系统用户目录下的 .m2/repository，修改 Maven 的配置文件 conf/settings.xml 来修改仓库位置 修改本地仓库位置：找到 标签，修改默认值 123456&lt;!-- localRepository| The path to the local repository maven will use to store artifacts.| Default: $&#123;user.home&#125;/.m2/repository&lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt;--&gt;&lt;localRepository&gt;E:\\Workspace\\Java\\Project\\.m2\\repository&lt;/localRepository&gt; 注意：在仓库的同级目录即 .m2 也应该包含一个 settings.xml 配置文件，局部用户配置优先与全局配置 全局 setting 定义了 Maven 的公共配置 用户 setting 定义了当前用户的配置 修改远程仓库：在配置文件中找到 &lt;mirrors&gt; 标签，在这组标签下添加国内镜像 123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;!--必须是central--&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 修改默认 JDK：在配置文件中找到 &lt;profiles&gt; 标签，添加配置 123456789101112&lt;profile&gt; &lt;id&gt;jdk-10&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;10&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;10&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;10&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;/profile&gt; 项目搭建手动搭建 在 E 盘下创建目录 mvnproject 进入该目录，作为我们的操作目录 创建我们的 Maven 项目，创建一个目录 project-java 作为我们的项目文件夹，并进入到该目录 创建 Java 代码（源代码）所在目录，即创建 src/main/java 创建配置文件所在目录，即创建 src/main/resources 创建测试源代码所在目录，即创建 src/test/java 创建测试存放配置文件存放目录，即 src/test/resources 在 src/main/java 中创建一个包（注意在 Windos 文件夹下就是创建目录）demo，在该目录下创建 Demo.java 文件，作为演示所需 Java 程序，内容如下 1234567package demo;public class Demo&#123;\tpublic String say(String name)&#123; System.out.println(&quot;hello &quot;+name); return &quot;hello &quot;+name;\t&#125;&#125; 在 src/test/java 中创建一个测试包（目录）demo，在该包下创建测试程序 DemoTest.java 12345678910package demo;import org.junit.*;public class DemoTest&#123;\t@Test\tpublic void testSay()&#123; Demo d = new Demo(); String ret = d.say(&quot;maven&quot;); Assert.assertEquals(&quot;hello maven&quot;,ret);\t&#125;&#125; 在 project-java/src 下创建 pom.xml 文件，格式如下： 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;!--指定pom的模型版本--&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!--打包方式，web工程打包为war，java工程打包为jar --&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;!--组织id--&gt; &lt;groupId&gt;demo&lt;/groupId&gt;\t&lt;!--项目id--&gt; &lt;artifactId&gt;project-java&lt;/artifactId&gt; &lt;!--版本号:release,snapshot--&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;!--设置当前工程的所有依赖--&gt; &lt;dependencies&gt; &lt;!--具体的依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 搭建完成 Maven 的项目结构，通过 Maven 来构建项目。Maven 的构建命令以 mvn 开头，后面添加功能参数，可以一次性执行多个命令，用空格分离 mvn compile：编译 mvn clean：清理 mvn test：测试 mvn package：打包 mvn install：安装到本地仓库 注意：执行某一条命令，则会把前面所有的都执行一遍 IDEA搭建不用原型 在 IDEA 中配置 Maven，选择 maven3.6.1 防止依赖问题 创建 Maven，New Module → Maven → 不选中 Create from archetype 填写项目的坐标 GroupId：demo ArtifactId：project-java 查看各目录颜色标记是否正确 IDEA 右侧侧栏有 Maven Project，打开后有 Lifecycle 生命周期 自定义 Maven 命令：Run → Edit Configurations → 左上角 + → Maven 使用原型普通工程： 创建 Maven 项目的时候选择使用原型骨架 创建完成后发现通过这种方式缺少一些目录，需要手动去补全目录，并且要对补全的目录进行标记 Web 工程： 选择 Web 对应的原型骨架（选择 Maven 开头的是简化的） 通过原型创建 Web 项目得到的目录结构是不全的，因此需要我们自行补全，同时要标记正确 Web 工程创建之后需要启动运行，使用 tomcat 插件来运行项目，在 pom.xml 中添加插件的坐标： 1234567891011121314151617181920212223242526272829303132333435&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;web01&lt;/name&gt; &lt;groupId&gt;demo&lt;/groupId&gt; &lt;artifactId&gt;web01&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;/dependencies&gt; &lt;!--构建--&gt; &lt;build&gt; &lt;!--设置插件--&gt; &lt;plugins&gt; &lt;!--具体的插件配置--&gt; &lt;plugin&gt; &lt;!--https://mvnrepository.com/ 搜索--&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;80&lt;/port&gt; &lt;!--80端口默认不显示--&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 插件配置以后，在 IDEA 右侧 maven-project 操作面板看到该插件，并且可以利用该插件启动项目，web01 → Plugins → tomcat7 → tomcat7:run 依赖管理依赖配置依赖是指在当前项目中运行所需的 jar，依赖配置的格式如下： 123456789101112&lt;!--设置当前项目所依赖的所有jar--&gt;&lt;dependencies&gt; &lt;!--设置具体的依赖--&gt; &lt;dependency&gt; &lt;!--依赖所属群组id--&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;!--依赖所属项目id--&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;!--依赖版本号--&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 依赖传递依赖具有传递性，分两种： 直接依赖：在当前项目中通过依赖配置建立的依赖关系 间接依赖：被依赖的资源如果依赖其他资源，则表明当前项目间接依赖其他资源 注意：直接依赖和间接依赖其实也是一个相对关系 依赖传递的冲突问题：在依赖传递过程中产生了冲突，有三种优先法则 路径优先：当依赖中出现相同资源时，层级越深，优先级越低，反之则越高 声明优先：当资源在相同层级被依赖时，配置顺序靠前的覆盖靠后的 特殊优先：当同级配置了相同资源的不同版本时，后配置的覆盖先配置的 可选依赖：对外隐藏当前所依赖的资源，不透明 1234567&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;!--默认是false，true以后就变得不透明--&gt;&lt;/dependency&gt; 排除依赖：主动断开依赖的资源，被排除的资源无需指定版本 1234567891011&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hamcrest&lt;/groupId&gt; &lt;!--排除这个资源--&gt; &lt;artifactId&gt;hamcrest-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 依赖范围依赖的 jar 默认情况可以在任何地方可用，可以通过 scope 标签设定其作用范围，有三种： 主程序范围有效（src&#x2F;main 目录范围内） 测试程序范围内有效（src&#x2F;test 目录范围内） 是否参与打包（package 指令范围内） scope 标签的取值有四种：compile,test,provided,runtime 依赖范围的传递性： 生命周期相关事件Maven 的构建生命周期描述的是一次构建过程经历了多少个事件 最常用的一套流程：compile → test-compile → test → package → install clean：清理工作 pre-clean：执行一些在 clean 之前的工作 clean：移除上一次构建产生的所有文件 post-clean：执行一些在 clean 之后立刻完成的工作 default：核心工作，例如编译，测试，打包，部署等，每个事件在执行之前都会将之前的所有事件依次执行一遍 site：产生报告，发布站点等 pre-site：执行一些在生成站点文档之前的工作 site：生成项目的站点文档 post-site：执行一些在生成站点文档之后完成的工作，并为部署做准备 site-deploy：将生成的站点文档部署到特定的服务器上 执行事件Maven 的插件用来执行生命周期中的相关事件 插件与生命周期内的阶段绑定，在执行到对应生命周期时执行对应的插件 Maven 默认在各个生命周期上都绑定了预先设定的插件来完成相应功能 插件还可以完成一些自定义功能 1234567891011121314151617181920212223&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;!--执行--&gt; &lt;excutions&gt; &lt;!--具体执行位置--&gt; &lt;excution&gt; &lt;goals&gt; &lt;!--对源码进行打包，打包放在target目录--&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;!--对测试代码进行打包--&gt; &lt;goal&gt;test-jar&lt;/goal&gt; &lt;/goals&gt; &lt;!--执行的生命周期--&gt; &lt;phase&gt;generate-test-resources&lt;/phase&gt; &lt;/excution&gt; &lt;/excutions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 模块开发拆分工程模块与模块划分： ssm_pojo 拆分 新建模块，拷贝原始项目中对应的相关内容到 ssm_pojo 模块中 实体类（User） 配置文件（无） ssm_dao 拆分 新建模块 拷贝原始项目中对应的相关内容到 ssm_dao 模块中 数据层接口（UserDao） 配置文件：保留与数据层相关配置文件(3 个） 注意：分页插件在配置中与 SqlSessionFactoryBean 绑定，需要保留 pom.xml：引入数据层相关坐标即可，删除 SpringMVC 相关坐标 Spring MyBatis Spring 整合 MyBatis MySQL druid pagehelper 直接依赖 ssm_pojo（对 ssm_pojo 模块执行 install 指令，将其安装到本地仓库） 1234567891011121314&lt;dependencies&gt; &lt;!--导入资源文件pojo--&gt; &lt;dependency&gt; &lt;groupId&gt;demo&lt;/groupId&gt; &lt;artifactId&gt;ssm_pojo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!--spring环境--&gt; &lt;!--mybatis环境--&gt; &lt;!--mysql环境--&gt; &lt;!--spring整合jdbc--&gt; &lt;!--spring整合mybatis--&gt; &lt;!--druid连接池--&gt; &lt;!--分页插件坐标--&gt; &lt;/dependencies&gt; ssm_service 拆分 新建模块 拷贝原始项目中对应的相关内容到 ssm_service 模块中 业务层接口与实现类（UserService、UserServiceImpl） 配置文件：保留与数据层相关配置文件(1 个） pom.xml：引入数据层相关坐标即可，删除 SpringMVC 相关坐标 spring junit spring 整合 junit 直接依赖 ssm_dao（对 ssm_dao 模块执行 install 指令，将其安装到本地仓库） 间接依赖 ssm_pojo（由 ssm_dao 模块负责依赖关系的建立） 修改 service 模块 Spring 核心配置文件名，添加模块名称，格式：applicationContext-service.xml 修改 dao 模块 Spring 核心配置文件名，添加模块名称，格式：applicationContext-dao.xml 修改单元测试引入的配置文件名称，由单个文件修改为多个文件 ssm_control 拆分 新建模块（使用 webapp 模板） 拷贝原始项目中对应的相关内容到 ssm_controller 模块中 现层控制器类与相关设置类（UserController、异常相关……） 配置文件：保留与表现层相关配置文件(1 个）、服务器相关配置文件（1 个） pom.xml：引入数据层相关坐标即可，删除 SpringMVC 相关坐标 spring springmvc jackson servlet tomcat 服务器插件 直接依赖 ssm_service（对 ssm_service 模块执行 install 指令，将其安装到本地仓库） 间接依赖 ssm_dao、ssm_pojo 12345678910111213141516171819&lt;dependencies&gt; &lt;!--导入资源文件service--&gt; &lt;dependency&gt; &lt;groupId&gt;demo&lt;/groupId&gt; &lt;artifactId&gt;ssm_service&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!--springmvc环境--&gt; &lt;!--jackson相关坐标3个--&gt; &lt;!--servlet环境--&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;!--设置插件--&gt; &lt;plugins&gt; &lt;!--具体的插件配置--&gt; &lt;plugin&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 修改 web.xml 配置文件中加载 Spring 环境的配置文件名称，使用*通配，加载所有 applicationContext- 开始的配置文件： 12345&lt;!--加载配置文件--&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:applicationContext-*.xml&lt;/param-value&gt;&lt;/context-param&gt; spring-mvc 1&lt;mvc:annotation-driven/&gt;&lt;context:component-scan base-package=&quot;controller&quot;/&gt; 聚合作用：聚合用于快速构建 Maven 工程，一次性构建多个项目&#x2F;模块 制作方式： 创建一个空模块，打包类型定义为 pom 1&lt;packaging&gt;pom&lt;/packaging&gt; 定义当前模块进行构建操作时关联的其他模块名称 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;............&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;demo&lt;/groupId&gt; &lt;artifactId&gt;ssm&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!--定义该工程用于构建管理--&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;!--管理的工程列表--&gt; &lt;modules&gt; &lt;!--具体的工程名称--&gt; &lt;module&gt;../ssm_pojo&lt;/module&gt; &lt;module&gt;../ssm_dao&lt;/module&gt; &lt;module&gt;../ssm_service&lt;/module&gt; &lt;module&gt;../ssm_controller&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; 注意事项：参与聚合操作的模块最终执行顺序与模块间的依赖关系有关，与配置顺序无关 继承Maven 中的继承与 Java 中的继承相似，可以实现在子工程中沿用父工程中的配置 dependencyManagement 里只是声明依赖，并不实现引入，所以子工程需要显式声明需要用的依赖 如果子工程中未声明依赖，则不会从父项目继承下来 在子工程中声明该依赖项，并且不指定具体版本，才会从父项目中继承该项，version 和 scope 都继承取自父工程 pom 文件 如果子工程中指定了版本号，那么使用子工程中指定的 jar 版本 制作方式： 在子工程中声明其父工程坐标与对应的位置 12345678&lt;!--定义该工程的父工程--&gt;&lt;parent&gt; &lt;groupId&gt;com.seazean&lt;/groupId&gt; &lt;artifactId&gt;ssm&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!--填写父工程的pom文件--&gt; &lt;relativePath&gt;../ssm/pom.xml&lt;/relativePath&gt;&lt;/parent&gt; 继承依赖的定义：在父工程中定义依赖管理 12345678910111213&lt;!--声明此处进行依赖管理，版本锁定--&gt;&lt;dependencyManagement&gt; &lt;!--具体的依赖--&gt; &lt;dependencies&gt; &lt;!--spring环境--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.9.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!--等等所有--&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 继承依赖的使用：在子工程中定义依赖关系，无需声明依赖版本，版本参照父工程中依赖的版本 1234567&lt;dependencies&gt; &lt;!--spring环境--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 继承的资源： 12345678910111213141516171819groupId：项目组ID，项目坐标的核心元素version：项目版本，项目坐标的核心因素description：项目的描述信息organization：项目的组织信息inceptionYear：项目的创始年份url：项目的URL地址developers：项目的开发者信息contributors：项目的贡献者信息distributionManagement：项目的部署配置issueManagement：项目的缺陷跟踪系统信息ciManagement：项目的持续集成系统信息scm：项目的版本控制系统信息malilingLists：项目的邮件列表信息properties：自定义的Maven属性dependencies：项目的依赖配置dependencyManagement：项目的依赖管理配置repositories：项目的仓库配置build：包括项目的源码目录配置、输出目录配置、插件配置、插件管理配置等reporting：包括项目的报告输出目录配置、报告插件配置等 继承与聚合： 作用： 聚合用于快速构建项目 继承用于快速配置 相同点： 聚合与继承的 pom.xml 文件打包方式均为 pom，可以将两种关系制作到同一个 pom 文件中 聚合与继承均属于设计型模块，并无实际的模块内容 不同点： 聚合是在当前模块中配置关系，聚合可以感知到参与聚合的模块有哪些 继承是在子模块中配置关系，父模块无法感知哪些子模块继承了自己 属性 版本统一的重要性： 属性类别： 自定义属性 内置属性 setting 属性 Java 系统属性 环境变量属性 自定义属性： 作用：等同于定义变量，方便统一维护 定义格式： 12345&lt;!--定义自定义属性，放在dependencyManagement上方--&gt;&lt;properties&gt; &lt;spring.version&gt;5.1.9.RELEASE&lt;/spring.version&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt;&lt;/properties&gt; 聚合与继承的 pom.xml 文件打包方式均为 pom，可以将两种关系制作到同一个 pom 文件中 聚合与继承均属于设计型模块，并无实际的模块内容 调用格式： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt; 内置属性： 作用：使用 Maven 内置属性，快速配置 调用格式： 1$&#123;project.basedir&#125; or $&#123;project.basedir&#125; &lt;!--../ssm根目录--&gt;$&#123;version&#125; or $&#123;project.version&#125; vresion 是 1.0-SNAPSHOT 123&lt;groupId&gt;demo&lt;/groupId&gt;&lt;artifactId&gt;ssm&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; setting 属性 使用 Maven 配置文件 setting.xml 中的标签属性，用于动态配置 调用格式： 1$&#123;settings.localRepository&#125; Java 系统属性： 作用：读取 Java 系统属性 调用格式： 1$&#123;user.home&#125; 系统属性查询方式 cmd 命令： 1mvn help:system 环境变量属性 作用：使用 Maven 配置文件 setting.xml 中的标签属性，用于动态配置 调用格式： 1$&#123;env.JAVA_HOME&#125; 环境变量属性查询方式： 1mvn help:system 工程版本SNAPSHOT（快照版本） 项目开发过程中，为方便团队成员合作，解决模块间相互依赖和时时更新的问题，开发者对每个模块进行构建的时候，输出的临时性版本叫快照版本（测试阶段版本） 快照版本会随着开发的进展不断更新 RELEASE（发布版本） 项目开发到进入阶段里程碑后，向团队外部发布较为稳定的版本，这种版本所对应的构件文件是稳定的，即便进行功能的后续开发，也不会改变当前发布版本内容，这种版本称为发布版本 约定规范： &lt;主版本&gt;.&lt;次版本&gt;.&lt;增量版本&gt;.&lt;里程碑版本&gt; 主版本：表示项目重大架构的变更，如：Spring5 相较于 Spring4 的迭代 次版本：表示有较大的功能增加和变化，或者全面系统地修复漏洞 增量版本：表示有重大漏洞的修复 里程碑版本：表明一个版本的里程碑（版本内部）。这样的版本同下一个正式版本相比，相对来说不是很稳定，有待更多的测试 资源配置作用：在任意配置文件中加载 pom 文件中定义的属性 父文件 pom.xml 12&lt;properties&gt; &lt;jdbc.url&gt;jdbc:mysql://192.168.0.137:3306/ssm_db?useSSL=false&lt;/jdbc.url&gt;&lt;/properties&gt; 开启配置文件加载 pom 属性： 123456789&lt;!--配置资源文件对应的信息--&gt;&lt;resources&gt; &lt;resource&gt; &lt;!--设定配置文件对应的位置目录，支持使用属性动态设定路径--&gt; &lt;directory&gt;$&#123;project.basedir&#125;/src/main/resources&lt;/directory&gt; &lt;!--开启对配置文件的资源加载过滤--&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt;&lt;/resources&gt; properties 文件中调用格式： 12jdbc.driver=com.mysql.jdbc.Driverjdbc.url=$&#123;jdbc.url&#125;jdbc.username=rootjdbc.password=123456 多环境配置 环境配置 123456789101112131415161718192021&lt;!--创建多环境--&gt;&lt;profiles&gt; &lt;!--定义具体的环境：生产环境--&gt; &lt;profile&gt; &lt;!--定义环境对应的唯一名称--&gt; &lt;id&gt;pro_env&lt;/id&gt; &lt;!--定义环境中专用的属性值--&gt; &lt;properties&gt; &lt;jdbc.url&gt;jdbc:mysql://127.1.1.1:3306/ssm_db&lt;/jdbc.url&gt; &lt;/properties&gt; &lt;!--设置默认启动--&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;!--定义具体的环境：开发环境--&gt; &lt;profile&gt; &lt;id&gt;dev_env&lt;/id&gt; …… &lt;/profile&gt;&lt;/profiles&gt; 加载指定环境 作用：加载指定环境配置 调用格式： 1mvn 指令 –P 环境定义id 范例： 1mvn install –P pro_env 跳过测试命令： 1mvn 指令 –D skipTests 注意事项：执行的指令生命周期必须包含测试环节 IEDA 界面： 配置跳过： 1234567891011121314&lt;plugin&gt; &lt;!--&lt;groupId&gt;org.apache.maven&lt;/groupId&gt;--&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.22.1&lt;/version&gt; &lt;configuration&gt; &lt;skipTests&gt;true&lt;/skipTests&gt;&lt;!--设置跳过测试--&gt; &lt;includes&gt; &lt;!--包含指定的测试用例--&gt; &lt;include&gt;**/User*Test.java&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt;&lt;!--排除指定的测试用例--&gt; &lt;exclude&gt;**/User*TestCase.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt;&lt;/plugin&gt; 私服NexusNexus 是 Sonatype 公司的一款 Maven 私服产品 下载地址：https://help.sonatype.com/repomanager3/download 启动服务器（命令行启动）： 1nexus.exe /run nexus 访问服务器（默认端口：8081）： 1http://localhost:8081 修改基础配置信息 安装路径下 etc 目录中 nexus-default.properties 文件保存有 nexus 基础配置信息，例如默认访问端口 修改服务器运行配置信息 安装路径下 bin 目录中 nexus.vmoptions 文件保存有 nexus 服务器启动的配置信息，例如默认占用内存空间 资源操作 仓库分类： 宿主仓库 hosted 保存无法从中央仓库获取的资源 自主研发 第三方非开源项目 代理仓库 proxy 代理远程仓库，通过 nexus 访问其他公共仓库，例如中央仓库 仓库组 group 将若干个仓库组成一个群组，简化配置 仓库组不能保存资源，属于设计型仓库 资源上传，上传资源时提供对应的信息 保存的位置（宿主仓库） 资源文件 对应坐标 IDEA操作上传下载 访问私服本地访问配置本地仓库访问私服的权限（setting.xml） 123456789101112&lt;servers&gt; &lt;server&gt; &lt;id&gt;heima-release&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;heima-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 配置本地仓库资源来源（setting.xml） 1234567&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-heima&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;url&gt;http://localhost:8081/repository/maven-public/&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 工程访问配置当前项目访问私服上传资源的保存位置（pom.xml） 12345678910&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;heima-release&lt;/id&gt; &lt;url&gt;http://localhost:8081/repository/heima-release/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;heima-snapshots&lt;/id&gt; &lt;url&gt;http://localhost:8081/repository/heima-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 发布资源到私服命令 1mvn deploy 日志Log4j程序中的日志可以用来记录程序在运行时候的详情，并可以进行永久存储。 输出语句 日志技术 取消日志 需要修改代码，灵活性比较差 不需要修改代码，灵活性比较好 输出位置 只能是控制台 可以将日志信息写入到文件或者数据库中 多线程 和业务代码处于一个线程中 多线程方式记录日志，不影响业务代码的性能 Log4j 是 Apache 的一个开源项目。使用 Log4j，通过一个配置文件来灵活地进行配置，而不需要修改应用的代码。我们可以控制日志信息输送的目的地是控制台、文件等位置，也可以控制每一条日志的输出格式。 配置文件配置文件的三个核心： 配置根 Logger 格式：log4j.rootLogger&#x3D;日志级别，appenderName1，appenderName2，… 日志级别：常见的五个级别：DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL（可以自定义）Log4j 规则：只输出级别不低于设定级别的日志信息 appenderName1：指定日志信息要输出地址。可以同时指定多个输出目的地，用逗号隔开： 例如：log4j.rootLogger＝INFO，ca，fa Appenders（输出源）：日志要输出的地方，如控制台（Console）、文件（Files）等 Appenders 取值： org.apache.log4j.ConsoleAppender（控制台） org.apache.log4j.FileAppender（文件） ConsoleAppender 常用参数 ImmediateFlush=true：表示所有消息都会被立即输出，设为 false 则不输出，默认值是 true Target=System.err：默认值是 System.out FileAppender常用的选项 ImmediateFlush=true：表示所有消息都会被立即输出。设为 false 则不输出，默认值是 true Append=false：true 表示将消息添加到指定文件中，原来的消息不覆盖。默认值是 true File=E:/logs/logging.log4j：指定消息输出到 logging.log4j 文件中 Layouts (布局)：日志输出的格式，常用的布局管理器： org.apache.log4j.PatternLayout（可以灵活地指定布局模式） org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串） org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等信息） PatternLayout 常用的选项 日志应用 log4j 的配置文件,名字为 log4j.properties, 放在 src 根目录下 12345678910111213141516log4j.rootLogger=I### direct log messages to my ###log4j.appender.my=org.apache.log4j.ConsoleAppenderlog4j.appender.my.ImmediateFlush = truelog4j.appender.my.Target=System.outlog4j.appender.my.layout=org.apache.log4j.PatternLayoutlog4j.appender.my.layout.ConversionPattern=%d %t %5p %c&#123;1&#125;:%L - %m%n# fileAppender演示log4j.appender.fileAppender=org.apache.log4j.FileAppenderlog4j.appender.fileAppender.ImmediateFlush = truelog4j.appender.fileAppender.Append=truelog4j.appender.fileAppender.File=E:/log4j-log.loglog4j.appender.fileAppender.layout=org.apache.log4j.PatternLayoutlog4j.appender.fileAppender.layout.ConversionPattern=%d %5p %c&#123;1&#125;:%L - %m%n 测试类 123456789101112131415161718192021// 测试类public class Log4JTest01 &#123; //使用log4j的api来获取日志的对象 //弊端：如果以后我们更换日志的实现类，那么下面的代码就需要跟着改 //不推荐使用 //private static final Logger LOGGER = Logger.getLogger(Log4JTest01.class); //使用slf4j里面的api来获取日志的对象 //好处：如果以后我们更换日志的实现类，那么下面的代码不需要跟着修改 //推荐使用 private static final Logger LOGGER = LoggerFactory.getLogger(Log4JTest01.class); public static void main(String[] args) &#123; //1.导入jar包 //2.编写配置文件 //3.在代码中获取日志的对象 //4.按照日志级别设置日志信息 LOGGER.debug(&quot;debug级别的日志&quot;); LOGGER.info(&quot;info级别的日志&quot;); LOGGER.warn(&quot;warn级别的日志&quot;); LOGGER.error(&quot;error级别的日志&quot;); &#125;&#125; Netty基本介绍Netty 是一个异步事件驱动的网络应用程序框架，用于快速开发可维护、高性能的网络服务器和客户端 Netty 官网：https://netty.io/ Netty 的对 JDK 自带的 NIO 的 API 进行封装，解决上述问题，主要特点有： 设计优雅，适用于各种传输类型的统一 API， 阻塞和非阻塞 Socket 基于灵活且可扩展的事件模型 使用方便，详细记录的 Javadoc、用户指南和示例，没有其他依赖项 高性能，吞吐量更高，延迟更低，减少资源消耗，最小化不必要的内存复制 安全，完整的 SSL&#x2F;TLS 和 StartTLS 支持 Netty 的功能特性： 传输服务：支持 BIO 和 NIO 容器集成：支持 OSGI、JBossMC、Spring、Guice 容器 协议支持：HTTP、Protobuf、二进制、文本、WebSocket 等一系列协议都支持，也支持通过实行编码解码逻辑来实现自定义协议 Core 核心：可扩展事件模型、通用通信 API、支持零拷贝的 ByteBuf 缓冲对象 线程模型阻塞模型传统阻塞型 I&#x2F;O 模式，每个连接都需要独立的线程完成数据的输入，业务处理，数据返回 模型缺点： 当并发数较大时，需要创建大量线程来处理连接，系统资源占用较大 连接建立后，如果当前线程暂时没有数据可读，则线程就阻塞在 read 操作上，造成线程资源浪费 参考文章：https://www.jianshu.com/p/2965fca6bb8f Reactor设计思想Reactor 模式，通过一个或多个输入同时传递给服务处理器的事件驱动处理模式。 服务端程序处理传入的多路请求，并将它们同步分派给对应的处理线程，Reactor 模式也叫 Dispatcher 模式，即 I&#x2F;O 多路复用统一监听事件，收到事件后分发（Dispatch 给某线程） I&#x2F;O 复用结合线程池，就是 Reactor 模式基本设计思想： Reactor 模式关键组成： Reactor：在一个单独的线程中运行，负责监听和分发事件，分发给适当的处理程序来对 I&#x2F;O 事件做出反应 Handler：处理程序执行 I&#x2F;O 要完成的实际事件，Reactor 通过调度适当的处理程序来响应 I&#x2F;O 事件，处理程序执行非阻塞操作 Reactor 模式具有如下的优点： 响应快，不必为单个同步时间所阻塞，虽然 Reactor 本身依然是同步的 编程相对简单，可以最大程度的避免复杂的多线程及同步问题，并且避免了多线程&#x2F;进程的切换开销 可扩展性，可以方便的通过增加 Reactor 实例个数来充分利用 CPU 资源 可复用性，Reactor 模型本身与具体事件处理逻辑无关，具有很高的复用性 根据 Reactor 的数量和处理资源池线程的数量不同，有三种典型的实现： 单 Reactor 单线程 单 Reactor 多线程 主从 Reactor 多线程 单R单线程Reactor 对象通过 select 监控客户端请求事件，收到事件后通过 dispatch 进行分发： 如果是建立连接请求事件，则由 Acceptor 通过 accept 处理连接请求，然后创建一个 Handler 对象处理连接完成后的后续业务处理 如果不是建立连接事件，则 Reactor 会分发给连接对应的 Handler 来响应，Handler 会完成 read、业务处理、send 的完整流程 说明：Handler 和 Acceptor 属于同一个线程 模型优点：模型简单，没有多线程、进程通信、竞争的问题，全部都在一个线程中完成 模型缺点： 性能问题：只有一个线程，无法发挥多核 CPU 的性能，Handler 在处理某个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈 可靠性问题：线程意外跑飞，或者进入死循环，会导致整个系统通信模块不可用，不能接收和处理外部消息，造成节点故障 使用场景：客户端的数量有限，业务处理非常快速，比如 Redis，业务处理的时间复杂度 O(1) 单R多线程执行流程通同单 Reactor 单线程，不同的是： Handler 只负责响应事件，不做具体业务处理，通过 read 读取数据后，会分发给后面的 Worker 线程池进行业务处理 Worker 线程池会分配独立的线程完成真正的业务处理，将响应结果发给 Handler 进行处理，最后由 Handler 收到响应结果后通过 send 将响应结果返回给 Client 模型优点：可以充分利用多核 CPU 的处理能力 模型缺点： 多线程数据共享和访问比较复杂 Reactor 承担所有事件的监听和响应，在单线程中运行，高并发场景下容易成为性能瓶颈 主从模型采用多个 Reactor ，执行流程： Reactor 主线程 MainReactor 通过 select 监控建立连接事件，收到事件后通过 Acceptor 接收，处理建立连接事件，处理完成后 MainReactor 会将连接分配给 Reactor 子线程的 SubReactor（有多个）处理 SubReactor 将连接加入连接队列进行监听其他事件，并创建一个 Handler 用于处理该连接的事件，当有新的事件发生时，SubReactor 会调用连接对应的 Handler 进行响应 Handler 通过 read 读取数据后，会分发给 Worker 线程池进行业务处理 Worker 线程池会分配独立的线程完成真正的业务处理，将响应结果发给 Handler 进行处理，最后由 Handler 收到响应结果后通过 send 将响应结果返回给 Client 模型优点 父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理 父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据 使用场景：Nginx 主从 Reactor 多进程模型，Memcached 主从多线程，Netty 主从多线程模型的支持 ProactorReactor 模式中，Reactor 等待某个事件的操作状态发生变化（文件描述符可读写，socket 可读写），然后把事件传递给事先注册的 Handler 来做实际的读写操作，其中的读写操作都需要应用程序同步操作，所以 Reactor 是非阻塞同步网络模型（NIO） 把 I&#x2F;O 操作改为异步，交给操作系统来完成就能进一步提升性能，这就是异步网络模型 Proactor（AIO）： 工作流程： ProactorInitiator 创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 通过 Asynchronous Operation Processor（AsyOptProcessor）注册到内核 AsyOptProcessor 处理注册请求，并处理 I&#x2F;O 操作，完成I&#x2F;O后通知 Proactor Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理，最后由 Handler 完成业务处理 对比：Reactor 在事件发生时就通知事先注册的处理器（读写在应用程序线程中处理完成）；Proactor 是在事件发生时基于异步 I&#x2F;O 完成读写操作（内核完成），I&#x2F;O 完成后才回调应用程序的处理器进行业务处理 模式优点：异步 I&#x2F;O 更加充分发挥 DMA（Direct Memory Access 直接内存存取）的优势 模式缺点： 编程复杂性，由于异步操作流程的事件的初始化和事件完成在时间和空间上都是相互分离的，因此开发异步应用程序更加复杂，应用程序还可能因为反向的流控而变得更加难以调试 内存使用，缓冲区在读或写操作的时间段内必须保持住，可能造成持续的不确定性，并且每个并发操作都要求有独立的缓存，Reactor 模式在 socket 准备好读或写之前是不要求开辟缓存的 操作系统支持，Windows 下通过 IOCP 实现了真正的异步 I&#x2F;O，而在 Linux 系统下，Linux2.6 才引入异步 I&#x2F;O，目前还不完善，所以在 Linux 下实现高并发网络编程都是以 Reactor 模型为主 NettyNetty 主要基于主从 Reactors 多线程模型做了一定的改进，Netty 的工作架构图： 工作流程： Netty 抽象出两组线程池 BossGroup 专门负责接收客户端的连接，WorkerGroup 专门负责网络的读写 BossGroup 和 WorkerGroup 类型都是 NioEventLoopGroup，该 Group 相当于一个事件循环组，含有多个事件循环，每一个事件循环是 NioEventLoop，所以可以有多个线程 NioEventLoop 表示一个循环处理任务的线程，每个 NioEventLoop 都有一个 Selector，用于监听绑定在其上的 Socket 的通讯 每个 Boss NioEventLoop 循环执行的步骤： 轮询 accept 事件 处理 accept 事件，与 client 建立连接，生成 NioScocketChannel，并将其注册到某个 Worker 中的某个 NioEventLoop 上的 Selector，连接就与 NioEventLoop 绑定 处理任务队列的任务，即 runAllTasks 每个 Worker NioEventLoop 循环执行的步骤： 轮询 read、write 事件 处理 I&#x2F;O 事件，即 read，write 事件，在对应 NioSocketChannel 处理 处理任务队列的任务，即 runAllTasks 每个 Worker NioEventLoop 处理业务时，会使用 Pipeline（管道），Pipeline 中包含了 Channel，即通过 Pipeline 可以获取到对应通道，管道中维护了很多的处理器 Handler 基本实现开发简单的服务器端和客户端，基本介绍： Channel 理解为数据的通道，把 msg 理解为流动的数据，最开始输入是 ByteBuf，但经过 Pipeline 的加工，会变成其它类型对象，最后输出又变成 ByteBuf Handler 理解为数据的处理工序，Pipeline 负责发布事件传播给每个 Handler，Handler 对自己感兴趣的事件进行处理（重写了相应事件处理方法），分 Inbound 和 Outbound 两类 EventLoop 理解为处理数据的执行者，既可以执行 IO 操作，也可以进行任务处理。每个执行者有任务队列，队列里可以堆放多个 Channel 的待处理任务，任务分为普通任务、定时任务。按照 Pipeline 顺序，依次按照 Handler 的规划（代码）处理数据 代码实现： pom.xml 12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.20.Final&lt;/version&gt;&lt;/dependency&gt; Server.java 12345678910111213141516171819202122232425262728293031323334public class HelloServer &#123; public static void main(String[] args) &#123; EventLoopGroup boss = new NioEventLoopGroup(); EventLoopGroup worker = new NioEventLoopGroup(2); // 1. 启动器，负责组装 netty 组件，启动服务器 new ServerBootstrap() // 2. 线程组，boss 只负责【处理 accept 事件】， worker 只【负责 channel 上的读写】 .group(boss, worker) //.option() // 给 ServerSocketChannel 配置参数 //.childOption() // 给 SocketChannel 配置参数 // 3. 选择服务器的 ServerSocketChannel 实现 .channel(NioServerSocketChannel.class) // 4. boss 负责处理连接，worker(child) 负责处理读写，决定了能执行哪些操作(handler) .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; // 5. channel 代表和客户端进行数据读写的通道 Initializer 初始化，负责添加别的 handler // 7. 连接建立后，执行初始化方法 @Override protected void initChannel(NioSocketChannel ch) throws Exception &#123; // 添加具体的 handler ch.pipeline().addLast(new StringDecoder());// 将 ByteBuf 转成字符串 ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123; // 自定义 handler // 读事件 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; // 打印转换好的字符串 System.out.println(msg); &#125; &#125;); &#125; &#125;) // 6. 绑定监听端口 .bind(8080); &#125;&#125; Client.java 12345678910111213141516171819202122232425262728public class HelloClient &#123; public static void main(String[] args) throws InterruptedException &#123; // 1. 创建启动器类 new Bootstrap() // 2. 添加 EventLoop .group(new NioEventLoopGroup()) //.option()，给 SocketChannel 配置参数 // 3. 选择客户端 channel 实现 .channel(NioSocketChannel.class) // 4. 添加处理器 .handler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; // 4.1 连接建立后被调用 @Override protected void initChannel(NioSocketChannel ch) throws Exception &#123; // 将 Hello World 转为 ByteBuf ch.pipeline().addLast(new StringEncoder()); &#125; &#125;) // 5. 连接到服务器，然后调用 4.1 .connect(new InetSocketAddress(&quot;127.0.0.1&quot;,8080)) // 6. 阻塞方法，直到连接建立 .sync() // 7. 代表连接对象 .channel() // 8. 向服务器发送数据 .writeAndFlush(&quot;Hello World&quot;); &#125;&#125; 参考视频：https://www.bilibili.com/video/BV1py4y1E7oA 组件介绍EventLoop基本介绍事件循环对象 EventLoop，本质是一个单线程执行器同时维护了一个 Selector，有 run 方法处理 Channel 上源源不断的 IO 事件 事件循环组 EventLoopGroup 是一组 EventLoop，Channel 会调用 Boss EventLoopGroup 的 register 方法来绑定其中一个 Worker 的 EventLoop，后续这个 Channel 上的 IO 事件都由此 EventLoop 来处理，保证了事件处理时的线程安全 EventLoopGroup 类 API： EventLoop next()：获取集合中下一个 EventLoop，EventLoopGroup 实现了 Iterable 接口提供遍历 EventLoop 的能力 Future&lt;?&gt; shutdownGracefully()：优雅关闭的方法，会首先切换 EventLoopGroup 到关闭状态从而拒绝新的任务的加入，然后在任务队列的任务都处理完成后，停止线程的运行，从而确保整体应用是在正常有序的状态下退出的 &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task)：提交任务 ScheduledFuture&lt;?&gt; scheduleWithFixedDelay：提交定时任务 任务传递把要调用的代码封装为一个任务对象，由下一个 handler 的线程来调用 12345678910111213141516171819202122232425262728public class EventLoopServer &#123; public static void main(String[] args) &#123; EventLoopGroup group = new DefaultEventLoopGroup(); new ServerBootstrap() .group(new NioEventLoopGroup(), new NioEventLoopGroup(2)) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(&quot;handler1&quot;, new ChannelInboundHandlerAdapter() &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ByteBuf buf = (ByteBuf) msg; log.debug(buf.toString(Charset.defaultCharset())); ctx.fireChannelRead(msg); // 让消息【传递】给下一个 handler &#125; &#125;).addLast(group, &quot;handler2&quot;, new ChannelInboundHandlerAdapter() &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ByteBuf buf = (ByteBuf) msg; log.debug(buf.toString(Charset.defaultCharset())); &#125; &#125;); &#125; &#125;) .bind(8080); &#125;&#125; 源码分析： 123456789101112131415161718192021public ChannelHandlerContext fireChannelRead(final Object msg) &#123; invokeChannelRead(findContextInbound(MASK_CHANNEL_READ), msg); return this;&#125;static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) &#123; final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, &quot;msg&quot;), next); EventExecutor executor = next.executor(); // 下一个 handler 的事件循环是否与当前的事件循环是同一个线程 if (executor.inEventLoop()) &#123; // 是，直接调用 next.invokeChannelRead(m); &#125; else &#123; // 不是，将要执行的代码作为任务提交给下一个 handler 处理 executor.execute(new Runnable() &#123; @Override public void run() &#123; next.invokeChannelRead(m); &#125; &#125;); &#125;&#125; Channel连接操作Channel 类 API： ChannelFuture close()：关闭通道 ChannelPipeline pipeline()：添加处理器 ChannelFuture write(Object msg)：数据写入缓冲区 ChannelFuture writeAndFlush(Object msg)：数据写入缓冲区并且刷出 ChannelFuture 类 API： ChannelFuture sync()：同步阻塞等待连接成功 ChannelFuture addListener(GenericFutureListener listener)：异步等待 代码实现： connect 方法是异步的，不等连接建立完成就返回，因此 channelFuture 对象中不能立刻获得到正确的 Channel 对象，需要等待 连接未建立 channel 打印为 [id: 0x2e1884dd]；建立成功打印为 [id: 0x2e1884dd, L:/127.0.0.1:57191 - R:/127.0.0.1:8080] 12345678910111213141516171819202122232425262728293031323334353637public class ChannelClient &#123; public static void main(String[] args) throws InterruptedException &#123; ChannelFuture channelFuture = new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new StringEncoder()); &#125; &#125;) // 1. 连接服务器，【异步非阻塞】，main 调用 connect 方法，真正执行连接的是 nio 线程 .connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8080)); // 2.1 使用 sync 方法【同步】处理结果，阻塞当前线程，直到 nio 线程连接建立完毕 channelFuture.sync(); Channel channel = channelFuture.channel(); System.out.println(channel); // 【打印】 // 向服务器发送数据 channel.writeAndFlush(&quot;hello world&quot;); **************************************************************************************二选一 // 2.2 使用 addListener 方法【异步】处理结果 channelFuture.addListener(new ChannelFutureListener() &#123; @Override // nio 线程连接建立好以后，回调该方法 public void operationComplete(ChannelFuture future) throws Exception &#123; if (future.isSuccess()) &#123; Channel channel = future.channel(); channel.writeAndFlush(&quot;hello, world&quot;); &#125; else &#123; // 建立失败，需要关闭 future.channel().close(); &#125; &#125; &#125;); &#125;&#125; 关闭操作关闭 EventLoopGroup 的运行，分为同步关闭和异步关闭 12345678910111213141516171819202122232425262728293031323334353637public class CloseFutureClient &#123; public static void main(String[] args) throws InterruptedException &#123; NioEventLoopGroup group = new NioEventLoopGroup(); ChannelFuture channelFuture = new Bootstrap() // .... .connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8080)); Channel channel = channelFuture.sync().channel(); // 发送数据 new Thread(() -&gt; &#123; Scanner sc = new Scanner(System.in); while (true) &#123; String line = sc.nextLine(); if (line.equals(&quot;q&quot;)) &#123; channel.close(); break; &#125; channel.writeAndFlush(line); &#125; &#125;, &quot;input&quot;).start(); // 获取 CloseFuture 对象 ChannelFuture closeFuture = channel.closeFuture(); // 1. 同步处理关闭 System.out.println(&quot;waiting close...&quot;); closeFuture.sync(); System.out.println(&quot;处理关闭后的操作&quot;);**************************************************** // 2. 异步处理关闭 closeFuture.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; System.out.println(&quot;处理关闭后的操作&quot;); group.shutdownGracefully(); &#125; &#125;); &#125;&#125; Future基本介绍Netty 中的 Future 与 JDK 中的 Future 同名，但是功能的实现不同 12package io.netty.util.concurrent;public interface Future&lt;V&gt; extends java.util.concurrent.Future&lt;V&gt; Future 类 API： V get()：阻塞等待获取任务执行结果 V getNow()：非阻塞获取任务结果，还未产生结果时返回 null Throwable cause()：非阻塞获取失败信息，如果没有失败，返回 null Future&lt;V&gt; sync()：等待任务结束，如果任务失败，抛出异常 boolean cancel(boolean mayInterruptIfRunning)：取消任务 Future&lt;V&gt; addListener(GenericFutureListener listener)：添加回调，异步接收结果 boolean isSuccess()：判断任务是否成功 boolean isCancellable()：判断任务是否取消 1234567891011121314151617public class NettyFutureDemo &#123; public static void main(String[] args) throws Exception &#123; NioEventLoopGroup group = new NioEventLoopGroup(); EventLoop eventLoop = group.next(); Future&lt;Integer&gt; future = eventLoop.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; System.out.println(&quot;执行计算&quot;); Thread.sleep(1000); return 70; &#125; &#125;); future.getNow(); System.out.println(new Date() + &quot;等待结果&quot;); System.out.println(new Date() + &quot;&quot; + future.get()); &#125;&#125; 扩展子类Promise 类是 Future 的子类，可以脱离任务独立存在，作为两个线程间传递结果的容器 1public interface Promise&lt;V&gt; extends Future&lt;V&gt; Promise 类 API： Promise&lt;V&gt; setSuccess(V result)：设置成功结果 Promise&lt;V&gt; setFailure(Throwable cause)：设置失败结果 123456789101112131415161718192021public class NettyPromiseDemo &#123; public static void main(String[] args) throws Exception &#123; // 1. 准备 EventLoop 对象 EventLoop eventLoop = new NioEventLoopGroup().next(); // 2. 主动创建 promise DefaultPromise&lt;Integer&gt; promise = new DefaultPromise&lt;&gt;(eventLoop); // 3. 任意一个线程执行计算，计算完毕后向 promise 填充结果 new Thread(() -&gt; &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; promise.setSuccess(200); &#125;).start(); // 4. 接受结果的线程 System.out.println(new Date() + &quot;等待结果&quot;); System.out.println(new Date() + &quot;&quot; + promise.get()); &#125;&#125; PipelineChannelHandler 用来处理 Channel 上的各种事件，分为入站出站两种，所有 ChannelHandler 连接成双向链表就是 Pipeline 入站处理器通常是 ChannelInboundHandlerAdapter 的子类，主要用来读取客户端数据，写回结果 出站处理器通常是 ChannelOutboundHandlerAdapter 的子类，主要对写回结果进行加工（入站和出站是对于服务端来说的） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public static void main(String[] args) &#123; new ServerBootstrap() .group(new NioEventLoopGroup()) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) throws Exception &#123; // 1. 通过 channel 拿到 pipeline ChannelPipeline pipeline = ch.pipeline(); // 2. 添加处理器 head -&gt; h1 -&gt; h2 -&gt; h3 -&gt; h4 -&gt; tail pipeline.addLast(&quot;h1&quot;, new ChannelInboundHandlerAdapter() &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; log.debug(&quot;1&quot;); ByteBuf buf = (ByteBuf) msg; String s = buf.toString(Charset.defaultCharset()); // 将数据传递给下一个【入站】handler，如果不调用该方法则链会断开 super.channelRead(ctx, s); &#125; &#125;); pipeline.addLast(&quot;h2&quot;, new ChannelInboundHandlerAdapter() &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; log.debug(&quot;2&quot;); // 从【尾部开始向前触发】出站处理器 ch.writeAndFlush(ctx.alloc().buffer().writeBytes(&quot;server&quot;.getBytes())); // 该方法会让管道从【当前 handler 向前】寻找出站处理器 // ctx.writeAndFlush(); &#125; &#125;); pipeline.addLast(&quot;h3&quot;, new ChannelOutboundHandlerAdapter() &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; log.debug(&quot;3&quot;); super.write(ctx, msg, promise); &#125; &#125;); pipeline.addLast(&quot;h4&quot;, new ChannelOutboundHandlerAdapter() &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; log.debug(&quot;4&quot;); super.write(ctx, msg, promise); &#125; &#125;); &#125; &#125;) .bind(8080);&#125; 服务器端依次打印：1 2 4 3 ，所以入站是按照 addLast 的顺序执行的，出站是按照 addLast 的逆序执行 一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中关联着一个 ChannelHandler 入站事件和出站事件在一个双向链表中，两种类型的 handler 互不干扰： 入站事件会从链表 head 往后传递到最后一个入站的 handler 出站事件会从链表 tail 往前传递到最前一个出站的 handler ByteBuf基本介绍ByteBuf 是对字节数据的封装，优点： 池化，可以重用池中 ByteBuf 实例，更节约内存，减少内存溢出的可能 读写指针分离，不需要像 ByteBuffer 一样切换读写模式 可以自动扩容 支持链式调用，使用更流畅 零拷贝思想，例如 slice、duplicate、CompositeByteBuf 创建方法创建方式 ByteBuf buffer = ByteBufAllocator.DEFAULT.buffer(10)：创建了一个默认的 ByteBuf，初始容量是 10 123456public ByteBuf buffer() &#123; if (directByDefault) &#123; return directBuffer(); &#125; return heapBuffer();&#125; ByteBuf buffer = ByteBufAllocator.DEFAULT.heapBuffer(10)：创建池化基于堆的 ByteBuf ByteBuf buffer = ByteBufAllocator.DEFAULT.directBuffer(10)：创建池化基于直接内存的 ByteBuf 推荐的创建方式：在添加处理器的方法中 123456pipeline.addLast(new ChannelInboundHandlerAdapter() &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf buffer = ctx.alloc().buffer(); &#125;&#125;); 直接内存对比堆内存： 直接内存创建和销毁的代价昂贵，但读写性能高（少一次内存复制），适合配合池化功能一起用 直接内存对 GC 压力小，因为这部分内存不受 JVM 垃圾回收的管理，但也要注意及时主动释放 池化的意义在于可以重用 ByteBuf，高并发时池化功能更节约内存，减少内存溢出的可能，与非池化对比： 非池化，每次都要创建新的 ByteBuf 实例，这个操作对直接内存代价昂贵，堆内存会增加 GC 压力 池化，可以重用池中 ByteBuf 实例，并且采用了与 jemalloc 类似的内存分配算法提升分配效率 池化功能的开启，可以通过下面的系统环境变量来设置： 1-Dio.netty.allocator.type=&#123;unpooled|pooled&#125; # VM 参数 4.1 以后，非 Android 平台默认启用池化实现，Android 平台启用非池化实现 4.1 之前，池化功能还不成熟，默认是非池化实现 读写操作ByteBuf 由四部分组成，最开始读写指针（双指针）都在 0 位置 写入方法： 方法名 说明 备注 writeBoolean(boolean value) 写入 boolean 值 用一字节 01|00 代表 true|false writeByte(int value) 写入 byte 值 writeInt(int value) 写入 int 值 Big Endian，即 0x250，写入后 00 00 02 50 writeIntLE(int value) 写入 int 值 Little Endian，即 0x250，写入后 50 02 00 00 writeBytes(ByteBuf src) 写入 ByteBuf writeBytes(byte[] src) 写入 byte[] writeBytes(ByteBuffer src) 写入 NIO 的 ByteBuffer int writeCharSequence(CharSequence s, Charset c) 写入字符串 这些方法的未指明返回值的，其返回值都是 ByteBuf，意味着可以链式调用 写入几位写指针后移几位，指向可以写入的位置 网络传输，默认习惯是 Big Endian 扩容：写入数据时，容量不够了（初始容量是 10），这时会引发扩容 如果写入后数据大小未超过 512，则选择下一个 16 的整数倍，例如写入后大小为 12 ，则扩容后 capacity 是 16 如果写入后数据大小超过 512，则选择下一个 2^n，例如写入后大小为 513，则扩容后 capacity 是 2^10 &#x3D; 1024（2^9&#x3D;512 不够） 扩容不能超过 max capacity 会报错 读取方法： byte readByte()：读取一个字节，读指针后移 byte getByte(int index)：读取指定索引位置的字节，读指针不动 ByteBuf markReaderIndex()：标记读数据的位置 ByteBuf resetReaderIndex()：重置到标记位置，可以重复读取标记位置向后的数据 内存释放Netty 中三种内存的回收： UnpooledHeapByteBuf 使用的是 JVM 内存，只需等 GC 回收内存 UnpooledDirectByteBuf 使用的就是直接内存了，需要特殊的方法来回收内存 PooledByteBuf 和子类使用了池化机制，需要更复杂的规则来回收内存 Netty 采用了引用计数法来控制回收内存，每个 ByteBuf 都实现了 ReferenceCounted 接口，回收的规则： 每个 ByteBuf 对象的初始计数为 1 调用 release 方法计数减 1，如果计数为 0，ByteBuf 内存被回收 调用 retain 方法计数加 1，表示调用者没用完之前，其它 handler 即使调用了 release 也不会造成回收 当计数为 0 时，底层内存会被回收，这时即使 ByteBuf 对象还在，其各个方法均无法正常使用 123456ByteBuf buf = .ByteBufAllocator.DEFAULT.buffer(10)try &#123; // 逻辑处理&#125; finally &#123; buf.release();&#125; Pipeline 的存在，需要将 ByteBuf 传递给下一个 ChannelHandler，如果在 finally 中 release 了，就失去了传递性，处理规则： 创建 ByteBuf 放入 Pipeline 入站 ByteBuf 处理原则 对原始 ByteBuf 不做处理，调用 ctx.fireChannelRead(msg) 向后传递，这时无须 release，反之不传递需要 将原始 ByteBuf 转换为其它类型的 Java 对象，这时 ByteBuf 就没用了，此时必须 release 如果出现异常，ByteBuf 没有成功传递到下一个 ChannelHandler，必须 release 假设消息一直向后传，那么 TailContext 会负责释放未处理消息（原始的 ByteBuf） 123456789101112131415// io.netty.channel.DefaultChannelPipeline#onUnhandledInboundMessage(java.lang.Object)protected void onUnhandledInboundMessage(Object msg) &#123; try &#123; logger.debug(); &#125; finally &#123; ReferenceCountUtil.release(msg); &#125;&#125;// io.netty.util.ReferenceCountUtil#release(java.lang.Object)public static boolean release(Object msg) &#123; if (msg instanceof ReferenceCounted) &#123; return ((ReferenceCounted) msg).release(); &#125; return false;&#125; 出站 ByteBuf 处理原则 出站消息最终都会转为 ByteBuf 输出，一直向前传，由 HeadContext flush 后 release 不确定 ByteBuf 被引用了多少次，但又必须彻底释放，可以循环调用 release 直到返回 true 拷贝操作零拷贝方法： ByteBuf slice(int index, int length)：对原始 ByteBuf 进行切片成多个 ByteBuf，切片后的 ByteBuf 并没有发生内存复制，共用原始 ByteBuf 的内存，切片后的 ByteBuf 维护独立的 read，write 指针 12345678910public static void main(String[] args) &#123; ByteBuf buf = ByteBufAllocator.DEFAULT.buffer(10); buf.writeBytes(new byte[]&#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;&#125;); // 在切片过程中并没有发生数据复制 ByteBuf f1 = buf.slice(0, 5); f1.retain(); ByteBuf f2 = buf.slice(5, 5); f2.retain(); // 对 f1 进行相关的操作也会体现在 buf 上&#125; ByteBuf duplicate()：截取原始 ByteBuf 所有内容，并且没有 max capacity 的限制，也是与原始 ByteBuf 使用同一块底层内存，只是读写指针是独立的 CompositeByteBuf addComponents(boolean increaseWriterIndex, ByteBuf... buffers)：合并多个 ByteBuf 1234567891011public static void main(String[] args) &#123; ByteBuf buf1 = ByteBufAllocator.DEFAULT.buffer(); buf1.writeBytes(new byte[]&#123;1, 2, 3, 4, 5&#125;); ByteBuf buf2 = ByteBufAllocator.DEFAULT.buffer(); buf1.writeBytes(new byte[]&#123;6, 7, 8, 9, 10&#125;); CompositeByteBuf buf = ByteBufAllocator.DEFAULT.compositeBuffer(); // true 表示增加新的 ByteBuf 自动递增 write index, 否则 write index 会始终为 0 buf.addComponents(true, buf1, buf2);&#125; CompositeByteBuf 是一个组合的 ByteBuf，内部维护了一个 Component 数组，每个 Component 管理一个 ByteBuf，记录了这个 ByteBuf 相对于整体偏移量等信息，代表着整体中某一段的数据 优点：对外是一个虚拟视图，组合这些 ByteBuf 不会产生内存复制 缺点：复杂了很多，多次操作会带来性能的损耗 深拷贝： ByteBuf copy()：将底层内存数据进行深拷贝，因此无论读写，都与原始 ByteBuf 无关 池化相关： Unpooled 是一个工具类，提供了非池化的 ByteBuf 创建、组合、复制等操作 1234567ByteBuf buf1 = ByteBufAllocator.DEFAULT.buffer(5);buf1.writeBytes(new byte[]&#123;1, 2, 3, 4, 5&#125;);ByteBuf buf2 = ByteBufAllocator.DEFAULT.buffer(5);buf2.writeBytes(new byte[]&#123;6, 7, 8, 9, 10&#125;);// 当包装 ByteBuf 个数超过一个时, 底层使用了 CompositeByteBuf，零拷贝思想ByteBuf buf = Unpooled.wrappedBuffer(buf1, buf2); 粘包半包现象演示在 TCP 传输中，客户端发送消息时，实际上是将数据写入 TCP 的缓存，此时数据的大小和缓存的大小就会造成粘包和半包 当数据超过 TCP 缓存容量时，就会被拆分成多个包，通过 Socket 多次发送到服务端，服务端每次从缓存中取数据，产生半包问题 当数据小于 TCP 缓存容量时，缓存中可以存放多个包，客户端和服务端一次通信就可能传递多个包，这时候服务端就可能一次读取多个包，产生粘包的问题 代码演示： 客户端代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class HelloWorldClient &#123; public static void main(String[] args) &#123; send(); &#125; private static void send() &#123; NioEventLoopGroup worker = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(worker); bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123; // 【在连接 channel 建立成功后，会触发 active 方法】 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; // 发送内容随机的数据包 Random r = new Random(); char c = &#x27;0&#x27;; ByteBuf buf = ctx.alloc().buffer(); for (int i = 0; i &lt; 10; i++) &#123; byte[] bytes = new byte[10]; for (int j = 0; j &lt; r.nextInt(9) + 1; j++) &#123; bytes[j] = (byte) c; &#125; c++; buf.writeBytes(bytes); &#125; ctx.writeAndFlush(buf); &#125; &#125;); &#125; &#125;); ChannelFuture channelFuture = bootstrap.connect(&quot;127.0.0.1&quot;, 8080).sync(); channelFuture.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; log.error(&quot;client error&quot;, e); &#125; finally &#123; worker.shutdownGracefully(); &#125; &#125;&#125; 服务器代码： 123456789101112131415161718192021222324252627282930313233public class HelloWorldServer &#123; public static void main(String[] args) &#123; NioEventLoopGroup boss = new NioEventLoopGroup(1); NioEventLoopGroup worker = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.channel(NioServerSocketChannel.class); // 调整系统的接受缓冲区【滑动窗口】 //serverBootstrap.option(ChannelOption.SO_RCVBUF, 10); // 调整 netty 的接受缓冲区（ByteBuf） //serverBootstrap.childOption(ChannelOption.RCVBUF_ALLOCATOR, // new AdaptiveRecvByteBufAllocator(16, 16, 16)); serverBootstrap.group(boss, worker); serverBootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 【这里可以添加解码器】 // LoggingHandler 用来打印消息 ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); &#125; &#125;); ChannelFuture channelFuture = serverBootstrap.bind(8080); channelFuture.sync(); channelFuture.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; log.error(&quot;server error&quot;, e); &#125; finally &#123; boss.shutdownGracefully(); worker.shutdownGracefully(); log.debug(&quot;stop&quot;); &#125; &#125;&#125; 粘包效果展示： 12345678910111209:57:27.140 [nioEventLoopGroup-3-1] DEBUG io.netty.handler.logging.LoggingHandler - [id: 0xddbaaef6, L:/127.0.0.1:8080 - R:/127.0.0.1:8701] READ: 100B\t// 读了 100 字节，发生粘包 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f |+--------+-------------------------------------------------+----------------+|00000000| 30 30 30 30 30 00 00 00 00 00 31 00 00 00 00 00 |00000.....1.....||00000010| 00 00 00 00 32 32 32 32 00 00 00 00 00 00 33 00 |....2222......3.||00000020| 00 00 00 00 00 00 00 00 34 34 00 00 00 00 00 00 |........44......||00000030| 00 00 35 35 35 35 00 00 00 00 00 00 36 36 36 00 |..5555......666.||00000040| 00 00 00 00 00 00 37 37 37 37 00 00 00 00 00 00 |......7777......||00000050| 38 38 38 38 38 00 00 00 00 00 39 39 00 00 00 00 |88888.....99....||00000060| 00 00 00 00 |.... |+--------+-------------------------------------------------+----------------+ 解决方法：通过调整系统的接受缓冲区的滑动窗口和 Netty 的接受缓冲区保证每条包只含有一条数据，滑动窗口的大小仅决定了 Netty 读取的最小单位，实际每次读取的一般是它的整数倍 解决方案短连接发一个包建立一次连接，这样连接建立到连接断开之间就是消息的边界，缺点就是效率很低 客户端代码改造： 12345678public class HelloWorldClient &#123; public static void main(String[] args) &#123; // 分 10 次发送 for (int i = 0; i &lt; 10; i++) &#123; send(); &#125; &#125;&#125; 固定长度服务器端加入定长解码器，每一条消息采用固定长度。如果是半包消息，会缓存半包消息并等待下个包到达之后进行拼包合并，直到读取一个完整的消息包；如果是粘包消息，空余的位置会进行补 0，会浪费空间 12345678serverBootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new FixedLengthFrameDecoder(10)); // LoggingHandler 用来打印消息 ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); &#125;&#125;); 12345678910111210:29:06.522 [nioEventLoopGroup-3-1] DEBUG io.netty.handler.logging.LoggingHandler - [id: 0x38a70fbf, L:/127.0.0.1:8080 - R:/127.0.0.1:10144] READ: 10B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f |+--------+-------------------------------------------------+----------------+|00000000| 31 31 00 00 00 00 00 00 00 00 |11........ |+--------+-------------------------------------------------+----------------+10:29:06.522 [nioEventLoopGroup-3-1] DEBUG io.netty.handler.logging.LoggingHandler - [id: 0x38a70fbf, L:/127.0.0.1:8080 - R:/127.0.0.1:10144] READ: 10B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f |+--------+-------------------------------------------------+----------------+|00000000| 32 32 32 32 32 32 00 00 00 00 |222222.... |+--------+-------------------------------------------------+----------------+ 分隔符服务端加入行解码器，默认以 或 \\r 作为分隔符，如果超出指定长度仍未出现分隔符，则抛出异常： 1234567serverBootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new FixedLengthFrameDecoder(8)); ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); &#125;&#125;); 客户端在每条消息之后，加入 分隔符： 1234567891011121314public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Random r = new Random(); char c = &#x27;a&#x27;; ByteBuf buffer = ctx.alloc().buffer(); for (int i = 0; i &lt; 10; i++) &#123; for (int j = 1; j &lt;= r.nextInt(16)+1; j++) &#123; buffer.writeByte((byte) c); &#125; // 10 代表 &#x27; &#x27; buffer.writeByte(10); c++; &#125; ctx.writeAndFlush(buffer);&#125; 预设长度LengthFieldBasedFrameDecoder 解码器自定义长度解决 TCP 粘包黏包问题 12345int maxFrameLength // 数据最大长度int lengthFieldOffset // 长度字段偏移量，从第几个字节开始是内容的长度字段int lengthFieldLength\t// 长度字段本身的长度int lengthAdjustment // 长度字段为基准，几个字节后才是内容int initialBytesToStrip\t// 从头开始剥离几个字节解码后显示 12345678910lengthFieldOffset = 1 (= the length of HDR1)lengthFieldLength = 2lengthAdjustment = 1 (= the length of HDR2)initialBytesToStrip = 3 (= the length of HDR1 + LEN)BEFORE DECODE (16 bytes) AFTER DECODE (13 bytes)//解码+------+--------+------+----------------+ +------+----------------+| HDR1 | Length | HDR2 | Actual Content |-----&gt;| HDR2 | Actual Content || 0xCA | 0x000C | 0xFE | &quot;HELLO, WORLD&quot; | | 0xFE | &quot;HELLO, WORLD&quot; |+------+--------+------+----------------+ +------+----------------+ 代码实现： 123456789101112131415161718192021222324public class LengthFieldDecoderDemo &#123; public static void main(String[] args) &#123; EmbeddedChannel channel = new EmbeddedChannel( // int 占 4 字节，版本号一个字节 new LengthFieldBasedFrameDecoder(1024, 0, 4, 1,5), new LoggingHandler(LogLevel.DEBUG) ); // 4 个字节的内容长度， 实际内容 ByteBuf buffer = ByteBufAllocator.DEFAULT.buffer(); send(buffer, &quot;Hello, world&quot;); send(buffer, &quot;Hi!&quot;); // 写出缓存 channel.writeInbound(buffer); &#125; // 写入缓存 private static void send(ByteBuf buffer, String content) &#123; byte[] bytes = content.getBytes(); // 实际内容 int length = bytes.length; // 实际内容长度 buffer.writeInt(length); buffer.writeByte(1); // 表示版本号 buffer.writeBytes(bytes); &#125;&#125; 12345678910111210:49:59.344 [main] DEBUG io.netty.handler.logging.LoggingHandler - [id: 0xembedded, L:embedded - R:embedded] READ: 12B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f |+--------+-------------------------------------------------+----------------+|00000000| 48 65 6c 6c 6f 2c 20 77 6f 72 6c 64 |Hello, world |+--------+-------------------------------------------------+----------------+10:49:59.344 [main] DEBUG io.netty.handler.logging.LoggingHandler - [id: 0xembedded, L:embedded - R:embedded] READ: 3B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f |+--------+-------------------------------------------------+----------------+|00000000| 48 69 21 |Hi! |+--------+-------------------------------------------------+----------------+ 协议设计HTTP访问 URL：http://localhost:8080/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class HttpDemo &#123; public static void main(String[] args) &#123; NioEventLoopGroup boss = new NioEventLoopGroup(); NioEventLoopGroup worker = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.channel(NioServerSocketChannel.class); serverBootstrap.group(boss, worker); serverBootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); ch.pipeline().addLast(new HttpServerCodec()); // 只针对某一种类型的请求处理，此处针对 HttpRequest ch.pipeline().addLast(new SimpleChannelInboundHandler&lt;HttpRequest&gt;() &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, HttpRequest msg) &#123; // 获取请求 log.debug(msg.uri()); // 返回响应 DefaultFullHttpResponse response = new DefaultFullHttpResponse( msg.protocolVersion(), HttpResponseStatus.OK); byte[] bytes = &quot;&lt;h1&gt;Hello, world!&lt;/h1&gt;&quot;.getBytes(); response.headers().setInt(CONTENT_LENGTH, bytes.length); response.content().writeBytes(bytes); // 写回响应 ctx.writeAndFlush(response); &#125; &#125;); &#125; &#125;); ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; log.error(&quot;n3.server error&quot;, e); &#125; finally &#123; boss.shutdownGracefully(); worker.shutdownGracefully(); &#125; &#125;&#125; 自定义处理器代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Slf4jpublic class MessageCodec extends ByteToMessageCodec&lt;Message&gt; &#123; // 编码 @Override public void encode(ChannelHandlerContext ctx, Message msg, ByteBuf out) throws Exception &#123; // 4 字节的魔数 out.writeBytes(new byte[]&#123;1, 2, 3, 4&#125;); // 1 字节的版本, out.writeByte(1); // 1 字节的序列化方式 jdk 0 , json 1 out.writeByte(0); // 1 字节的指令类型 out.writeByte(msg.getMessageType()); // 4 个字节 out.writeInt(msg.getSequenceId()); // 无意义，对齐填充, 1 字节 out.writeByte(0xff); // 获取内容的字节数组，msg 对象序列化 ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(msg); byte[] bytes = bos.toByteArray(); // 长度 out.writeInt(bytes.length); // 写入内容 out.writeBytes(bytes); &#125; // 解码 @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; int magicNum = in.readInt(); byte version = in.readByte(); byte serializerType = in.readByte(); byte messageType = in.readByte(); int sequenceId = in.readInt(); in.readByte(); int length = in.readInt(); byte[] bytes = new byte[length]; in.readBytes(bytes, 0, length); ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(bytes)); Message message = (Message) ois.readObject(); log.debug(&quot;&#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;&quot;, magicNum, version, serializerType, messageType, sequenceId, length); log.debug(&quot;&#123;&#125;&quot;, message); out.add(message); &#125;&#125; 测试代码： 1234567891011121314151617public static void main(String[] args) throws Exception &#123; EmbeddedChannel channel = new EmbeddedChannel(new LoggingHandler(), new MessageCodec()); // encode LoginRequestMessage message = new LoginRequestMessage(&quot;zhangsan&quot;, &quot;123&quot;); channel.writeOutbound(message); // decode ByteBuf buf = ByteBufAllocator.DEFAULT.buffer(); new MessageCodec().encode(null, message, buf); // 入站 channel.writeInbound(buf);&#125;public class LoginRequestMessage extends Message &#123; private String username; private String password; // set + get &#125; Sharable@Sharable 注解的添加时机： 当 handler 不保存状态时，就可以安全地在多线程下被共享 对于编解码器类不能继承 ByteToMessageCodec 或 CombinedChannelDuplexHandler，它们的构造方法对 @Sharable 有限制 12345protected ByteToMessageCodec(boolean preferDirect) &#123; ensureNotSharable(); outboundMsgMatcher = TypeParameterMatcher.find(this, ByteToMessageCodec.class, &quot;I&quot;); encoder = new Encoder(preferDirect);&#125; 123456protected void ensureNotSharable() &#123; // 如果类上有该注解 if (isSharable()) &#123; throw new IllegalStateException(); &#125;&#125; 如果能确保编解码器不会保存状态，可以继承 MessageToMessageCodec 父类 123456789101112131415161718@Slf4j@ChannelHandler.Sharable// 必须和 LengthFieldBasedFrameDecoder 一起使用，确保接到的 ByteBuf 消息是完整的public class MessageCodecSharable extends MessageToMessageCodec&lt;ByteBuf, Message&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, Message msg, List&lt;Object&gt; outList) throws Exception &#123; ByteBuf out = ctx.alloc().buffer(); // 4 字节的魔数 out.writeBytes(new byte[]&#123;1, 2, 3, 4&#125;); // .... outList.add(out); &#125; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; //.... &#125;&#125; 场景优化空闲检测连接假死连接假死就是客户端数据发不出去，服务端也一直收不到数据，保持这种状态，假死的连接占用的资源不能自动释放，而且向假死连接发送数据，得到的反馈是发送超时 解决方案：每隔一段时间就检查这段时间内是否接收到客户端数据，没有就可以判定为连接假死 IdleStateHandler 是 Netty 提供的处理空闲状态的处理器，用来判断是不是读空闲时间或写空闲时间过长 参数一 long readerIdleTime：读空闲，表示多长时间没有读 参数二 long writerIdleTime：写空闲，表示多长时间没有写 参数三 long allIdleTime：读写空闲，表示多长时间没有读写 12345678910111213141516171819202122serverBootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;\t@Override\tprotected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new LengthFieldBasedFrameDecoder(1024, 12, 4, 0, 0)); ch.pipeline().addLast(new MessageCodec()); // 5s 内如果没有收到 channel 的数据，会触发一个 IdleState#READER_IDLE 事件， ch.pipeline().addLast(new IdleStateHandler(5, 0, 0)); // ChannelDuplexHandler 【可以同时作为入站和出站】处理器 ch.pipeline().addLast(new ChannelDuplexHandler() &#123; // 用来触发特殊事件 @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception&#123; IdleStateEvent event = (IdleStateEvent) evt; // 触发了读空闲事件 if (event.state() == IdleState.READER_IDLE) &#123; log.debug(&quot;已经 5s 没有读到数据了&quot;); ctx.channel().close(); &#125; &#125; &#125;); &#125;&#125; 心跳机制客户端定时向服务器端发送数据，时间间隔要小于服务器定义的空闲检测的时间间隔，就能防止误判连接假死，这就是心跳机制 12345678910111213141516171819202122bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new LengthFieldBasedFrameDecoder(1024, 12, 4, 0, 0)); ch.pipeline().addLast(new MessageCodec()); // 3s 内如果没有向服务器写数据，会触发一个 IdleState#WRITER_IDLE 事件 ch.pipeline().addLast(new IdleStateHandler(0, 3, 0)); // ChannelDuplexHandler 可以同时作为入站和出站处理器 ch.pipeline().addLast(new ChannelDuplexHandler() &#123; // 用来触发特殊事件 @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; IdleStateEvent event = (IdleStateEvent) evt; // 触发了写空闲事件 if (event.state() == IdleState.WRITER_IDLE) &#123; // 3s 没有写数据了，【发送一个心跳包】 ctx.writeAndFlush(new PingMessage()); &#125; &#125; &#125;); &#125;&#125; 序列化普通方式序列化，反序列化主要用在消息正文的转换上 序列化时，需要将 Java 对象变为要传输的数据（可以是 byte[]，或 json 等，最终都需要变成 byte[]） 反序列化时，需要将传入的正文数据还原成 Java 对象，便于处理 代码实现： 抽象一个 Serializer 接口 123456public interface Serializer &#123; // 反序列化方法 &lt;T&gt; T deserialize(Class&lt;T&gt; clazz, byte[] bytes); // 序列化方法 &lt;T&gt; byte[] serialize(T object);&#125; 提供两个实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748enum SerializerAlgorithm implements Serializer &#123;\t// Java 实现 Java &#123; @Override public &lt;T&gt; T deserialize(Class&lt;T&gt; clazz, byte[] bytes) &#123; try &#123; ObjectInputStream in = new ObjectInputStream(new ByteArrayInputStream(bytes)); Object object = in.readObject(); return (T) object; &#125; catch (IOException | ClassNotFoundException e) &#123; throw new RuntimeException(&quot;SerializerAlgorithm.Java 反序列化错误&quot;, e); &#125; &#125; @Override public &lt;T&gt; byte[] serialize(T object) &#123; try &#123; ByteArrayOutputStream out = new ByteArrayOutputStream(); new ObjectOutputStream(out).writeObject(object); return out.toByteArray(); &#125; catch (IOException e) &#123; throw new RuntimeException(&quot;SerializerAlgorithm.Java 序列化错误&quot;, e); &#125; &#125; &#125;, // Json 实现(引入了 Gson 依赖) Json &#123; @Override public &lt;T&gt; T deserialize(Class&lt;T&gt; clazz, byte[] bytes) &#123; return new Gson().fromJson(new String(bytes, StandardCharsets.UTF_8), clazz); &#125; @Override public &lt;T&gt; byte[] serialize(T object) &#123; return new Gson().toJson(object).getBytes(StandardCharsets.UTF_8); &#125; &#125;; // 需要从协议的字节中得到是哪种序列化算法 public static SerializerAlgorithm getByInt(int type) &#123; SerializerAlgorithm[] array = SerializerAlgorithm.values(); if (type &lt; 0 || type &gt; array.length - 1) &#123; throw new IllegalArgumentException(&quot;超过 SerializerAlgorithm 范围&quot;); &#125; return array[type]; &#125;&#125; ProtoBuf基本介绍Codec（编解码器）的组成部分有两个：Decoder（解码器）和 Encoder（编码器）。Encoder 负责把业务数据转换成字节码数据，Decoder 负责把字节码数据转换成业务数据 Protobuf 是 Google 发布的开源项目，全称 Google Protocol Buffers ，是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。很适合做数据存储或 RPC（远程过程调用 remote procedure call）数据交换格式。目前很多公司从 HTTP + Json 转向 TCP + Protobuf ，效率会更高 Protobuf 是以 message 的方式来管理数据，支持跨平台、跨语言（客户端和服务器端可以是不同的语言编写的），高性能、高可靠性 工作过程：使用 Protobuf 编译器自动生成代码，Protobuf 是将类的定义使用 .proto 文件进行描述，然后通过 protoc.exe 编译器根据 .proto 自动生成 .java 文件 代码实现 单个 message： 1234567syntax = &quot;proto3&quot;; // 版本option java_outer_classname = &quot;StudentPOJO&quot;;\t// 生成的外部类名，同时也是文件名message Student &#123; // 在 StudentPOJO 外部类种生成一个内部类 Student，是真正发送的 POJO 对象 int32 id = 1; // Student 类中有一个属性：名字为 id 类型为 int32(protobuf类型) ，1表示属性序号，不是值 string name = 2;&#125; 编译 protoc.exe --java_out=.Student.proto（cmd 窗口输入） 将生成的 StudentPOJO 放入到项目使用 Server 端： 123456789101112new ServerBootstrap() //... .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;\t// 创建一个通道初始化对象 // 给pipeline 设置处理器 @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 在pipeline加入ProtoBufDecoder，指定对哪种对象进行解码 ch.pipeline().addLast(&quot;decoder&quot;, new ProtobufDecoder( StudentPOJO.Student.getDefaultInstance())); ch.pipeline().addLast(new NettyServerHandler()); &#125; &#125;); &#125; Client 端： 12345678910new Bootstrap().group(group) // 设置线程组 .channel(NioSocketChannel.class) // 设置客户端通道的实现类(反射) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 在pipeline中加入 ProtoBufEncoder ch.pipeline().addLast(&quot;encoder&quot;, new ProtobufEncoder()); ch.pipeline().addLast(new NettyClientHandler()); // 加入自定义的业务处理器 &#125; &#125;); 多个 message：Protobuf 可以使用 message 管理其他的 message。假设某个项目需要传输 20 个对象，可以在一个文件里定义 20 个 message，最后再用一个总的 message 来决定在实际传输时真正需要传输哪一个对象 12345678910111213141516171819202122232425262728293031323334syntax = &quot;proto3&quot;;option optimize_for = SPEED; // 加快解析option java_package=&quot;com.atguigu.netty.codec2&quot;;\t// 指定生成到哪个包下option java_outer_classname=&quot;MyDataInfo&quot;; // 外部类名, 文件名message MyMessage &#123; // 定义一个枚举类型，DataType 如果是 0 则表示一个 Student 对象实例，DataType 这个名称自定义 enum DataType &#123; StudentType = 0; //在 proto3 要求 enum 的编号从 0 开始 WorkerType = 1; &#125; // 用 data_type 来标识传的是哪一个枚举类型，这里才真正开始定义 Message 的数据类型 DataType data_type = 1; // 所有后面的数字都只是编号而已 // oneof 关键字，表示每次枚举类型进行传输时，限制最多只能传输一个对象。 // dataBody名称也是自定义的 // MyMessage 里出现的类型只有两个 DataType 类型，Student 或者 Worker 类型，在真正传输的时候只会有一个出现 oneof dataBody &#123; Student student = 2; //注意这后面的数字也都只是编号而已，上面DataType data_type = 1 占了第一个序号了 Worker worker = 3; &#125;&#125;message Student &#123; int32 id = 1; // Student类的属性 string name = 2; //&#125;message Worker &#123; string name=1; int32 age=2;&#125; 编译： Server 端： 1ch.pipeline().addLast(&quot;decoder&quot;, new ProtobufDecoder(MyDataInfo.MyMessage.getDefaultInstance())); Client 端： 1pipeline.addLast(&quot;encoder&quot;, new ProtobufEncoder()); 长连接HTTP 协议是无状态的，浏览器和服务器间的请求响应一次，下一次会重新创建连接。实现基于 WebSocket 的长连接的全双工的交互，改变 HTTP 协议多次请求的约束 开发需求： 实现长连接，服务器与浏览器相互通信客户端 浏览器和服务器端会相互感知，比如服务器关闭了，浏览器会感知，同样浏览器关闭了，服务器会感知 代码实现： WebSocket： WebSocket 的数据是以帧（frame）形式传递，WebSocketFrame 下面有六个子类，代表不同的帧格式 浏览器请求 URL：ws:&#x2F;&#x2F;localhost:8080&#x2F;xxx 12345678910111213141516171819202122232425262728293031323334353637383940414243public class MyWebSocket &#123; public static void main(String[] args) throws Exception &#123; // 创建两个线程组 EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup); serverBootstrap.channel(NioServerSocketChannel.class); serverBootstrap.handler(new LoggingHandler(LogLevel.INFO)); serverBootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); // 基于 http 协议，使用 http 的编码和解码器 pipeline.addLast(new HttpServerCodec()); // 是以块方式写，添加 ChunkedWriteHandler 处理器 pipeline.addLast(new ChunkedWriteHandler()); // http 数据在传输过程中是分段, HttpObjectAggregator 就是可以将多个段聚合 // 这就就是为什么，当浏览器发送大量数据时，就会发出多次 http 请求 pipeline.addLast(new HttpObjectAggregator(8192)); // WebSocketServerProtocolHandler 核心功能是【将 http 协议升级为 ws 协议】，保持长连接 pipeline.addLast(new WebSocketServerProtocolHandler(&quot;/hello&quot;)); // 自定义的handler ，处理业务逻辑 pipeline.addLast(new MyTextWebSocketFrameHandler()); &#125; &#125;); // 启动服务器 ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 处理器： 12345678910111213141516171819202122232425262728public class MyTextWebSocketFrameHandler extends SimpleChannelInboundHandler&lt;TextWebSocketFrame&gt; &#123; // TextWebSocketFrame 类型，表示一个文本帧(frame) @Override protected void channelRead0(ChannelHandlerContext ctx, TextWebSocketFrame msg) throws Exception &#123; System.out.println(&quot;服务器收到消息 &quot; + msg.text()); // 回复消息 ctx.writeAndFlush(new TextWebSocketFrame(&quot;服务器时间&quot; + LocalDateTime.now() + &quot; &quot; + msg.text())); &#125; // 当web客户端连接后， 触发方法 @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; // id 表示唯一的值，LongText 是唯一的 ShortText 不是唯一 System.out.println(&quot;handlerAdded 被调用&quot; + ctx.channel().id().asLongText()); System.out.println(&quot;handlerAdded 被调用&quot; + ctx.channel().id().asShortText()); &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(&quot;handlerRemoved 被调用&quot; + ctx.channel().id().asLongText()); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println(&quot;异常发生 &quot; + cause.getMessage()); ctx.close(); // 关闭连接 &#125;&#125; HTML： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;script&gt; var socket; // 判断当前浏览器是否支持websocket if(window.WebSocket) &#123; //go on socket = new WebSocket(&quot;ws://localhost:8080/hello&quot;); //相当于channelReado, ev 收到服务器端回送的消息 socket.onmessage = function (ev) &#123; var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot; &quot; + ev.data; &#125; //相当于连接开启(感知到连接开启) socket.onopen = function (ev) &#123; var rt = document.getElementById(&quot;responseText&quot;); rt.value = &quot;连接开启了..&quot; &#125; //相当于连接关闭(感知到连接关闭) socket.onclose = function (ev) &#123; var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot; &quot; + &quot;连接关闭了..&quot; &#125; &#125; else &#123; alert(&quot;当前浏览器不支持websocket&quot;) &#125; // 发送消息到服务器 function send(message) &#123; // 先判断socket是否创建好 if(!window.socket) &#123; return; &#125; if(socket.readyState == WebSocket.OPEN) &#123; // 通过socket 发送消息 socket.send(message) &#125; else &#123; alert(&quot;连接没有开启&quot;); &#125; &#125;&lt;/script&gt; &lt;form onsubmit=&quot;return false&quot;&gt; &lt;textarea name=&quot;message&quot; style=&quot;height: 300px; width: 300px&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;button&quot; value=&quot;发生消息&quot; onclick=&quot;send(this.form.message.value)&quot;&gt; &lt;textarea id=&quot;responseText&quot; style=&quot;height: 300px; width: 300px&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;button&quot; value=&quot;清空内容&quot; onclick=&quot;document.getElementById(&#x27;responseText&#x27;).value=&#x27;&#x27;&quot;&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 参数调优CONNECT参数配置方式： 客户端通过 .option() 方法配置参数，给 SocketChannel 配置参数 服务器端： new ServerBootstrap().option()： 给 ServerSocketChannel 配置参数 new ServerBootstrap().childOption()：给 SocketChannel 配置参数 CONNECT_TIMEOUT_MILLIS 参数： 属于 SocketChannal 参数 在客户端建立连接时，如果在指定毫秒内无法连接，会抛出 timeout 异常 SO_TIMEOUT 主要用在阻塞 IO，阻塞 IO 中 accept，read 等都是无限等待的，如果不希望永远阻塞，可以调整超时时间 12345678910111213141516171819public class ConnectionTimeoutTest &#123; public static void main(String[] args) &#123; NioEventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap() .group(group) .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 5000) .channel(NioSocketChannel.class) .handler(new LoggingHandler()); ChannelFuture future = bootstrap.connect(&quot;127.0.0.1&quot;, 8080); future.sync().channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); log.debug(&quot;timeout&quot;); &#125; finally &#123; group.shutdownGracefully(); &#125; &#125;&#125; SO_BACKLOG属于 ServerSocketChannal 参数，通过 option(ChannelOption.SO_BACKLOG, value) 来设置大小 在 Linux 2.2 之前，backlog 大小包括了两个队列的大小，在 2.2 之后，分别用下面两个参数来控制 sync queue：半连接队列，大小通过 /proc/sys/net/ipv4/tcp_max_syn_backlog 指定，在 syncookies 启用的情况下，逻辑上没有最大值限制 accept queue：全连接队列，大小通过 /proc/sys/net/core/somaxconn 指定，在使用 listen 函数时，内核会根据传入的 backlog 参数与系统参数，取二者的较小值。如果 accpet queue 队列满了，server 将发送一个拒绝连接的错误信息到 client 其他参数ALLOCATOR：属于 SocketChannal 参数，用来分配 ByteBuf， ctx.alloc() RCVBUF_ALLOCATOR：属于 SocketChannal 参数 控制 Netty 接收缓冲区大小 负责入站数据的分配，决定入站缓冲区的大小（并可动态调整），统一采用 direct 直接内存，具体池化还是非池化由 allocator 决定 RocketMQ基本介绍消息队列应用场景消息队列是一种先进先出的数据结构，常见的应用场景： 应用解耦：系统的耦合性越高，容错性就越低 实例：用户创建订单后，耦合调用库存系统、物流系统、支付系统，任何一个子系统出了故障都会造成下单异常，影响用户使用体验。使用消息队列解耦合，比如物流系统发生故障，需要几分钟恢复，将物流系统要处理的数据缓存到消息队列中，用户的下单操作正常完成。等待物流系统正常后处理存在消息队列中的订单消息即可，终端系统感知不到物流系统发生过几分钟故障 流量削峰：应用系统如果遇到系统请求流量的瞬间猛增，有可能会将系统压垮，使用消息队列可以将大量请求缓存起来，分散到很长一段时间处理，这样可以提高系统的稳定性和用户体验 数据分发：让数据在多个系统更加之间进行流通，数据的产生方不需要关心谁来使用数据，只需要将数据发送到消息队列，数据使用方直接在消息队列中直接获取数据 参考视频：https://www.bilibili.com/video/BV1L4411y7mn 技术选型RocketMQ 对比 Kafka 的优点 支持 Pull和 Push 两种消息模式 支持延时消息、死信队列、消息重试、消息回溯、消息跟踪、事务消息等高级特性 对消息可靠性做了改进，保证消息不丢失并且至少消费一次，与 Kafka 一样是先写 PageCache 再落盘，并且数据有多副本 RocketMQ 存储模型是所有的 Topic 都写到同一个 Commitlog 里，是一个 append only 操作，在海量 Topic 下也能将磁盘的性能发挥到极致，并且保持稳定的写入时延。Kafka 的吞吐非常高（零拷贝、操作系统页缓存、磁盘顺序写），但是在多 Topic 下时延不够稳定（顺序写入特性会被破坏从而引入大量的随机 I&#x2F;O），不适合实时在线业务场景 经过阿里巴巴多年双 11 验证过、可以支持亿级并发的开源消息队列 Kafka 比 RocketMQ 吞吐量高： Kafka 将 Producer 端将多个小消息合并，采用异步批量发送的机制，当发送一条消息时，消息并没有发送到 Broker 而是缓存起来，直接向业务返回成功，当缓存的消息达到一定数量时再批量发送 减少了网络 I&#x2F;O，提高了消息发送的性能，但是如果消息发送者宕机，会导致消息丢失，降低了可靠性 RocketMQ 缓存过多消息会导致频繁 GC，并且为了保证可靠性没有采用这种方式 Topic 的 partition 数量过多时，Kafka 的性能不如 RocketMQ： 两者都使用文件存储，但是 Kafka 是一个分区一个文件，Topic 过多时分区的总量也会增加，过多的文件导致对消息刷盘时出现文件竞争磁盘，造成性能的下降。一个分区只能被一个消费组中的一个消费线程进行消费，因此可以同时消费的消费端也比较少 RocketMQ 所有队列都存储在一个文件中，每个队列存储的消息量也比较小，因此多 Topic 的对 RocketMQ 的性能的影响较小 安装测试安装需要 Java 环境，下载解压后进入安装目录，进行启动： 启动 NameServer 1234# 1.启动 NameServernohup sh bin/mqnamesrv &amp;# 2.查看启动日志tail -f ~/logs/rocketmqlogs/namesrv.log RocketMQ 默认的虚拟机内存较大，需要编辑如下两个配置文件，修改 JVM 内存大小 123# 编辑runbroker.sh和runserver.sh修改默认JVM大小vi runbroker.shvi runserver.sh 参考配置：JAVA_OPT&#x3D;”${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize&#x3D;128m -XX:MaxMetaspaceSize&#x3D;320m” 启动 Broker 1234# 1.启动 Brokernohup sh bin/mqbroker -n localhost:9876 autoCreateTopicEnable=true &amp;# 2.查看启动日志tail -f ~/logs/rocketmqlogs/broker.log 发送消息： 1234# 1.设置环境变量export NAMESRV_ADDR=localhost:9876# 2.使用安装包的 Demo 发送消息sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer 接受消息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576 # 1.设置环境变量 export NAMESRV_ADDR=localhost:9876 # 2.接收消息 sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer* 关闭 RocketMQ： ```sh # 1.关闭 NameServer sh bin/mqshutdown namesrv # 2.关闭 Broker sh bin/mqshutdown broker***### 相关概念RocketMQ 主要由 Producer、Broker、Consumer 三部分组成，其中 Producer 负责生产消息，Consumer 负责消费消息，Broker 负责存储消息，NameServer 负责管理 Broker* 代理服务器（Broker Server）：消息中转角色，负责**存储消息、转发消息**。在 RocketMQ 系统中负责接收从生产者发送来的消息并存储、同时为消费者的拉取请求作准备，也存储消息相关的元数据，包括消费者组、消费进度偏移和主题和队列消息等* 名字服务（Name Server）：充当**路由消息**的提供者。生产者或消费者能够通过名字服务查找各主题相应的 Broker IP 列表* 消息生产者（Producer）：负责**生产消息**，把业务应用系统里产生的消息发送到 Broker 服务器。RocketMQ 提供多种发送方式，同步发送、异步发送、顺序发送、单向发送，同步和异步方式均需要 Broker 返回确认信息，单向发送不需要；可以通过 MQ 的负载均衡模块选择相应的 Broker 集群队列进行消息投递，投递的过程支持快速失败并且低延迟* 消息消费者（Consumer）：负责**消费消息**，一般是后台系统负责异步消费，一个消息消费者会从 Broker 服务器拉取消息、并将其提供给应用程序。从用户应用的角度而提供了两种消费形式： * 拉取式消费（Pull Consumer）：应用通主动调用 Consumer 的拉消息方法从 Broker 服务器拉消息，主动权由应用控制，一旦获取了批量消息，应用就会启动消费过程 * 推动式消费（Push Consumer）：该模式下 Broker 收到数据后会主动推送给消费端，实时性较高* 生产者组（Producer Group）：同一类 Producer 的集合，发送同一类消息且发送逻辑一致。如果发送的是事务消息且原始生产者在发送之后崩溃，**则 Broker 服务器会联系同一生产者组的其他生产者实例以提交或回溯消费*** 消费者组（Consumer Group）：同一类 Consumer 的集合，消费者实例必须订阅完全相同的 Topic，消费同一类消息且消费逻辑一致。消费者组使得在消息消费方面更容易的实现负载均衡和容错。RocketMQ 支持两种消息模式： * 集群消费（Clustering）：相同 Consumer Group 的每个 Consumer 实例平均分摊消息 * 广播消费（Broadcasting）：相同 Consumer Group 的每个 Consumer 实例都接收全量的消息每个 Broker 可以存储多个 Topic 的消息，每个 Topic 的消息也可以分片存储于不同的 Broker，Message Queue（消息队列）是用于存储消息的物理地址，每个 Topic 中的消息地址存储于多个 Message Queue 中* 主题（Topic）：表示一类消息的集合，每个主题包含若干条消息，每条消息只属于一个主题，是 RocketMQ 消息订阅的基本单位* 消息（Message）：消息系统所传输信息的物理载体，生产和消费数据的最小单位，每条消息必须属于一个主题。RocketMQ 中每个消息拥有唯一的 Message ID，且可以携带具有业务标识的 Key，系统提供了通过 Message ID 和 Key 查询消息的功能* 标签（Tag）：为消息设置的标志，用于同一主题下区分不同类型的消息。标签能够有效地保持代码的清晰度和连贯性，并优化 RocketMQ 提供的查询系统，消费者可以根据 Tag 实现对不同子主题的不同消费逻辑，实现更好的扩展性* 普通顺序消息（Normal Ordered Message）：消费者通过同一个消息队列（Topic 分区）收到的消息是有顺序的，不同消息队列收到的消息则可能是无顺序的* 严格顺序消息（Strictly Ordered Message）：消费者收到的所有消息均是有顺序的官方文档：https://github.com/apache/rocketmq/tree/master/docs/cn（基础知识部分的笔记参考官方文档编写）****## 消息操作### 基本样例#### 订阅发布消息的发布是指某个生产者向某个 Topic 发送消息，消息的订阅是指某个消费者关注了某个 Topic 中带有某些 Tag 的消息，进而从该 Topic 消费数据导入 MQ 客户端依赖：```xml&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.4.0&lt;/version&gt;&lt;/dependency&gt; 消息发送者步骤分析： 创建消息生产者 Producer，并制定生产者组名 指定 Nameserver 地址 启动 Producer 创建消息对象，指定主题 Topic、Tag 和消息体 发送消息 关闭生产者 Producer 消息消费者步骤分析： 创建消费者 Consumer，制定消费者组名 指定 Nameserver 地址 订阅主题 Topic 和 Tag 设置回调函数，处理消息 启动消费者 Consumer 官方文档：https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md 发送消息同步发送使用 RocketMQ 发送三种类型的消息：同步消息、异步消息和单向消息，其中前两种消息是可靠的，因为会有发送是否成功的应答 这种可靠性同步地发送方式使用的比较广泛，比如：重要的消息通知，短信通知 123456789101112131415161718192021222324public class SyncProducer &#123;\tpublic static void main(String[] args) throws Exception &#123; // 实例化消息生产者Producer DefaultMQProducer producer = new DefaultMQProducer(&quot;please_rename_unique_group_name&quot;); // 设置NameServer的地址 producer.setNamesrvAddr(&quot;localhost:9876&quot;); // 启动Producer实例 producer.start(); for (int i = 0; i &lt; 100; i++) &#123; // 创建消息，并指定Topic，Tag和消息体 Message msg = new Message( &quot;TopicTest&quot; /* Topic */, &quot;TagA&quot; /* Tag */, (&quot;Hello RocketMQ &quot; + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */); // 发送消息到一个Broker SendResult sendResult = producer.send(msg); // 通过sendResult返回消息是否成功送达 System.out.printf(&quot;%s%n&quot;, sendResult); &#125; // 如果不再发送消息，关闭Producer实例。 producer.shutdown(); &#125;&#125; 异步发送异步消息通常用在对响应时间敏感的业务场景，即发送端不能容忍长时间地等待 Broker 的响应 123456789101112131415161718192021222324252627282930313233343536373839404142public class AsyncProducer &#123;\tpublic static void main(String[] args) throws Exception &#123; // 实例化消息生产者Producer DefaultMQProducer producer = new DefaultMQProducer(&quot;please_rename_unique_group_name&quot;); // 设置NameServer的地址 producer.setNamesrvAddr(&quot;localhost:9876&quot;); // 启动Producer实例 producer.start(); producer.setRetryTimesWhenSendAsyncFailed(0); int messageCount = 100; // 根据消息数量实例化倒计时计算器 final CountDownLatch2 countDownLatch = new CountDownLatch2(messageCount); for (int i = 0; i &lt; messageCount; i++) &#123; final int index = i; // 创建消息，并指定Topic，Tag和消息体 Message msg = new Message(&quot;TopicTest&quot;, &quot;TagA&quot;, &quot;OrderID188&quot;, &quot;Hello world&quot;.getBytes(RemotingHelper.DEFAULT_CHARSET)); // SendCallback接收异步返回结果的回调 producer.send(msg, new SendCallback() &#123; // 发送成功回调函数 @Override public void onSuccess(SendResult sendResult) &#123; countDownLatch.countDown(); System.out.printf(&quot;%-10d OK %s %n&quot;, index, sendResult.getMsgId()); &#125; @Override public void onException(Throwable e) &#123; countDownLatch.countDown(); System.out.printf(&quot;%-10d Exception %s %n&quot;, index, e); e.printStackTrace(); &#125; &#125;); &#125; // 等待5s countDownLatch.await(5, TimeUnit.SECONDS); // 如果不再发送消息，关闭Producer实例。 producer.shutdown(); &#125;&#125; 单向发送单向发送主要用在不特别关心发送结果的场景，例如日志发送 12345678910111213141516171819public class OnewayProducer &#123;\tpublic static void main(String[] args) throws Exception&#123; // 实例化消息生产者Producer DefaultMQProducer producer = new DefaultMQProducer(&quot;please_rename_unique_group_name&quot;); // 设置NameServer的地址 producer.setNamesrvAddr(&quot;localhost:9876&quot;); // 启动Producer实例 producer.start(); for (int i = 0; i &lt; 100; i++) &#123; // 创建消息，并指定Topic，Tag和消息体 Message msg = new Message(&quot;TopicTest&quot;,&quot;TagA&quot;, (&quot;Hello RocketMQ &quot; + i).getBytes(RemotingHelper.DEFAULT_CHARSET)); // 发送单向消息，没有任何返回结果 producer.sendOneway(msg); &#125; // 如果不再发送消息，关闭Producer实例。 producer.shutdown(); &#125;&#125; 消费消息123456789101112131415161718192021222324public class Consumer &#123;\tpublic static void main(String[] args) throws InterruptedException, MQClientException &#123; // 实例化消费者 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;please_rename_unique_group_name&quot;); // 设置NameServer的地址 consumer.setNamesrvAddr(&quot;localhost:9876&quot;); // 订阅一个或者多个Topic，以及Tag来过滤需要消费的消息 consumer.subscribe(&quot;TopicTest&quot;, &quot;*&quot;); // 注册消息监听器，回调实现类来处理从broker拉取回来的消息 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; // 接受消息内容 @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; System.out.printf(&quot;%s Receive New Messages: %s %n&quot;, Thread.currentThread().getName(), msgs); // 标记该消息已经被成功消费 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); // 启动消费者实例 consumer.start(); System.out.printf(&quot;Consumer Started.%n&quot;);\t&#125;&#125; 顺序消息原理解析消息有序指的是一类消息消费时，能按照发送的顺序来消费。例如：一个订单产生了三条消息分别是订单创建、订单付款、订单完成。消费时要按照这个顺序消费才能有意义，但是同时订单之间是可以并行消费的，RocketMQ 可以严格的保证消息有序。 顺序消息分为全局顺序消息与分区顺序消息， 全局顺序：对于指定的一个 Topic，所有消息按照严格的先入先出（FIFO）的顺序进行发布和消费，适用于性能要求不高，所有的消息严格按照 FIFO 原则进行消息发布和消费的场景 分区顺序：对于指定的一个 Topic，所有消息根据 Sharding key 进行分区，同一个分组内的消息按照严格的 FIFO 顺序进行发布和消费。Sharding key 是顺序消息中用来区分不同分区的关键字段，和普通消息的 Key 是完全不同的概念，适用于性能要求高的场景 在默认的情况下消息发送会采取 Round Robin 轮询方式把消息发送到不同的 queue（分区队列），而消费消息是从多个 queue 上拉取消息，这种情况发送和消费是不能保证顺序。但是如果控制发送的顺序消息只依次发送到同一个 queue 中，消费的时候只从这个 queue 上依次拉取，则就保证了顺序。当发送和消费参与的 queue 只有一个，则是全局有序；如果多个queue 参与，则为分区有序，即相对每个 queue，消息都是有序的 代码实现一个订单的顺序流程是：创建、付款、推送、完成，订单号相同的消息会被先后发送到同一个队列中，消费时同一个 OrderId 获取到的肯定是同一个队列 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class Producer &#123; public static void main(String[] args) throws Exception &#123; DefaultMQProducer producer = new DefaultMQProducer(&quot;please_rename_unique_group_name&quot;); producer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;); producer.start(); // 标签集合 String[] tags = new String[]&#123;&quot;TagA&quot;, &quot;TagC&quot;, &quot;TagD&quot;&#125;; // 订单列表 List&lt;OrderStep&gt; orderList = new Producer().buildOrders(); Date date = new Date(); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); String dateStr = sdf.format(date); for (int i = 0; i &lt; 10; i++) &#123; // 加个时间前缀 String body = dateStr + &quot; Hello RocketMQ &quot; + orderList.get(i); Message msg = new Message(&quot;OrderTopic&quot;, tags[i % tags.length], &quot;KEY&quot; + i, body.getBytes()); /** * 参数一：消息对象 * 参数二：消息队列的选择器 * 参数三：选择队列的业务标识（订单 ID） */ SendResult sendResult = producer.send(msg, new MessageQueueSelector() &#123; @Override /** * mqs：队列集合 * msg：消息对象 * arg：业务标识的参数 */ public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Long id = (Long) arg; long index = id % mqs.size(); // 根据订单id选择发送queue return mqs.get((int) index); &#125; &#125;, orderList.get(i).getOrderId());//订单id System.out.println(String.format(&quot;SendResult status:%s, queueId:%d, body:%s&quot;, sendResult.getSendStatus(), sendResult.getMessageQueue().getQueueId(), body)); &#125; producer.shutdown(); &#125; // 订单的步骤 private static class OrderStep &#123; private long orderId; private String desc; // set + get &#125; // 生成模拟订单数据 private List&lt;OrderStep&gt; buildOrders() &#123; List&lt;OrderStep&gt; orderList = new ArrayList&lt;OrderStep&gt;(); OrderStep orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc(&quot;创建&quot;); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111065L); orderDemo.setDesc(&quot;创建&quot;); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc(&quot;付款&quot;); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103117235L); orderDemo.setDesc(&quot;创建&quot;); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111065L); orderDemo.setDesc(&quot;付款&quot;); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103117235L); orderDemo.setDesc(&quot;付款&quot;); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111065L); orderDemo.setDesc(&quot;完成&quot;); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc(&quot;推送&quot;); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103117235L); orderDemo.setDesc(&quot;完成&quot;); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc(&quot;完成&quot;); orderList.add(orderDemo); return orderList; &#125;&#125; 1234567891011121314151617181920212223242526// 顺序消息消费，带事务方式（应用可控制Offset什么时候提交）public class ConsumerInOrder &#123; public static void main(String[] args) throws Exception &#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;please_rename_unique_group_name_3&quot;); consumer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;); // 设置Consumer第一次启动是从队列头部开始消费还是队列尾部开始消费 // 如果非第一次启动，那么按照上次消费的位置继续消费 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); // 订阅三个tag consumer.subscribe(&quot;OrderTopic&quot;, &quot;TagA || TagC || TagD&quot;); consumer.registerMessageListener(new MessageListenerOrderly() &#123; Random random = new Random(); @Override public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123; context.setAutoCommit(true); for (MessageExt msg : msgs) &#123; // 可以看到每个queue有唯一的consume线程来消费, 订单对每个queue(分区)有序 System.out.println(&quot;consumeThread=&quot; + Thread.currentThread().getName() + &quot;queueId=&quot; + msg.getQueueId() + &quot;, content:&quot; + new String(msg.getBody())); &#125; return ConsumeOrderlyStatus.SUCCESS; &#125; &#125;); consumer.start(); System.out.println(&quot;Consumer Started.&quot;); &#125;&#125; 延时消息原理解析定时消息（延迟队列）是指消息发送到 Broker 后，不会立即被消费，等待特定时间投递给真正的 Topic RocketMQ 并不支持任意时间的延时，需要设置几个固定的延时等级，从 1s 到 2h 分别对应着等级 1 到 18，消息消费失败会进入延时消息队列，消息发送时间与设置的延时等级和重试次数有关，详见代码 SendMessageProcessor.java 1private String messageDelayLevel = &quot;1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h&quot;; Broker 可以配置 messageDelayLevel，该属性是 Broker 的属性，不属于某个 Topic 发消息时，可以设置延迟等级 msg.setDelayLevel(level)，level 有以下三种情况： level &#x3D;&#x3D; 0：消息为非延迟消息 1&lt;&#x3D;level&lt;&#x3D;maxLevel：消息延迟特定时间，例如 level&#x3D;&#x3D;1，延迟 1s level &gt; maxLevel：则 level&#x3D;&#x3D; maxLevel，例如 level&#x3D;&#x3D;20，延迟 2h 定时消息会暂存在名为 SCHEDULE_TOPIC_XXXX 的 Topic 中，并根据 delayTimeLevel 存入特定的 queue，队列的标识 queueId = delayTimeLevel – 1，即一个 queue 只存相同延迟的消息，保证具有相同发送延迟的消息能够顺序消费。Broker 会为每个延迟级别提交一个定时任务，调度地消费 SCHEDULE_TOPIC_XXXX，将消息写入真实的 Topic 注意：定时消息在第一次写入和调度写入真实 Topic 时都会计数，因此发送数量、tps 都会变高 代码实现提交了一个订单就可以发送一个延时消息，1h 后去检查这个订单的状态，如果还是未付款就取消订单释放库存 12345678910111213141516171819public class ScheduledMessageProducer &#123; public static void main(String[] args) throws Exception &#123; // 实例化一个生产者来产生延时消息 DefaultMQProducer producer = new DefaultMQProducer(&quot;ExampleProducerGroup&quot;); producer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;); // 启动生产者 producer.start(); int totalMessagesToSend = 100; for (int i = 0; i &lt; totalMessagesToSend; i++) &#123; Message message = new Message(&quot;DelayTopic&quot;, (&quot;Hello scheduled message &quot; + i).getBytes()); // 设置延时等级3,这个消息将在10s之后发送(现在只支持固定的几个时间,详看delayTimeLevel) message.setDelayTimeLevel(3); // 发送消息 producer.send(message); &#125; // 关闭生产者 producer.shutdown(); &#125;&#125; 123456789101112131415161718192021public class ScheduledMessageConsumer &#123; public static void main(String[] args) throws Exception &#123; // 实例化消费者 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;ExampleConsumer&quot;); consumer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;); // 订阅Topics consumer.subscribe(&quot;DelayTopic&quot;, &quot;*&quot;); // 注册消息监听者 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; messages, ConsumeConcurrentlyContext context) &#123; for (MessageExt message : messages) &#123; // 打印延迟的时间段 System.out.println(&quot;Receive message[msgId=&quot; + message.getMsgId() + &quot;] &quot; + (System.currentTimeMillis() - message.getBornTimestamp()) + &quot;ms later&quot;);&#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); // 启动消费者 consumer.start(); &#125;&#125; 批量消息批量发送消息能显著提高传递小消息的性能，限制是这些批量消息应该有相同的 topic，相同的 waitStoreMsgOK，而且不能是延时消息，并且这一批消息的总大小不应超过 4MB 12345678910111213141516171819202122232425public class Producer &#123; public static void main(String[] args) throws Exception &#123; DefaultMQProducer producer = new DefaultMQProducer(&quot;ExampleProducerGroup&quot;) producer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;); //启动producer producer.start(); List&lt;Message&gt; msgs = new ArrayList&lt;Message&gt;(); // 创建消息对象，指定主题Topic、Tag和消息体 Message msg1 = new Message(&quot;BatchTopic&quot;, &quot;Tag1&quot;, (&quot;Hello World&quot; + 1).getBytes()); Message msg2 = new Message(&quot;BatchTopic&quot;, &quot;Tag1&quot;, (&quot;Hello World&quot; + 2).getBytes()); Message msg3 = new Message(&quot;BatchTopic&quot;, &quot;Tag1&quot;, (&quot;Hello World&quot; + 3).getBytes()); msgs.add(msg1); msgs.add(msg2); msgs.add(msg3); // 发送消息 SendResult result = producer.send(msgs); System.out.println(&quot;发送结果:&quot; + result); // 关闭生产者producer producer.shutdown(); &#125;&#125; 当发送大批量数据时，可能不确定消息是否超过了大小限制（4MB），所以需要将消息列表分割一下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class ListSplitter implements Iterator&lt;List&lt;Message&gt;&gt; &#123; private final int SIZE_LIMIT = 1024 * 1024 * 4; private final List&lt;Message&gt; messages; private int currIndex; public ListSplitter(List&lt;Message&gt; messages) &#123; this.messages = messages; &#125; @Override public boolean hasNext() &#123; return currIndex &lt; messages.size(); &#125; @Override public List&lt;Message&gt; next() &#123; int startIndex = getStartIndex(); int nextIndex = startIndex; int totalSize = 0; for (; nextIndex &lt; messages.size(); nextIndex++) &#123; Message message = messages.get(nextIndex); int tmpSize = calcMessageSize(message); // 单个消息超过了最大的限制 if (tmpSize + totalSize &gt; SIZE_LIMIT) &#123; break; &#125; else &#123; totalSize += tmpSize; &#125; &#125; List&lt;Message&gt; subList = messages.subList(startIndex, nextIndex); currIndex = nextIndex; return subList; &#125; private int getStartIndex() &#123; Message currMessage = messages.get(currIndex); int tmpSize = calcMessageSize(currMessage); while (tmpSize &gt; SIZE_LIMIT) &#123; currIndex += 1; Message message = messages.get(curIndex); tmpSize = calcMessageSize(message); &#125; return currIndex; &#125; private int calcMessageSize(Message message) &#123; int tmpSize = message.getTopic().length() + message.getBody().length; Map&lt;String, String&gt; properties = message.getProperties(); for (Map.Entry&lt;String, String&gt; entry : properties.entrySet()) &#123; tmpSize += entry.getKey().length() + entry.getValue().length(); &#125; tmpSize = tmpSize + 20; // 增加⽇日志的开销20字节 return tmpSize; &#125; public static void main(String[] args) &#123; //把大的消息分裂成若干个小的消息 ListSplitter splitter = new ListSplitter(messages); while (splitter.hasNext()) &#123; try &#123; List&lt;Message&gt; listItem = splitter.next(); producer.send(listItem); &#125; catch (Exception e) &#123; e.printStackTrace(); //处理error &#125; &#125; &#125;&#125; 过滤消息基本语法RocketMQ 定义了一些基本语法来支持过滤特性，可以很容易地扩展： 数值比较，比如：&gt;，&gt;&#x3D;，&lt;，&lt;&#x3D;，BETWEEN，&#x3D; 字符比较，比如：&#x3D;，&lt;&gt;，IN IS NULL 或者 IS NOT NULL 逻辑符号 AND，OR，NOT 常量支持类型为： 数值，比如 123，3.1415 字符，比如 ‘abc’，必须用单引号包裹起来 NULL，特殊的常量 布尔值，TRUE 或 FALSE 只有使用 push 模式的消费者才能用使用 SQL92 标准的 sql 语句，接口如下： 1public void subscribe(final String topic, final MessageSelector messageSelector) 例如：消费者接收包含 TAGA 或 TAGB 或 TAGC 的消息 12DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;CID_EXAMPLE&quot;);consumer.subscribe(&quot;TOPIC&quot;, &quot;TAGA || TAGB || TAGC&quot;); 原理解析RocketMQ 分布式消息队列的消息过滤方式是在 Consumer 端订阅消息时再做消息过滤的，所以是在 Broker 端实现的，优点是减少了对于 Consumer 无用消息的网络传输，缺点是增加了 Broker 的负担，而且实现相对复杂 RocketMQ 在 Producer 端写入消息和在 Consumer 端订阅消息采用分离存储的机制实现，Consumer 端订阅消息是需要通过 ConsumeQueue 这个消息消费的逻辑队列拿到一个索引，然后再从 CommitLog 里面读取真正的消息实体内容 ConsumeQueue 的存储结构如下，有 8 个字节存储的 Message Tag 的哈希值，基于 Tag 的消息过滤就是基于这个字段 Tag 过滤：Consumer 端订阅消息时指定 Topic 和 TAG，然后将订阅请求构建成一个 SubscriptionData，发送一个 Pull 消息的请求给 Broker 端。Broker 端用这些数据先构建一个 MessageFilter，然后传给文件存储层 Store。Store 从 ConsumeQueue 读取到一条记录后，会用它记录的消息 tag hash 值去做过滤。因为在服务端只是根据 hashcode 进行判断，无法精确对 tag 原始字符串进行过滤，所以消费端拉取到消息后，还需要对消息的原始 tag 字符串进行比对，如果不同，则丢弃该消息，不进行消息消费 SQL92 过滤：工作流程和 Tag 过滤大致一样，只是在 Store 层的具体过滤方式不一样。真正的 SQL expression 的构建和执行由 rocketmq-filter 模块负责，每次过滤都去执行 SQL 表达式会影响效率，所以 RocketMQ 使用了 BloomFilter 来避免了每次都去执行 代码实现发送消息时，通过 putUserProperty 来设置消息的属性，SQL92 的表达式上下文为消息的属性 123456789101112131415public class Producer &#123; public static void main(String[] args) throws Exception &#123; DefaultMQProducer producer = new DefaultMQProducer(&quot;please_rename_unique_group_name&quot;); producer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;); producer.start(); for (int i = 0; i &lt; 10; i++) &#123; Message msg = new Message(&quot;FilterTopic&quot;, &quot;tag&quot;, (&quot;Hello RocketMQ &quot; + i).getBytes(RemotingHelper.DEFAULT_CHARSET)); // 设置一些属性 msg.putUserProperty(&quot;i&quot;, String.valueOf(i)); SendResult sendResult = producer.send(msg); &#125; producer.shutdown(); &#125;&#125; 使用 SQL 筛选过滤消息： 12345678910111213141516171819202122public class Consumer &#123; public static void main(String[] args) throws Exception &#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;please_rename_unique_group_name&quot;); consumer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;); // 过滤属性大于 5 的消息 consumer.subscribe(&quot;FilterTopic&quot;, MessageSelector.bySql(&quot;i&gt;5&quot;)); // 设置回调函数，处理消息 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; //接受消息内容 @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; for (MessageExt msg : msgs) &#123; System.out.println(&quot;consumeThread=&quot; + Thread.currentThread().getName() + &quot;,&quot; + new String(msg.getBody())); &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); // 启动消费者consumer consumer.start(); &#125;&#125; 事务消息工作流程RocketMQ 支持分布式事务消息，采用了 2PC 的思想来实现了提交事务消息，同时增加一个补偿逻辑来处理二阶段超时或者失败的消息，如下图所示： 事务消息的大致方案分为两个流程：正常事务消息的发送及提交、事务消息的补偿流程 事务消息发送及提交： 发送消息（Half 消息），服务器将消息的主题和队列改为半消息状态，并放入半消息队列 服务端响应消息写入结果（如果写入失败，此时 Half 消息对业务不可见） 根据发送结果执行本地事务 根据本地事务状态执行 Commit 或者 Rollback 补偿机制：用于解决消息 Commit 或者 Rollback 发生超时或者失败的情况，比如出现网络问题 Broker 服务端通过对比 Half 消息和 Op 消息，对未确定状态的消息推进 CheckPoint 没有 Commit&#x2F;Rollback 的事务消息，服务端根据根据半消息的生产者组，到 ProducerManager 中获取生产者（同一个 Group 的 Producer）的会话通道，发起一次回查（单向请求） Producer 收到回查消息，检查事务消息状态表内对应的本地事务的状态 根据本地事务状态，重新 Commit 或者 Rollback RocketMQ 并不会无休止的进行事务状态回查，最大回查 15 次，如果 15 次回查还是无法得知事务状态，则默认回滚该消息， 回查服务：TransactionalMessageCheckService#run 两阶段一阶段事务消息相对普通消息最大的特点就是一阶段发送的消息对用户是不可见的，因为对于 Half 消息，会备份原消息的主题与消息消费队列，然后改变主题为 RMQ_SYS_TRANS_HALF_TOPIC，由于消费组未订阅该主题，故消费端无法消费 Half 类型的消息 RocketMQ 会开启一个定时任务，从 Topic 为 RMQ_SYS_TRANS_HALF_TOPIC 中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息 RocketMQ 的具体实现策略：如果写入的是事务消息，对消息的 Topic 和 Queue 等属性进行替换，同时将原来的 Topic 和 Queue 信息存储到消息的属性中，因为消息的主题被替换，所以消息不会转发到该原主题的消息消费队列，消费者无法感知消息的存在，不会消费 二阶段一阶段写入不可见的消息后，二阶段操作： 如果执行 Commit 操作，则需要让消息对用户可见，构建出 Half 消息的索引。一阶段的 Half 消息写到一个特殊的 Topic，构建索引时需要读取出 Half 消息，然后通过一次普通消息的写入操作将 Topic 和 Queue 替换成真正的目标 Topic 和 Queue，生成一条对用户可见的消息。其实就是利用了一阶段存储的消息的内容，在二阶段时恢复出一条完整的普通消息，然后走一遍消息写入流程 如果是 Rollback 则需要撤销一阶段的消息，因为消息本就不可见，所以并不需要真正撤销消息（实际上 RocketMQ 也无法去删除一条消息，因为是顺序写文件的）。RocketMQ 为了区分这条消息没有确定状态的消息，采用 Op 消息标识已经确定状态的事务消息（Commit 或者 Rollback） 事务消息无论是 Commit 或者 Rollback 都会记录一个 Op 操作，两者的区别是 Commit 相对于 Rollback 在写入 Op 消息前将原消息的主题和队列恢复。如果一条事务消息没有对应的 Op 消息，说明这个事务的状态还无法确定（可能是二阶段失败了） RocketMQ 将 Op 消息写入到全局一个特定的 Topic 中，通过源码中的方法 TransactionalMessageUtil.buildOpTopic()，这个主题是一个内部的 Topic（像 Half 消息的 Topic 一样），不会被用户消费。Op 消息的内容为对应的 Half 消息的存储的 Offset，这样通过 Op 消息能索引到 Half 消息 基本使用使用方式事务消息共有三种状态，提交状态、回滚状态、中间状态： TransactionStatus.CommitTransaction：提交事务，允许消费者消费此消息。 TransactionStatus.RollbackTransaction：回滚事务，代表该消息将被删除，不允许被消费 TransactionStatus.Unknown：中间状态，代表需要检查消息队列来确定状态 使用限制： 事务消息不支持延时消息和批量消息 Broker 配置文件中的参数 transactionTimeout 为特定时间，事务消息将在特定时间长度之后被检查。当发送事务消息时，还可以通过设置用户属性 CHECK_IMMUNITY_TIME_IN_SECONDS 来改变这个限制，该参数优先于 transactionTimeout 参数 为了避免单个消息被检查太多次而导致半队列消息累积，默认将单个消息的检查次数限制为 15 次，开发者可以通过 Broker 配置文件的 transactionCheckMax 参数来修改此限制。如果已经检查某条消息超过 N 次（N &#x3D; transactionCheckMax）， 则 Broker 将丢弃此消息，在默认情况下会打印错误日志。可以通过重写 AbstractTransactionalMessageCheckListener 类来修改这个行为 事务性消息可能不止一次被检查或消费 提交给用户的目标主题消息可能会失败，可以查看日志的记录。事务的高可用性通过 RocketMQ 本身的高可用性机制来保证，如果希望事务消息不丢失、并且事务完整性得到保证，可以使用同步的双重写入机制 事务消息的生产者 ID 不能与其他类型消息的生产者 ID 共享。与其他类型的消息不同，事务消息允许反向查询，MQ 服务器能通过消息的生产者 ID 查询到消费者 代码实现实现事务的监听接口，当发送半消息成功时： executeLocalTransaction 方法来执行本地事务，返回三个事务状态之一 checkLocalTransaction 方法检查本地事务状态，响应消息队列的检查请求，返回三个事务状态之一 123456789101112131415161718192021222324252627282930public class TransactionListenerImpl implements TransactionListener &#123; private AtomicInteger transactionIndex = new AtomicInteger(0); private ConcurrentHashMap&lt;String, Integer&gt; localTrans = new ConcurrentHashMap&lt;&gt;(); @Override public LocalTransactionState executeLocalTransaction(Message msg, Object arg) &#123; int value = transactionIndex.getAndIncrement(); int status = value % 3; // 将事务ID和状态存入 map 集合 localTrans.put(msg.getTransactionId(), status); return LocalTransactionState.UNKNOW; &#125; @Override public LocalTransactionState checkLocalTransaction(MessageExt msg) &#123; // 从 map 集合读出当前事务对应的状态 Integer status = localTrans.get(msg.getTransactionId()); if (null != status) &#123; switch (status) &#123; case 0: return LocalTransactionState.UNKNOW; case 1: return LocalTransactionState.COMMIT_MESSAGE; case 2: return LocalTransactionState.ROLLBACK_MESSAGE; &#125; &#125; return LocalTransactionState.COMMIT_MESSAGE; &#125;&#125; 使用 TransactionMQProducer 类创建事务性生产者，并指定唯一的 ProducerGroup，就可以设置自定义线程池来处理这些检查请求，执行本地事务后，需要根据执行结果对消息队列进行回复 123456789101112131415161718192021222324252627282930public class Producer &#123;\tpublic static void main(String[] args) throws MQClientException, InterruptedException &#123; // 创建消息生产者 TransactionMQProducer producer = new TransactionMQProducer(&quot;please_rename_unique_group_name&quot;); ExecutorService executorService = new ThreadPoolExecutor(2, 5, 100, TimeUnit.SECONDS); producer.setExecutorService(executorService); // 创建事务监听器 TransactionListener transactionListener = new TransactionListenerImpl(); // 生产者的监听器 producer.setTransactionListener(transactionListener); // 启动生产者 producer.start(); String[] tags = new String[] &#123;&quot;TagA&quot;, &quot;TagB&quot;, &quot;TagC&quot;, &quot;TagD&quot;, &quot;TagE&quot;&#125;; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Message msg = new Message(&quot;TransactionTopic&quot;, tags[i % tags.length], &quot;KEY&quot; + i, (&quot;Hello RocketMQ &quot; + i).getBytes(RemotingHelper.DEFAULT_CHARSET)); // 发送消息 SendResult sendResult = producer.sendMessageInTransaction(msg, null); System.out.printf(&quot;%s%n&quot;, sendResult); Thread.sleep(10); &#125; catch (MQClientException | UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; //Thread.sleep(1000000); //producer.shutdown();暂时不关闭 &#125;&#125; 消费者代码和前面的实例相同的 系统特性工作流程模块介绍NameServer 是一个简单的 Topic 路由注册中心，支持 Broker 的动态注册与发现，生产者或消费者能够通过名字服务查找各主题相应的 Broker IP 列表 NameServer 主要包括两个功能： Broker 管理，NameServer 接受 Broker 集群的注册信息，保存下来作为路由信息的基本数据，提供心跳检测机制检查 Broker 是否还存活，每 10 秒清除一次两小时没有活跃的 Broker 路由信息管理，每个 NameServer 将保存关于 Broker 集群的整个路由信息和用于客户端查询的队列信息，然后 Producer 和 Conumser 通过 NameServer 就可以知道整个 Broker 集群的路由信息，从而进行消息的投递和消费 NameServer 特点： NameServer 通常是集群的方式部署，各实例间相互不进行信息通讯 Broker 向每一台 NameServer（集群）注册自己的路由信息，所以每个 NameServer 实例上面都保存一份完整的路由信息 当某个 NameServer 因某种原因下线了，Broker 仍可以向其它 NameServer 同步其路由信息 BrokerServer 主要负责消息的存储、投递和查询以及服务高可用保证，在 RocketMQ 系统中接收从生产者发送来的消息并存储、同时为消费者的拉取请求作准备，也存储消息相关的元数据，包括消费者组、消费进度偏移和主题和队列消息等 Broker 包含了以下几个重要子模块： Remoting Module：整个 Broker 的实体，负责处理来自 Clients 端的请求 Client Manager：负责管理客户端（Producer&#x2F;Consumer）和维护 Consumer 的 Topic 订阅信息 Store Service：提供方便简单的 API 接口处理消息存储到物理硬盘和查询功能 HA Service：高可用服务，提供 Master Broker 和 Slave Broker 之间的数据同步功能 Index Service：根据特定的 Message key 对投递到 Broker 的消息进行索引服务，以提供消息的快速查询 总体流程RocketMQ 的工作流程： 启动 NameServer 监听端口，等待 Broker、Producer、Consumer 连上来，相当于一个路由控制中心 Broker 启动，跟所有的 NameServer 保持长连接，每隔 30s 时间向 NameServer 上报 Topic 路由信息（心跳包）。心跳包中包含当前 Broker 信息（IP、端口等）以及存储所有 Topic 信息。注册成功后，NameServer 集群中就有 Topic 跟 Broker 的映射关系 收发消息前，先创建 Topic，创建 Topic 时需要指定该 Topic 要存储在哪些 Broker 上，也可以在发送消息时自动创建 Topic Producer 启动时先跟 NameServer 集群中的其中一台建立长连接，并从 NameServer 中获取当前发送的 Topic 存在哪些 Broker 上，同时 Producer 会默认每隔 30s 向 NameServer 定时拉取一次路由信息 Producer 发送消息时，根据消息的 Topic 从本地缓存的 TopicPublishInfoTable 获取路由信息，如果没有则会从 NameServer 上重新拉取并更新，轮询队列列表并选择一个队列 MessageQueue，然后与队列所在的 Broker 建立长连接，向 Broker 发消息 Consumer 跟 Producer 类似，跟其中一台 NameServer 建立长连接，定时获取路由信息，根据当前订阅 Topic 存在哪些 Broker 上，直接跟 Broker 建立连接通道，在完成客户端的负载均衡后，选择其中的某一个或者某几个 MessageQueue 来拉取消息并进行消费 生产消费At least Once：至少一次，指每个消息必须投递一次，Consumer 先 Pull 消息到本地，消费完成后才向服务器返回 ACK，如果没有消费一定不会 ACK 消息 回溯消费：指 Consumer 已经消费成功的消息，由于业务上需求需要重新消费，Broker 在向 Consumer 投递成功消息后，消息仍然需要保留。并且重新消费一般是按照时间维度，例如由于 Consumer 系统故障，恢复后需要重新消费 1 小时前的数据，RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒 分布式队列因为有高可靠性的要求，所以数据要进行持久化存储 消息生产者发送消息 MQ 收到消息，将消息进行持久化，在存储中新增一条记录 返回 ACK 给生产者 MQ push 消息给对应的消费者，然后等待消费者返回 ACK 如果消息消费者在指定时间内成功返回 ACK，那么 MQ 认为消息消费成功，在存储中删除消息；如果 MQ 在指定时间内没有收到 ACK，则认为消息消费失败，会尝试重新 push 消息，重复执行 4、5、6 步骤 MQ 删除消息 存储机制存储结构RocketMQ 中 Broker 负责存储消息转发消息，所以以下的结构是存储在 Broker Server 上的，生产者和消费者与 Broker 进行消息的收发是通过主题对应的 Message Queue 完成，类似于通道 RocketMQ 消息的存储是由 ConsumeQueue 和 CommitLog 配合完成 的，CommitLog 是消息真正的物理存储文件，ConsumeQueue 是消息的逻辑队列，类似数据库的索引节点，存储的是指向物理存储的地址。每个 Topic 下的每个 Message Queue 都有一个对应的 ConsumeQueue 文件 每条消息都会有对应的索引信息，Consumer 通过 ConsumeQueue 这个结构来读取消息实体内容 CommitLog：消息主体以及元数据的存储主体，存储 Producer 端写入的消息内容，消息内容不是定长的。消息主要是顺序写入日志文件，单个文件大小默认 1G，偏移量代表下一次写入的位置，当文件写满了就继续写入下一个文件 ConsumerQueue：消息消费队列，存储消息在 CommitLog 的索引。RocketMQ 消息消费时要遍历 CommitLog 文件，并根据主题 Topic 检索消息，这是非常低效的。引入 ConsumeQueue 作为消费消息的索引，保存了指定 Topic 下的队列消息在 CommitLog 中的起始物理偏移量 offset，消息大小 size 和消息 Tag 的 HashCode 值，每个 ConsumeQueue 文件大小约 5.72M IndexFile：为了消息查询提供了一种通过 Key 或时间区间来查询消息的方法，通过 IndexFile 来查找消息的方法不影响发送与消费消息的主流程。IndexFile 的底层存储为在文件系统中实现的 HashMap 结构，故 RocketMQ 的索引文件其底层实现为 hash 索引 RocketMQ 采用的是混合型的存储结构，即为 Broker 单个实例下所有的队列共用一个日志数据文件（CommitLog）来存储，多个 Topic 的消息实体内容都存储于一个 CommitLog 中。混合型存储结构针对 Producer 和 Consumer 分别采用了数据和索引部分相分离的存储结构，Producer 发送消息至 Broker 端，然后 Broker 端使用同步或者异步的方式对消息刷盘持久化，保存至 CommitLog 中。只要消息被持久化至磁盘文件 CommitLog 中，Producer 发送的消息就不会丢失，Consumer 也就肯定有机会去消费这条消息 服务端支持长轮询模式，当消费者无法拉取到消息后，可以等下一次消息拉取，Broker 允许等待 30s 的时间，只要这段时间内有新消息到达，将直接返回给消费端。RocketMQ 的具体做法是，使用 Broker 端的后台服务线程 ReputMessageService 不停地分发请求并异步构建 ConsumeQueue（逻辑消费队列）和 IndexFile（索引文件）数据 内存映射操作系统分为用户态和内核态，文件操作、网络操作需要涉及这两种形态的切换，需要进行数据复制。一台服务器把本机磁盘文件的内容发送到客户端，分为两个步骤： read：读取本地文件内容 write：将读取的内容通过网络发送出去 补充：Prog → NET → I&#x2F;O → 零拷贝部分的笔记详解相关内容 通过使用 mmap 的方式，可以省去向用户态的内存复制，RocketMQ 充分利用零拷贝技术，提高消息存盘和网络发送的速度 RocketMQ 通过 MappedByteBuffer 对文件进行读写操作，利用了 NIO 中的 FileChannel 模型将磁盘上的物理文件直接映射到用户态的内存地址中，将对文件的操作转化为直接对内存地址进行操作，从而极大地提高了文件的读写效率 MappedByteBuffer 内存映射的方式限制一次只能映射 1.5~2G 的文件至用户态的虚拟内存，所以 RocketMQ 默认设置单个 CommitLog 日志数据文件为 1G。RocketMQ 的文件存储使用定长结构来存储，方便一次将整个文件映射至内存 页面缓存页缓存（PageCache）是 OS 对文件的缓存，每一页的大小通常是 4K，用于加速对文件的读写。因为 OS 将一部分的内存用作 PageCache，所以程序对文件进行顺序读写的速度几乎接近于内存的读写速度 对于数据的写入，OS 会先写入至 Cache 内，随后通过异步的方式由 pdflush 内核线程将 Cache 内的数据刷盘至物理磁盘上 对于数据的读取，如果一次读取文件时出现未命中 PageCache 的情况，OS 从物理磁盘上访问读取文件的同时，会顺序对其他相邻块的数据文件进行预读取（局部性原理，最大 128K） 在 RocketMQ 中，ConsumeQueue 逻辑消费队列存储的数据较少，并且是顺序读取，在 PageCache 机制的预读取作用下，Consume Queue 文件的读性能几乎接近读内存，即使在有消息堆积情况下也不会影响性能。但是 CommitLog 消息存储的日志数据文件读取内容时会产生较多的随机访问读取，严重影响性能。选择合适的系统 IO 调度算法和固态硬盘，比如设置调度算法为 Deadline，随机读的性能也会有所提升 刷盘机制两种持久化的方案： 关系型数据库 DB：IO 读写性能比较差，如果 DB 出现故障，则 MQ 的消息就无法落盘存储导致线上故障，可靠性不高 文件系统：消息刷盘至所部署虚拟机&#x2F;物理机的文件系统来做持久化，分为异步刷盘和同步刷盘两种模式。消息刷盘为消息存储提供了一种高效率、高可靠性和高性能的数据持久化方式，除非部署 MQ 机器本身或是本地磁盘挂了，一般不会出现无法持久化的问题 RocketMQ 采用文件系统的方式，无论同步还是异步刷盘，都使用顺序 IO，因为磁盘的顺序读写要比随机读写快很多 同步刷盘：只有在消息真正持久化至磁盘后 RocketMQ 的 Broker 端才会真正返回给 Producer 端一个成功的 ACK 响应，保障 MQ 消息的可靠性，但是性能上会有较大影响，一般适用于金融业务应用该模式较多 异步刷盘：利用 OS 的 PageCache，只要消息写入内存 PageCache 即可将成功的 ACK 返回给 Producer 端，降低了读写延迟，提高了 MQ 的性能和吞吐量。消息刷盘采用后台异步线程提交的方式进行，当内存里的消息量积累到一定程度时，触发写磁盘动作 通过 Broker 配置文件里的 flushDiskType 参数设置采用什么方式，可以配置成 SYNC_FLUSH、ASYNC_FLUSH 中的一个 官方文档：https://github.com/apache/rocketmq/blob/master/docs/cn/design.md 集群设计集群模式常用的以下几种模式： 单 Master 模式：这种方式风险较大，一旦 Broker 重启或者宕机，会导致整个服务不可用 多 Master 模式：一个集群无 Slave，全是 Master 优点：配置简单，单个 Master 宕机或重启维护对应用无影响，在磁盘配置为 RAID10 时，即使机器宕机不可恢复情况下，由于 RAID10 磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢），性能最高 缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响 多 Master 多 Slave 模式（同步）：每个 Master 配置一个 Slave，有多对 Master-Slave，HA 采用同步双写方式，即只有主备都写成功，才向应用返回成功 优点：数据与服务都无单点故障，Master 宕机情况下，消息无延迟，服务可用性与数据可用性都非常高 缺点：性能比异步复制略低（大约低 10% 左右），发送单个消息的 RT 略高，目前不能实现主节点宕机，备机自动切换为主机 多 Master 多 Slave 模式（异步）：HA 采用异步复制的方式，会造成主备有短暂的消息延迟（毫秒级别） 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，同时 Master 宕机后，消费者仍然可以从 Slave 消费，而且此过程对应用透明，不需要人工干预，性能同多 Master 模式几乎一样 缺点：Master 宕机，磁盘损坏情况下会丢失少量消息 集群架构RocketMQ 网络部署特点： NameServer 是一个几乎无状态节点，节点之间相互独立，无任何信息同步 Broker 部署相对复杂，Broker 分为 Master 与 Slave，Master 可以部署多个，一个 Master 可以对应多个 Slave，但是一个 Slave 只能对应一个 Master，Master 与 Slave 的对应关系通过指定相同 BrokerName、不同 BrokerId 来定义，BrokerId 为 0 是 Master，非 0 表示 Slave。每个 Broker 与 NameServer 集群中的所有节点建立长连接，定时注册 Topic 信息到所有 NameServer 说明：部署架构上也支持一 Master 多 Slave，但只有 BrokerId&#x3D;1 的从服务器才会参与消息的读负载（读写分离） Producer 与 NameServer 集群中的其中一个节点（随机选择）建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Master 建立长连接，且定时向 Master 发送心跳。Producer 完全无状态，可集群部署 Consumer 与 NameServer 集群中的其中一个节点（随机选择）建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Master、Slave 建立长连接，且定时向 Master、Slave 发送心跳 Consumer 既可以从 Master 订阅消息，也可以从 Slave 订阅消息，在向 Master 拉取消息时，Master 服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读 I&#x2F;O），以及从服务器是否可读等因素建议下一次是从 Master 还是 Slave 拉取 官方文档：https://github.com/apache/rocketmq/blob/master/docs/cn/architecture.md 高可用性NameServer 节点是无状态的，且各个节点直接的数据是一致的，部分 NameServer 不可用也可以保证 MQ 服务正常运行 BrokerServer 的高可用通过 Master 和 Slave 的配合： Slave 只负责读，当 Master 不可用，对应的 Slave 仍能保证消息被正常消费 配置多组 Master-Slave 组，其他的 Master-Slave 组也会保证消息的正常发送和消费 目前不支持把 Slave 自动转成 Master，需要手动停止 Slave 角色的 Broker，更改配置文件，用新的配置文件启动 Broker 所以需要配置多个 Master 保证可用性，否则一个 Master 挂了导致整体系统的写操作不可用 生产端的高可用：在创建 Topic 的时候，把 Topic 的多个 Message Queue 创建在多个 Broker 组上（相同 Broker 名称，不同 brokerId 的机器），当一个 Broker 组的 Master 不可用后，其他组的 Master 仍然可用，Producer 仍然可以发送消息 消费端的高可用：在 Consumer 的配置文件中，并不需要设置是从 Master Broker 读还是从 Slave 读，当 Master 不可用或者繁忙的时候，Consumer 会被自动切换到从 Slave 读。有了自动切换的机制，当一个 Master 机器出现故障后，Consumer 仍然可以从 Slave 读取消息，不影响 Consumer 程序，达到了消费端的高可用性 主从复制如果一个 Broker 组有 Master 和 Slave，消息需要从 Master 复制到 Slave 上，有同步和异步两种复制方式： 同步复制方式：Master 和 Slave 均写成功后才反馈给客户端写成功状态（写 Page Cache）。在同步复制方式下，如果 Master 出故障， Slave 上有全部的备份数据，容易恢复，但是同步复制会增大数据写入延迟，降低系统吞吐量 异步复制方式：只要 Master 写成功，即可反馈给客户端写成功状态，系统拥有较低的延迟和较高的吞吐量，但是如果 Master 出了故障，有些数据因为没有被写入 Slave，有可能会丢失 同步复制和异步复制是通过 Broker 配置文件里的 brokerRole 参数进行设置的，可以设置成 ASYNC_MASTE、RSYNC_MASTER、SLAVE 三个值中的一个 一般把刷盘机制配置成 ASYNC_FLUSH，主从复制为 SYNC_MASTER，这样即使有一台机器出故障，仍然能保证数据不丢 RocketMQ 支持消息的高可靠，影响消息可靠性的几种情况： Broker 非正常关闭 Broker 异常 Crash OS Crash 机器掉电，但是能立即恢复供电情况 机器无法开机（可能是 CPU、主板、内存等关键设备损坏） 磁盘设备损坏 前四种情况都属于硬件资源可立即恢复情况，RocketMQ 在这四种情况下能保证消息不丢，或者丢失少量数据（依赖刷盘方式） 后两种属于单点故障，且无法恢复，一旦发生，在此单点上的消息全部丢失。RocketMQ 在这两种情况下，通过主从异步复制，可保证 99% 的消息不丢，但是仍然会有极少量的消息可能丢失。通过同步双写技术可以完全避免单点，但是会影响性能，适合对消息可靠性要求极高的场合，RocketMQ 从 3.0 版本开始支持同步双写 一般而言，我们会建议采取同步双写 + 异步刷盘的方式，在消息的可靠性和性能间有一个较好的平衡 负载均衡生产端RocketMQ 中的负载均衡可以分为 Producer 端发送消息时候的负载均衡和 Consumer 端订阅消息的负载均衡 Producer 端在发送消息时，会先根据 Topic 找到指定的 TopicPublishInfo，在获取了 TopicPublishInfo 路由信息后，RocketMQ 的客户端在默认方式调用 selectOneMessageQueue() 方法从 TopicPublishInfo 中的 messageQueueList 中选择一个队列 MessageQueue 进行发送消息 默认会轮询所有的 Message Queue 发送，以让消息平均落在不同的 queue 上，而由于 queue可以散落在不同的 Broker，所以消息就发送到不同的 Broker 下，图中箭头线条上的标号代表顺序，发布方会把第一条消息发送至 Queue 0，然后第二条消息发送至 Queue 1，以此类推： 容错策略均在 MQFaultStrategy 这个类中定义，有一个 sendLatencyFaultEnable 开关变量： 如果开启，会在随机（只有初始化索引变量时才随机，正常都是递增）递增取模的基础上，再过滤掉 not available 的 Broker 如果关闭，采用随机递增取模的方式选择一个队列（MessageQueue）来发送消息 LatencyFaultTolerance 机制是实现消息发送高可用的核心关键所在，对之前失败的，按一定的时间做退避。例如上次请求的 latency 超过 550Lms，就退避 3000Lms；超过 1000L，就退避 60000L 消费端在 RocketMQ 中，Consumer 端的两种消费模式（Push&#x2F;Pull）都是基于拉模式来获取消息的，而在 Push 模式只是对 Pull 模式的一种封装，其本质实现为消息拉取线程在从服务器拉取到一批消息，提交到消息消费线程池后，又继续向服务器再次尝试拉取消息，如果未拉取到消息，则延迟一下又继续拉取 在两种基于拉模式的消费方式（Push&#x2F;Pull）中，均需要 Consumer 端在知道从 Broker 端的哪一个消息队列中去获取消息，所以在 Consumer 端来做负载均衡，即 Broker 端中多个 MessageQueue 分配给同一个 Consumer Group 中的哪些 Consumer 消费 广播模式下要求一条消息需要投递到一个消费组下面所有的消费者实例，所以不存在负载均衡，在实现上，Consumer 分配 queue 时，所有 Consumer 都分到所有的 queue。 在集群消费模式下，每条消息只需要投递到订阅这个 Topic 的 Consumer Group 下的一个实例即可，RocketMQ 采用主动拉取的方式拉取并消费消息，在拉取的时候需要明确指定拉取哪一条 Message Queue 集群模式下，每当消费者实例的数量有变更，都会触发一次所有实例的负载均衡，这时候会按照 queue 的数量和实例的数量平均分配 queue 给每个实例。默认的分配算法是 AllocateMessageQueueAveragely： 还有一种平均的算法是 AllocateMessageQueueAveragelyByCircle，以环状轮流均分 queue 的形式： 集群模式下，queue 都是只允许分配一个实例，如果多个实例同时消费一个 queue 的消息，由于拉取哪些消息是 Consumer 主动控制的，会导致同一个消息在不同的实例下被消费多次 通过增加 Consumer 实例去分摊 queue 的消费，可以起到水平扩展的消费能力的作用。而当有实例下线时，会重新触发负载均衡，这时候原来分配到的 queue 将分配到其他实例上继续消费。但是如果 Consumer 实例的数量比 Message Queue 的总数量还多的话，多出来的 Consumer 实例将无法分到 queue，也就无法消费到消息，也就无法起到分摊负载的作用了，所以需要控制让 queue 的总数量大于等于 Consumer 的数量 原理解析在 Consumer 启动后，会通过定时任务不断地向 RocketMQ 集群中的所有 Broker 实例发送心跳包。Broker 端在收到 Consumer 的心跳消息后，会将它维护在 ConsumerManager 的本地缓存变量 consumerTable，同时并将封装后的客户端网络通道信息保存在本地缓存变量 channelInfoTable 中，为 Consumer 端的负载均衡提供可以依据的元数据信息 Consumer 端实现负载均衡的核心类 RebalanceImpl 在 Consumer 实例的启动流程中的会启动 MQClientInstance 实例，完成负载均衡服务线程 RebalanceService 的启动（每隔 20s 执行一次负载均衡），RebalanceService 线程的 run() 方法最终调用的是 RebalanceImpl 类的 rebalanceByTopic() 方法，该方法是实现 Consumer 端负载均衡的核心。rebalanceByTopic() 方法会根据广播模式还是集群模式做不同的逻辑处理。主要看集群模式： 从 rebalanceImpl 实例的本地缓存变量 topicSubscribeInfoTable 中，获取该 Topic 主题下的消息消费队列集合 mqSet 根据 Topic 和 consumerGroup 为参数调用 mQClientFactory.findConsumerIdList() 方法向 Broker 端发送获取该消费组下消费者 ID 列表的 RPC 通信请求（Broker 端基于前面 Consumer 端上报的心跳包数据而构建的 consumerTable 做出响应返回，业务请求码 GET_CONSUMER_LIST_BY_GROUP） 先对 Topic 下的消息消费队列、消费者 ID 排序，然后用消息队列分配策略算法（默认是消息队列的平均分配算法），计算出待拉取的消息队列。平均分配算法类似于分页的算法，将所有 MessageQueue 排好序类似于记录，将所有消费端 Consumer 排好序类似页数，并求出每一页需要包含的平均 size 和每个页面记录的范围 range，最后遍历整个 range 而计算出当前 Consumer 端应该分配到的记录（这里即为 MessageQueue） 调用 updateProcessQueueTableInRebalance() 方法，先将分配到的消息队列集合 mqSet 与 processQueueTable 做一个过滤比对 processQueueTable 标注的红色部分，表示与分配到的消息队列集合 mqSet 互不包含，将这些队列设置 Dropped 属性为 true，然后查看这些队列是否可以移除出 processQueueTable 缓存变量。具体执行 removeUnnecessaryMessageQueue() 方法，即每隔 1s 查看是否可以获取当前消费处理队列的锁，拿到的话返回 true；如果等待 1s 后，仍然拿不到当前消费处理队列的锁则返回 false。如果返回 true，则从 processQueueTable 缓存变量中移除对应的 Entry processQueueTable 的绿色部分，表示与分配到的消息队列集合 mqSet 的交集，判断该 ProcessQueue 是否已经过期了，在 Pull 模式的不用管，如果是 Push 模式的，设置 Dropped 属性为 true，并且调用 removeUnnecessaryMessageQueue() 方法，像上面一样尝试移除 Entry 为过滤后的消息队列集合 mqSet 中每个 MessageQueue 创建 ProcessQueue 对象存入 RebalanceImpl 的 processQueueTable 队列中（其中调用 RebalanceImpl 实例的 computePullFromWhere(MessageQueue mq) 方法获取该 MessageQueue 对象的下一个进度消费值 offset，随后填充至接下来要创建的 pullRequest 对象属性中），并创建拉取请求对象 pullRequest 添加到拉取列表 pullRequestList 中，最后执行 dispatchPullRequest() 方法，将 Pull 消息的请求对象 PullRequest 放入 PullMessageService 服务线程的阻塞队列 pullRequestQueue 中，待该服务线程取出后向 Broker 端发起 Pull 消息的请求 对比下 RebalancePushImpl 和 RebalancePullImpl 两个实现类的 dispatchPullRequest() 方法，RebalancePullImpl 类里面的该方法为空 消息消费队列在同一消费组不同消费者之间的负载均衡，其核心设计理念是在一个消息消费队列在同一时间只允许被同一消费组内的一个消费者消费，一个消息消费者能同时消费多个消息队列 消息查询查询方式RocketMQ 支持按照两种维度进行消息查询：按照 Message ID 查询消息、按照 Message Key 查询消息 RocketMQ 中的 MessageID 的长度总共有 16 字节，其中包含了消息存储主机地址（IP 地址和端口），消息 Commit Log offset 实现方式：Client 端从 MessageID 中解析出 Broker 的地址（IP 地址和端口）和 Commit Log 的偏移地址，封装成一个 RPC 请求后通过 Remoting 通信层发送（业务请求码 VIEW_MESSAGE_BY_ID）。Broker 端走的是 QueryMessageProcessor，读取消息的过程用其中的 CommitLog 的 offset 和 size 去 CommitLog 中找到真正的记录并解析成一个完整的消息返回 按照 Message Key 查询消息，IndexFile 索引文件为提供了通过 Message Key 查询消息的服务 实现方式：通过 Broker 端的 QueryMessageProcessor 业务处理器来查询，读取消息的过程用 Topic 和 Key 找到 IndexFile 索引文件中的一条记录，根据其中的 CommitLog Offset 从 CommitLog 文件中读取消息的实体内容 索引机制RocketMQ 的索引文件逻辑结构，类似 JDK 中 HashMap 的实现，具体结构如下： IndexFile 文件的存储在 $HOME\\store\\index$&#123;fileName&#125;，文件名 fileName 是以创建时的时间戳命名，文件大小是固定的，等于 40+500W*4+2000W*20= 420000040 个字节大小。如果消息的 properties 中设置了 UNIQ_KEY 这个属性，就用 topic + “#” + UNIQ_KEY 作为 key 来做写入操作；如果消息设置了 KEYS 属性（多个 KEY 以空格分隔），也会用 topic + “#” + KEY 来做索引 整个 Index File 的结构如图，40 Byte 的 Header 用于保存一些总的统计信息，4*500W 的 Slot Table 并不保存真正的索引数据，而是保存每个槽位对应的单向链表的头指针，即一个 Index File 可以保存 2000W 个索引，20*2000W 是真正的索引数据 索引数据包含了 Key Hash&#x2F;CommitLog Offset&#x2F;Timestamp&#x2F;NextIndex offset 这四个字段，一共 20 Byte NextIndex offset 即前面读出来的 slotValue，如果有 hash 冲突，就可以用这个字段将所有冲突的索引用链表的方式串起来 Timestamp 记录的是消息 storeTimestamp 之间的差，并不是一个绝对的时间 参考文档：https://github.com/apache/rocketmq/blob/master/docs/cn/design.md 消息重试消息重投生产者在发送消息时，同步消息和异步消息失败会重投，oneway 没有任何保证。消息重投保证消息尽可能发送成功、不丢失，但当出现消息量大、网络抖动时，可能会造成消息重复；生产者主动重发、Consumer 负载变化也会导致重复消息 如下方法可以设置消息重投策略： retryTimesWhenSendFailed：同步发送失败重投次数，默认为 2，因此生产者会最多尝试发送 retryTimesWhenSendFailed + 1 次。不会选择上次失败的 Broker，尝试向其他 Broker 发送，最大程度保证消息不丢。超过重投次数抛出异常，由客户端保证消息不丢。当出现 RemotingException、MQClientException 和部分 MQBrokerException 时会重投 retryTimesWhenSendAsyncFailed：异步发送失败重试次数，异步重试不会选择其他 Broker，仅在同一个 Broker 上做重试，不保证消息不丢 retryAnotherBrokerWhenNotStoreOK：消息刷盘（主或备）超时或 slave 不可用（返回状态非 SEND_OK），是否尝试发送到其他 Broker，默认 false，十分重要消息可以开启 注意点： 如果同步模式发送失败，则选择到下一个 Broker，如果异步模式发送失败，则只会在当前 Broker 进行重试 发送消息超时时间默认 3000 毫秒，就不会再尝试重试 消息重试Consumer 消费消息失败后，提供了一种重试机制，令消息再消费一次。Consumer 消费消息失败可以认为有以下几种情况： 由于消息本身的原因，例如反序列化失败，消息数据本身无法处理等。这种错误通常需要跳过这条消息，再消费其它消息，而这条失败的消息即使立刻重试消费，99% 也不成功，所以需要提供一种定时重试机制，即过 10 秒后再重试 由于依赖的下游应用服务不可用，例如 DB 连接不可用，外系统网络不可达等。这种情况即使跳过当前失败的消息，消费其他消息同样也会报错，这种情况建议应用 sleep 30s，再消费下一条消息，这样可以减轻 Broker 重试消息的压力 RocketMQ 会为每个消费组都设置一个 Topic 名称为 %RETRY%+consumerGroup 的重试队列（这个 Topic 的重试队列是针对消费组，而不是针对每个 Topic 设置的），用于暂时保存因为各种异常而导致 Consumer 端无法消费的消息 顺序消息的重试，当消费者消费消息失败后，消息队列 RocketMQ 会自动不断进行消息重试（每次间隔时间为 1 秒），这时应用会出现消息消费被阻塞的情况。所以在使用顺序消息时，必须保证应用能够及时监控并处理消费失败的情况，避免阻塞现象的发生 无序消息（普通、定时、延时、事务消息）的重试，可以通过设置返回状态达到消息重试的结果。无序消息的重试只针对集群消费方式生效，广播方式不提供失败重试特性，即消费失败后，失败消息不再重试，继续消费新的消息 无序消息情况下，因为异常恢复需要一些时间，会为重试队列设置多个重试级别，每个重试级别都有对应的重新投递延时，重试次数越多投递延时就越大。RocketMQ 对于重试消息的处理是先保存至 Topic 名称为 SCHEDULE_TOPIC_XXXX 的延迟队列中，后台定时任务按照对应的时间进行 Delay 后重新保存至 %RETRY%+consumerGroup 的重试队列中 消息队列 RocketMQ 默认允许每条消息最多重试 16 次，每次重试的间隔时间如下表示： 第几次重试 与上次重试的间隔时间 第几次重试 与上次重试的间隔时间 1 10 秒 9 7 分钟 2 30 秒 10 8 分钟 3 1 分钟 11 9 分钟 4 2 分钟 12 10 分钟 5 3 分钟 13 20 分钟 6 4 分钟 14 30 分钟 7 5 分钟 15 1 小时 8 6 分钟 16 2 小时 如果消息重试 16 次后仍然失败，消息将不再投递，如果严格按照上述重试时间间隔计算，某条消息在一直消费失败的前提下，将会在接下来的 4 小时 46 分钟之内进行 16 次重试，超过这个时间范围消息将不再重试投递 时间间隔不支持自定义配置，最大重试次数可通过自定义参数 MaxReconsumeTimes 取值进行配置，若配置超过 16 次，则超过的间隔时间均为 2 小时 说明：一条消息无论重试多少次，消息的 Message ID 是不会改变的 重试操作集群消费方式下，消息消费失败后期望消息重试，需要在消息监听器接口的实现中明确进行配置（三种方式任选一种）： 返回 Action.ReconsumeLater （推荐） 返回 null 抛出异常 12345678910111213public class MessageListenerImpl implements MessageListener &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; // 处理消息 doConsumeMessage(message); //方式1：返回 Action.ReconsumeLater，消息将重试 return Action.ReconsumeLater; //方式2：返回 null，消息将重试 return null; //方式3：直接抛出异常， 消息将重试 throw new RuntimeException(&quot;Consumer Message exceotion&quot;); &#125;&#125; 集群消费方式下，消息失败后期望消息不重试，需要捕获消费逻辑中可能抛出的异常，最终返回 Action.CommitMessage，此后这条消息将不会再重试 12345678910111213public class MessageListenerImpl implements MessageListener &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; try &#123; doConsumeMessage(message); &#125; catch (Throwable e) &#123; // 捕获消费逻辑中的所有异常，并返回 Action.CommitMessage; return Action.CommitMessage; &#125; //消息处理正常，直接返回 Action.CommitMessage; return Action.CommitMessage; &#125;&#125; 自定义消息最大重试次数，RocketMQ 允许 Consumer 启动的时候设置最大重试次数，重试时间间隔将按照如下策略： 最大重试次数小于等于 16 次，则重试时间间隔同上表描述 最大重试次数大于 16 次，超过 16 次的重试时间间隔均为每次 2 小时 1234Properties properties = new Properties();// 配置对应 Group ID 的最大消息重试次数为 20 次properties.put(PropertyKeyConst.MaxReconsumeTimes,&quot;20&quot;);Consumer consumer = ONSFactory.createConsumer(properties); 注意： 消息最大重试次数的设置对相同 Group ID 下的所有 Consumer 实例有效。例如只对相同 Group ID 下两个 Consumer 实例中的其中一个设置了 MaxReconsumeTimes，那么该配置对两个 Consumer 实例均生效 配置采用覆盖的方式生效，即最后启动的 Consumer 实例会覆盖之前的启动实例的配置 消费者收到消息后，可按照如下方式获取消息的重试次数： 12345678public class MessageListenerImpl implements MessageListener &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; // 获取消息的重试次数 System.out.println(message.getReconsumeTimes()); return Action.CommitMessage; &#125;&#125; 死信队列正常情况下无法被消费的消息称为死信消息（Dead-Letter Message），存储死信消息的特殊队列称为死信队列（Dead-Letter Queue） 当一条消息初次消费失败，消息队列 RocketMQ 会自动进行消息重试，达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时 RocketMQ 不会立刻将消息丢弃，而是将其发送到该消费者对应的死信队列中 死信消息具有以下特性： 不会再被消费者正常消费 有效期与正常消息相同，均为 3 天，3 天后会被自动删除，所以请在死信消息产生后的 3 天内及时处理 死信队列具有以下特性： 一个死信队列对应一个 Group ID， 而不是对应单个消费者实例 如果一个 Group ID 未产生死信消息，消息队列 RocketMQ 不会为其创建相应的死信队列 一个死信队列包含了对应 Group ID 产生的所有死信消息，不论该消息属于哪个 Topic 一条消息进入死信队列，需要排查可疑因素并解决问题后，可以在消息队列 RocketMQ 控制台重新发送该消息，让消费者重新消费一次 高可靠性RocketMQ 消息丢失可能发生在以下三个阶段： 生产阶段：消息在 Producer 发送端创建出来，经过网络传输发送到 Broker 存储端 生产者得到一个成功的响应，就可以认为消息的存储和消息的消费都是可靠的 消息重投机制 存储阶段：消息在 Broker 端存储，如果是主备或者多副本，消息会在这个阶段被复制到其他的节点或者副本上 单点：刷盘机制（同步或异步） 主从：消息同步机制（异步复制或同步双写，主从复制章节详解） 过期删除：操作 CommitLog、ConsumeQueue 文件是基于文件内存映射机制，并且在启动的时候会将所有的文件加载，为了避免内存与磁盘的浪费，让磁盘能够循环利用，防止磁盘不足导致消息无法写入等引入了文件过期删除机制。最终使得磁盘水位保持在一定水平，最终保证新写入消息的可靠存储 消费阶段：Consumer 消费端从 Broker存储端拉取消息，经过网络传输发送到 Consumer 消费端上 消息重试机制来最大限度的保证消息的消费 消费失败的进行消息回退，重试次数过多的消息放入死信队列 推荐文章：https://cdn.modb.pro/db/394751 幂等消费消息队列 RocketMQ 消费者在接收到消息以后，需要根据业务上的唯一 Key 对消息做幂等处理 At least Once 机制保证消息不丢失，但是可能会造成消息重复，RocketMQ 中无法避免消息重复（Exactly-Once），在互联网应用中，尤其在网络不稳定的情况下，几种情况： 发送时消息重复：当一条消息已被成功发送到服务端并完成持久化，此时出现了网络闪断或客户端宕机，导致服务端对客户端应答失败。此时生产者意识到消息发送失败并尝试再次发送消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息 投递时消息重复：消息消费的场景下，消息已投递到消费者并完成业务处理，当客户端给服务端反馈应答的时候网络闪断。为了保证消息至少被消费一次，消息队列 RocketMQ 的服务端将在网络恢复后再次尝试投递之前已被处理过的消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息 负载均衡时消息重复：当消息队列 RocketMQ 的 Broker 或客户端重启、扩容或缩容时，会触发 Rebalance，此时消费者可能会收到重复消息 处理方式： 因为 Message ID 有可能出现冲突（重复）的情况，所以真正安全的幂等处理，不建议以 Message ID 作为处理依据，最好的方式是以业务唯一标识作为幂等处理的关键依据，而业务的唯一标识可以通过消息 Key 进行设置： 123Message message = new Message();message.setKey(&quot;ORDERID_100&quot;);SendResult sendResult = producer.send(message); 订阅方收到消息时可以根据消息的 Key 进行幂等处理： 123456consumer.subscribe(&quot;ons_test&quot;, &quot;*&quot;, new MessageListener() &#123; public Action consume(Message message, ConsumeContext context) &#123; String key = message.getKey() // 根据业务唯一标识的 key 做幂等处理 &#125;&#125;); 流量控制生产者流控，因为 Broker 处理能力达到瓶颈；消费者流控，因为消费能力达到瓶颈 生产者流控： CommitLog 文件被锁时间超过 osPageCacheBusyTimeOutMills 时，参数默认为 1000ms，返回流控 如果开启 transientStorePoolEnable &#x3D;&#x3D; true，且 Broker 为异步刷盘的主机，且 transientStorePool 中资源不足，拒绝当前 send 请求，返回流控 Broker 每隔 10ms 检查 send 请求队列头部请求的等待时间，如果超过 waitTimeMillsInSendQueue，默认 200ms，拒绝当前 send 请求，返回流控。 Broker 通过拒绝 send 请求方式实现流量控制 注意：生产者流控，不会尝试消息重投 消费者流控： 消费者本地缓存消息数超过 pullThresholdForQueue 时，默认 1000 消费者本地缓存消息大小超过 pullThresholdSizeForQueue 时，默认 100MB 消费者本地缓存消息跨度超过 consumeConcurrentlyMaxSpan 时，默认 2000 消费者流控的结果是降低拉取频率 原理解析Namesrv服务启动启动方法NamesrvStartup 类中有 Namesrv 服务的启动方法： 123456789101112131415161718public static void main(String[] args) &#123; // 如果启动时 使用 -c -p 设置参数了，这些参数存储在 args 中 main0(args);&#125;public static NamesrvController main0(String[] args) &#123; try &#123; // 创建 namesrv 控制器，用来初始化 namesrv 启动 namesrv 关闭 namesrv NamesrvController controller = createNamesrvController(args); // 启动 controller start(controller); return controller; &#125; catch (Throwable e) &#123; // 出现异常，停止系统 System.exit(-1); &#125; return null;&#125; NamesrvStartup#createNamesrvController：读取配置信息，初始化 Namesrv 控制器 ServerUtil.parseCmdLine(&quot;mqnamesrv&quot;, args, buildCommandlineOptions(options)，..)：解析启动时的参数信息 namesrvConfig = new NamesrvConfig()：创建 Namesrv 配置对象 private String rocketmqHome：获取 ROCKETMQ_HOME 值 private boolean orderMessageEnable = false：顺序消息功能是否开启 nettyServerConfig = new NettyServerConfig()：Netty 的服务器配置对象 nettyServerConfig.setListenPort(9876)：Namesrv 服务器的监听端口设置为 9876 if (commandLine.hasOption(&#39;c&#39;))：读取命令行 -c 的参数值 in = new BufferedInputStream(new FileInputStream(file))：读取指定目录的配置文件 properties.load(in)：将配置文件信息加载到 properties 对象，相关属性会复写到 Namesrv 配置和 Netty 配置对象 namesrvConfig.setConfigStorePath(file)：将配置文件的路径保存到配置保存字段 if (null == namesrvConfig.getRocketmqHome())：检查 ROCKETMQ_HOME 配置是否是空，是空就报错 lc = (LoggerContext) LoggerFactory.getILoggerFactory()：创建日志对象 controller = new NamesrvController(namesrvConfig, nettyServerConfig)：创建 Namesrv 控制器 NamesrvStartup#start：启动 Namesrv 控制器 boolean initResult = controller.initialize()：初始化方法 Runtime.getRuntime().addShutdownHook(new ShutdownHookThread())：JVM HOOK 平滑关闭的逻辑， 当 JVM 被关闭时，主动调用 controller.shutdown() 方法，让服务器平滑关机 controller.start()：启动服务器 源码解析参考视频：https://space.bilibili.com/457326371 控制器类NamesrvController 用来初始化和启动 Namesrv 服务器 成员变量： 1234private final ScheduledExecutorService scheduledExecutorService;\t// 调度线程池，用来执行定时任务private final RouteInfoManager routeInfoManager; // 管理【路由信息】的对象private RemotingServer remotingServer; // 【网络层】封装对象private BrokerHousekeepingService brokerHousekeepingService; // 用于监听 channel 状态 private ExecutorService remotingExecutor：业务线程池，netty 线程解析报文成 RemotingCommand 对象，然后将该对象交给业务线程池再继续处理 初始化： 1234567891011121314151617181920212223242526272829303132public boolean initialize() &#123; // 加载本地kv配置（我还不明白 kv 配置是啥） this.kvConfigManager.load(); // 创建网络服务器对象，【将 netty 的配置和监听器传入】 // 监听器监听 channel 状态的改变，会向事件队列发起事件，最后交由 service 处理 this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.brokerHousekeepingService); // 【创建业务线程池，默认线程数 8】 this.remotingExecutor = Executors.newFixedThreadPool(nettyServerConfig.getServerWorkerThreads().); // 注册协议处理器（缺省协议处理器），【处理器是 DefaultRequestProcessor】，线程使用的是刚创建的业务的线程池 this.registerProcessor(); // 定时任务1：每 10 秒钟检查 broker 存活状态，将 IDLE 状态的 broker 移除【扫描机制，心跳检测】 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; // 扫描 brokerLiveTable 表，将两小时没有活动的 broker 关闭， // 通过 next.getKey() 获取 broker 的地址，然后【关闭服务器与broker物理节点的 channel】 NamesrvController.this.routeInfoManager.scanNotActiveBroker(); &#125; &#125;, 5, 10, TimeUnit.SECONDS); // 定时任务2：每 10 分钟打印一遍 kv 配置。 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.kvConfigManager.printAllPeriodically(); &#125; &#125;, 1, 10, TimeUnit.MINUTES); return true;&#125; 启动方法： 12345678public void start() throws Exception &#123; // 服务器网络层启动。 this.remotingServer.start(); if (this.fileWatchService != null) &#123; this.fileWatchService.start(); &#125;&#125; 网络通信通信原理RocketMQ 的 RPC 通信采用 Netty 组件作为底层通信库，同样也遵循了 Reactor 多线程模型，NettyRemotingServer 类负责框架的通信服务，同时又在这之上做了一些扩展和优化 RocketMQ 基于 NettyRemotingServer 的 Reactor 多线程模型： 一个 Reactor 主线程（eventLoopGroupBoss）负责监听 TCP 网络连接请求，建立好连接创建 SocketChannel（RocketMQ 会自动根据 OS 的类型选择 NIO 和 Epoll，也可以通过参数配置），并注册到 Selector 上，然后监听真正的网络数据 拿到网络数据交给 Worker 线程池（eventLoopGroupSelector，默认设置为 3），在真正执行业务逻辑之前需要进行 SSL 验证、编解码、空闲检查、网络连接管理，这些工作交给 defaultEventExecutorGroup（默认设置为 8）去做 处理业务操作放在业务线程池中执行，根据 RomotingCommand 的业务请求码 code 去 processorTable 这个本地缓存变量中找到对应的 processor，封装成 task 任务提交给对应的 processor 处理线程池来执行（sendMessageExecutor，以发送消息为例） 从入口到业务逻辑的几个步骤中线程池一直再增加，这跟每一步逻辑复杂性相关，越复杂，需要的并发通道越宽 线程数 线程名 线程具体说明 1 NettyBoss_%d Reactor 主线程 N NettyServerEPOLLSelector_%d_%d Reactor 线程池 M1 NettyServerCodecThread_%d Worker 线程池 M2 RemotingExecutorThread_%d 业务 processor 处理线程池 RocketMQ 的异步通信流程： &#x3D;&#x3D;todo：后期对 Netty 有了更深的认知后会进行扩充，现在暂时 copy 官方文档&#x3D;&#x3D; 官方文档：https://github.com/apache/rocketmq/blob/master/docs/cn/design.md#2-%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6 成员属性NettyRemotingServer 类成员变量： 服务器相关属性： 12345private final ServerBootstrap serverBootstrap; // netty 服务端启动对象private final EventLoopGroup eventLoopGroupSelector; // netty worker 组线程池，【默认 3 个线程】private final EventLoopGroup eventLoopGroupBoss; // netty boss 组线程池，【一般是 1 个线程】private final NettyServerConfig nettyServerConfig; // netty 服务端网络配置private int port = 0; // 服务器绑定的端口 公共线程池：注册处理器时如果未指定线程池，则业务处理使用公共线程池，线程数量默认是 4 1private final ExecutorService publicExecutor; 事件监听器：Nameserver 使用 BrokerHouseKeepingService，Broker 使用 ClientHouseKeepingService 1private final ChannelEventListener channelEventListener; 事件处理线程池：默认是 8 1private DefaultEventExecutorGroup defaultEventExecutorGroup; 定时器：执行循环任务，并且将定时器线程设置为守护线程 1private final Timer timer = new Timer(&quot;ServerHouseKeepingService&quot;, true); 处理器：多个 Channel 共享的处理器 Handler，多个通道使用同一个对象 Netty 配置对象： 12345678910111213141516171819202122232425public class NettyServerConfig implements Cloneable &#123; // 服务端启动时监听的端口号 private int listenPort = 8888; // 【业务线程池】 线程数量 private int serverWorkerThreads = 8; // 根据该值创建 remotingServer 内部的一个 publicExecutor private int serverCallbackExecutorThreads = 0; // netty 【worker】线程数 private int serverSelectorThreads = 3; // 【单向访问】时的并发限制 private int serverOnewaySemaphoreValue = 256; // 【异步访问】时的并发限制 private int serverAsyncSemaphoreValue = 64; // channel 最大的空闲存活时间 默认是 2min private int serverChannelMaxIdleTimeSeconds = 120; // 发送缓冲区大小 65535 private int serverSocketSndBufSize = NettySystemConfig.socketSndbufSize; // 接收缓冲区大小 65535 private int serverSocketRcvBufSize = NettySystemConfig.socketRcvbufSize; // 是否启用 netty 内存池 默认开启 private boolean serverPooledByteBufAllocatorEnable = true; // 默认 linux 会启用 【epoll】 private boolean useEpollNativeSelector = false;&#125; 构造方法： 无监听器构造： 123public NettyRemotingServer(final NettyServerConfig nettyServerConfig) &#123; this(nettyServerConfig, null);&#125; 有参构造方法： 1234567891011121314151617181920212223public NettyRemotingServer(final NettyServerConfig nettyServerConfig, final ChannelEventListener channelEventListener) &#123; // 服务器对客户端主动发起请求时并发限制。【单向请求和异步请求】的并发限制 super(nettyServerConfig.getServerOnewaySemaphoreValue(), nettyServerConfig.getServerAsyncSemaphoreValue());\t// Netty 的启动器，负责组装 netty 组件 this.serverBootstrap = new ServerBootstrap(); // 成员变量的赋值 this.nettyServerConfig = nettyServerConfig; this.channelEventListener = channelEventListener; // 公共线程池的线程数量，默认给的0，这里最终修改为4. int publicThreadNums = nettyServerConfig.getServerCallbackExecutorThreads(); if (publicThreadNums &lt;= 0) &#123; publicThreadNums = 4; &#125; // 创建公共线程池，指定线程工厂，设置线程名称前缀：NettyServerPublicExecutor_[数字] this.publicExecutor = Executors.newFixedThreadPool(publicThreadNums, new ThreadFactory()&#123;.&#125;); // 创建两个 netty 的线程组，一个是boss组，一个是worker组，【linux 系统默认启用 epoll】 if (useEpoll()) &#123;...&#125; else &#123;...&#125;\t// SSL 相关 loadSslContext();&#125; 启动方法核心方法的解析： start()：启动方法，创建 BootStrap，并添加 NettyServerHandler 处理器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public void start() &#123; // Channel Pipeline 内的 handler 使用的线程资源，【线程分配给 handler 处理事件】 this.defaultEventExecutorGroup = new DefaultEventExecutorGroup(...); // 创建通用共享的处理器 handler，【非常重要的 NettyServerHandler】 prepareSharableHandlers(); ServerBootstrap childHandler = // 配置工作组 boss（数量1） 和 worker（数量3） 组 this.serverBootstrap.group(this.eventLoopGroupBoss, this.eventLoopGroupSelector) // 设置服务端 ServerSocketChannel 类型， Linux 用 epoll .channel(useEpoll() ? EpollServerSocketChannel.class : NioServerSocketChannel.class) // 设置服务端 channel 选项 .option(ChannelOption.SO_BACKLOG, 1024) // 客户端 channel 选项 .childOption(ChannelOption.TCP_NODELAY, true) // 设置服务器端口 .localAddress(new InetSocketAddress(this.nettyServerConfig.getListenPort())) // 向 channel pipeline 添加了很多 handler，【包括 NettyServerHandler】 .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;&#125;); // 客户端开启 内存池，使用的内存池是 PooledByteBufAllocator.DEFAULT if (nettyServerConfig.isServerPooledByteBufAllocatorEnable()) &#123; childHandler.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT); &#125; try &#123; // 同步等待建立连接，并绑定端口。 ChannelFuture sync = this.serverBootstrap.bind().sync(); InetSocketAddress addr = (InetSocketAddress) sync.channel().localAddress(); // 将服务器成功绑定的端口号赋值给字段 port。 this.port = addr.getPort(); &#125; catch (InterruptedException e1) &#123;&#125; // housekeepingService 不为空，则创建【网络异常事件处理器】 if (this.channelEventListener != null) &#123; // 线程一直轮询 nettyEvent 状态，根据 CONNECT,CLOSE,IDLE,EXCEPTION 四种事件类型 // CONNECT 不做操作，其余都是回调 onChannelDestroy 【关闭服务器与 Broker 物理节点的 Channel】 this.nettyEventExecutor.start(); &#125; // 提交定时任务，每一秒 执行一次。扫描 responseTable 表，将过期的数据移除 this.timer.scheduleAtFixedRate(new TimerTask() &#123; @Override public void run() &#123; NettyRemotingServer.this.scanResponseTable(); &#125; &#125;, 1000 * 3, 1000);&#125; registerProcessor()：注册业务处理器 123456789101112public void registerProcessor(int requestCode, NettyRequestProcessor processor, ExecutorService executor) &#123; ExecutorService executorThis = executor; if (null == executor) &#123; // 未指定线程池资源，将公共线程池赋值 executorThis = this.publicExecutor; &#125; // pair 对象，第一个参数代表的是处理器， 第二个参数是线程池，默认是公共的线程池 Pair&lt;NettyRequestProcessor, ExecutorService&gt; pair = new Pair&lt;NettyRequestProcessor, ExecutorService&gt;(processor, executorThis); // key 是请求码，value 是 Pair 对象 this.processorTable.put(requestCode, pair);&#125; getProcessorPair()：根据请求码获取对应的处理器和线程池资源 123public Pair&lt;NettyRequestProcessor, ExecutorService&gt; getProcessorPair(int requestCode) &#123; return processorTable.get(requestCode);&#125; 请求方法在 RocketMQ 消息队列中支持通信的方式主要有同步（sync）、异步（async）、单向（oneway）三种，其中单向通信模式相对简单，一般用在发送心跳包场景下，无需关注其 Response 服务器主动向客户端发起请求时，使用三种方法 invokeSync()： 同步调用，服务器需要阻塞等待调用的返回结果 int opaque = request.getOpaque()：获取请求 ID（与请求码不同） responseFuture = new ResponseFuture(...)：创建响应对象，没有回调函数和 Once this.responseTable.put(opaque, responseFuture)：加入到响应映射表中，key 为请求 ID SocketAddress addr = channel.remoteAddress()：获取客户端的地址信息 channel.writeAndFlush(request).addListener(...)：将业务 Command 信息写入通道，业务线程将数据交给 Netty ，Netty 的 IO 线程接管写刷数据的操作，监听器由 IO 线程在写刷后回调 if (f.isSuccess())：写入成功会将响应对象设置为成功状态直接 return，写入失败设置为失败状态 responseTable.remove(opaque)：将当前请求的 responseFuture 从映射表移除 responseFuture.setCause(f.cause())：设置错误的信息 responseFuture.putResponse(null)：响应 Command 设置为 null responseCommand = responseFuture.waitResponse(timeoutMillis)：当前线程设置超时时间挂起，同步等待响应 if (null == responseCommand)：超时或者出现异常，直接报错 return responseCommand：返回响应 Command 信息 invokeAsync()：异步调用，有回调对象，无返回值 boolean acquired = this.semaphoreAsync.tryAcquire(timeoutMillis, TimeUnit.MILLISECONDS)：获取信号量的许可证，信号量用来限制异步请求的数量 if (acquired)：许可证获取失败说明并发较高，会抛出异常 once = new SemaphoreReleaseOnlyOnce(this.semaphoreAsync)：Once 对象封装了释放信号量的操作 costTime = System.currentTimeMillis() - beginStartTime：计算一下耗费的时间，超时不再发起请求 responseFuture = new ResponseFuture()：创建响应对象，包装了回调函数和 Once 对象 this.responseTable.put(opaque, responseFuture)：加入到响应映射表中，key 为请求 ID channel.writeAndFlush(request).addListener(...)：写刷数据 if (f.isSuccess())：写刷成功，设置 responseFuture 发生状态为 true requestFail(opaque)：写入失败，使用 publicExecutor 公共线程池异步执行回调对象的函数 responseFuture.release()：出现异常会释放信号量 invokeOneway()：单向调用，不关注响应结果 request.markOnewayRPC()：设置单向标记，对端检查标记可知该请是单向请求 boolean acquired = this.semaphoreOneway.tryAcquire(timeoutMillis, TimeUnit.MILLISECONDS)：获取信号量的许可证，信号量用来限制单向请求的数量 处理器类协议设计在 Client 和 Server 之间完成一次消息发送时，需要对发送的消息进行一个协议约定，所以自定义 RocketMQ 的消息协议。在 RocketMQ 中，为了高效地在网络中传输消息和对收到的消息读取，就需要对消息进行编解码，RemotingCommand 这个类在消息传输过程中对所有数据内容的封装，不但包含了所有的数据结构，还包含了编码解码操作 Header字段 类型 Request 说明 Response 说明 code int 请求操作码，应答方根据不同的请求码进行不同的处理 应答响应码，0 表示成功，非 0 则表示各种错误 language LanguageCode 请求方实现的语言 应答方实现的语言 version int 请求方程序的版本 应答方程序的版本 opaque int 相当于 requestId，在同一个连接上的不同请求标识码，与响应消息中的相对应 应答不做修改直接返回 flag int 区分是普通 RPC 还是 onewayRPC 的标志 区分是普通 RPC 还是 onewayRPC的标志 remark String 传输自定义文本信息 传输自定义文本信息 extFields HashMap&lt;String, String&gt; 请求自定义扩展信息 响应自定义扩展信息 传输内容主要可以分为以下四部分： 消息长度：总长度，四个字节存储，占用一个 int 类型 序列化类型&amp;消息头长度：同样占用一个 int 类型，第一个字节表示序列化类型，后面三个字节表示消息头长度 消息头数据：经过序列化后的消息头数据 消息主体数据：消息主体的二进制字节数据内容 官方文档：https://github.com/apache/rocketmq/blob/master/docs/cn/design.md 处理方法NettyServerHandler 类用来处理 Channel 上的事件，在 NettyRemotingServer 启动时注册到 Netty 中，可以处理 RemotingCommand 相关的数据，针对某一种类型的请求处理 1234567891011121314151617181920212223class NettyServerHandler extends SimpleChannelInboundHandler&lt;RemotingCommand&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, RemotingCommand msg) throws Exception &#123; // 服务器处理接受到的请求信息 processMessageReceived(ctx, msg); &#125;&#125;public void processMessageReceived(ChannelHandlerContext ctx, RemotingCommand msg) throws Exception &#123; final RemotingCommand cmd = msg; if (cmd != null) &#123; // 根据请求的类型进行处理 switch (cmd.getType()) &#123; case REQUEST_COMMAND:// 客户端发起的请求，走这里 processRequestCommand(ctx, cmd); break; case RESPONSE_COMMAND:// 客户端响应的数据，走这里【当前类本身是服务器类也是客户端类】 processResponseCommand(ctx, cmd); break; default: break; &#125; &#125;&#125; NettyRemotingAbstract#processRequestCommand：处理请求的数据 matched = this.processorTable.get(cmd.getCode())：根据业务请求码获取 Pair 对象，包含处理器和线程池资源 pair = null == matched ? this.defaultRequestProcessor : matched：未找到处理器则使用缺省处理器 int opaque = cmd.getOpaque()：获取请求 ID Runnable run = new Runnable()：创建任务对象，任务在提交到线程池后开始执行 doBeforeRpcHooks()：RPC HOOK 前置处理 callback = new RemotingResponseCallback()：封装响应客户端的逻辑 doAfterRpcHooks()：RPC HOOK 后置处理 if (!cmd.isOnewayRPC())：条件成立说明不是单向请求，需要结果 response.setOpaque(opaque)：将请求 ID 设置到 response response.markResponseType()：设置当前请求是响应 ctx.writeAndFlush(response)： 将响应数据交给 Netty IO 线程，完成数据写和刷 if (pair.getObject1() instanceof AsyncNettyRequestProcessor)：Nameserver 默认使用 DefaultRequestProcessor 处理器，是一个 AsyncNettyRequestProcessor 子类 processor = (AsyncNettyRequestProcessor)pair.getObject1()：获取处理器 processor.asyncProcessRequest(ctx, cmd, callback)：异步调用，首先 processRequest，然后 callback 响应客户端 DefaultRequestProcessor.processRequest：根据业务码处理请求，执行对应的操作 ClientRemotingProcessor.processRequest：处理事务回查消息，或者回执消息，需要消费者回执一条消息给生产者 requestTask = new RequestTask(run, ctx.channel(), cmd)：将任务对象、通道、请求封装成 RequestTask 对象 pair.getObject2().submit(requestTask)：获取处理器对应的线程池，将 task 提交，从 IO 线程切换到业务线程 NettyRemotingAbstract#processResponseCommand：处理响应的数据 int opaque = cmd.getOpaque()：获取请求 ID responseFuture = responseTable.get(opaque)：从响应映射表中获取对应的对象 responseFuture.setResponseCommand(cmd)：设置响应的 Command 对象 responseTable.remove(opaque)：从映射表中移除对象，代表处理完成 if (responseFuture.getInvokeCallback() != null)：包含回调对象，异步执行回调对象 responseFuture.putResponse(cmd)：不包含回调对象，同步调用时，唤醒等待的业务线程 流程：客户端 invokeSync → 服务器的 processRequestCommand → 客户端的 processResponseCommand → 结束 路由信息信息管理RouteInfoManager 类负责管理路由信息，NamesrvController 的构造方法中创建该类的实例对象，管理服务端的路由数据 12345678910111213141516public class RouteInfoManager &#123; // Broker 两个小时不活跃，视为离线，被定时任务删除 private final static long BROKER_CHANNEL_EXPIRED_TIME = 1000 * 60 * 2; // 读写锁，保证线程安全 private final ReadWriteLock lock = new ReentrantReadWriteLock(); // 主题队列数据，一个主题对应多个队列 private final HashMap&lt;String/* topic */, List&lt;QueueData&gt;&gt; topicQueueTable; // Broker 数据列表 private final HashMap&lt;String/* brokerName */, BrokerData&gt; brokerAddrTable; // 集群 private final HashMap&lt;String/* clusterName */, Set&lt;String/* brokerName */&gt;&gt; clusterAddrTable; // Broker 存活信息 private final HashMap&lt;String/* brokerAddr */, BrokerLiveInfo&gt; brokerLiveTable; // 服务过滤 private final HashMap&lt;String/* brokerAddr */, List&lt;String&gt;/* Filter Server */&gt; filterServerTable;&#125; 路由注册DefaultRequestProcessor REGISTER_BROKER 方法解析： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public RemotingCommand registerBroker(ChannelHandlerContext ctx, RemotingCommand request) &#123; // 创建响应请求的对象，设置为响应类型，【先设置响应的状态码时系统错误码】 // 反射创建 RegisterBrokerResponseHeader 对象设置到 response.customHeader 属性中 final RemotingCommand response = RemotingCommand.createResponseCommand(RegisterBrokerResponseHeader.class); // 获取出反射创建的 RegisterBrokerResponseHeader 用户自定义header对象。 final RegisterBrokerResponseHeader responseHeader = (RegisterBrokerResponseHeader) response.readCustomHeader(); // 反射创建 RegisterBrokerRequestHeader 对象，并且将 request.extFields 中的数据写入到该对象中 final RegisterBrokerRequestHeader requestHeader = request.decodeCommandCustomHeader(RegisterBrokerRequestHeader.class); // CRC 校验，计算请求中的 CRC 值和请求头中包含的是否一致 if (!checksum(ctx, request, requestHeader)) &#123; response.setCode(ResponseCode.SYSTEM_ERROR); response.setRemark(&quot;crc32 not match&quot;); return response; &#125; TopicConfigSerializeWrapper topicConfigWrapper; if (request.getBody() != null) &#123; // 【解析请求体 body】，解码出来的数据就是当前机器的主题信息 topicConfigWrapper = TopicConfigSerializeWrapper.decode(request.getBody(), TopicConfigSerializeWrapper.class); &#125; else &#123; topicConfigWrapper = new TopicConfigSerializeWrapper(); topicConfigWrapper.getDataVersion().setCounter(new AtomicLong(0)); topicConfigWrapper.getDataVersion().setTimestamp(0); &#125; // 注册方法 // 参数1 集群、参数2：节点ip地址、参数3：brokerName、参数4：brokerId 注意brokerId=0的节点为主节点 // 参数5：ha节点ip地址、参数6当前节点主题信息、参数7：过滤服务器列表、参数8：当前服务器和客户端通信的channel RegisterBrokerResult result = this.namesrvController.getRouteInfoManager().registerBroker(..); // 将结果信息 写到 responseHeader 中 responseHeader.setHaServerAddr(result.getHaServerAddr()); responseHeader.setMasterAddr(result.getMasterAddr()); // 获取 kv配置，写入 response body 中，【kv 配置是顺序消息相关的】 byte[] jsonValue = this.namesrvController.getKvConfigManager().getKVListByNamespace(NamesrvUtil.NAMESPACE_ORDER_TOPIC); response.setBody(jsonValue); // code 设置为 SUCCESS response.setCode(ResponseCode.SUCCESS); response.setRemark(null);\t// 返回 response ，【返回的 response 由 callback 对象处理】 return response;&#125; RouteInfoManager#registerBroker：注册 Broker 的信息 RegisterBrokerResult result = new RegisterBrokerResult()：返回结果的封装对象 this.lock.writeLock().lockInterruptibly()：加写锁后同步执行 brokerNames = this.clusterAddrTable.get(clusterName)：获取当前集群上的 Broker 名称列表，是空就新建列表 brokerNames.add(brokerName)：将当前 Broker 名字加入到集群列表 brokerData = this.brokerAddrTable.get(brokerName)：获取当前 Broker 的 brokerData，是空就新建放入映射表 brokerAddrsMap = brokerData.getBrokerAddrs()：获取当前 Broker 的物理节点 map 表，进行遍历，如果物理节点角色发生变化（slave → master），先将旧数据从物理节点 map 中移除，然后重写放入，保证节点的唯一性 if (null != topicConfigWrapper &amp;&amp; MixAll.MASTER_ID == brokerId)：Broker 上的 Topic 不为 null，并且当前物理节点是 Broker 上的 master 节点 tcTable = topicConfigWrapper.getTopicConfigTable()：获取当前 Broker 信息中的主题映射表 if (tcTable != null)：映射表不空就加入或者更新到 Namesrv 内 prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr)：添加当前节点的 BrokerLiveInfo ，返回上一次心跳时当前 Broker 节点的存活对象数据。NamesrvController 中的定时任务会扫描映射表 brokerLiveTable 12BrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr, new BrokerLiveInfo( System.currentTimeMillis(),topicConfigWrapper.getDataVersion(), channel,haServerAddr)); if (MixAll.MASTER_ID != brokerId)：当前 Broker 不是 master 节点，获取主节点的信息设置到结果对象 this.lock.writeLock().unlock()：释放写锁 BrokerMappedFile成员属性MappedFile 类是最基础的存储类，继承自 ReferenceResource 类，用来保证线程安全 MappedFile 类成员变量： 内存相关： 123public static final int OS_PAGE_SIZE = 1024 * 4;// 内存页大小：默认是 4kprivate AtomicLong TOTAL_MAPPED_VIRTUAL_MEMORY;\t// 当前进程下所有的 mappedFile 占用的总虚拟内存大小private AtomicInteger TOTAL_MAPPED_FILES; // 当前进程下所有的 mappedFile 个数 数据位点： 1234protected final AtomicInteger wrotePosition;\t// 当前 mappedFile 的数据写入点protected final AtomicInteger committedPosition;// 当前 mappedFile 的数据提交点private final AtomicInteger flushedPosition;\t// 数据落盘位点，在这之前的数据是持久化的安全数据 // flushedPosition-wrotePosition 之间的数据属于脏页 文件相关：CL 是 CommitLog，CQ 是 ConsumeQueue 123private String fileName;\t// 文件名称，CL和CQ文件名是【第一条消息的物理偏移量】，索引文件是【年月日时分秒】private long fileFromOffset;// 文件名转long，代表该对象的【起始偏移量】\tprivate File file; // 文件对象 MF 中以物理偏移量作为文件名，可以更好的寻址和进行判断 内存映射： 12protected FileChannel fileChannel; // 文件通道private MappedByteBuffer mappedByteBuffer;\t// 内存映射缓冲区，访问虚拟内存 ReferenceResource 类成员变量： 引用数量：当 refCount &lt;= 0 时，表示该资源可以释放了，没有任何其他程序依赖它了，用原子类保证线程安全 1protected final AtomicLong refCount = new AtomicLong(1);\t// 初始值为 1 存活状态：表示资源的存活状态 1protected volatile boolean available = true; 是否清理：默认值 false，当执行完子类对象的 cleanup() 清理方法后，该值置为 true ，表示资源已经全部释放 1protected volatile boolean cleanupOver = false; 第一次关闭资源的时间：用来记录超时时间 1private volatile long firstShutdownTimestamp = 0; 成员方法MappedFile 类核心方法： appendMessage()：提供上层向内存映射中追加消息的方法，消息如何追加由 AppendMessageCallback 控制 12// 参数一：消息 参数二：追加消息回调public AppendMessageResult appendMessage(MessageExtBrokerInner msg, AppendMessageCallback cb) 12// 将字节数组写入到文件通道public boolean appendMessage(final byte[] data) flush()：刷盘接口，参数 flushLeastPages 代表刷盘的最小页数 ，等于 0 时属于强制刷盘；&gt; 0 时需要脏页（计算方法在数据位点）达到该值才进行物理刷盘；文件写满时强制刷盘 1public int flush(final int flushLeastPages) selectMappedBuffer()：该方法以 pos 为开始位点 ，到有效数据为止，创建一个切片 ByteBuffer 作为数据副本，供业务访问数据 1public SelectMappedBufferResult selectMappedBuffer(int pos) destroy()：销毁映射文件对象，并删除关联的系统文件，参数是强制关闭资源的时间 1public boolean destroy(final long intervalForcibly) cleanup()：释放堆外内存，更新总虚拟内存和总内存映射文件数 1public boolean cleanup(final long currentRef) warmMappedFile()：内存预热，当要新建的 MappedFile 对象大于 1g 时执行该方法。mappedByteBuffer 已经通过mmap映射，此时操作系统中只是记录了该文件和该 Buffer 的映射关系，而并没有映射到物理内存中，对该 MappedFile 的每个 Page Cache 进行写入一个字节分配内存，将映射文件全部加载到内存 1public void warmMappedFile(FlushDiskType type, int pages) mlock()：锁住指定的内存区域避免被操作系统调到 swap 空间，减少了缺页异常的产生 1public void mlock() swap space 是磁盘上的一块区域，可以是一个分区或者一个文件或者是组合。当系统物理内存不足时，Linux 会将内存中不常访问的数据保存到 swap 区域上，这样系统就可以有更多的物理内存为各个进程服务，而当系统需要访问 swap 上存储的内容时，需要通过缺页中断将 swap 上的数据加载到内存中 ReferenceResource 类核心方法： hold()：增加引用记数 refCount，方法加锁 1public synchronized boolean hold() shutdown()：关闭资源，参数代表强制关闭资源的时间间隔 12// 系统当前时间 - firstShutdownTimestamp 时间 &gt; intervalForcibly 进行【强制关闭】public void shutdown(final long intervalForcibly) release()：引用计数减 1，当 refCount 为 0 时，调用子类的 cleanup 方法 1public void release() MapQueue成员属性MappedFileQueue 用来管理 MappedFile 文件 成员变量： 管理目录：CommitLog 是 ../store/commitlog， ConsumeQueue 是 ../store/xxx_topic/0 1private final String storePath; 文件属性： 12private final int mappedFileSize;\t// 目录下每个文件大小，CL文件默认 1g，CQ文件 默认 600w字节private final CopyOnWriteArrayList&lt;MappedFile&gt; mappedFiles;\t//目录下的每个 mappedFile 都加入该集合 数据位点： 12private long flushedWhere = 0; // 目录的刷盘位点，值为 mf.fileName + mf.wrotePositionprivate long committedWhere = 0;\t// 目录的提交位点 消息存储： 1private volatile long storeTimestamp = 0;\t// 当前目录下最后一条 msg 的存储时间 创建服务：新建 MappedFile 实例，继承自 ServiceThread 是一个任务对象，run 方法用来创建实例 1private final AllocateMappedFileService allocateMappedFileService; 成员方法核心方法： load()：Broker 启动时，加载本地磁盘数据，该方法读取 storePath 目录下的文件，创建 MappedFile 对象放入集合内 1public boolean load() getLastMappedFile()：获取当前正在顺序写入的 MappedFile 对象，如果最后一个 MappedFile 写满了，或者不存在 MappedFile 对象，则创建新的 MappedFile 12// 参数一：文件起始偏移量；参数二：当list为空时，是否新建 MappedFilepublic MappedFile getLastMappedFile(final long startOffset, boolean needCreate) flush()：根据 flushedWhere 属性查找合适的 MappedFile，调用该 MappedFile 的落盘方法，并更新全局的 flushedWhere 12//参数：0 表示强制刷新， &gt; 0 脏页数据必须达到 flushLeastPages 才刷新public boolean flush(final int flushLeastPages) findMappedFileByOffset()：根据偏移量查询对象 1public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) deleteExpiredFileByTime()：CL 删除过期文件，根据文件的保留时长决定是否删除 12// 参数一：过期时间； 参数二：删除两个文件之间的时间间隔； 参数三：mf.destory传递的参数； 参数四：true 强制删除public int deleteExpiredFileByTime(final long expiredTime,final int deleteFilesInterval, final long intervalForcibly, final boolean cleanImmediately) deleteExpiredFileByOffset()：CQ 删除过期文件，遍历每个 MF 文件，获取当前文件最后一个数据单元的物理偏移量，小于 offset 说明当前 MF 文件内都是过期数据 123// 参数一：consumeLog 目录下最小物理偏移量，就是第一条消息的 offset； // 参数二：ConsumerQueue 文件内每个数据单元固定大小public int deleteExpiredFileByOffset(long offset, int unitSize) CommitLog成员属性成员变量： 魔数： 12public final static int MESSAGE_MAGIC_CODE = -626843481;\t// 消息的第一个字段是大小，第二个字段就是魔数\tprotected final static int BLANK_MAGIC_CODE = -875286124;\t// 文件尾消息的魔法值 MappedFileQueue：用于管理 ../store/commitlog 目录下的文件 1protected final MappedFileQueue mappedFileQueue; 存储服务： 12protected final DefaultMessageStore defaultMessageStore;\t// 存储模块对象，上层服务private final FlushCommitLogService flushCommitLogService;\t// 刷盘服务，默认实现是异步刷盘 回调器：控制消息的哪些字段添加到 MappedFile 1private final AppendMessageCallback appendMessageCallback; 队列偏移量字典表：key 是主题队列 id，value 是偏移量 1protected HashMap&lt;String, Long&gt; topicQueueTable = new HashMap&lt;String, Long&gt;(1024); 锁相关： 12private volatile long beginTimeInLock = 0; // 写数据时加锁的开始时间protected final PutMessageLock putMessageLock; // 写锁，两个实现类：自旋锁和重入锁 因为发送消息是需要持久化的，在 Broker 端持久化时会获取该锁，保证发送的消息的线程安全 构造方法： 有参构造： 1234567891011public CommitLog(final DefaultMessageStore defaultMessageStore) &#123; // 创建 MappedFileQueue 对象 // 参数1：../store/commitlog； 参数2：【1g】； 参数3：allocateMappedFileService this.mappedFileQueue = new MappedFileQueue(...); // 默认 异步刷盘，创建这个对象 this.flushCommitLogService = new FlushRealTimeService(); // 控制消息哪些字段追加到 mappedFile，【消息最大是 4M】 this.appendMessageCallback = new DefaultAppendMessageCallback(...); // 默认使用自旋锁 this.putMessageLock = ...;&#125; 成员方法CommitLog 类核心方法： start()：会启动刷盘服务 1public void start() shutdown()：关闭刷盘服务 1public void shutdown() load()：加载 CommitLog 目录下的文件 1public boolean load() getMessage()：根据 offset 查询单条信息，返回的结果对象内部封装了一个 ByteBuffer，该 Buffer 表示 [offset, offset + size] 区间的 MappedFile 的数据 1public SelectMappedBufferResult getMessage(final long offset, final int size) deleteExpiredFile()：删除过期文件，方法由 DefaultMessageStore 的定时任务调用 1public int deleteExpiredFile() asyncPutMessage()：存储消息 1public CompletableFuture&lt;PutMessageResult&gt; asyncPutMessage(final MessageExtBrokerInner msg) msg.setStoreTimestamp(System.currentTimeMillis())：设置存储时间，后面获取到写锁后这个事件会重写 msg.setBodyCRC(UtilAll.crc32(msg.getBody()))：获取消息的 CRC 值 topic、queueId：获取主题和队列 ID if (msg.getDelayTimeLevel() &gt; 0) ：获取消息的延迟级别，这里是延迟消息实现的关键 topic = TopicValidator.RMQ_SYS_SCHEDULE_TOPIC：修改消息的主题为 SCHEDULE_TOPIC_XXXX queueId = ScheduleMessageService.delayLevel2QueueId()：队列 ID 为延迟级别 -1 MessageAccessor.putProperty：将原来的消息主题和 ID 存入消息的属性 REAL_TOPIC 中 mappedFile = this.mappedFileQueue.getLastMappedFile()：获取当前顺序写的 MappedFile 对象 putMessageLock.lock()：获取写锁 msg.setStoreTimestamp(beginLockTimestamp)：设置消息的存储时间为获取锁的时间 if (null == mappedFile || mappedFile.isFull())：文件写满了创建新的 MF 对象 result = mappedFile.appendMessage(msg, this.appendMessageCallback)：消息追加，核心逻辑在回调器类 putMessageLock.unlock()：释放写锁 this.defaultMessageStore.unlockMappedFile(..)：将 MappedByteBuffer 从 lock 切换为 unlock 状态 putMessageResult = new PutMessageResult(PutMessageStatus.PUT_OK, result)：结果封装 flushResultFuture = submitFlushRequest(result, msg)：唤醒刷盘线程 replicaResultFuture = submitReplicaRequest(result, msg)：HA 消息同步 recoverNormally()：正常关机时的恢复方法，存储模块启动时先恢复所有的 ConsumeQueue 数据，再恢复 CommitLog 数据 12// 参数表示恢复阶段 ConsumeQueue 中已知的最大的消息 offsetpublic void recoverNormally(long maxPhyOffsetOfConsumeQueue) int index = mappedFiles.size() - 3：从倒数第三个 file 开始向后恢复 dispatchRequest = this.checkMessageAndReturnSize()：每次从切片内解析出一条 msg 封装成 DispatchRequest 对象 size = dispatchRequest.getMsgSize()：获取消息的大小，检查 DispatchRequest 对象的状态 情况 1：正常数据，则 mappedFileOffset += size 情况 2：文件尾数据，处理下一个文件，mappedFileOffset 置为 0，magic_code 表示文件尾 processOffset += mappedFileOffset：计算出正确的数据存储位点，并设置 MappedFileQueue 的目录刷盘位点 this.mappedFileQueue.truncateDirtyFiles(processOffset)：调整 MFQ 中文件的刷盘位点 if (maxPhyOffsetOfConsumeQueue &gt;= processOffset)：删除冗余数据，将超过全局位点的 CQ 下的文件删除，将包含全局位点的 CQ 下的文件重新定位 recoverAbnormally()：异常关机时的恢复方法 1public void recoverAbnormally(long maxPhyOffsetOfConsumeQueue) int index = mappedFiles.size() - 1：从尾部开始遍历 MFQ，验证 MF 的第一条消息，找到第一个验证通过的文件对象 dispatchRequest = this.checkMessageAndReturnSize()：每次解析出一条 msg 封装成 DispatchRequest 对象 this.defaultMessageStore.doDispatch(dispatchRequest)：重建 ConsumerQueue 和 Index，避免上次异常停机导致 CQ 和 Index 与 CommitLog 不对齐 剩余逻辑与正常关机的恢复方法相似 服务线程AppendMessageCallback 消息追加服务实现类为 DefaultAppendMessageCallback doAppend()： 1public AppendMessageResult doAppend() long wroteOffset = fileFromOffset + byteBuffer.position()：消息写入的位置，物理偏移量 phyOffset String msgId：消息 ID，规则是客户端 IP + 消息偏移量 phyOffset byte[] topicData：序列化消息，将消息的字段压入到 msgStoreItemMemory 这个 Buffer 中 byteBuffer.put(this.msgStoreItemMemory.array(), 0, msgLen)：将 msgStoreItemMemory 中的数据写入 MF 对象的内存映射的 Buffer 中，数据还没落盘 AppendMessageResult result：构造结果对象，包括存储位点、是否成功、队列偏移量等信息 CommitLog.this.topicQueueTable.put(key, ++queueOffset)：更新队列偏移量 FlushRealTimeService 刷盘 CL 数据，默认是异步刷盘类 FlushRealTimeService run()：运行方法 1public void run() while (!this.isStopped())：stopped为 true 才跳出循环 boolean flushCommitLogTimed：控制线程的休眠方式，默认是 false，使用 CountDownLatch.await() 休眠，设置为 true 时使用 Thread.sleep() 休眠 int interval：获取配置中的刷盘时间间隔 int flushPhysicQueueLeastPages：获取最小刷盘页数，默认是 4 页，脏页达到指定页数才刷盘 int flushPhysicQueueThoroughInterval：获取强制刷盘周期，默认是 10 秒，达到周期后强制刷盘，不考虑脏页 if (flushCommitLogTimed)：休眠逻辑，避免 CPU 占用太长时间，导致无法执行其他更紧急的任务 CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages)：刷盘 for (int i = 0; i &lt; RETRY_TIMES_OVER &amp;&amp; !result; i++)：stopped 停止标记为 true 时，需要确保所有的数据都已经刷盘，所以此处尝试 10 次强制刷盘， result = CommitLog.this.mappedFileQueue.flush(0)：强制刷盘 同步刷盘类 GroupCommitService run()：运行方法 1public void run() while (!this.isStopped())：stopped为 true 才跳出循环 this.waitForRunning(10)：线程休眠 10 毫秒，最后调用 onWaitEnd() 进行请求的交换 swapRequests() this.doCommit()：做提交逻辑 if (!this.requestsRead.isEmpty()) ：读请求集合不为空 for (GroupCommitRequest req : this.requestsRead)：遍历所有的读请求，请求中的属性： private final long nextOffset：本条消息存储之后，下一条消息开始的 offset private CompletableFuture&lt;PutMessageStatus&gt; flushOKFuture：Future 对象 boolean flushOK = ...：当前请求关注的数据是否全部落盘，落盘成功唤醒消费者线程 for (int i = 0; i &lt; 2 &amp;&amp; !flushOK; i++)：尝试进行两次强制刷盘，保证刷盘成功 CommitLog.this.mappedFileQueue.flush(0)：强制刷盘 req.wakeupCustomer(flushOK ? ...)：设置 Future 结果，在 Future 阻塞的线程在这里会被唤醒 this.requestsRead.clear()：清理 reqeustsRead 列表，方便交换时成为 requestsWrite 使用 else：读请求集合为空 CommitLog.this.mappedFileQueue.flush(0)：强制刷盘 this.swapRequests()：交换读写请求 this.doCommit()：交换后做一次提交 ConsQueue成员属性ConsumerQueue 是消息消费队列，存储消息在 CommitLog 的索引，便于快速定位消息 成员变量： 数据单元：ConsumerQueueData 数据单元的固定大小是 20 字节，默认申请 20 字节的缓冲区 1public static final int CQ_STORE_UNIT_SIZE = 20; 文件管理： 123private final MappedFileQueue mappedFileQueue;\t// 文件管理器，管理 CQ 目录下的文件private final String storePath; // 目录，比如../store/consumequeue/xxx_topic/0private final int mappedFileSize; // 每一个 CQ 存储文件大小，默认 20 * 30w = 600w byte 存储主模块：上层的对象 1private final DefaultMessageStore defaultMessageStore; 消息属性： 12345private final String topic; // CQ 主题private final int queueId; // CQ 队列，每一个队列都有一个 ConsumeQueue 对象进行管理private final ByteBuffer byteBufferIndex;\t// 临时缓冲区，插新的 CQData 时使用private long maxPhysicOffset = -1; // 当前ConsumeQueue内存储的最大消息物理偏移量private volatile long minLogicOffset = 0;\t// 当前ConsumeQueue内存储的最小消息物理偏移量 构造方法： 有参构造： 1234public ConsumeQueue() &#123; // 申请了一个 20 字节大小的 临时缓冲区 this.byteBufferIndex = ByteBuffer.allocate(CQ_STORE_UNIT_SIZE);&#125; 成员方法ConsumeQueue 启动阶段方法： load()：第一步，加载 storePath 目录下的文件，初始化 MappedFileQueue recover()：第二步，恢复 ConsumeQueue 数据 从倒数第三个 MF 文件开始向后遍历，依次读取 MF 中 20 个字节的 CQData 数据，检查 offset 和 size 是否是有效数据 找到无效的 CQData 的位点，该位点就是 CQ 的刷盘点和数据顺序写入点 删除无效的 MF 文件，调整当前顺序写的 MF 文件的数据位点 其他方法： truncateDirtyLogicFiles()：CommitLog 恢复阶段调用，将 ConsumeQueue 有效数据文件与 CommitLog 对齐，将超出部分的数据文删除掉，并调整当前文件的数据位点。Broker 启动阶段先恢复 CQ 的数据，再恢复 CL 数据，但是数据要以 CL 为基准 12// 参数是最大消息物理偏移量public void truncateDirtyLogicFiles(long phyOffet) flush()：刷盘，调用 MFQ 的刷盘方法 1public boolean flush(final int flushLeastPages) deleteExpiredFile()：删除过期文件，将小于 offset 的所有 MF 文件删除，offset 是 CommitLog 目录下最小的物理偏移量，小于该值的 CL 文件已经没有了，所以 CQ 也没有存在的必要 1public int deleteExpiredFile(long offset) putMessagePositionInfoWrapper()：向 CQ 中追加 CQData 数据，由存储主模块 DefaultMessageStore 内部的异步线程调用，负责构建 ConsumeQueue 文件和 Index 文件的，该线程会持续关注 CommitLog 文件，当 CommitLog 文件内有新数据写入，就读出来封装成 DispatchRequest 对象，转发给 ConsumeQueue 或者 IndexService 1public void putMessagePositionInfoWrapper(DispatchRequest request) getIndexBuffer()：转换 startIndex 为 offset，获取包含该 offset 的 MappedFile 文件，读取 [offset%maxSize, mfPos] 范围的数据，包装成结果对象返回 1public SelectMappedBufferResult getIndexBuffer(final long startIndex) IndexFile成员属性IndexFile 类成员属性 哈希： 12private static int hashSlotSize = 4;\t// 每个 hash 桶的大小是 4 字节，【用来存放索引的编号】private final int hashSlotNum; // hash 桶的个数，默认 500 万 索引： 1234private static int indexSize = 20; // 每个 index 条目的大小是 20 字节private static int invalidIndex = 0;\t// 无效索引编号：0 特殊值private final int indexNum; // 默认值：2000wprivate final IndexHeader indexHeader;\t// 索引头 映射： 123private final MappedFile mappedFile; // 【索引文件使用的 MF 文件】private final FileChannel fileChannel; // 文件通道private final MappedByteBuffer mappedByteBuffer;// 从 MF 中获取的内存映射缓冲区 构造方法： 有参构造 12345678910111213// endPhyOffset 上个索引文件 最后一条消息的 物理偏移量// endTimestamp 上个索引文件 最后一条消息的 存储时间public IndexFile(final String fileName, final int hashSlotNum, final int indexNum, final long endPhyOffset, final long endTimestamp) throws IOException &#123; // 文件大小 40 + 500w * 4 + 2000w * 20 int fileTotalSize = IndexHeader.INDEX_HEADER_SIZE + (hashSlotNum * hashSlotSize) + (indexNum * indexSize); // 创建 mf 对象，会在disk上创建文件 this.mappedFile = new MappedFile(fileName, fileTotalSize); // 创建 索引头对象，传递 索引文件mf 的切片数据 this.indexHeader = new IndexHeader(byteBuffer);\t//...&#125; 成员方法IndexFile 类方法 load()：加载 IndexHeader 1public void load() flush()：MappedByteBuffer 内的数据强制落盘 1public void flush() isWriteFull()：检查当前的 IndexFile 已写索引数是否 &gt;&#x3D; indexNum，达到该值则当前 IndexFile 不能继续追加 IndexData 了 1public boolean isWriteFull() destroy()：删除文件时使用的方法 1public boolean destroy(final long intervalForcibly) putKey()：添加索引数据，解决哈希冲突使用头插法 123// 参数一：消息的 key，uniq_key 或者 keys=&quot;aaa bbb ccc&quot; 会分别为 aaa bbb ccc 创建索引// 参数二：消息的物理偏移量； 参数三：消息存储时间public boolean putKey(final String key, final long phyOffset, final long storeTimestamp) int slotPos = keyHash % this.hashSlotNum：对 key 计算哈希后，取模得到对应的哈希槽 slot 下标，然后计算出哈希槽的存储位置 absSlotPos int slotValue = this.mappedByteBuffer.getInt(absSlotPos)：获取槽中的值，如果是无效值说明没有哈希冲突 timeDiff = timeDiff / 1000：计算当前 msg 存储时间减去索引文件内第一条消息存储时间的差值，转化为秒进行存储 int absIndexPos：计算当前索引数据存储的位置，开始填充索引数据到对应的位置 this.mappedByteBuffer.putInt(absIndexPos + 4 + 8 + 4, slotValue)：hash 桶的原值，头插法 this.mappedByteBuffer.putInt(absSlotPos, this.indexHeader...)：在 slot 放入当前索引的索引编号 if (this.indexHeader.getIndexCount() &lt;= 1)：索引文件插入的第一条数据，需要设置起始偏移量和存储时间 if (invalidIndex == slotValue)：没有哈希冲突，说明占用了一个新的 hash slot this.indexHeader：设置索引头的相关属性 selectPhyOffset()：从索引文件查询消息的物理偏移量 12// 参数一：查询结果全部放到该list内； 参数二：查询key； 参数三：结果最大数限制； 参数四五：时间范围public void selectPhyOffset(final List&lt;Long&gt; phyOffsets, final String key, final int maxNum,final long begin, final long end, boolean lock) if (this.mappedFile.hold())： MF 的引用记数 +1，查询期间 MF 资源不能被释放 int slotValue = this.mappedByteBuffer.getInt(absSlotPos)：获取槽中的值，可能是无效值或者索引编号，如果是无效值说明查询未命中 int absIndexPos：计算出索引编号对应索引数据的开始位点 this.mappedByteBuffer：读取索引数据 long timeRead = this.indexHeader.getBeginTimestamp() + timeDiff：计算出准确的存储时间 boolean timeMatched = (timeRead &gt;= begin) &amp;&amp; (timeRead &lt;= end)：时间范围的匹配 phyOffsets.add(phyOffsetRead)：将命中的消息索引的消息偏移量加入到 list 集合中 nextIndexToRead = prevIndexRead：遍历前驱节点 IndexServ成员属性IndexService 类用来管理 IndexFile 文件 成员变量： 存储主模块： 1private final DefaultMessageStore defaultMessageStore; 索引文件存储目录：../store/index 1private final String storePath; 索引对象集合：目录下的每个文件都有一个 IndexFile 对象 1private final ArrayList&lt;IndexFile&gt; indexFileList = new ArrayList&lt;IndexFile&gt;(); 索引文件： 12private final int hashSlotNum; // 每个索引文件包含的 哈希桶数量 ：500wprivate final int indexNum; // 每个索引文件包含的 索引条目数量 ：2000w 成员方法 load()：加载 storePath 目录下的文件，为每个文件创建一个 IndexFile 实例对象，并加载 IndexHeader 信息 1public boolean load(final boolean lastExitOK) deleteExpiredFile()：删除过期索引文件 12// 参数 offset 表示 CommitLog 内最早的消息的 phyOffsetpublic void deleteExpiredFile(long offset) this.readWriteLock.readLock().lock()：加锁判断 long endPhyOffset = this.indexFileList.get(0).getEndPhyOffset()：获取目录中第一个文件的结束偏移量 if (endPhyOffset &lt; offset)：索引目录内存在过期的索引文件，并且当前的 IndexFile 都是过期的数据 for (int i = 0; i &lt; (files.length - 1); i++)：遍历文件列表，删除过期的文件 buildIndex()：存储主模块 DefaultMessageStore 内部的异步线程调用，构建 Index 数据 1public void buildIndex(DispatchRequest req) indexFile = retryGetAndCreateIndexFile()：获取或者创建顺序写的索引文件对象 buildKey(topic, req.getUniqKey())：构建索引 key，topic + # + uniqKey indexFile = putKey()：插入索引文件 if (keys != null &amp;&amp; keys.length() &gt; 0)：消息存在自定义索引 keys for (int i = 0; i &lt; keyset.length; i++)：遍历每个索引，为每个 key 调用一次 putKey getAndCreateLastIndexFile()：获取当前顺序写的 IndexFile，没有就创建 1public IndexFile getAndCreateLastIndexFile() HAServiceHAServiceServiceHAService 类成员变量： 主节点属性： 123456// master 节点当前有多少个 slave 节点与其进行数据同步private final AtomicInteger connectionCount = new AtomicInteger(0);// master 节点会给每个发起连接的 slave 节点的通道创建一个 HAConnection，【控制 master 端向 slave 端传输数据】private final List&lt;HAConnection&gt; connectionList = new LinkedList&lt;&gt;();// master 向 slave 节点推送的最大的 offset，表示数据同步的进度private final AtomicLong push2SlaveMaxOffset = new AtomicLong(0) 内部类属性： 123456// 封装了绑定服务器指定端口，监听 slave 的连接的逻辑，没有使用 Netty，使用了原生态的 NIO 去做private final AcceptSocketService acceptSocketService;// 控制生产者线程阻塞等待的逻辑private final GroupTransferService groupTransferService;// slave 节点的客户端对象，【slave 端才会正常运行该实例】private final HAClient haClient; 线程通信对象： 1private final WaitNotifyObject waitNotifyObject = new WaitNotifyObject() 成员方法： start()：启动高可用服务 12345678910public void start() throws Exception &#123; // 监听从节点 this.acceptSocketService.beginAccept(); // 启动监听服务 this.acceptSocketService.start(); // 启动转移服务 this.groupTransferService.start(); // 启动从节点客户端实例 this.haClient.start();&#125; AcceptAcceptSocketService 类用于监听从节点的连接，创建 HAConnection 连接对象 成员变量： 端口信息：Master 绑定监听的端口信息 1private final SocketAddress socketAddressListen; 服务端通道： 1private ServerSocketChannel serverSocketChannel; 多路复用器： 1private Selector selector; 成员方法： beginAccept()：开始监听连接，NIO 标准模板 1public void beginAccept() run()：服务启动 1public void run() this.selector.select(1000)：多路复用器阻塞获取就绪的通道，最多等待 1 秒钟 Set&lt;SelectionKey&gt; selected = this.selector.selectedKeys()：获取选择器中所有注册的通道中已经就绪好的事件 for (SelectionKey k : selected)：遍历所有就绪的事件 if ((k.readyOps() &amp; SelectionKey.OP_ACCEPT) != 0)：说明 OP_ACCEPT 事件就绪 SocketChannel sc = ((ServerSocketChannel) k.channel()).accept()：获取到客户端连接的通道 HAConnection conn = new HAConnection(HAService.this, sc)：为每个连接 master 服务器的 slave 创建连接对象 conn.start()：启动 HAConnection 对象，内部启动两个服务为读数据服务、写数据服务 HAService.this.addConnection(conn)：加入到 HAConnection 集合内 GroupGroupTransferService 用来控制数据同步 成员方法： doWaitTransfer()：等待主从数据同步 1private void doWaitTransfer() if (!this.requestsRead.isEmpty())：读请求不为空 boolean transferOK = HAService.this.push2SlaveMaxOffset... &gt;= req.getNextOffset()：主从同步是否完成 req.wakeupCustomer(transferOK ? ...)：唤醒消费者 this.requestsRead.clear()：清空读请求 swapRequests()：交换读写请求 1private void swapRequests() HAClient成员属性HAClient 是 slave 端运行的代码，用于和 master 服务器建立长连接，上报本地同步进度，消费服务器发来的 msg 数据 成员变量： 缓冲区： 123private static final int READ_MAX_BUFFER_SIZE = 1024 * 1024 * 4;\t// 默认大小：4 MBprivate ByteBuffer byteBufferRead = ByteBuffer.allocate(READ_MAX_BUFFER_SIZE);private ByteBuffer byteBufferBackup = ByteBuffer.allocate(READ_MAX_BUFFER_SIZE); 主节点地址：格式为 ip:port 1private final AtomicReference&lt;String&gt; masterAddress = new AtomicReference&lt;&gt;() NIO 属性： 123private final ByteBuffer reportOffset;\t// 通信使用NIO，所以消息使用块传输，上报 slave offset 使用private SocketChannel socketChannel;\t// 客户端与 master 的会话通道 private Selector selector; // 多路复用器 通信时间：上次会话通信时间，用于控制 socketChannel 是否关闭的 1private long lastWriteTimestamp = System.currentTimeMillis(); 进度信息： 12private long currentReportedOffset = 0;\t// slave 当前的进度信息private int dispatchPosition = 0; // 控制 byteBufferRead position 指针 成员方法 run()：启动方法 1public void run() if (this.connectMaster())：连接主节点，连接失败会休眠 5 秒 String addr = this.masterAddress.get()：获取 master 暴露的 HA 地址端口信息 this.socketChannel = RemotingUtil.connect(socketAddress)：建立连接 this.socketChannel.register(this.selector, SelectionKey.OP_READ)：注册到多路复用器，关注读事件 this.currentReportedOffset： 初始化上报进度字段为 slave 的 maxPhyOffset if (this.isTimeToReportOffset())：slave 每 5 秒会上报一次 slave 端的同步进度信息给 master boolean result = this.reportSlaveMaxOffset()：上报同步信息，上报失败关闭连接 this.selector.select(1000)：多路复用器阻塞获取就绪的通道，最多等待 1 秒钟，获取到就绪事件或者超时后结束 boolean ok = this.processReadEvent()：处理读事件 if (!reportSlaveMaxOffsetPlus())：检查是否重新上报同步进度 reportSlaveMaxOffset()：上报 slave 同步进度 1private boolean reportSlaveMaxOffset(final long maxOffset) 首先向缓冲区写入 slave 端最大偏移量，写完以后切换为指定置为初始状态 for (int i = 0; i &lt; 3 &amp;&amp; this.reportOffset.hasRemaining(); i++)：尝试三次写数据 this.socketChannel.write(this.reportOffset)：写数据 return !this.reportOffset.hasRemaining()：写成功之后 pos &#x3D; limit processReadEvent()：处理 master 发送给 slave 数据，返回 true 表示处理成功 false 表示 Socket 处于半关闭状态，需要上层重建 haClient 1private boolean processReadEvent() int readSizeZeroTimes = 0：控制 while 循环的一个条件变量，当值为 3 时跳出循环 while (this.byteBufferRead.hasRemaining())：byteBufferRead 有空间可以去 Socket 读缓冲区加载数据 int readSize = this.socketChannel.read(this.byteBufferRead)：从通道读数据 if (readSize &gt; 0)：加载成功，有新数据 readSizeZeroTimes = 0：置为 0 boolean result = this.dispatchReadRequest()：处理数据的核心逻辑 else if (readSize == 0) ：连续无新数据 3 次，跳出循环 else：readSize &#x3D; -1 就表示 Socket 处于半关闭状态，对端已经关闭了 dispatchReadRequest()：处理数据的核心逻辑，master 与 slave 传输的数据格式 &#123;[phyOffset][size][data...]&#125;，phyOffset 表示数据区间的开始偏移量，data 代表数据块，最大 32kb，可能包含多条消息的数据 1private boolean dispatchReadRequest() final int msgHeaderSize = 8 + 4：协议头大小 12 int readSocketPos = this.byteBufferRead.position()：记录缓冲区处理数据前的 pos 位点，用于恢复指针 int diff = ...：当前 byteBufferRead 还剩多少 byte 未处理，每处理一条帧数据都会更新 dispatchPosition if (diff &gt;= msgHeaderSize)：缓冲区还有完整的协议头 header 数据 if (diff &gt;= (msgHeaderSize + bodySize))：说明缓冲区内是包含当前帧的全部数据的，开始处理帧数据 HAService...appendToCommitLog(masterPhyOffset, bodyData)：存储数据到 CommitLog，并构建 Index 和 CQ this.byteBufferRead.position(readSocketPos)：恢复 byteBufferRead 的 pos 指针 this.dispatchPosition += msgHeaderSize + bodySize：加一帧数据长度，处理下一条数据使用 if (!reportSlaveMaxOffsetPlus())：上报 slave 同步信息 if (!this.byteBufferRead.hasRemaining())：缓冲区写满了，重新分配缓冲区 reallocateByteBuffer()：重新分配缓冲区 1private void reallocateByteBuffer() int remain = READ_MAX_BUFFER_SIZE - this.dispatchPosition：表示缓冲区尚未处理过的字节数量 if (remain &gt; 0)：条件成立，说明缓冲区最后一帧数据是半包数据，但是不能丢失数据 this.byteBufferBackup.put(this.byteBufferRead)：将半包数据拷贝到 backup 缓冲区 this.swapByteBuffer()：交换 backup 成为 read this.byteBufferRead.position(remain)：设置 pos 为 remain ，后续加载数据 pos 从remain 开始向后移动 this.dispatchPosition = 0：当前缓冲区交换之后，相当于是一个全新的 byteBuffer，所以分配指针归零 HAConnConnectionHAConnection 类成员变量： 会话通道：master 和 slave 之间通信的 SocketChannel 1private final SocketChannel socketChannel; 客户端地址： 1private final String clientAddr; 服务类： 12private WriteSocketService writeSocketService;\t// 写数据服务private ReadSocketService readSocketService;\t// 读数据服务 请求位点：在 slave 上报本地的进度之后被赋值，该值大于 0 后同步逻辑才会运行，master 如果不知道 slave 节点当前消息的存储进度，就无法给 slave 推送数据 1private volatile long slaveRequestOffset = -1; 应答位点： 保存最新的 slave 上报的 offset 信息，slaveAckOffset 之前的数据都可以认为 slave 已经同步完成 1private volatile long slaveAckOffset = -1; 核心方法： 构造方法： 1234567891011public HAConnection(final HAService haService, final SocketChannel socketChannel) &#123; // 初始化一些东西 // 设置 socket 读写缓冲区为 64kb 大小 this.socketChannel.socket().setReceiveBufferSize(1024 * 64); this.socketChannel.socket().setSendBufferSize(1024 * 64); // 创建读写服务 this.writeSocketService = new WriteSocketService(this.socketChannel); this.readSocketService = new ReadSocketService(this.socketChannel); // 自增 this.haService.getConnectionCount().incrementAndGet();&#125; 启动方法： 1234public void start() &#123; this.readSocketService.start(); this.writeSocketService.start();&#125; ReadSocketReadSocketService 类是一个任务对象，slave 向 master 传输的帧格式为 [long][long][long]，上报的是 slave 本地的同步进度，同步进度是一个 long 值 成员变量： 读缓冲： 12private static final int READ_MAX_BUFFER_SIZE = 1024 * 1024;\t// 默认大小 1MBprivate final ByteBuffer byteBufferRead = ByteBuffer.allocate(READ_MAX_BUFFER_SIZE); NIO 属性： 12private final Selector selector; // 多路复用器private final SocketChannel socketChannel;\t// master 与 slave 之间的会话 SocketChannel 处理位点：缓冲区处理位点 1private int processPosition = 0; 上次读操作的时间： 1private volatile long lastReadTimestamp = System.currentTimeMillis(); 核心方法： 构造方法： 1public ReadSocketService(final SocketChannel socketChannel) this.socketChannel.register(this.selector, SelectionKey.OP_READ)：通道注册到多路复用器，关注读事件 this.setDaemon(true)：设置为守护线程 运行方法： 1public void run() this.selector.select(1000)：多路复用器阻塞获取就绪的通道，最多等待 1 秒钟，获取到就绪事件或者超时后结束 boolean ok = this.processReadEvent()：读数据的核心方法，返回 true 表示处理成功 false 表示 Socket 处于半关闭状态，需要上层重建 HAConnection 对象 int readSizeZeroTimes = 0：控制 while 循环，当连续从 Socket 读取失败 3 次（未加载到数据）跳出循环 if (!this.byteBufferRead.hasRemaining())：byteBufferRead 已经全部使用完，需要清理数据并更新位点 while (this.byteBufferRead.hasRemaining())：byteBufferRead 有空间可以去 Socket 读缓冲区加载数据 int readSize = this.socketChannel.read(this.byteBufferRead)：从通道读数据 if (readSize &gt; 0)：加载成功，有新数据 if ((byteBufferRead.position() - processPosition) &gt;= 8)：缓冲区的可读数据最少包含一个数据帧 int pos = ...：获取可读帧数据中最后一个完整的帧数据的位点，后面的数据丢弃 long readOffset = ...byteBufferRead.getLong(pos - 8)：读取最后一帧数据，slave 端当前的同步进度信息 this.processPosition = pos：更新处理位点 HAConnection.this.slaveAckOffset = readOffset：更新应答位点 if (HAConnection.this.slaveRequestOffset &lt; 0)：条件成立给 slaveRequestOffset 赋值 HAConnection...notifyTransferSome(slaveAckOffset)：唤醒阻塞的生产者线程 else if (readSize == 0) ：读取 3 次无新数据跳出循环 else：readSize &#x3D; -1 就表示 Socket 处于半关闭状态，对端已经关闭了 if (interval &gt; 20)：超过 20 秒未发生通信，直接结束循环 WriteSocketWriteSocketService 类是一个任务对象，master 向 slave 传输的数据帧格式为 &#123;[phyOffset][size][data...]&#125;&#123;[phyOffset][size][data...]&#125; phyOffset：数据区间的开始偏移量，并不表示某一条具体的消息，表示的数据块开始的偏移量位置 size：同步的数据块的大小 data：数据块，最大 32kb，可能包含多条消息的数据 成员变量： 协议头： 12private final int headerSize = 8 + 4; // 协议头大小：12private final ByteBuffer byteBufferHeader;\t// 帧头缓冲区 NIO 属性： 12private final Selector selector; // 多路复用器private final SocketChannel socketChannel;\t// master 与 slave 之间的会话 SocketChannel 处理位点：下一次传输同步数据的位置信息，master 给当前 slave 同步的位点 1private long nextTransferFromWhere = -1; 上次写操作： 12private boolean lastWriteOver = true; // 上一轮数据是否传输完毕private long lastWriteTimestamp = System.currentTimeMillis();\t// 上次写操作的时间 核心方法： 构造方法： 1public WriteSocketService(final SocketChannel socketChannel) this.socketChannel.register(this.selector, SelectionKey.OP_WRITE)：通道注册到多路复用器，关注写事件 this.setDaemon(true)：设置为守护线程 运行方法： 1public void run() this.selector.select(1000)：多路复用器阻塞获取就绪的通道，最多等待 1 秒钟，获取到就绪事件或者超时后结束 if (-1 == HAConnection.this.slaveRequestOffset)：等待 slave 同步完数据 if (-1 == this.nextTransferFromWhere)：条件成立，需要初始化该变量 if (0 == HAConnection.this.slaveRequestOffset)：slave 是一个全新节点，从正在顺序写的 MF 开始同步数据 long masterOffset = ...：获取 master 最大的 offset，并计算归属的 mappedFile 文件的开始 offset this.nextTransferFromWhere = masterOffset：赋值给下一次传输同步数据的位置信息 this.nextTransferFromWhere = HAConnection.this.slaveRequestOffset：大部分情况走这个赋值逻辑 if (this.lastWriteOver)：上一次待发送数据全部发送完成 if (interval &gt; 5)：超过 5 秒未同步数据，发送一个 header 心跳数据包，维持长连接 else：上一轮的待发送数据未全部发送，需要同步数据到 slave 节点 SelectMappedBufferResult selectResult：到 CommitLog 中查询 nextTransferFromWhere 开始位置的数据 if (size &gt; 32k)：一次最多同步 32k 数据 this.nextTransferFromWhere += size：增加 size，下一轮传输跳过本帧数据 selectResult.getByteBuffer().limit(size)：设置 byteBuffer 可访问数据区间为 [pos, size] this.selectMappedBufferResult = selectResult：待发送的数据 this.byteBufferHeader.put：构建帧头数据 this.lastWriteOver = this.transferData()：处理数据，返回是否处理完成 同步方法：同步数据到 slave 节点，返回 true 表示本轮数据全部同步完成，false 表示本轮同步未完成（Header 和 Body 其中一个未同步完成都会返回 false） 1private boolean transferData() int writeSizeZeroTimes= 0：控制 while 循环，当写失败连续 3 次时，跳出循环）跳出循环 while (this.byteBufferHeader.hasRemaining())：帧头数据缓冲区有待发送的数据 int writeSize = this.socketChannel.write(this.byteBufferHeader)：向通道写帧头数据 if (null == this.selectMappedBufferResult)：说明是心跳数据，返回心跳数据是否发送完成 if (!this.byteBufferHeader.hasRemaining())：Header写成功之后，才进行写 Body while (this.selectMappedBufferResult.getByteBuffer().hasRemaining())：数据缓冲区有待发送的数据 int writeSize = this.socketChannel.write(this.selectMappedBufferResult...)：向通道写帧头数据 if (writeSize &gt; 0)：写数据成功，但是不代表 SMBR 中的数据全部写完成 boolean result：判断是否发送完成，返回该值 MesStore生命周期DefaultMessageStore 类核心是整个存储服务的调度类 构造方法： 1public DefaultMessageStore() this.allocateMappedFileService.start()：启动创建 MappedFile 文件服务 this.indexService.start()：启动索引服务 load()：先加载 CommitLog，再加载 ConsumeQueue，最后加载 IndexFile，加载完进入恢复阶段，先恢复 CQ，在恢复 CL 1public boolean load() start()：核心启动方法 1public void start() lock = lockFile.getChannel().tryLock(0, 1, false)：获取文件锁，获取失败说明当前目录已经启动过 Broker long maxPhysicalPosInLogicQueue = commitLog.getMinOffset()：遍历全部的 CQ 对象，获取 CQ 中消息的最大偏移量 this.reputMessageService.start()：设置分发服务的分发位点，启动分发服务，构建 ConsumerQueue 和 IndexFile if (dispatchBehindBytes() &lt;= 0)：线程等待分发服务将分发数据全部处理完毕 this.recoverTopicQueueTable()：因为修改了 CQ 数据，所以再次构建队列偏移量字段表 this.haService.start()：启动 HA 服务 this.handleScheduleMessageService()：启动消息调度服务 this.flushConsumeQueueService.start()：启动 CQ 消费队列刷盘服务 this.commitLog.start()：启动 CL 刷盘服务 this.storeStatsService.start()：启动状态存储服务 this.createTempFile()：创建 AbortFile，正常关机时 JVM HOOK 会删除该文件，异常宕机时该文件不会删除，开机数据恢复阶段根据是否存在该文件，执行不同的恢复策略 this.addScheduleTask()：添加定时任务 DefaultMessageStore.this.cleanFilesPeriodically()：定时清理过期文件，周期是 10 秒 this.cleanCommitLogService.run()：启动清理过期的 CL 文件服务 this.cleanConsumeQueueService.run()：启动清理过期的 CQ 文件服务 DefaultMessageStore.this.checkSelf()：每 10 分种进行健康检查 DefaultMessageStore.this.cleanCommitLogService.isSpaceFull()：磁盘预警定时任务，每 10 秒一次 if (physicRatio &gt; this.diskSpaceWarningLevelRatio)：检查磁盘是否到达 waring 阈值，默认 90% boolean diskok = ...runningFlags.getAndMakeDiskFull()：设置磁盘写满标记 boolean diskok = ...this.runningFlags.getAndMakeDiskOK()：设置磁盘可写标记 this.shutdown = false：刚启动，设置为 false shutdown()：关闭各种服务和线程资源，设置存储模块状态为关闭状态 1public void shutdown() destroy()：销毁 Broker 的工作目录 1public void destroy() 服务线程ServiceThread 类被很多服务继承，本身是一个 Runnable 任务对象，继承者通过重写 run 方法来实现服务的逻辑 run()：一般实现方式 12345public void run() &#123; while (!this.isStopped()) &#123; // 业务逻辑 &#125;&#125; 通过参数 stopped 控制服务的停止，使用 volatile 修饰保证可见性 1protected volatile boolean stopped = false shutdown()：停止线程，首先设置 stopped 为 true，然后进行唤醒，默认不直接打断线程 1public void shutdown() waitForRunning()：挂起线程，设置唤醒标记 hasNotified 为 false 1protected void waitForRunning(long interval) wakeup()：唤醒线程，设置 hasNotified 为 true 1public void wakeup() 构建服务AllocateMappedFileService 创建 MappedFile 服务 mmapOperation()：核心服务 1private boolean mmapOperation() req = this.requestQueue.take()： 从 requestQueue 阻塞队列（优先级）中获取 AllocateRequest 任务 if (...isTransientStorePoolEnable())：条件成立使用直接内存写入数据， 从直接内存中 commit 到 FileChannel 中 mappedFile = new MappedFile(req.getFilePath(), req.getFileSize())：根据请求的路径和大小创建对象 mappedFile.warmMappedFile()：判断 mappedFile 大小，只有 CommitLog 才进行文件预热 req.setMappedFile(mappedFile)：将创建好的 MF 对象的赋值给请求对象的成员属性 req.getCountDownLatch().countDown()：唤醒请求的阻塞线程 putRequestAndReturnMappedFile()：MappedFileQueue 中用来创建 MF 对象的方法 1public MappedFile putRequestAndReturnMappedFile(String nextFilePath, String nextNextFilePath, int fileSize) AllocateRequest nextReq = new AllocateRequest(...)：创建 nextFilePath 的 AllocateRequest 对象，放入请求列表和阻塞队列，然后创建 nextNextFilePath 的 AllocateRequest 对象，放入请求列表和阻塞队列 AllocateRequest result = this.requestTable.get(nextFilePath)：从请求列表获取 nextFilePath 的请求对象 result.getCountDownLatch().await(...)：线程挂起，直到超时或者 nextFilePath 对应的 MF 文件创建完成 return result.getMappedFile()：返回创建好的 MF 文件对象 ReputMessageService 消息分发服务，用于构建 ConsumerQueue 和 IndexFile 文件 run()：循环执行 doReput 方法，所以发送的消息存储进 CL 就可以产生对应的 CQ，每执行一次线程休眠 1 毫秒 1public void run() doReput()：实现分发的核心逻辑 1private void doReput() for (boolean doNext = true; this.isCommitLogAvailable() &amp;&amp; doNext; )：循环遍历 SelectMappedBufferResult result： 从 CommitLog 拉取数据，数据范围 [reputFromOffset, 包含该偏移量的 MF 的最大 Pos]，封装成结果对象 DispatchRequest dispatchRequest：从结果对象读取出一条 DispatchRequest 数据 DefaultMessageStore.this.doDispatch(dispatchRequest)：将数据交给分发器进行分发，用于构建 CQ 和索引文件 this.reputFromOffset += size：更新数据范围 刷盘服务FlushConsumeQueueService 刷盘 CQ 数据 run()：每隔 1 秒执行一次刷盘服务，跳出循环后还会执行一次强制刷盘 1public void run() doFlush()：刷盘 1private void doFlush(int retryTimes) int flushConsumeQueueLeastPages：脏页阈值，默认是 2 if (retryTimes == RETRY_TIMES_OVER)：重试次数是 3 时设置强制刷盘，设置脏页阈值为 0 int flushConsumeQueueThoroughInterval：两次刷新的时间间隔超过 60 秒会强制刷盘 for (ConsumeQueue cq : maps.values())：遍历所有的 CQ，进行刷盘 DefaultMessageStore.this.getStoreCheckpoint().flush()：强制刷盘时将 StoreCheckpoint 瞬时数据刷盘 FlushCommitLogService 刷盘 CL 数据，默认是异步刷盘 run()：运行方法 1public void run() while (!this.isStopped())：stopped为 true 才跳出循环 boolean flushCommitLogTimed：控制线程的休眠方式，默认是 false，使用 CountDownLatch.await() 休眠，设置为 true 时使用 Thread.sleep() 休眠 int interval：获取配置中的刷盘时间间隔 int flushPhysicQueueLeastPages：获取最小刷盘页数，默认是 4 页，脏页达到指定页数才刷盘 int flushPhysicQueueThoroughInterval：获取强制刷盘周期，默认是 10 秒，达到周期后强制刷盘，不考虑脏页 if (flushCommitLogTimed)：休眠逻辑，避免 CPU 占用太长时间，导致无法执行其他更紧急的任务 CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages)：刷盘 for (int i = 0; i &lt; RETRY_TIMES_OVER &amp;&amp; !result; i++)：stopped 停止标记为 true 时，需要确保所有的数据都已经刷盘，所以此处尝试 10 次强制刷盘， result = CommitLog.this.mappedFileQueue.flush(0)：强制刷盘 清理服务CleanCommitLogService 清理过期的 CL 数据，定时任务 10 秒调用一次，先清理 CL，再清理 CQ，因为 CQ 依赖于 CL 的数据 run()：运行方法 1public void run() deleteExpiredFiles()：删除过期 CL 文件 1private void deleteExpiredFiles() long fileReservedTime：默认 72，代表文件的保留时间 boolean timeup = this.isTimeToDelete()：当前时间是否是凌晨 4 点 boolean spacefull = this.isSpaceToDelete()：CL 或者 CQ 的目录磁盘使用率达到阈值标准 85% boolean manualDelete = this.manualDeleteFileSeveralTimes &gt; 0：手动删除文件 fileReservedTime *= 60 * 60 * 1000：默认保留 72 小时 deleteCount = DefaultMessageStore.this.commitLog.deleteExpiredFile()：调用 MFQ 对象的删除方法 CleanConsumeQueueService 清理过期的 CQ 数据 run()：运行方法 1public void run() deleteExpiredFiles()：删除过期 CQ 文件 1private void deleteExpiredFiles() int deleteLogicsFilesInterval：清理 CQ 的时间间隔，默认 100 毫秒 long minOffset = DefaultMessageStore.this.commitLog.getMinOffset()：获取 CL 文件中最小的物理偏移量 if (minOffset &gt; this.lastPhysicalMinOffset)：CL 最小的偏移量大于 CQ 最小的，说明有过期数据 this.lastPhysicalMinOffset = minOffset：更新 CQ 的最小偏移量 for (ConsumeQueue logic : maps.values())：遍历所有的 CQ 文件 logic.deleteExpiredFile(minOffset)：调用 MFQ 对象的删除方法 DefaultMessageStore.this.indexService.deleteExpiredFile(minOffset)：删除过期的索引文件 获取消息DefaultMessageStore#getMessage 用于获取消息，在 PullMessageProcessor#processRequest 方法中被调用 （提示：建议学习消费者源码时再阅读） 12// offset: 客户端拉消息使用位点； maxMsgNums: 32； messageFilter: 一般这里是 tagCode 过滤 public GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset, final int maxMsgNums, final MessageFilter messageFilter) if (this.shutdown)：检查运行状态 GetMessageResult getResult：创建查询结果对象 final long maxOffsetPy = this.commitLog.getMaxOffset()：获取 CommitLog 最大物理偏移量 ConsumeQueue consumeQueue = findConsumeQueue(topic, queueId)：根据主题和队列 ID 获取 ConsumeQueue对象 minOffset, maxOffset：获取当前 ConsumeQueue 的最小 offset 和 最大 offset，判断是否满足本次 Pull 的 offset if (maxOffset == 0)：说明队列内无数据，设置状态为 NO_MESSAGE_IN_QUEUE，外层进行长轮询 else if (offset &lt; minOffset)：说明 offset 太小了，设置状态为 OFFSET_TOO_SMALL else if (offset == maxOffset)：消费进度持平，设置状态为 OFFSET_OVERFLOW_ONE，外层进行长轮询 else if (offset &gt; maxOffset)：说明 offset 越界了，设置状态为 OFFSET_OVERFLOW_BADLY SelectMappedBufferResult bufferConsumeQueue：查询 CQData 获取包含该 offset 的 MappedFile 文件，如果该文件不是顺序写的文件，就读取 [offset%maxSize, 文件尾] 范围的数据，反之读取 [offset%maxSize, 文件名+wrotePosition尾] 先查 CQ 的原因：因为 CQ 时 CL 的索引，通过 CQ 查询 CL 更加快捷 if (bufferConsumeQueue != null)：只有再 CQ 删除过期数据的逻辑执行时，条件才不成立，一般都是成立的 long nextPhyFileStartOffset = Long.MIN_VALUE：下一个 commitLog 物理文件名，初始值为最小值 long maxPhyOffsetPulling = 0：本次拉消息最后一条消息的物理偏移量 for ()：处理数据，每次处理 20 字节处理字节数大于 16000 时跳出循环 offsetPy, sizePy, tagsCode：读取 20 个字节后，获取消息物理偏移量、消息大小、消息 tagCode boolean isInDisk = checkInDiskByCommitOffset(...)：检查消息是热数据还是冷数据，false 为热数据 long memory：Broker 系统 40% 内存的字节数，写数据时内存不够会使用 LRU 算法淘汰数据，将淘汰数据持久化到磁盘 return (maxOffsetPy - offsetPy) &gt; memory：返回 true 说明数据已经持久化到磁盘，为冷数据 if (this.isTheBatchFull())：控制是否跳出循环 if (0 == bufferTotal || 0 == messageTotal)：本次 pull 消息未拉取到任何东西，需要外层 for 循环继续，返回 false if (maxMsgNums &lt;= messageTotal)：结果对象内消息数已经超过了最大消息数量，可以结束循环了 if (isInDisk)：冷数据 if ((bufferTotal + sizePy) &gt; ...)：冷数据一次 pull 请求最大允许获取 64kb 的消息 if (messageTotal &gt; ...)：冷数据一次 pull 请求最大允许获取8 条消息 else：热数据 if ((bufferTotal + sizePy) &gt; ...)：热数据一次 pull 请求最大允许获取 256kb 的消息 if (messageTotal &gt; ...)：冷数据一次 pull 请求最大允许获取 32 条消息 if (messageFilter != null)：按照消息 tagCode 进行过滤 selectResult = this.commitLog.getMessage(offsetPy, sizePy)：根据 CQ 消息物理偏移量和消息大小到 commitLog 中查询这条 msg if (null == selectResult)：条件成立说明 commitLog 执行了删除过期文件的定时任务，因为是先清理的 CL，所以 CQ 还有该索引数据 nextPhyFileStartOffset = this.commitLog.rollNextFile(offsetPy)：获取包含该 offsetPy 的下一个数据文件的文件名 getResult.addMessage(selectResult)：将本次循环查询出来的 msg 加入到 getResult 内 status = GetMessageStatus.FOUND：查询状态设置为 FOUND nextPhyFileStartOffset = Long.MIN_VALUE：设置为最小值，跳过期 CQData 数据的逻辑 nextBeginOffset = offset + (i / ConsumeQueue.CQ_STORE_UNIT_SIZE)：计算客户端下一次 pull 时使用的位点信息 getResult.setSuggestPullingFromSlave(diff &gt; memory)：选择主从节点的建议 diff &gt; memory =&gt; true：表示本轮查询最后一条消息为冷数据，Broker 建议客户端下一次 pull 时到 slave 节点 diff &gt; memory =&gt; false：表示本轮查询最后一条消息为热数据，Broker 建议客户端下一次 pull 时到 master 节点 getResult.setStatus(status)：设置结果状态 getResult.setNextBeginOffset(nextBeginOffset)：设置客户端下一次 pull 时的 offset getResult.setMaxOffset(maxOffset)：设置 queue 的最大 offset 和最小 offset return getResult：返回结果对象 BrokerBrokerStartup 启动方法 123456public static void main(String[] args) &#123; start(createBrokerController(args));&#125;public static BrokerController start(BrokerController controller) &#123; controller.start();\t// 启动&#125; BrokerStartup#createBrokerController：构造控制器，并初始化 final BrokerController controller()：创建实例对象 boolean initResult = controller.initialize()：控制器初始化 this.registerProcessor()：注册了处理器，包括发送消息、拉取消息、查询消息等核心处理器 initialTransaction()：初始化了事务服务，用于进行事务回查 BrokerController#start：核心启动方法 this.messageStore.start()：启动存储服务 this.remotingServer.start()：启动 Netty 通信服务 this.fileWatchService.start()：启动文件监听服务 startProcessorByHa(messageStoreConfig.getBrokerRole())：启动事务回查 this.scheduledExecutorService.scheduleAtFixedRate()：每隔 30s 向 NameServer 上报 Topic 路由信息，心跳机制 BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister()) Producer生产者类生产者类DefaultMQProducer 是生产者的默认实现类 成员变量： 生产者实现类： 1protected final transient DefaultMQProducerImpl defaultMQProducerImpl 生产者组：发送事务消息，Broker 端进行事务回查（补偿机制）时，选择当前生产者组的下一个生产者进行事务回查 1private String producerGroup; 默认主题：isAutoCreateTopicEnable 开启时，当发送消息指定的 Topic 在 Namesrv 未找到路由信息，使用该值创建 Topic 信息 12private String createTopicKey = TopicValidator.AUTO_CREATE_TOPIC_KEY_TOPIC;// 值为【TBW102】，Just for testing or demo program 消息重投：系统特性消息重试部分详解了三个参数的作用 123private int retryTimesWhenSendFailed = 2; // 同步发送失败后重试的发送次数，加上第一次发送，一共三次private int retryTimesWhenSendAsyncFailed = 2;\t// 异步private boolean retryAnotherBrokerWhenNotStoreOK = false;\t// 消息未存储成功，选择其他 Broker 重试 消息队列： 1private volatile int defaultTopicQueueNums = 4; // 默认 Broker 创建的队列数 消息属性： 12345678910111213141516 private int sendMsgTimeout = 3000; // 发送消息的超时限制 private int compressMsgBodyOverHowmuch = 1024 * 4;\t// 压缩阈值，当 msg body 超过 4k 后使用压缩 private int maxMessageSize = 1024 * 1024 * 4; // 消息体的最大限制，默认 4M private TraceDispatcher traceDispatcher = null; // 消息轨迹构造方法：* 构造方法： ```java public DefaultMQProducer(final String namespace, final String producerGroup, RPCHook rpcHook) &#123; this.namespace = namespace; this.producerGroup = producerGroup; // 创建生产者实现对象 defaultMQProducerImpl = new DefaultMQProducerImpl(this, rpcHook); &#125; 成员方法： start()：启动方法 12345678910public void start() throws MQClientException &#123; // 重置生产者组名，如果传递了命名空间，则 【namespace%group】 this.setProducerGroup(withNamespace(this.producerGroup)); // 生产者实现对象启动 this.defaultMQProducerImpl.start(); if (null != traceDispatcher) &#123; // 消息轨迹的逻辑 traceDispatcher.start(this.getNamesrvAddr(), this.getAccessChannel()); &#125;&#125; send()：发送消息： 1234567public SendResult send(Message msg)&#123; // 校验消息 Validators.checkMessage(msg, this); // 设置消息 Topic msg.setTopic(withNamespace(msg.getTopic())); return this.defaultMQProducerImpl.send(msg);&#125; request()：请求方法，需要消费者回执消息 1234public Message request(final Message msg, final MessageQueue mq, final long timeout) &#123; msg.setTopic(withNamespace(msg.getTopic())); return this.defaultMQProducerImpl.request(msg, mq, timeout);&#125; 实现者类DefaultMQProducerImpl 类是默认的生产者实现类 成员变量： 实例对象： 12private final DefaultMQProducer defaultMQProducer;\t// 持有默认生产者对象，用来获取对象中的配置信息private MQClientInstance mQClientFactory; // 客户端实例对象，生产者启动后需要注册到该客户端对象内 主题发布信息映射表：key 是 Topic，value 是发布信息 1private final ConcurrentMap&lt;String, TopicPublishInfo&gt; topicPublishInfoTable = new ConcurrentHashMap&lt;String, TopicPublishInfo&gt;(); 异步发送消息：相关信息 123private final BlockingQueue&lt;Runnable&gt; asyncSenderThreadPoolQueue;// 异步发送消息，异步线程池使用的队列private final ExecutorService defaultAsyncSenderExecutor;\t// 异步发送消息默认使用的线程池private ExecutorService asyncSenderExecutor; // 异步消息发送线程池，指定后就不使用默认线程池了 定时器：执行定时任务 1private final Timer timer = new Timer(&quot;RequestHouseKeepingService&quot;, true);\t// 守护线程 状态信息：服务的状态，默认创建状态 1private ServiceState serviceState = ServiceState.CREATE_JUST; 压缩等级：ZIP 压缩算法的等级，默认是 5，越高压缩效果好，但是压缩的更慢 1private int zipCompressLevel = Integer.parseInt(System.getProperty..., &quot;5&quot;)); 容错策略：选择队列的容错策略 1private MQFaultStrategy mqFaultStrategy = new MQFaultStrategy(); 钩子：用来进行前置或者后置处理 123ArrayList&lt;SendMessageHook&gt; sendMessageHookList; // 发送消息的钩子，留给用户扩展使用ArrayList&lt;CheckForbiddenHook&gt; checkForbiddenHookList;\t// 对比上面的钩子，可以抛异常，控制消息是否可以发送private final RPCHook rpcHook; // 传递给 NettyRemotingClient 构造方法： 默认构造： 1234public DefaultMQProducerImpl(final DefaultMQProducer defaultMQProducer) &#123; // 默认 RPC HOOK 是空 this(defaultMQProducer, null);&#125; 有参构造： 1234567891011public DefaultMQProducerImpl(final DefaultMQProducer defaultMQProducer, RPCHook rpcHook) &#123; // 属性赋值 this.defaultMQProducer = defaultMQProducer; this.rpcHook = rpcHook; // 创建【异步消息线程池任务队列】，长度是 5w this.asyncSenderThreadPoolQueue = new LinkedBlockingQueue&lt;Runnable&gt;(50000); // 创建默认的异步消息任务线程池 this.defaultAsyncSenderExecutor = new ThreadPoolExecutor( // 核心线程数和最大线程数都是 系统可用的计算资源（8核16线程的系统就是 16）...&#125; 实现方法 start()：启动方法，参数默认是 true，代表正常的启动路径 1public void start(final boolean startFactory) this.serviceState = ServiceState.START_FAILED：先修改为启动失败，成功后再修改，这种思想很常见 this.checkConfig()：判断生产者组名不能是空，也不能是 default_PRODUCER if (!getProducerGroup().equals(MixAll.CLIENT_INNER_PRODUCER_GROUP))：条件成立说明当前生产者不是内部产生者，内部生产者是处理消息回退的这种情况使用的生产者 this.defaultMQProducer.changeInstanceNameToPID()：修改生产者实例名称为当前进程的 PID this.mQClientFactory = ...：获取当前进程的 MQ 客户端实例对象，从 factoryTable 中获取 key 为 客户端 ID，格式是ip@pid，一个 JVM 进程只有一个 PID，也只有一个 MQClientInstance boolean registerOK = mQClientFactory.registerProducer(...)：将生产者注册到 RocketMQ 客户端实例内 this.topicPublishInfoTable.put(...)：添加一个主题发布信息，key 是 TBW102 ，value 是一个空对象 mQClientFactory.start()：启动 RocketMQ 客户端实例对象 this.mQClientFactory.sendHeartbeatToAllBrokerWithLock()：RocketMQ 客户端实例向已知的 Broker 节点发送一次心跳（也是定时任务） this.timer.scheduleAtFixedRate()： request 发送的消息需要消费着回执信息，启动定时任务每秒一次删除超时请求 生产者 msg 添加信息关联 ID 发送到 Broker 消费者从 Broker 拿到消息后会检查 msg 类型是一个需要回执的消息，处理完消息后会根据 msg 关联 ID 和客户端 ID 生成一条响应结果消息发送到 Broker，Broker 判断为回执消息，会根据客户端ID 找到 channel 推送给生产者 生产者拿到回执消息后，读取出来关联 ID 找到对应的 RequestFuture，将阻塞线程唤醒 sendDefaultImpl()：发送消息 12//参数1：消息；参数2：发送模式（同步异步单向）；参数3：回调函数，异步发送时需要；参数4：发送超时时间, 默认 3 秒private SendResult sendDefaultImpl(msg, communicationMode, sendCallback,timeout) &#123;&#125; this.makeSureStateOK()：校验生产者状态是运行中，否则抛出异常 topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic())：获取当前消息主题的发布信息 this.topicPublishInfoTable.get(topic)：先尝试从本地主题发布信息映射表获取信息，获取不到继续执行 this.mQClientFactory.update...FromNameServer(topic)：然后从 Namesrv 更新该 Topic 的路由数据 this.mQClientFactory.update...FromNameServer(...)：路由数据是空，获取默认 TBW102 的数据 return topicPublishInfo：返回 TBW102 主题的发布信息 String[] brokersSent = new String[timesTotal]：下标索引代表第几次发送，值代表这次发送选择 Broker name for (; times &lt; timesTotal; times++)：循环发送，发送成功或者发送尝试次数达到上限，结束循环 String lastBrokerName = null == mq ? null : mq.getBrokerName()：获取上次发送失败的 BrokerName mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName)：从发布信息中选择一个队列，生产者的负载均衡策略，参考系统特性章节 brokersSent[times] = mq.getBrokerName()：将本次选择的 BrokerName 存入数组 msg.setTopic(this.defaultMQProducer.withNamespace(msg.getTopic()))：产生重投，重投消息需要加上标记 sendResult = this.sendKernelImpl：核心发送方法 switch (communicationMode)：异步或者单向消息直接返回 null，异步通过回调函数处理，同步发送进入逻辑判断 if (sendResult.getSendStatus() != SendStatus.SEND_OK)：服务端 Broker 存储失败，需要重试其他 Broker throw new MQClientException()：未找到当前主题的路由数据，无法发送消息，抛出异常 sendKernelImpl()：核心发送方法 12//参数1：消息；参数2：选择的队列；参数3：发送模式（同步异步单向）；参数4：回调函数，异步发送时需要；参数5：主题发布信息；参数6：剩余超时时间限制private SendResult sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout) brokerAddr = this.mQClientFactory(...)：获取指定 BrokerName 对应的 mater 节点的地址，master 节点的 ID 为 0，集群模式下，发送消息要发到主节点 brokerAddr = MixAll.brokerVIPChannel()：Broker 启动时会绑定两个服务器端口，一个是普通端口，一个是 VIP 端口，服务器端根据不同端口创建不同的的 NioSocketChannel byte[] prevBody = msg.getBody()：获取消息体 if (!(msg instanceof MessageBatch))：非批量消息，需要重新设置消息 ID MessageClientIDSetter.setUniqID(msg)：msg id 由两部分组成，一部分是 ip 地址、进程号、Classloader 的 hashcode，另一部分是时间差（当前时间减去当月一号的时间）和计数器的值 if (this.tryToCompressMessage(msg))：判断消息是否压缩，压缩需要设置压缩标记 hasCheckForbiddenHook、hasSendMessageHook：执行钩子方法 requestHeader = new SendMessageRequestHeader()：设置发送消息的消息头 if (requestHeader.getTopic().startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX))：重投的发送消息 switch (communicationMode)：异步发送一种处理方式，单向和同步同样的处理逻辑 sendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage()：发送消息 request = RemotingCommand.createRequestCommand()：创建一个 RequestCommand 对象 request.setBody(msg.getBody())：将消息放入请求体 switch (communicationMode)：根据不同的模式 invoke 不同的方法 request()：请求方法，消费者回执消息，这种消息是异步消息 requestResponseFuture = new RequestResponseFuture(correlationId, timeout, null)：创建请求响应对象 getRequestFutureTable().put(correlationId, requestResponseFuture)：放入RequestFutureTable 映射表中 this.sendDefaultImpl(msg, CommunicationMode.ASYNC, new SendCallback())：发送异步消息，有回调函数 return waitResponse(msg, timeout, requestResponseFuture, cost)：用来挂起请求的方法 12345678910111213 public Message waitResponseMessage(final long timeout) throws InterruptedException &#123; // 请求挂起 this.countDownLatch.await(timeout, TimeUnit.MILLISECONDS); return this.responseMsg; &#125;* 当消息被消费后，客户端处理响应时通过消息的关联 ID，从映射表中获取消息的 RequestResponseFuture，执行下面的方法唤醒挂起线程 ```java public void putResponseMessage(final Message responseMsg) &#123; this.responseMsg = responseMsg; this.countDownLatch.countDown(); &#125; 路由信息TopicPublishInfo 类用来存储路由信息 成员变量： 顺序消息： 1private boolean orderTopic = false; 消息队列： 12private List&lt;MessageQueue&gt; messageQueueList = new ArrayList&lt;&gt;(); // 主题全部的消息队列private volatile ThreadLocalIndex sendWhichQueue = new ThreadLocalIndex();\t// 消息队列索引 123456// 【消息队列类】public class MessageQueue implements Comparable&lt;MessageQueue&gt;, Serializable &#123; private String topic; private String brokerName; private int queueId;// 队列 ID&#125; 路由数据：主题对应的路由数据 1private TopicRouteData topicRouteData; 123456public class TopicRouteData extends RemotingSerializable &#123; private String orderTopicConf; private List&lt;QueueData&gt; queueDatas; // 队列数据 private List&lt;BrokerData&gt; brokerDatas;\t// Broker 数据 private HashMap&lt;String/* brokerAddr */, List&lt;String&gt;/* Filter Server */&gt; filterServerTable;&#125; 1234567public class QueueData implements Comparable&lt;QueueData&gt; &#123; private String brokerName;\t// 节点名称 private int readQueueNums;\t// 读队列数 private int writeQueueNums;\t// 写队列数 private int perm; // 权限 private int topicSynFlag;&#125; 12345public class BrokerData implements Comparable&lt;BrokerData&gt; &#123; private String cluster; // 集群名 private String brokerName;\t// Broker节点名称 private HashMap&lt;Long/* brokerId */, String/* broker address */&gt; brokerAddrs;&#125; 核心方法： selectOneMessageQueue()：选择消息队列使用 1234567891011121314151617181920212223// 参数是上次失败时的 brokerName，可以为 nullpublic MessageQueue selectOneMessageQueue(final String lastBrokerName) &#123; if (lastBrokerName == null) &#123; return selectOneMessageQueue(); &#125; else &#123; // 遍历消息队列 for (int i = 0; i &lt; this.messageQueueList.size(); i++) &#123; // 【获取队列的索引，+1】 int index = this.sendWhichQueue.getAndIncrement(); // 获取队列的下标位置 int pos = Math.abs(index) % this.messageQueueList.size(); if (pos &lt; 0) pos = 0; // 获取消息队列 MessageQueue mq = this.messageQueueList.get(pos); // 与上次选择的不同就可以返回 if (!mq.getBrokerName().equals(lastBrokerName)) &#123; return mq; &#125; &#125; return selectOneMessageQueue(); &#125;&#125; 公共配置公共的配置信息类 ClientConfig 类 123456789101112131415161718192021222324252627public class ClientConfig &#123; // Namesrv 地址配置 private String namesrvAddr = NameServerAddressUtils.getNameServerAddresses(); // 客户端的 IP 地址 private String clientIP = RemotingUtil.getLocalAddress(); // 客户端实例名称 private String instanceName = System.getProperty(&quot;rocketmq.client.name&quot;, &quot;DEFAULT&quot;); // 客户端回调线程池的数量，平台核心数，8核16线程的电脑返回16 private int clientCallbackExecutorThreads = Runtime.getRuntime().availableProcessors(); // 命名空间 protected String namespace; protected AccessChannel accessChannel = AccessChannel.LOCAL; // 获取路由信息的间隔时间 30s private int pollNameServerInterval = 1000 * 30; // 客户端与 broker 之间的心跳周期 30s private int heartbeatBrokerInterval = 1000 * 30; // 消费者持久化消费的周期 5s private int persistConsumerOffsetInterval = 1000 * 5; private long pullTimeDelayMillsWhenException = 1000; private boolean unitMode = false; private String unitName; // vip 通道，broker 启动时绑定两个端口，其中一个是 vip 通道 private boolean vipChannelEnabled = Boolean.parseBoolean(); // 语言，默认是 Java private LanguageCode language = LanguageCode.JAVA;&#125; NettyClientConfig 123456789101112131415161718192021222324public class NettyClientConfig &#123; // 客户端工作线程数 private int clientWorkerThreads = 4; // 回调处理线程池 线程数：平台核心数 private int clientCallbackExecutorThreads = Runtime.getRuntime().availableProcessors(); // 单向请求并发数，默认 65535 private int clientOnewaySemaphoreValue = NettySystemConfig.CLIENT_ONEWAY_SEMAPHORE_VALUE; // 异步请求并发数，默认 65535 private int clientAsyncSemaphoreValue = NettySystemConfig.CLIENT_ASYNC_SEMAPHORE_VALUE; // 客户端连接服务器的超时时间限制 3秒 private int connectTimeoutMillis = 3000; // 客户端未激活周期，60s（指定时间内 ch 未激活，需要关闭） private long channelNotActiveInterval = 1000 * 60; // 客户端与服务器 ch 最大空闲时间 2分钟 private int clientChannelMaxIdleTimeSeconds = 120; // 底层 Socket 写和收 缓冲区的大小 65535 64k private int clientSocketSndBufSize = NettySystemConfig.socketSndbufSize; private int clientSocketRcvBufSize = NettySystemConfig.socketRcvbufSize; // 客户端 netty 是否启动内存池 private boolean clientPooledByteBufAllocatorEnable = false; // 客户端是否超时关闭 Socket 连接 private boolean clientCloseSocketIfTimeout = false;&#125; 客户端类成员属性MQClientInstance 是 RocketMQ 客户端实例，在一个 JVM 进程中只有一个客户端实例，既服务于生产者，也服务于消费者 成员变量： 配置信息： 1234private final int instanceIndex; // 索引一般是 0，因为客户端实例一般都是一个进程只有一个private final String clientId; // 客户端 ID ip@pidprivate final long bootTimestamp; // 客户端的启动时间private ServiceState serviceState; // 客户端状态 生产者消费者的映射表：key 是组名 123private final ConcurrentMap&lt;String, MQProducerInner&gt; producerTableprivate final ConcurrentMap&lt;String, MQConsumerInner&gt; consumerTableprivate final ConcurrentMap&lt;String, MQAdminExtInner&gt; adminExtTable 网络层配置： 1private final NettyClientConfig nettyClientConfig; 核心功能的实现：负责将 MQ 业务层的数据转换为网络层的 RemotingCommand 对象，使用内部持有的 NettyRemotingClient 对象的 invoke 系列方法，完成网络 IO（同步、异步、单向） 1private final MQClientAPIImpl mQClientAPIImpl; 本地路由数据：key 是主题名称，value 路由信息 1private final ConcurrentMap&lt;String, TopicRouteData&gt; topicRouteTable = new ConcurrentHashMap&lt;&gt;(); 锁信息：两把锁，锁不同的数据 12private final Lock lockNamesrv = new ReentrantLock();private final Lock lockHeartbeat = new ReentrantLock(); 调度线程池：单线程，执行定时任务 1private final ScheduledExecutorService scheduledExecutorService; Broker 映射表：key 是 BrokerName 1234// 物理节点映射表，value：Long 是 brokerID，【ID=0 的是主节点，其他是从节点】，String 是地址 ip:portprivate final ConcurrentMap&lt;String, HashMap&lt;Long, String&gt;&gt; brokerAddrTable;// 物理节点版本映射表，String 是地址 ip:port，Integer 是版本ConcurrentMap&lt;String, HashMap&lt;String, Integer&gt;&gt; brokerVersionTable; 客户端的协议处理器：用于处理 IO 事件 1private final ClientRemotingProcessor clientRemotingProcessor; 消息服务： 123private final PullMessageService pullMessageService; // 拉消息服务private final RebalanceService rebalanceService; // 消费者负载均衡服务private final ConsumerStatsManager consumerStatsManager;\t// 消费者状态管理 内部生产者实例：处理消费端消息回退，用该生产者发送回退消息 1private final DefaultMQProducer defaultMQProducer; 心跳次数统计： 1private final AtomicLong sendHeartbeatTimesTotal = new AtomicLong(0) 构造方法： MQClientInstance 有参构造： 123456789101112131415161718192021public MQClientInstance(ClientConfig clientConfig, int instanceIndex, String clientId, RPCHook rpcHook) &#123; this.clientConfig = clientConfig; this.instanceIndex = instanceIndex; // Netty 相关的配置信息 this.nettyClientConfig = new NettyClientConfig(); // 平台核心数 this.nettyClientConfig.setClientCallbackExecutorThreads(...); this.nettyClientConfig.setUseTLS(clientConfig.isUseTLS()); // 【创建客户端协议处理器】 this.clientRemotingProcessor = new ClientRemotingProcessor(this); // 创建 API 实现对象 // 参数一：客户端网络配置 // 参数二：客户端协议处理器，注册到客户端网络层 // 参数三：rpcHook，注册到客户端网络层 // 参数四：客户端配置 this.mQClientAPIImpl = new MQClientAPIImpl(this.nettyClientConfig, this.clientRemotingProcessor, rpcHook, clientConfig); //... // 内部生产者，指定内部生产者的组 this.defaultMQProducer = new DefaultMQProducer(MixAll.CLIENT_INNER_PRODUCER_GROUP);&#125; MQClientAPIImpl 有参构造： 12345678910111213public MQClientAPIImpl(nettyClientConfig, clientRemotingProcessor, rpcHook, clientConfig) &#123; this.clientConfig = clientConfig; topAddressing = new TopAddressing(MixAll.getWSAddr(), clientConfig.getUnitName()); // 创建网络层对象，参数二为 null 说明客户端并不关心 channel event this.remotingClient = new NettyRemotingClient(nettyClientConfig, null); // 业务处理器 this.clientRemotingProcessor = clientRemotingProcessor; // 注册 RpcHook this.remotingClient.registerRPCHook(rpcHook);\t// ... // 注册回退消息的请求码 this.remotingClient.registerProcessor(RequestCode.PUSH_REPLY_MESSAGE_TO_CLIENT, this.clientRemotingProcessor, null);&#125; 成员方法 start()：启动方法 synchronized (this)：加锁保证线程安全，保证只有一个实例对象启动 this.mQClientAPIImpl.start()：启动客户端网络层，底层调用 RemotingClient 类 this.startScheduledTask()：启动定时任务 this.pullMessageService.start()：启动拉取消息服务 this.rebalanceService.start()：启动负载均衡服务 this.defaultMQProducer...start(false)：启动内部生产者，参数为 false 代表不启动实例 startScheduledTask()：启动定时任务，调度线程池是单线程 if (null == this.clientConfig.getNamesrvAddr())：Namesrv 地址是空，需要两分钟拉取一次 Namesrv 地址 定时任务 1：从 Namesrv 更新客户端本地的路由数据，周期 30 秒一次 12// 获取生产者和消费者订阅的主题集合，遍历集合，对比从 namesrv 拉取最新的主题路由数据和本地数据，是否需要更新MQClientInstance.this.updateTopicRouteInfoFromNameServer(); 定时任务 2：周期 30 秒一次，两个任务 清理下线的 Broker 节点，遍历客户端的 Broker 物理节点映射表，将所有主题数据都不包含的 Broker 物理节点清理掉，如果被清理的 Broker 下所有的物理节点都没有了，就将该 Broker 的映射数据删除掉 向在线的所有的 Broker 发送心跳数据，同步发送的方式，返回值是 Broker 物理节点的版本号，更新版本映射表 12MQClientInstance.this.cleanOfflineBroker();MQClientInstance.this.sendHeartbeatToAllBrokerWithLock(); 123456789// 心跳数据public class HeartbeatData extends RemotingSerializable &#123; // 客户端 ID ip@pid private String clientID; // 存储客户端所有生产者数据 private Set&lt;ProducerData&gt; producerDataSet = new HashSet&lt;ProducerData&gt;(); // 存储客户端所有消费者数据 private Set&lt;ConsumerData&gt; consumerDataSet = new HashSet&lt;ConsumerData&gt;();&#125; 定时任务 3：消费者持久化消费数据，周期 5 秒一次 1MQClientInstance.this.persistAllConsumerOffset(); 定时任务 4：动态调整消费者线程池，周期 1 分钟一次 1MQClientInstance.this.adjustThreadPool(); updateTopicRouteInfoFromNameServer()：更新路由数据，通过加锁保证当前实例只有一个线程去更新 if (isDefault &amp;&amp; defaultMQProducer != null)：需要默认数据 topicRouteData = ...getDefaultTopicRouteInfoFromNameServer()：从 Namesrv 获取默认的 TBW102 的路由数据 topicRouteData = ...getTopicRouteInfoFromNameServer(topic)：需要从 Namesrv 获取路由数据（同步） old = this.topicRouteTable.get(topic)：获取客户端实例本地的该主题的路由数据 boolean changed = topicRouteDataIsChange(old, topicRouteData)：对比本地和最新下拉的数据是否一致 if (changed)：不一致进入更新逻辑 this.brokerAddrTable.put(...)：更新客户端 broker 物理节点映射表 Update Pub info：更新生产者信息 publishInfo = topicRouteData2TopicPublishInfo(topic, topicRouteData)：将主题路由数据转化为发布数据，会创建消息队列 MQ，放入发布数据对象的集合中 impl.updateTopicPublishInfo(topic, publishInfo)：生产者将主题的发布数据保存到它本地，方便发送消息使用 Update sub info：更新消费者信息，创建 MQ 队列，更新订阅信息，用于负载均衡 this.topicRouteTable.put(topic, cloneTopicRouteData)：将数据放入本地路由表 网络通信成员属性NettyRemotingClient 类负责客户端的网络通信 成员变量： Netty 服务相关属性： 123private final NettyClientConfig nettyClientConfig; // 客户端的网络层配置private final Bootstrap bootstrap = new Bootstrap(); // 客户端网络层启动对象private final EventLoopGroup eventLoopGroupWorker; // 客户端网络层 Netty IO 线程组 Channel 映射表： 12private final ConcurrentMap&lt;String, ChannelWrapper&gt; channelTables;// key 是服务器的地址，value 是通道对象private final Lock lockChannelTables = new ReentrantLock(); // 锁，控制并发安全 定时器：启动定时任务 1private final Timer timer = new Timer(&quot;ClientHouseKeepingService&quot;, true) 线程池： 12private ExecutorService publicExecutor; // 公共线程池private ExecutorService callbackExecutor; // 回调线程池，客户端发起异步请求，服务器的响应数据由回调线程池处理 事件监听器：客户端这里是 null 1private final ChannelEventListener channelEventListener; 构造方法 无参构造： 123public NettyRemotingClient(final NettyClientConfig nettyClientConfig) &#123; this(nettyClientConfig, null);&#125; 有参构造： 1234567891011121314151617181920public NettyRemotingClient(nettyClientConfig, channelEventListener) &#123; // 父类创建了2个信号量，1、控制单向请求的并发度，2、控制异步请求的并发度 super(nettyClientConfig.getClientOnewaySemaphoreValue(), nettyClientConfig.getClientAsyncSemaphoreValue()); this.nettyClientConfig = nettyClientConfig; this.channelEventListener = channelEventListener; // 创建公共线程池 int publicThreadNums = nettyClientConfig.getClientCallbackExecutorThreads(); if (publicThreadNums &lt;= 0) &#123; publicThreadNums = 4; &#125; this.publicExecutor = Executors.newFixedThreadPool(publicThreadNums,); // 创建 Netty IO 线程，1个线程 this.eventLoopGroupWorker = new NioEventLoopGroup(1, ); if (nettyClientConfig.isUseTLS()) &#123; sslContext = TlsHelper.buildSslContext(true); &#125;&#125; 成员方法 start()：启动方法 1234567891011121314151617181920212223242526public void start() &#123; // channel pipeline 内的 handler 使用的线程资源，默认 4 个 this.defaultEventExecutorGroup = new DefaultEventExecutorGroup(); // 配置 netty 客户端启动类对象 Bootstrap handler = this.bootstrap.group(this.eventLoopGroupWorker).channel(NioSocketChannel.class) //... .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); // 加几个handler pipeline.addLast( // 服务端的数据，都会来到这个 new NettyClientHandler()); &#125; &#125;); // 注意 Bootstrap 只是配置好客户端的元数据了，【在这里并没有创建任何 channel 对象】 // 定时任务 扫描 responseTable 中超时的 ResponseFuture，避免客户端线程长时间阻塞 this.timer.scheduleAtFixedRate(() -&gt; &#123; NettyRemotingClient.this.scanResponseTable(); &#125;, 1000 * 3, 1000); // 这里是 null，不启动 if (this.channelEventListener != null) &#123; this.nettyEventExecutor.start(); &#125;&#125; 单向通信： 1234567891011121314151617181920212223242526272829public RemotingCommand invokeSync(String addr, final RemotingCommand request, long timeoutMillis) &#123; // 开始时间 long beginStartTime = System.currentTimeMillis(); // 获取或者创建客户端与服务端（addr）的通道 channel final Channel channel = this.getAndCreateChannel(addr); // 条件成立说明客户端与服务端 channel 通道正常，可以通信 if (channel != null &amp;&amp; channel.isActive()) &#123; try &#123; // 执行 rpcHook 拓展点 doBeforeRpcHooks(addr, request); // 计算耗时，如果当前耗时已经超过 timeoutMillis 限制，则直接抛出异常，不再进行系统通信 long costTime = System.currentTimeMillis() - beginStartTime; if (timeoutMillis &lt; costTime) &#123; throw new RemotingTimeoutException(&quot;invokeSync call timeout&quot;); &#125; // 参数1：客户端-服务端通道channel // 参数二：网络层传输对象，封装着请求数据 // 参数三：剩余的超时限制 RemotingCommand response = this.invokeSyncImpl(channel, request, ...); // 后置处理 doAfterRpcHooks(RemotingHelper.parseChannelRemoteAddr(channel), request, response); // 返回响应数据 return response; &#125; catch (RemotingSendRequestException e) &#123;&#125; &#125; else &#123; this.closeChannel(addr, channel); throw new RemotingConnectException(addr); &#125;&#125; 延迟消息消息处理BrokerStartup 初始化 BrokerController 调用 registerProcessor() 方法将 SendMessageProcessor 注册到 NettyRemotingServer 中，对应的请求 ID 为 SEND_MESSAGE = 10，NettyServerHandler 在处理请求时通过 CMD 会获取处理器执行 processRequest 123456// 参数一：处理通道的事件； 参数二：客户端public RemotingCommand processRequest(ChannelHandlerContext ctx, RemotingCommand request) &#123; RemotingCommand response = null; response = asyncProcessRequest(ctx, request).get(); return response;&#125; SendMessageProcessor#asyncConsumerSendMsgBack：异步发送消费者的回调消息 final RemotingCommand response：创建一个服务器响应对象 final ConsumerSendMsgBackRequestHeader requestHeader：解析出客户端请求头信息，几个核心字段： private Long offset：回退消息的 CommitLog offset private Integer delayLevel：延迟级别，一般是 0 private String originMsgId, originTopic：原始的消息 ID，主题 private Integer maxReconsumeTimes：最大重试次数，默认是 16 次 if ()：鉴权，是否找到订阅组配置、Broker 是否支持写请求、订阅组是否支持消息重试 String newTopic = MixAll.getRetryTopic(...)：获取消费者组的重试主题，规则是 %RETRY%GroupName int queueIdInt = Math.abs()：重试主题下的队列 ID 是 0 TopicConfig topicConfig：获取重试主题的配置信息 MessageExt msgExt：根据消息的物理 offset 到存储模块查询，内部先查询出这条消息的 size，然后再根据 offset 和 size 查询出整条 msg final String retryTopic：获取消息的原始主题 if (null == retryTopic)：条件成立说明当前消息是第一次被回退， 添加 RETRY_TOPIC 属性 msgExt.setWaitStoreMsgOK(false)：异步刷盘 if (msgExt...() &gt;= maxReconsumeTimes || delayLevel &lt; 0)：消息重试次数超过最大次数，不支持重试 newTopic = MixAll.getDLQTopic()：获取消费者的死信队列，规则是 %DLQ%GroupName queueIdInt, topicConfig：死信队列 ID 为 0，创建死信队列的配置 if (0 == delayLevel)：说明延迟级别由 Broker 控制 delayLevel = 3 + msgExt.getReconsumeTimes()：延迟级别默认从 3 级开始，每重试一次，延迟级别 +1 msgExt.setDelayTimeLevel(delayLevel)：将延迟级别设置进消息属性，存储时会检查该属性，该属性值 &gt; 0 会将消息的主题和队列修改为调度主题和调度队列 ID MessageExtBrokerInner msgInner：创建一条空消息，消息属性从 offset 查询出来的 msg 中拷贝 msgInner.setReconsumeTimes)：重试次数设置为原 msg 的次数 +1 UtilAll.isBlank(originMsgId)：判断消息是否是初次返回到服务器 true：说明 msgExt 消息是第一次被返回到服务器，此时使用该 msg 的 id 作为 originMessageId false：说明原始消息已经被重试不止 1 次，此时使用 offset 查询出来的 msg 中的 originMessageId CompletableFuture putMessageResult = ..asyncPutMessage(msgInner)：调用存储模块存储消息 DefaultMessageStore#asyncPutMessage： PutMessageResult result = this.commitLog.asyncPutMessage(msg)：将新消息存储到 CommitLog 中 调度服务DefaultMessageStore 中有成员属性 ScheduleMessageService，在 start 方法中会启动该调度服务 成员变量： 延迟级别属性表： 1234// 存储延迟级别对应的 延迟时间长度 （单位：毫秒）private final ConcurrentMap&lt;Integer /* level */, Long/* delay timeMillis */&gt; delayLevelTable;// 存储延迟级别 queue 的消费进度 offset，该 table 每 10 秒钟，会持久化一次，持久化到本地磁盘private final ConcurrentMap&lt;Integer /* level */, Long/* offset */&gt; offsetTable; 最大延迟级别： 1private int maxDelayLevel; 模块启动状态： 1private final AtomicBoolean started = new AtomicBoolean(false); 定时器：内部有线程资源，可执行调度任务 1private Timer timer; 成员方法： load()：加载调度消息，初始化 delayLevelTable 和 offsetTable 1public boolean load() start()：启动消息调度服务 1public void start() if (started.compareAndSet(false, true))：将启动状态设为 true this.timer：创建定时器对象 for (... : this.delayLevelTable.entrySet())：为每个延迟级别创建一个延迟任务提交到 timer ，周期执行，这样就可以将延迟消息得到及时的消费 this.timer.scheduleAtFixedRate()：提交周期型任务，延迟 10 秒执行，周期为 10 秒，持久化延迟队列消费进度任务 ScheduleMessageService.this.persist()：持久化消费进度 调度任务DeliverDelayedMessageTimerTask 是一个任务类 成员变量： 延迟级别：延迟队列任务处理的延迟级别 1private final int delayLevel; 消费进度：延迟队列任务处理的延迟队列的消费进度 1private final long offset; 成员方法： run()：执行任务 1234public void run() &#123; if (isStarted()) &#123; this.executeOnTimeup();&#125; executeOnTimeup()：执行任务 1public void executeOnTimeup() ConsumeQueue cq：获取出该延迟队列任务处理的延迟队列 ConsumeQueue SelectMappedBufferResult bufferCQ：根据消费进度查询出 SMBR 对象 for (; i &lt; bufferCQ.getSize(); i += ConsumeQueue.CQ_STORE_UNIT_SIZE)：每次读取 20 各字节的数据 offsetPy, sizePy：延迟消息的物理偏移量和消息大小 long tagsCode：延迟消息的交付时间，在 ReputMessageService 转发时根据消息的 DELAY 属性是否 &gt;0 ，会在 tagsCode 字段存储交付时间 long deliver... = this.correctDeliverTimestamp(..)：校准交付时间，延迟时间过长会调整为当前时间立刻执行 long countdown = deliverTimestamp - now：计算差值 if (countdown &lt;= 0)：消息已经到达交付时间了 MessageExt msgExt：根据物理偏移量和消息大小获取这条消息 MessageExtBrokerInner msgInner：构建一条新消息，将原消息的属性拷贝过来 long tagsCodeValue：不再是交付时间了 MessageAccessor.clearProperty(msgInner, DELAY..)：清理新消息的 DELAY 属性，避免存储时重定向到延迟队列 msgInner.setTopic()：修改主题为原始的主题 %RETRY%GroupName String queueIdStr：修改队列 ID 为原始的 ID PutMessageResult putMessageResult：将新消息存储到 CommitLog，消费者订阅的是目标主题，会再次消费该消息 else：消息还未到达交付时间 ScheduleMessageService.this.timer.schedule()：创建该延迟级别的任务，延迟 countDown 毫秒之后再执行 ScheduleMessageService.this.updateOffset()：更新延迟级别队列的消费进度 PutMessageResult putMessageResult bufferCQ == null：说明通过消费进度没有获取到数据 if (offset &lt; cqMinOffset)：如果消费进度比最小位点都小，说明是过期数据，重置为最小位点 ScheduleMessageService.this.timer.schedule()：重新提交该延迟级别对应的延迟队列任务，延迟 100 毫秒后执行 事务消息生产者类TransactionMQProducer 类发送事务消息时使用 成员变量： 事务回查线程池资源： 123456 private ExecutorService executorService;* 事务监听器： ```java private TransactionListener transactionListener; 核心方法： start()：启动方法 1public void start() this.defaultMQProducerImpl.initTransactionEnv()：初始化生产者实例和回查线程池资源 super.start()：启动生产者实例 sendMessageInTransaction()：发送事务消息 12345public TransactionSendResult sendMessageInTransaction(final Message msg, final Object arg) &#123; msg.setTopic(NamespaceUtil.wrapNamespace(this.getNamespace(), msg.getTopic())); // 调用实现类的发送方法 return this.defaultMQProducerImpl.sendMessageInTransaction(msg, null, arg);&#125; TransactionListener transactionListener = getCheckListener()：获取监听器 if (null == localTransactionExecuter &amp;&amp; null == transactionListener)：两者都为 null 抛出异常 MessageAccessor.putProperty(msg, MessageConst.PROPERTY_TRANSACTION_PREPARED, &quot;true&quot;)：设置事务标志 sendResult = this.send(msg)：发送消息，同步发送 switch (sendResult.getSendStatus())：判断发送消息的结果状态 case SEND_OK：消息发送成功 msg.setTransactionId(transactionId)：设置事务 ID 为消息的 UNIQ_KEY 属性 localTransactionState = ...executeLocalTransactionBranch(msg, arg)：执行本地事务 case SLAVE_NOT_AVAILABLE：其他情况都需要回滚事务 localTransactionState = LocalTransactionState.ROLLBACK_MESSAGE：事务状态设置为回滚 this.endTransaction(sendResult, ...)：结束事务 EndTransactionRequestHeader requestHeader：构建事务结束头对象 this.mQClientFactory.getMQClientAPIImpl().endTransactionOneway()：向 Broker 发起事务结束的单向请求 接受消息SendMessageProcessor 是服务端处理客户端发送来的消息的处理器，processRequest() 方法处理请求 核心方法： asyncProcessRequest()：处理请求 12345678910111213141516171819202122232425public CompletableFuture&lt;RemotingCommand&gt; asyncProcessRequest(ChannelHandlerContext ctx, RemotingCommand request) &#123; final SendMessageContext mqtraceContext; switch (request.getCode()) &#123; // 回调消息回退 case RequestCode.CONSUMER_SEND_MSG_BACK: return this.asyncConsumerSendMsgBack(ctx, request); default: // 解析出请求头对象 SendMessageRequestHeader requestHeader = parseRequestHeader(request); if (requestHeader == null) &#123; return CompletableFuture.completedFuture(null); &#125; // 创建上下文对象 mqtraceContext = buildMsgContext(ctx, requestHeader); // 前置处理器 this.executeSendMessageHookBefore(ctx, request, mqtraceContext); // 判断是否是批量消息 if (requestHeader.isBatch()) &#123; return this.asyncSendBatchMessage(ctx, request, mqtraceContext, requestHeader); &#125; else &#123; return this.asyncSendMessage(ctx, request, mqtraceContext, requestHeader); &#125; &#125;&#125; asyncSendMessage()：异步处理发送消息 1private CompletableFuture&lt;RemotingCommand&gt; asyncSendMessage(ChannelHandlerContext ctx, RemotingCommand request, SendMessageContext mqtraceContext, SendMessageRequestHeader requestHeader) RemotingCommand response：创建响应对象 MessageExtBrokerInner msgInner = new MessageExtBrokerInner()：创建 msgInner 对象，并赋值相关的属性，主题和队列 ID 都是请求头中的 String transFlag：获取事务属性 if (transFlag != null &amp;&amp; Boolean.parseBoolean(transFlag))：判断事务属性是否是 true，走事务消息的存储流程 putMessageResult = ...asyncPrepareMessage(msgInner)：事务消息处理流程 1234public CompletableFuture&lt;PutMessageResult&gt; asyncPutHalfMessage(MessageExtBrokerInner messageInner) &#123; // 调用存储模块，将修改后的 msg 存储进 Broker(CommitLog) return store.asyncPutMessage(parseHalfMessageInner(messageInner));&#125; TransactionalMessageBridge#parseHalfMessageInner： MessageAccessor.putProperty(...)：将消息的原主题和队列 ID 放入消息的属性中 msgInner.setSysFlag(...)：消息设置为非事务状态 msgInner.setTopic(TransactionalMessageUtil.buildHalfTopic())：消息主题设置为半消息主题 msgInner.setQueueId(0)：队列 ID 设置为 0 else：普通消息存储 回查处理ClientRemotingProcessor 是客户端用于处理请求，创建 MQClientAPIImpl 时将该处理器注册到 Netty 中，processRequest() 方法根据请求的命令码，进行不同的处理，事务回查的处理命令码为 CHECK_TRANSACTION_STATE Broker 端有定时任务发送回查请求 成员方法： checkTransactionState()：检查事务状态 1public RemotingCommand checkTransactionState(ChannelHandlerContext ctx, RemotingCommand request) final CheckTransactionStateRequestHeader requestHeader：解析出请求头对象 final MessageExt messageExt：从请求 body 中解析出服务器回查的事务消息 String transactionId：提取 UNIQ_KEY 字段属性值赋值给事务 ID final String group：提取生产者组名 MQProducerInner producer = this...selectProducer(group)：根据生产者组获取生产者对象 String addr = RemotingHelper.parseChannelRemoteAddr()：解析出要回查的 Broker 服务器的地址 producer.checkTransactionState(addr, messageExt, requestHeader)：生产者的事务回查 Runnable request = new Runnable()：创建回查事务状态任务对象 获取生产者的 TransactionCheckListener 和 TransactionListener，选择一个不为 null 的监听器进行事务状态回查 this.processTransactionState()：处理回查状态 EndTransactionRequestHeader thisHeader：构建 EndTransactionRequestHeader 对象 DefaultMQProducerImpl...endTransactionOneway()：向 Broker 发起结束事务单向请求，二阶段提交 this.checkExecutor.submit(request)：提交到线程池运行 参考图：https://www.processon.com/view/link/61c8257e0e3e7474fb9dcbc0 参考视频：https://space.bilibili.com/457326371 事务提交EndTransactionProcessor 类是服务端用来处理客户端发来的提交或者回滚请求 processRequest()：处理请求 1public RemotingCommand processRequest(ChannelHandlerContext ctx, RemotingCommand request) EndTransactionRequestHeader requestHeader：从请求中解析出 EndTransactionRequestHeader if (MessageSysFlag.TRANSACTION_COMMIT_TYPE)：事务提交 result = this.brokerController...commitMessage(requestHeader)：根据 commitLogOffset 提取出 halfMsg 消息 MessageExtBrokerInner msgInner：根据 result 克隆出一条新消息 msgInner.setTopic(msgExt.getUserProperty(...))：设置回原主题 msgInner.setQueueId(Integer.parseInt(msgExt.getUserProperty(..)))：设置回原队列 ID MessageAccessor.clearProperty()：清理上面的两个属性 MessageAccessor.clearProperty(msgInner, ...)：清理事务属性 RemotingCommand sendResult = sendFinalMessage(msgInner)：调用存储模块存储至 Broker this.brokerController...deletePrepareMessage(result.getPrepareMessage())：向删除（OP）队列添加消息，消息体的数据是 halfMsg 的 queueOffset，表示半消息队列指定的 offset 的消息已被删除 if (this...putOpMessage(msgExt, TransactionalMessageUtil.REMOVETAG))：添加一条 OP 数据 MessageQueue messageQueue：新建一个消息队列，OP 队列 return addRemoveTagInTransactionOp(messageExt, messageQueue)：添加数据 Message message：创建 OP 消息 writeOp(message, messageQueue)：写入 OP 消息 else if (MessageSysFlag.TRANSACTION_ROLLBACK_TYPE)：事务回滚 this.brokerController...deletePrepareMessage(result.getPrepareMessage())：也需要向 OP 队列添加消息 Consumer消费者类默认消费DefaultMQPushConsumer 类是默认的消费者类 成员变量： 消费者实现类： 1protected final transient DefaultMQPushConsumerImpl defaultMQPushConsumerImpl; 消费属性： 12private String consumerGroup; // 消费者组private MessageModel messageModel = MessageModel.CLUSTERING;\t// 消费模式，默认集群模式 订阅信息：key 是主题，value 是过滤表达式，一般是 tag 1private Map&lt;String, String &gt; subscription = new HashMap&lt;String, String&gt;() 消息监听器：消息处理逻辑，并发消费 MessageListenerConcurrently，顺序（分区）消费 MessageListenerOrderly 1private MessageListener messageListener; 消费位点：当从 Broker 获取当前组内该 queue 的 offset 不存在时，consumeFromWhere 才有效，默认值代表从队列的最后 offset 开始消费，当队列内再有一条新的 msg 加入时，消费者才会去消费 1private ConsumeFromWhere consumeFromWhere = ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET; 消费时间戳：当消费位点配置的是 CONSUME_FROM_TIMESTAMP 时，并且服务器 Group 内不存在该 queue 的 offset 时，会使用该时间戳进行消费 1private String consumeTimestamp = UtilAll.timeMillisToHumanString3(System.currentTimeMillis() - (1000 * 60 * 30));// 消费者创建时间 - 30秒，转换成 格式： 年月日小时分钟秒，比如 20220203171201 队列分配策略：主题下的队列分配策略，RebalanceImpl 对象依赖该算法 1private AllocateMessageQueueStrategy allocateMessageQueueStrategy; 消费进度存储器： 1private OffsetStore offsetStore; 核心方法： start()：启动消费者 1public void start() shutdown()：关闭消费者 1public void shutdown() registerMessageListener()：注册消息监听器 1public void registerMessageListener(MessageListener messageListener) subscribe()：添加订阅信息，将订阅信息放入负载均衡对象的 subscriptionInner 中 1public void subscribe(String topic, String subExpression) unsubscribe()：删除订阅指定主题的信息 1public void unsubscribe(String topic) suspend()：停止消费 1public void suspend() resume()：恢复消费 1public void resume() 默认实现DefaultMQPushConsumerImpl 是默认消费者的实现类 成员变量： 客户端实例：整个进程内只有一个客户端实例对象 1private MQClientInstance mQClientFactory; 消费者实例：门面对象 1private final DefaultMQPushConsumer defaultMQPushConsumer; 负载均衡：分配订阅主题的队列给当前消费者，20 秒钟一个周期执行 Rebalance 算法（客户端实例触发） 1private final RebalanceImpl rebalanceImpl = new RebalancePushImpl(this); 消费者信息： 1234private final long consumerStartTimestamp;\t// 消费者启动时间private volatile ServiceState serviceState;\t// 消费者状态private volatile boolean pause = false; // 是否暂停private boolean consumeOrderly = false; // 是否顺序消费 拉取消息：封装拉消息的 API，服务器 Broker 返回结果中包含下次 Pull 时推荐的 BrokerId，根据本次请求数据的冷热程度进行推荐 1private PullAPIWrapper pullAPIWrapper; 消息消费服务：并发消费和顺序消费 1private ConsumeMessageService consumeMessageService; 流控： 12private long queueFlowControlTimes = 0; // 队列流控次数，默认每1000次流控，进行一次日志打印private long queueMaxSpanFlowControlTimes = 0;\t// 流控使用，控制打印日志 HOOK：钩子方法 1234// 过滤消息 hookprivate final ArrayList&lt;FilterMessageHook&gt; filterMessageHookList;// 消息执行hook，在消息处理前和处理后分别执行 hook.before hook.after 系列方法private final ArrayList&lt;ConsumeMessageHook&gt; consumeMessageHookList; 核心方法： start()：加锁保证线程安全 1public synchronized void start() this.checkConfig()：检查配置，包括组名、消费模式、订阅信息、消息监听器等 this.copySubscription()：拷贝订阅信息到 RebalanceImpl 对象 this.rebalanceImpl.getSubscriptionInner().put(topic, subscriptionData)：将订阅信息加入 rbl 的 map 中 this.messageListenerInner = ...getMessageListener()：将消息监听器保存到实例对象 switch (this.defaultMQPushConsumer.getMessageModel())：判断消费模式，广播模式下直接返回 final String retryTopic：创建当前消费者组重试的主题名，规则 %RETRY%ConsumerGroup SubscriptionData subscriptionData = FilterAPI.buildSubscriptionData()：创建重试主题的订阅数据对象 this.rebalanceImpl.getSubscriptionInner().put(retryTopic, subscriptionData)：将创建的重试主题加入到 rbl 对象的 map 中，消息重试时会加入到该主题，消费者订阅这个主题之后，就有机会再次拿到该消息进行消费处理 this.mQClientFactory = ...getOrCreateMQClientInstance()：获取客户端实例对象 this.rebalanceImpl.：初始化负载均衡对象，设置队列分配策略对象到属性中 this.pullAPIWrapper = new PullAPIWrapper()：创建拉消息 API 对象，内部封装了查询推荐主机算法 this.pullAPIWrapper.registerFilterMessageHook(filterMessageHookList)：将过滤 Hook 列表注册到该对象内，消息拉取下来之后会执行该 Hook，再进行一次自定义的消息过滤 this.offsetStore = new RemoteBrokerOffsetStore()：默认集群模式下创建消息进度存储器 this.consumeMessageService = ...：根据消息监听器的类型创建消费服务 this.consumeMessageService.start()：启动消费服务 boolean registerOK = mQClientFactory.registerConsumer()：将消费者注册到客户端实例中，客户端提供的服务： 心跳服务：把订阅数据同步到订阅主题的 Broker 拉消息服务：内部 PullMessageService 启动线程，基于 PullRequestQueue 工作，消费者负载均衡分配到队列后会向该队列提交 PullRequest 队列负载服务：每 20 秒调用一次 consumer.doRebalance() 接口 消息进度持久化 动态调整消费者、消费服务线程池 mQClientFactory.start()：启动客户端实例 this.updateTopic：从 nameserver 获取主题路由数据，生成主题集合放入 rbl 对象的 table this.mQClientFactory.checkClientInBroker()：检查服务器是否支持消息过滤模式，一般使用 tag 过滤，服务器默认支持 this.mQClientFactory.sendHeartbeatToAllBrokerWithLock()：向所有已知的 Broker 节点，发送心跳数据 this.mQClientFactory.rebalanceImmediately()：唤醒 rbl 线程，触发负载均衡执行 负载均衡实现方式MQClientInstance#start 中会启动负载均衡服务 RebalanceService： 123456789public void run() &#123;\t// 检查停止标记 while (!this.isStopped()) &#123; // 休眠 20 秒，防止其他线程饥饿，所以【每 20 秒负载均衡一次】 this.waitForRunning(waitInterval); // 调用客户端实例的负载均衡方法，底层【会遍历所有消费者，调用消费者的负载均衡】 this.mqClientFactory.doRebalance(); &#125;\t&#125; RebalanceImpl 类成员变量： 分配给当前消费者的处理队列：处理消息队列集合，ProcessQueue 是 MQ 队列在消费者端的快照 1protected final ConcurrentMap&lt;MessageQueue, ProcessQueue&gt; processQueueTable; 消费者订阅主题的队列信息： 1protected final ConcurrentMap&lt;String/* topic */, Set&lt;MessageQueue&gt;&gt; topicSubscribeInfoTable; 订阅数据： 1protected final ConcurrentMap&lt;String/* topic */, SubscriptionData&gt; subscriptionInner; 队列分配策略： 1protected AllocateMessageQueueStrategy allocateMessageQueueStrategy; 成员方法： doRebalance()：负载均衡方法，以每个消费者实例为粒度进行负载均衡 123456789101112131415public void doRebalance(final boolean isOrder) &#123; // 获取当前消费者的订阅数据 Map&lt;String, SubscriptionData&gt; subTable = this.getSubscriptionInner(); if (subTable != null) &#123; // 遍历所有的订阅主题 for (final Entry&lt;String, SubscriptionData&gt; entry : subTable.entrySet()) &#123; // 获取订阅的主题 final String topic = entry.getKey(); // 按照主题进行负载均衡 this.rebalanceByTopic(topic, isOrder); &#125; &#125; // 将分配到当前消费者的队列进行过滤，不属于当前消费者订阅主题的直接移除 this.truncateMessageQueueNotMyTopic();&#125; 集群模式下： Set&lt;MessageQueue&gt; mqSet = this.topicSubscribeInfoTable.get(topic)：订阅的主题下的全部队列信息 cidAll = this...findConsumerIdList(topic, consumerGroup)：从服务器获取消费者组下的全部消费者 ID Collections.sort(mqAll)：主题 MQ 队列和消费者 ID 都进行排序，保证每个消费者的视图一致性 allocateResult = strategy.allocate()： 调用队列分配策略，给当前消费者进行分配 MessageQueue（下一节） boolean changed = this.updateProcessQueueTableInRebalance(...)：更新队列处理集合，mqSet 是 rbl 算法分配到当前消费者的 MQ 集合 while (it.hasNext())：遍历当前消费者的所有处理队列 if (mq.getTopic().equals(topic))：该 MQ 是 本次 rbl 分配算法计算的主题 if (!mqSet.contains(mq))：该 MQ 经过 rbl 计算之后，被分配到其它 Consumer 节点 pq.setDropped(true)：将删除状态设置为 true if (this.removeUnnecessaryMessageQueue(mq, pq))：删除不需要的 MQ 队列 this...getOffsetStore().persist(mq)：在 MQ 归属的 Broker 节点持久化消费进度 this...getOffsetStore().removeOffset(mq)：删除该 MQ 在本地的消费进度 if (this.defaultMQPushConsumerImpl.isConsumeOrderly() &amp;&amp;)：是否是顺序消费和集群模式 if (pq.getLockConsume().tryLock(1000, ..))： 获取锁成功，说明顺序消费任务已经停止消费工作 return this.unlockDelay(mq, pq)：释放锁 Broker 端的队列锁，向服务器发起 oneway 的解锁请求 if (pq.hasTempMessage())：队列中有消息，延迟 20 秒释放队列分布式锁，确保全局范围内只有一个消费任务 运行中 else：当前消费者本地该消费任务已经退出，直接释放锁 else：顺序消费任务正在消费一批消息，不可打断，增加尝试获取锁的次数 it.remove()：从 processQueueTable 移除该 MQ else if (pq.isPullExpired())：说明当前 MQ 还是被当前 Consumer 消费，此时判断一下是否超过 2 分钟未到服务器 拉消息，如果条件成立进行上述相同的逻辑 for (MessageQueue mq : mqSet)：开始处理当前主题新分配到当前节点的队列 if (isOrder &amp;&amp; !this.lock(mq))：顺序消息为了保证有序性，需要获取队列锁 ProcessQueue pq = new ProcessQueue()：为每个新分配的消息队列创建快照队列 long nextOffset = this.computePullFromWhere(mq)：从服务端获取新分配的 MQ 的消费进度 ProcessQueue pre = this.processQueueTable.putIfAbsent(mq, pq)：保存到处理队列集合 PullRequest pullRequest = new PullRequest()：创建拉取请求对象 this.dispatchPullRequest(pullRequestList)：放入 PullMessageService 的本地阻塞队列内，用于拉取消息工作 lockAll()：续约锁，对消费者的所有队列进行续约 1public void lockAll() HashMap&lt;String, Set&lt;MessageQueue&gt;&gt; brokerMqs：将分配给当前消费者的全部 MQ 按照 BrokerName 分组 while (it.hasNext())：遍历所有的分组 final Set&lt;MessageQueue&gt; mqs：获取该 Broker 上分配给当前消费者的 queue 集合 FindBrokerResult findBrokerResult：查询 Broker 主节点信息 LockBatchRequestBody requestBody：创建请求对象，填充属性 Set&lt;MessageQueue&gt; lockOKMQSet：以组为单位向 Broker 发起批量续约锁的同步请求，返回成功的队列集合 for (MessageQueue mq : lockOKMQSet)：遍历续约锁成功的 MQ processQueue.setLocked(true)：分布式锁状态设置为 true，表示允许顺序消费 processQueue.setLastLockTimestamp(System.currentTimeMillis())：设置上次获取锁的时间为当前时间 for (MessageQueue mq : mqs)：遍历当前 Broker 上的所有队列集合 if (!lockOKMQSet.contains(mq))：条件成立说明续约锁失败 processQueue.setLocked(false)：分布式锁状态设置为 false，表示不允许顺序消费 队列分配AllocateMessageQueueStrategy 类是队列的分配策略 平均分配：AllocateMessageQueueAveragely 类 123456789101112131415161718192021// 参数一：消费者组 参数二：当前消费者id // 参数三：主题的全部队列，包括所有 broker 上该主题的 mq 参数四：全部消费者id集合public List&lt;MessageQueue&gt; allocate(String consumerGroup, String currentCID, List&lt;MessageQueue&gt; mqAll, List&lt;String&gt; cidAll) &#123; // 获取当前消费者在全部消费者中的位置，【全部消费者是已经排序好的，排在前面的优先分配更多的队列】 int index = cidAll.indexOf(currentCID); // 平均分配完以后，还剩余的待分配的 mq 的数量 int mod = mqAll.size() % cidAll.size(); // 首先判断整体的 mq 的数量是否小于消费者的数量，小于消费者的数量就说明不够分的，先分一个 int averageSize = mqAll.size() &lt;= cidAll.size() ? 1 : // 成立需要多分配一个队列，因为更靠前 (mod &gt; 0 &amp;&amp; index &lt; mod ? mqAll.size() / cidAll.size() + 1 : mqAll.size() / cidAll.size()); // 获取起始的分配位置 int startIndex = (mod &gt; 0 &amp;&amp; index &lt; mod) ? index * averageSize : index * averageSize + mod; // 防止索引越界 int range = Math.min(averageSize, mqAll.size() - startIndex); // 开始分配，【挨着分配，是直接就把当前的 消费者分配完成】 for (int i = 0; i &lt; range; i++) &#123; result.add(mqAll.get((startIndex + i) % mqAll.size())); &#125; return result;&#125; 队列排序后：Q1 → Q2 → Q3，消费者排序后 C1 → C2 → C3 轮流分配：AllocateMessageQueueAveragelyByCircle 指定机房平均分配：AllocateMessageQueueByMachineRoom，前提是 Broker 的命名规则为 机房名@BrokerName 拉取服务实现方式MQClientInstance#start 中会启动消息拉取服务：PullMessageService 12345678910111213public void run() &#123; // 检查停止标记，【循环拉取】 while (!this.isStopped()) &#123; try &#123; // 从阻塞队列中获取拉消息请求 PullRequest pullRequest = this.pullRequestQueue.take(); // 拉取消息，获取请求对应的使用当前消费者组中的哪个消费者，调用消费者的 pullMessage 方法 this.pullMessage(pullRequest); &#125; catch (Exception e) &#123; log.error(&quot;Pull Message Service Run Method exception&quot;, e); &#125; &#125;&#125; DefaultMQPushConsumerImpl#pullMessage： ProcessQueue processQueue = pullRequest.getProcessQueue()：获取请求对应的快照队列，并判断是否是删除状态 this.executePullRequestLater()：如果当前消费者不是运行状态，则拉消息任务延迟 3 秒后执行，如果是暂停状态延迟 1 秒 流控的逻辑： long cachedMessageCount = processQueue.getMsgCount().get()：获取消费者本地该 queue 快照内缓存的消息数量，如果大于 1000 条，进行流控，延迟 50 毫秒 long cachedMessageSizeInMiB： 消费者本地该 queue 快照内缓存的消息容量 size，超过 100m 消息未被消费进行流控 if(processQueue.getMaxSpan() &gt; 2000)：消费者本地缓存消息第一条消息最后一条消息跨度超过 2000 进行流控 SubscriptionData subscriptionData：本次拉消息请求订阅的主题数据，如果调用了 unsubscribe(主题) 将会获取为 null PullCallback pullCallback = new PullCallback()：拉消息处理回调对象 pullResult = ...processPullResult()：预处理 PullResult 结果，将服务器端指定 MQ 的拉消息下一次的推荐节点保存到 pullFromWhichNodeTable 中，并进行消息过滤 case FOUND：正常拉取到消息 pullRequest.setNextOffset(pullResult.getNextBeginOffset())：更新 pullRequest 对象下一次拉取消息的位点 if (pullResult.getMsgFoundList() == null...)：消息过滤导致消息全部被过滤掉，需要立马发起下一次拉消息 boolean .. = processQueue.putMessage()：将服务器拉取的消息集合加入到消费者本地的 processQueue 内 DefaultMQPushConsumerImpl...submitConsumeRequest()：提交消费任务，分为顺序消费和并发消费 Defaul..executePullRequestImmediately(pullRequest)：将更新过 nextOffset 字段的 PullRequest 对象，再次放到 pullMessageService 的阻塞队列中，形成闭环 case NO_NEW_MSG ||NO_MATCHED_MSG：表示本次 pull 没有新的可消费的信息 pullRequest.setNextOffset()：更新更新 pullRequest 对象下一次拉取消息的位点 Defaul..executePullRequestImmediately(pullRequest)：再次拉取请求 case OFFSET_ILLEGAL：本次 pull 时使用的 offset 是无效的，即 offset &gt; maxOffset || offset &lt; minOffset pullRequest.setNextOffset()：调整 pullRequest.nextOffset 为正确的 offset pullRequest.getProcessQueue().setDropped(true)：设置该 processQueue 为删除状态，如果有该 queue 的消费任务，消费任务会马上停止 DefaultMQPushConsumerImpl.this.executeTaskLater()：提交异步任务，10 秒后去执行 DefaultMQPushConsumerImpl...updateOffset()：更新 offsetStore 该 MQ 的 offset 为正确值，内部直接替换 DefaultMQPushConsumerImpl...persist()：持久化该 messageQueue 的 offset 到 Broker 端 DefaultMQPushConsumerImpl...removeProcessQueue()： 删除该消费者该 messageQueue 对应的 processQueue 这里没有再次提交 pullRequest 到 pullMessageService 的队列，那该队列不再拉消息了吗？ 负载均衡 rbl 程序会重建该队列的 processQueue，重建完之后会为该队列创建新的 PullRequest 对象 int sysFlag = PullSysFlag.buildSysFlag()：构建标志对象，sysFlag 高 4 位未使用，低 4 位使用，从左到右 0000 0011 第一位：表示是否提交消费者本地该队列的 offset，一般是 1 第二位：表示是否允许服务器端进行长轮询，一般是 1 第三位：表示是否提交消费者本地该主题的订阅数据，一般是 0 第四位：表示是否为类过滤，一般是 0 this.pullAPIWrapper.pullKernelImpl()：拉取消息的核心方法 封装对象PullAPIWrapper 类封装了拉取消息的 API 成员变量： 推荐拉消息使用的主机 ID： 1private ConcurrentMap&lt;MessageQueue, AtomicLong/* brokerId */&gt; pullFromWhichNodeTable 成员方法： pullKernelImpl()：拉消息 FindBrokerResult findBrokerResult：本地查询指定 BrokerName 的地址信息，推荐节点或者主节点 if (null == findBrokerResult)：查询不到，就到 Namesrv 获取指定 topic 的路由数据 if (findBrokerResult.isSlave())：成立说明 findBrokerResult 表示的主机为 slave 节点，slave 不存储 offset 信息 sysFlagInner = PullSysFlag.clearCommitOffsetFlag(sysFlagInner)：将 sysFlag 标记位中 CommitOffset 的位置为 0 PullMessageRequestHeader requestHeader：创建请求头对象，封装所有的参数 PullResult pullResult = this.mQClientFactory.getMQClientAPIImpl().pullMessage()：调用客户端实例的方法，核心逻辑就是将业务数据转化为 RemotingCommand 通过 NettyRemotingClient 的 IO 进行通信 RemotingCommand request：创建网络层传输对象 RemotingCommand 对象，请求 ID 为 PULL_MESSAGE = 11 return this.pullMessageSync(...)：此处是异步调用，处理结果放入 ResponseFuture 中，参考服务端小节的处理器类 NettyServerHandler#processMessageReceived 方法 RemotingCommand response = responseFuture.getResponseCommand()：获取服务器端响应数据 response PullResult pullResult：从 response 内提取出来拉消息结果对象，将响应头 PullMessageResponseHeader 对象中信息填充到 PullResult 中，列出两个重要的字段： private Long suggestWhichBrokerId：服务端建议客户端下次 Pull 时选择的 BrokerID private Long nextBeginOffset：客户端下次 Pull 时使用的 offset 信息 pullCallback.onSuccess(pullResult)：将 PullResult 交给拉消息结果处理回调对象，调用 onSuccess 方法 拉取处理处理器BrokerStartup#createBrokerController 方法中创建了 BrokerController 并进行初始化，调用 registerProcessor() 方法将处理器 PullMessageProcessor 注册到 NettyRemotingServer 中，对应的请求 ID 为 PULL_MESSAGE = 11，NettyServerHandler 在处理请求时通过请求 ID 会获取处理器执行 processRequest 方法 12// 参数一：服务器与客户端 netty 通道； 参数二：客户端请求； 参数三：是否允许服务器端长轮询，默认 trueprivate RemotingCommand processRequest(final Channel channel, RemotingCommand request, boolean brokerAllowSuspend) RemotingCommand response：创建响应对象，设置为响应类型的请求，响应头是 PullMessageResponseHeader final PullMessageResponseHeader responseHeader：获取响应对象的 header final PullMessageRequestHeader requestHeader：解析出请求头 PullMessageRequestHeader response.setOpaque(request.getOpaque())：设置 opaque 属性，客户端根据该字段获取 ResponseFuture 进行处理 进行一些鉴权的逻辑：是否允许长轮询、提交 offset、topicConfig 是否是空、队列 ID 是否合理 ConsumerGroupInfo consumerGroupInfo：获取消费者组信息，包含全部的消费者和订阅数据 subscriptionData = consumerGroupInfo.findSubscriptionData()：获取指定主题的订阅数据 if (!ExpressionType.isTagType()：表达式匹配 MessageFilter messageFilter：创建消息过滤器，一般是通过 tagCode 进行过滤 DefaultMessageStore.getMessage()：查询消息的核心逻辑，在 Broker 端查询消息（存储端笔记详解了该源码） response.setRemark()：设置此次响应的状态 responseHeader.set..：设置响应头对象的一些字段 switch (this.brokerController.getMessageStoreConfig().getBrokerRole())：如果当前主机节点角色为 slave 并且从节点读并未开启的话，直接给客户端 一个状态 PULL_RETRY_IMMEDIATELY，并设置为下次从主节点读 if (this.brokerController.getBrokerConfig().isSlaveReadEnable())：消费太慢，下次从另一台机器拉取 switch (getMessageResult.getStatus())：根据 getMessageResult 的状态设置 response 的 code 1234567891011public enum GetMessageStatus &#123; FOUND, // 查询成功 NO_MATCHED_MESSAGE, // 未查询到到消息，服务端过滤 tagCode MESSAGE_WAS_REMOVING,\t// 查询时赶上 CommitLog 清理过期文件，导致查询失败，立刻尝试 OFFSET_FOUND_NULL, // 查询时赶上 ConsumerQueue 清理过期文件，导致查询失败，【进行长轮询】 OFFSET_OVERFLOW_BADLY,\t// pullRequest.offset 越界 maxOffset OFFSET_OVERFLOW_ONE,\t// pullRequest.offset == CQ.maxOffset，【进行长轮询】 OFFSET_TOO_SMALL, // pullRequest.offset 越界 minOffset NO_MATCHED_LOGIC_QUEUE,\t// 没有匹配到逻辑队列 NO_MESSAGE_IN_QUEUE,\t// 空队列，创建队列也是因为查询导致，【进行长轮询】&#125; switch (response.getCode())：根据 response 状态做对应的业务处理 case ResponseCode.SUCCESS：查询成功 final byte[] r = this.readGetMessageResult()：本次 pull 出来的全部消息导入 byte 数组 response.setBody(r)：将消息的 byte 数组保存到 response body 字段 case ResponseCode.PULL_NOT_FOUND：产生这种情况大部分原因是 pullRequest.offset == queue.maxOffset，说明已经没有需要获取的消息，此时如果直接返回给客户端，客户端会立刻重新请求，还是继续返回该状态，频繁拉取服务器导致服务器压力大，所以此处需要长轮询 if (brokerAllowSuspend &amp;&amp; hasSuspendFlag)：brokerAllowSuspend &#x3D; true，当长轮询结束再次执行 processRequest 时该参数为 false，所以每次 Pull 请求至多在服务器端长轮询控制一次 PullRequest pullRequest = new PullRequest()：创建长轮询 PullRequest 对象 this.brokerController...suspendPullRequest(topic, queueId, pullRequest)：将长轮询请求对象交给长轮询服务 String key = this.buildKey(topic, queueId)：构建一个 topic@queueId 的 key ManyPullRequest mpr = this.pullRequestTable.get(key)：从拉请求表中获取对象 mpr.addPullRequest(pullRequest)：将 PullRequest 对象放入到长轮询的请求集合中 response = null：响应设置为 null 内部的 callBack 就不会给客户端发送任何数据，不进行通信，否则就又开始重新请求 boolean storeOffsetEnable：允许长轮询、sysFlag 表示提交消费者本地该队列的offset、当前 broker 节点角色为 master 节点三个条件成立，才在 Broker 端存储消费者组内该主题的指定 queue 的消费进度 return response：返回 response，不为 null 时外层 processRequestCommand 的 callback 会将数据写给客户端 长轮询PullRequestHoldService 类负责长轮询，BrokerController#start 方法中调用了 this.pullRequestHoldService.start() 启动该服务 核心方法： run()：核心运行方法 123456789101112131415public void run() &#123;\t// 循环运行 while (!this.isStopped()) &#123; if (this.brokerController.getBrokerConfig().isLongPollingEnable()) &#123; // 服务器开启长轮询开关：每次循环休眠5秒 this.waitForRunning(5 * 1000); &#125; else &#123; // 服务器关闭长轮询开关：每次循环休眠1秒 this.waitForRunning(...); &#125; // 检查持有的请求 this.checkHoldRequest(); // ..... &#125;&#125; checkHoldRequest()：检查所有的请求 for (String key : this.pullRequestTable.keySet())：处理所有的 topic@queueId 的逻辑 String[] kArray = key.split(TOPIC_QUEUEID_SEPARATOR)：key 按照 @ 拆分，得到 topic 和 queueId long offset = this...getMaxOffsetInQueue(topic, queueId)： 到存储模块查询该 ConsumeQueue 的最大 offset this.notifyMessageArriving(topic, queueId, offset)：通知消息到达 notifyMessageArriving()：通知消息到达的逻辑，ReputMessageService 消息分发服务也会调用该方法 ManyPullRequest mpr = this.pullRequestTable.get(key)：获取对应的的 manyPullRequest 对象 List&lt;PullRequest&gt; requestList：获取该队列下的所有 PullRequest，并进行遍历 List&lt;PullRequest&gt; replayList：当某个 pullRequest 不超时，并且对应的 CQ.maxOffset &lt;= pullRequest.offset，就将该 PullRequest 再放入该列表 long newestOffset：该值为 CQ 的 maxOffset if (newestOffset &gt; request.getPullFromThisOffset())：请求对应的队列内可以 pull 消息了，结束长轮询 boolean match：进行过滤匹配 this.brokerController...executeRequestWhenWakeup()：将满足条件的 pullRequest 再次提交到线程池内执行 final RemotingCommand response：执行 processRequest 方法，并且不会触发长轮询 channel.writeAndFlush(response).addListene()：将结果数据发送给客户端 if (System.currentTimeMillis() &gt;= ...)：判断该 pullRequest 是否超时，超时后的也是重新提交到线程池，并且不进行长轮询 mpr.addPullRequest(replayList)：将未满足条件的 PullRequest 对象再次添加到 ManyPullRequest 属性中 结果类GetMessageResult 类成员信息： 123456789101112131415161718192021public class GetMessageResult &#123; // 查询消息时，最底层都是 mappedFile 支持的查询，查询时返回给外层一个 SelectMappedBufferResult， // mappedFile 每查询一次都会 refCount++ ，通过SelectMappedBufferResult持有mappedFile，完成资源释放的句柄 private final List&lt;SelectMappedBufferResult&gt; messageMapedList = new ArrayList&lt;SelectMappedBufferResult&gt;(100); // 该List内存储消息，每一条消息都被转成 ByteBuffer 表示了 private final List&lt;ByteBuffer&gt; messageBufferList = new ArrayList&lt;ByteBuffer&gt;(100); // 查询结果状态 private GetMessageStatus status; // 客户端下次再向当前Queue拉消息时，使用的 offset private long nextBeginOffset; // 当前queue最小offset private long minOffset; // 当前queue最大offset private long maxOffset; // 消息总byte大小 private int bufferTotalSize = 0; // 服务器建议客户端下次到该 queue 拉消息时是否使用 【从节点】 private boolean suggestPullingFromSlave = false;&#125; 队列快照成员属性ProcessQueue 类是消费队列的快照 成员变量： 属性字段： 1234567private final AtomicLong msgCount = new AtomicLong();\t// 队列中消息数量private final AtomicLong msgSize = new AtomicLong();\t// 消息总大小private volatile long queueOffsetMax = 0L; // 快照中最大 offsetprivate volatile boolean dropped = false; // 快照是否移除private volatile long lastPullTimestamp = current; // 上一次拉消息的时间private volatile long lastConsumeTimestamp = current;\t// 上一次消费消息的时间private volatile long lastLockTimestamp = current; // 上一次获取锁的时间 消息容器：key 是消息偏移量，val 是消息 1private final TreeMap&lt;Long, MessageExt&gt; msgTreeMap = new TreeMap&lt;Long, MessageExt&gt;(); 顺序消费临时容器： 1private final TreeMap&lt;Long, MessageExt&gt; consumingMsgOrderlyTreeMap = new TreeMap&lt;Long, MessageExt&gt;(); 锁： 12private final ReadWriteLock lockTreeMap; // 读写锁private final Lock lockConsume; // 重入锁，【顺序消费使用】 顺序消费状态： 12private volatile boolean locked = false; // 是否是锁定状态private volatile boolean consuming = false; // 是否是消费中 成员方法核心成员方法 putMessage()：将 Broker 拉取下来的 msgs 存储到快照队列内，返回为 true 表示提交顺序消费任务，false 表示不提交 1public boolean putMessage(final List&lt;MessageExt&gt; msgs) this.lockTreeMap.writeLock().lockInterruptibly()：获取写锁 for (MessageExt msg : msgs)：遍历 msgs 全部加入 msgTreeMap，key 是消息的 queueOffset if (!msgTreeMap.isEmpty() &amp;&amp; !this.consuming)：消息容器中存在未处理的消息，并且不是消费中的状态 dispatchToConsume = true：代表需要提交顺序消费任务 this.consuming = true：设置为顺序消费执行中的状态 this.lockTreeMap.writeLock().unlock()：释放写锁 removeMessage()：移除已经消费的消息，参数是已经消费的消息集合，并发消费使用 1public long removeMessage(final List&lt;MessageExt&gt; msgs) long result = -1：结果初始化为 -1 this.lockTreeMap.writeLock().lockInterruptibly()：获取写锁 this.lastConsumeTimestamp = now：更新上一次消费消息的时间为现在 if (!msgTreeMap.isEmpty())：判断消息容器是否是空，是空直接返回 -1 result = this.queueOffsetMax + 1：设置结果，删除完后消息容器为空时返回 for (MessageExt msg : msgs)：将已经消费的消息全部从 msgTreeMap 移除 if (!msgTreeMap.isEmpty())：移除后容器内还有待消费的消息，获取第一条消息 offset 返回 this.lockTreeMap.writeLock().unlock()：释放写锁 takeMessages()：获取一批消息，顺序消费使用 1public List&lt;MessageExt&gt; takeMessages(final int batchSize) this.lockTreeMap.writeLock().lockInterruptibly()：获取写锁 this.lastConsumeTimestamp = now：更新上一次消费消息的时间为现在 for (int i = 0; i &lt; batchSize; i++)：从头节点开始获取消息 result.add(entry.getValue())：将消息放入结果集合 consumingMsgOrderlyTreeMap.put()：将消息加入顺序消费容器中 if (result.isEmpty())：条件成立说明顺序消费容器本地快照内的消息全部处理完了，当前顺序消费任务需要停止 consuming = false：消费状态置为 false this.lockTreeMap.writeLock().unlock()：释放写锁 commit()：处理完一批消息后调用，顺序消费使用 1public long commit() this.lockTreeMap.writeLock().lockInterruptibly()：获取写锁 Long offset = this.consumingMsgOrderlyTreeMap.lastKey()：获取顺序消费临时容器最后一条数据的 key msgCount, msgSize：更新顺序消费相关的字段 this.consumingMsgOrderlyTreeMap.clear()：清空顺序消费容器的数据 return offset + 1：消费者下一条消费的位点 this.lockTreeMap.writeLock().unlock()：释放写锁 cleanExpiredMsg()：清除过期消息 1public void cleanExpiredMsg(DefaultMQPushConsumer pushConsumer) if (pushConsumer.getDefaultMQPushConsumerImpl().isConsumeOrderly()) ：顺序消费不执行过期清理逻辑 int loop = msgTreeMap.size() &lt; 16 ? msgTreeMap.size() : 16：最多循环 16 次 if (!msgTreeMap.isEmpty() &amp;&amp;)：如果容器中第一条消息的消费开始时间与当前系统时间差值 &gt; 15min，则取出该消息 else：直接跳出循环，因为快照队列内的消息是有顺序的，第一条消息不过期，其他消息都不过期 pushConsumer.sendMessageBack(msg, 3)：消息回退到服务器，设置该消息的延迟级别为 3 if (!msgTreeMap.isEmpty() &amp;&amp; msg.getQueueOffset() == msgTreeMap.firstKey())：条件成立说明消息回退期间，该目标消息并没有被消费任务成功消费 removeMessage(Collections.singletonList(msg))：从 treeMap 将该回退成功的 msg 删除 并发消费成员属性ConsumeMessageConcurrentlyService 负责并发消费服务 成员变量： 消息监听器：封装处理消息的逻辑，该监听器由开发者实现，并注册到 defaultMQPushConsumer 1private final MessageListenerConcurrently messageListener; 消费属性： 12private final BlockingQueue&lt;Runnable&gt; consumeRequestQueue;\t// 消费任务队列private final String consumerGroup; // 消费者组 线程池： 123private final ThreadPoolExecutor consumeExecutor; // 消费任务线程池，默认 20private final ScheduledExecutorService scheduledExecutorService;// 调度线程池，延迟提交消费任务private final ScheduledExecutorService cleanExpireMsgExecutors;\t// 清理过期消息任务线程池，15min 一次 成员方法ConsumeMessageConcurrentlyService 并发消费核心方法 start()：启动消费服务，DefaultMQPushConsumerImpl 启动时会调用该方法 12345public void start() &#123; // 提交“清理过期消息任务”任务，延迟15min之后执行，之后每15min执行一次 this.cleanExpireMsgExecutors.scheduleAtFixedRate(() -&gt; cleanExpireMsg()&#125;, 15, 15, TimeUnit.MINUTES);&#125; cleanExpireMsg()：清理过期消息任务 1private void cleanExpireMsg() Iterator&lt;Map.Entry&lt;MessageQueue, ProcessQueue&gt;&gt; it ：获取分配给当前消费者的队列 while (it.hasNext())：遍历所有的队列 pq.cleanExpiredMsg(this.defaultMQPushConsumer)：调用队列快照 ProcessQueue 清理过期消息的方法 submitConsumeRequest()：提交消费请求 12345// 参数一：从服务器 pull 下来的这批消息// 参数二：消息归属 mq 在消费者端的 processQueue，提交消费任务之前，msgs已经加入到该pq内了// 参数三：消息归属队列// 参数四：并发消息此参数无效public void submitConsumeRequest(List&lt;MessageExt&gt; msgs, ProcessQueue processQueue, MessageQueue messageQueue, boolean dispatchToConsume) final int consumeBatchSize：一个消费任务可消费的消息数量，默认为 1 if (msgs.size() &lt;= consumeBatchSize)：判断一个消费任务是否可以提交 ConsumeRequest consumeRequest：封装为消费请求 this.consumeExecutor.submit(consumeRequest)：提交消费任务，异步执行消息的处理 else：说明消息较多，需要多个消费任务 for (int total = 0; total &lt; msgs.size(); )：将消息拆分成多个消费任务 processConsumeResult()：处理消费结果 12// 参数一：消费结果状态； 参数二：消费上下文； 参数三：当前消费任务public void processConsumeResult(status, context, consumeRequest) switch (status)：根据消费结果状态进行处理 case CONSUME_SUCCESS：消费成功 if (ackIndex &gt;= consumeRequest.getMsgs().size())：消费成功的话，ackIndex 设置成 消费消息数 - 1 的值，比如有 5 条消息，这里就设置为 4 ok, failed：ok 设置为消息数量，failed 设置为 0 case RECONSUME_LATER：消费失败 ackIndex = -1：设置为 -1 switch (this.defaultMQPushConsumer.getMessageModel())：判断消费模式，默认是集群模式 for (int i = ackIndex + 1; i &lt; msgs.size(); i++)：当消费失败时 ackIndex 为 -1，i 的起始值为 0，该消费任务内的全部消息都会尝试回退给服务器 MessageExt msg：提取一条消息 boolean result = this.sendMessageBack(msg, context)：发送消息回退，同步发送 if (!result)：回退失败的消息，将消息的重试属性加 1，并加入到回退失败的集合 if (!msgBackFailed.isEmpty())：回退失败集合不为空 consumeRequest.getMsgs().removeAll(msgBackFailed)：将回退失败的消息从当前消费任务的 msgs 集合内移除 this.submitConsumeRequestLater()：回退失败的消息会再次提交消费任务，延迟 5 秒钟后再次尝试消费 long offset = ...removeMessage(msgs)：从 pq 中删除已经消费成功的消息，返回 offset this...getOffsetStore().updateOffset()：更新消费者本地该 mq 的消费进度 消费请求ConsumeRequest 是 ConsumeMessageConcurrentlyService 的内部类，是一个 Runnable 任务对象 成员变量： 分配到该消费任务的消息： 1private final List&lt;MessageExt&gt; msgs; 消息队列： 12private final ProcessQueue processQueue;\t// 消息处理队列private final MessageQueue messageQueue;\t// 消息队列 核心方法： run()：执行任务 1public void run() if (this.processQueue.isDropped())：条件成立说明该 queue 经过 rbl 算法分配到其他的 consumer MessageListenerConcurrently listener：获取消息监听器 ConsumeConcurrentlyContext context：创建消费上下文对象 defaultMQPushConsumerImpl.resetRetryAndNamespace()：重置重试标记 final String groupTopic：获取当前消费者组的重试主题 %RETRY%GroupName for (MessageExt msg : msgs)：遍历所有的消息 String retryTopic = msg.getProperty(...)：原主题，一般消息没有该属性，只有被重复消费的消息才有 if (retryTopic != null &amp;&amp; groupTopic.equals(...))：条件成立说明该消息是被重复消费的消息 msg.setTopic(retryTopic)：将被重复消费的消息主题修改回原主题 if (ConsumeMessageConcurrentlyService...hasHook())：前置处理 boolean hasException = false：消费过程中，是否向外抛出异常 MessageAccessor.setConsumeStartTimeStamp()：给每条消息设置消费开始时间 status = listener.consumeMessage(Collections.unmodifiableList(msgs), context)：消费消息 if (ConsumeMessageConcurrentlyService...hasHook())：后置处理 ...processConsumeResult(status, context, this)：处理消费结果 顺序消费成员属性ConsumeMessageOrderlyService 负责顺序消费服务 成员变量： 消息监听器：封装处理消息的逻辑，该监听器由开发者实现，并注册到 defaultMQPushConsumer 1private final MessageListenerOrderly messageListener; 消费属性： 123private final BlockingQueue&lt;Runnable&gt; consumeRequestQueue;\t// 消费任务队列private final String consumerGroup; // 消费者组private volatile boolean stopped = false; // 消费停止状态 线程池： 12private final ThreadPoolExecutor consumeExecutor; // 消费任务线程池private final ScheduledExecutorService scheduledExecutorService;// 调度线程池，延迟提交消费任务 队列锁：消费者本地 MQ 锁，确保本地对于需要顺序消费的 MQ 同一时间只有一个任务在执行 1private final MessageQueueLock messageQueueLock = new MessageQueueLock(); 123456789101112131415public class MessageQueueLock &#123; private ConcurrentMap&lt;MessageQueue, Object&gt; mqLockTable = new ConcurrentHashMap&lt;MessageQueue, Object&gt;(); // 获取本地队列锁对象 public Object fetchLockObject(final MessageQueue mq) &#123; Object objLock = this.mqLockTable.get(mq); if (null == objLock) &#123; objLock = new Object(); Object prevLock = this.mqLockTable.putIfAbsent(mq, objLock); if (prevLock != null) &#123; objLock = prevLock; &#125; &#125; return objLock; &#125;&#125; 已经获取了 Broker 端该 Queue 的独占锁，为什么还要获取本地队列锁对象？（这里我也没太懂，先记录下来，本地多线程？） Broker queue 占用锁的角度是 Client 占用，Client 从 Broker 的某个占用了锁的 queue 拉取下来消息以后，将消息存储到消费者本地的 ProcessQueue 中，快照对象的 consuming 属性置为 true，表示本地的队列正在消费处理中 ProcessQueue 调用 takeMessages 方法时会获取下一批待处理的消息，获取不到会修改 consuming = false，本消费任务马上停止。 如果此时 Pull 再次拉取一批当前 ProcessQueue 的 msg，会再次向顺序消费服务提交消费任务，此时需要本地队列锁对象同步本地线程 成员方法 start()：启动消费服务，DefaultMQPushConsumerImpl 启动时会调用该方法 1public void start() this.scheduledExecutorService.scheduleAtFixedRate()：提交锁续约任务，延迟 1 秒执行，周期为 20 秒钟 ConsumeMessageOrderlyService.this.lockMQPeriodically()：锁续约任务 this.defaultMQPushConsumerImpl.getRebalanceImpl().lockAll()：对消费者的所有队列进行续约 submitConsumeRequest()：提交消费任务请求 12345678// 参数：true 表示创建消费任务并提交，false不创建消费任务，说明消费者本地已经有消费任务在执行了public void submitConsumeRequest(...., final boolean dispathToConsume) &#123; if (dispathToConsume) &#123; // 当前进程内不存在 顺序消费任务，创建新的消费任务，【提交到消费任务线程池】 ConsumeRequest consumeRequest = new ConsumeRequest(processQueue, messageQueue); this.consumeExecutor.submit(consumeRequest); &#125;&#125; processConsumeResult()：消费结果处理 1234// 参数1：msgs 本轮循环消费的消息集合 参数2：status 消费状态// 参数3：context 消费上下文 参数4：消费任务// 返回值：boolean 决定是否继续循环处理pq内的消息public boolean processConsumeResult(final List&lt;MessageExt&gt; msgs, final ConsumeOrderlyStatus status, final ConsumeOrderlyContext context, final ConsumeRequest consumeRequest) if (context.isAutoCommit()) ：默认自动提交 switch (status)：根据消费状态进行不同的处理 case SUCCESS：消费成功 commitOffset = ...commit()：调用 pq 提交方法，会将本次循环处理的消息从顺序消费 map 删除，并且返回消息进度 case SUSPEND_CURRENT_QUEUE_A_MOMENT：挂起当前队列 consumeRequest.getProcessQueue().makeMessageToConsumeAgain(msgs)：回滚消息 for (MessageExt msg : msgs)：遍历所有的消息 this.consumingMsgOrderlyTreeMap.remove(msg.getQueueOffset())：从顺序消费临时容器中移除 this.msgTreeMap.put(msg.getQueueOffset(), msg)：添加到消息容器 this.submitConsumeRequestLater()：再次提交消费任务，1 秒后执行 continueConsume = false：设置为 false，外层会退出本次的消费任务 this.defaultMQPushConsumerImpl.getOffsetStore().updateOffset(...)：更新本地消费进度 消费请求ConsumeRequest 是 ConsumeMessageOrderlyService 的内部类，是一个 Runnable 任务对象 核心方法： run()：执行任务 1public void run() final Object objLock：获取本地锁对象 synchronized (objLock)：本地队列锁，确保每个 MQ 的消费任务只有一个在执行，确保顺序消费 if(.. || (this.processQueue.isLocked() &amp;&amp; !this.processQueue.isLockExpired())))：当前队列持有分布式锁，并且锁未过期，持锁时间超过 30 秒算过期 final long beginTime：消费开始时间 for (boolean continueConsume = true; continueConsume; )：根据是否继续消费的标记判断是否继续 final int consumeBatchSize：获取每次循环处理的消息数量，一般是 1 List&lt;MessageExt&gt; msgs = this...takeMessages(consumeBatchSize)：到处理队列获取一批消息 if (!msgs.isEmpty())：获取到了待消费的消息 final ConsumeOrderlyContext context：创建消费上下文对象 this.processQueue.getLockConsume().lock()：获取 lockConsume 锁，与 RBL 线程同步使用 status = messageListener.consumeMessage(...)：监听器处理消息 this.processQueue.getLockConsume().unlock()：释放 lockConsume 锁 if (null == status)：处理消息状态返回 null，设置状态为挂起当前队列 continueConsume = ...processConsumeResult()：消费结果处理 else：获取到的消息是空 continueConsume = false：结束任务循环 else：当前队列未持有分布式锁，或者锁过期 ConsumeMessageOrderlyService.this.tryLockLaterAndReconsume()：重新提交任务，根据是否获取到队列锁，选择延迟 10 毫秒或者 300 毫秒 生产消费生产流程： 首先获取当前消息主题的发布信息，获取不到去 Namesrv 获取（默认有 TBW102），并将获取的到的路由数据转化为发布数据，创建 MQ 队列在多个 Broker 组（一组代表一主多从的 Broker 架构），客户端实例同样更新订阅数据，创建 MQ 队列，放入负载均衡服务 topicSubscribeInfoTable 中 然后从发布数据中选择一个 MQ 队列发送消息 Broker 端通过 SendMessageProcessor 对发送的消息进行持久化处理，存储到 CommitLog。将重试次数过多的消息加入死信队列，将延迟消息的主题和队列修改为调度主题和调度队列 ID Broker 启动 ScheduleMessageService 服务会为每个延迟级别创建一个延迟任务，让延迟消息得到有效的处理，将到达交付时间的消息修改为原始主题的原始 ID 存入 CommitLog，消费者就可以进行消费了 消费流程： 消息消费队列 ConsumerQueue 存储消息在 CommitLog 的索引，消费者通过该队列来读取消息实体内容，一个 MQ 就对应一个 CQ 首先通过负载均衡服务，将分配到当前消费者实例的 MQ 创建 PullRequest，并放入 PullMessageService 的本地阻塞队列内 PullMessageService 循环从阻塞队列获取请求对象，发起拉消息请求，并创建 PullCallback 回调对象，将正常拉取的消息提交到消费任务线程池，并设置请求的下一次拉取位点，重新放入阻塞队列，形成闭环 消费任务服务对消费失败的消息进行回退，通过内部生产者实例发送回退消息，回退失败的消息会再次提交消费任务重新消费 Broker 端对拉取消息的请求进行处理（processRequestCommand），查询成功将消息放入响应体，通过 Netty 写回客户端，当 pullRequest.offset == queue.maxOffset 说明该队列已经没有需要获取的消息，将请求放入长轮询集合等待有新消息 PullRequestHoldService 负责长轮询，每 5 秒遍历一次长轮询集合，将满足条件的 PullRequest 再次提交到线程池内处理 Zookeeper基本介绍框架特征Zookeeper 是 Apache Hadoop 项目子项目，为分布式框架提供协调服务，是一个树形目录服务 Zookeeper 是基于观察者模式设计的分布式服务管理框架，负责存储和管理共享数据，接受观察者的注册监控，一旦这些数据的状态发生变化，Zookeeper 会通知观察者 Zookeeper 是一个领导者（Leader），多个跟随者（Follower）组成的集群 集群中只要有半数以上节点存活就能正常服务，所以 Zookeeper 适合部署奇数台服务器 全局数据一致，每个 Server 保存一份相同的数据副本，Client 无论连接到哪个 Server，数据都是一致 更新的请求顺序执行，来自同一个 Client 的请求按其发送顺序依次执行 数据更新原子性，一次数据更新要么成功，要么失败 实时性，在一定的时间范围内，Client 能读到最新数据 心跳检测，会定时向各个服务提供者发送一个请求（实际上建立的是一个 Socket 长连接） 参考视频：https://www.bilibili.com/video/BV1to4y1C7gw 应用场景Zookeeper 提供的主要功能包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡、分布式锁等 在分布式环境中，经常对应用&#x2F;服务进行统一命名，便于识别，例如域名相对于 IP 地址更容易被接收 12/service/www.baidu.com # 节点路径192.168.1.1 192.168.1.2\t# 节点值 如果在节点中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求，可以实现负载均衡 12192.168.1.1 10\t# 次数192.168.1.1 15 配置文件同步可以通过 Zookeeper 实现，将配置信息写入某个 ZNode，其他客户端监视该节点，当节点数据被修改，通知各个客户端服务器 集群环境中，需要实时掌握每个集群节点的状态，可以将这些信息放入 ZNode，通过监控通知的机制实现 实现客户端实时观察服务器上下线的变化，通过心跳检测实现 基本操作安装搭建安装步骤： 安装 JDK 拷贝 apache-zookeeper-3.5.7-bin.tar.gz 安装包到 Linux 系统下，并解压到指定目录 conf 目录下的配置文件重命名： 1mv zoo_sample.cfg zoo.cfg 修改配置文件： 123vim zoo.cfg# 修改内容dataDir=/home/seazean/SoftWare/zookeeper-3.5.7/zkData 在对应目录创建 zkData 文件夹： 1mkdir zkData Zookeeper 中的配置文件 zoo.cfg 中参数含义解读： tickTime &#x3D; 2000：通信心跳时间，Zookeeper 服务器与客户端心跳时间，单位毫秒 initLimit &#x3D; 10：Leader 与 Follower 初始通信时限，初始连接时能容忍的最多心跳次数 syncLimit &#x3D; 5：Leader 与 Follower 同步通信时限，LF 通信时间超过 syncLimit * tickTime，Leader 认为 Follwer 下线 dataDir：保存 Zookeeper 中的数据目录，默认是 tmp目录，容易被 Linux 系统定期删除，所以建议修改 clientPort &#x3D; 2181：客户端连接端口，通常不做修改 操作命令服务端Linux 命令： 启动 ZooKeeper 服务：./zkServer.sh start 查看 ZooKeeper 服务：./zkServer.sh status 停止 ZooKeeper 服务：./zkServer.sh stop 重启 ZooKeeper 服务：./zkServer.sh restart 查看进程是否启动：jps 客户端Linux 命令： 连接 ZooKeeper 服务端： 12./zkCli.sh # 直接启动./zkCli.sh –server ip:port\t# 指定 host 启动 客户端命令： 基础操作： 12quit # 停止连接help # 查看命令帮助 创建命令：**/ 代表根目录** 1234create /path value # 创建节点，value 可选create -e /path value # 创建临时节点create -s /path value # 创建顺序节点create -es /path value # 创建临时顺序节点，比如node10000012 删除12后也会继续从13开始，只会增加 查询命令： 123456ls /path # 显示指定目录下子节点ls –s /path # 查询节点详细信息ls –w /path # 监听子节点数量的变化stat /path # 查看节点状态get –s /path # 查询节点详细信息get –w /path # 监听节点数据的变化 123456789101112# 属性，分为当前节点的属性和子节点属性czxid: 节点被创建的事务ID, 是ZooKeeper中所有修改总的次序，每次修改都有唯一的 zxid，谁小谁先发生ctime: 被创建的时间戳mzxid: 最后一次被更新的事务ID mtime: 最后修改的时间戳pzxid: 子节点列表最后一次被更新的事务IDcversion: 子节点的变化号，修改次数dataversion: 节点的数据变化号，数据的变化次数aclversion: 节点的访问控制列表变化号ephemeralOwner: 用于临时节点，代表节点拥有者的 session id，如果为持久节点则为0 dataLength: 节点存储的数据的长度 numChildren: 当前节点的子节点数量 删除命令： 12delete /path # 删除节点deleteall /path # 递归删除节点 数据结构ZooKeeper 是一个树形目录服务，类似 Unix 的文件系统，每一个节点都被称为 ZNode，每个 ZNode 默认存储 1MB 的数据，节点上会保存数据和节点信息，每个 ZNode 都可以通过其路径唯一标识 节点可以分为四大类： PERSISTENT：持久化节点 EPHEMERAL：临时节点，客户端和服务器端断开连接后，创建的节点删除 PERSISTENT_SEQUENTIAL：持久化顺序节点，创建 znode 时设置顺序标识，节点名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 EPHEMERAL_SEQUENTIAL：临时顺序节点 注意：在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 代码实现添加 Maven 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.5.7&lt;/version&gt;&lt;/dependency&gt; 实现代码： 1234567891011public static void main(String[] args) &#123; // 参数一：连接地址 // 参数二：会话超时时间 // 参数三：监听器 ZooKeeper zkClient = new ZooKeeper(&quot;192.168.3.128:2181&quot;, 20000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; System.out.println(&quot;监听处理函数&quot;); &#125; &#125;);&#125; 集群介绍相关概念Zookeepe 集群三个角色： Leader 领导者：处理客户端事务请求，负责集群内部各服务器的调度 Follower 跟随者：处理客户端非事务请求，转发事务请求给 Leader 服务器，参与 Leader 选举投票 Observer 观察者：观察集群的最新状态的变化，并将这些状态进行同步；处理非事务性请求，事务性请求会转发给 Leader 服务器进行处理；不会参与任何形式的投票。只提供非事务性的服务，通常用于在不影响集群事务处理能力的前提下，提升集群的非事务处理能力（提高集群读的能力，但是也降低了集群选主的复杂程度） 相关属性： SID：服务器 ID，用来唯一标识一台集群中的机器，和 myid 一致 ZXID：事务 ID，用来标识一次服务器状态的变更，在某一时刻集群中每台机器的 ZXID 值不一定完全一致，这和 ZooKeeper 服务器对于客户端更新请求的处理逻辑有关 Epoch：每个 Leader 任期的代号，同一轮选举投票过程中的该值是相同的，投完一次票就增加 选举机制：半数机制，超过半数的投票就通过 第一次启动选举规则：投票过半数时，服务器 ID 大的胜出 第二次启动选举规则： EPOCH 大的直接胜出 EPOCH 相同，事务 ID 大的胜出（事务 ID 越大，数据越新） 事务 ID 相同，服务器 ID 大的胜出 初次选举选举过程： 服务器 1 启动，发起一次选举，服务器 1 投自己一票，票数不超过半数，选举无法完成，服务器 1 状态保持为 LOOKING 服务器 2 启动，再发起一次选举，服务器 1 和 2 分别投自己一票并交换选票信息，此时服务器 1 会发现服务器 2 的 SID 比自己投票推举的（服务器 1）大，更改选票为推举服务器 2。投票结果为服务器 1 票数 0 票，服务器 2 票数 2 票，票数不超过半数，选举无法完成，服务器 1、2 状态保持 LOOKING 服务器 3 启动，发起一次选举，此时服务器 1 和 2 都会更改选票为服务器 3，投票结果为服务器 3 票数 3 票，此时服务器 3 的票数已经超过半数，服务器 3 当选 Leader，服务器 1、2 更改状态为 FOLLOWING，服务器 3 更改状态为 LEADING 服务器 4 启动，发起一次选举，此时服务器 1、2、3 已经不是 LOOKING 状态，不会更改选票信息，交换选票信息结果后服务器 3 为 3 票，服务器 4 为 1 票，此时服务器 4 更改选票信息为服务器 3，并更改状态为 FOLLOWING 服务器 5 启动，同 4 一样 再次选举ZooKeeper 集群中的一台服务器出现以下情况之一时，就会开始进入 Leader 选举： 服务器初始化启动 服务器运行期间无法和 Leader 保持连接 当一台服务器进入 Leader 选举流程时，当前集群可能会处于以下两种状态： 集群中本来就已经存在一个 Leader，服务器试图去选举 Leader 时会被告知当前服务器的 Leader 信息，对于该服务器来说，只需要和 Leader 服务器建立连接，并进行状态同步即可 集群中确实不存在 Leader，假设服务器 3 和 5 出现故障，开始进行 Leader 选举，SID 为 1、2、4 的机器投票情况 1(EPOCH，ZXID，SID): (1, 8, 1), (1, 8, 2), (1, 7, 4) 根据选举规则，服务器 2 胜出 数据写入写操作就是事务请求，写入请求直接发送给 Leader 节点：Leader 会先将数据写入自身，同时通知其他 Follower 写入，当集群中有半数以上节点写入完成，Leader 节点就会响应客户端数据写入完成 写入请求直接发送给 Follower 节点：Follower 没有写入权限，会将写请求转发给 Leader，Leader 将数据写入自身，通知其他 Follower 写入，当集群中有半数以上节点写入完成，Leader 会通知 Follower 写入完成，由 Follower 响应客户端数据写入完成 底层协议PaxosPaxos 算法：基于消息传递且具有高度容错特性的一致性算法 优点：快速正确的在一个分布式系统中对某个数据值达成一致，并且保证不论发生任何异常，都不会破坏整个系统的一致性 缺陷：在网络复杂的情况下，可能很久无法收敛，甚至陷入活锁的情况 ZAB算法介绍ZAB 协议借鉴了 Paxos 算法，是为 Zookeeper 设计的支持崩溃恢复的原子广播协议，基于该协议 Zookeeper 设计为只有一台客户端（Leader）负责处理外部的写事务请求，然后 Leader 将数据同步到其他 Follower 节点 Zab 协议包括两种基本的模式：消息广播、崩溃恢复 消息广播ZAB 协议针对事务请求的处理过程类似于一个两阶段提交过程：广播事务阶段、广播提交操作 客户端发起写操作请求，Leader 服务器将请求转化为事务 Proposal 提案，同时为 Proposal 分配一个全局的 ID，即 ZXID Leader 服务器为每个 Follower 分配一个单独的队列，将广播的 Proposal 依次放到队列中去，根据 FIFO 策略进行消息发送 Follower 接收到 Proposal 后，将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 ACK 响应消息 Leader 接收到超过半数以上 Follower 的 ACK 响应消息后，即认为消息发送成功，可以发送 Commit 消息 Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交，Follower 接收到 Commit 后，将上一条事务提交 两阶段提交模型可能因为 Leader 宕机带来数据不一致： Leader 发起一个事务 Proposal 后就宕机，Follower 都没有 Proposal Leader 收到半数 ACK 宕机，没来得及向 Follower 发送 Commit 崩溃恢复Leader 服务器出现崩溃或者由于网络原因导致 Leader 服务器失去了与过半 Follower的联系，那么就会进入崩溃恢复模式，崩溃恢复主要包括两部分：Leader 选举和数据恢复 Zab 协议崩溃恢复要求满足以下两个要求： 已经被 Leader 提交的提案 Proposal，必须最终被所有的 Follower 服务器正确提交 丢弃已经被 Leader 提出的，但是没有被提交的 Proposal Zab 协议需要保证选举出来的 Leader 需要满足以下条件： 新选举的 Leader 不能包含未提交的 Proposal，即新 Leader 必须都是已经提交了 Proposal 的 Follower 节点 新选举的 Leader 节点含有最大的 ZXID，可以避免 Leader 服务器检查 Proposal 的提交和丢弃工作 数据恢复阶段： 完成 Leader 选举后，在正式开始工作之前（接收事务请求提出新的 Proposal），Leader 服务器会首先确认事务日志中的所有 Proposal 是否已经被集群中过半的服务器 Commit Leader 服务器需要确保所有的 Follower 服务器能够接收到每一条事务的 Proposal，并且能将所有已经提交的事务 Proposal 应用到内存数据中，所以只有当 Follower 将所有尚未同步的事务 Proposal 都从 Leader 服务器上同步，并且应用到内存数据后，Leader 才会把该 Follower 加入到真正可用的 Follower 列表中 异常处理Zab 的事务编号 zxid 设计： zxid 是一个 64 位的数字，低 32 位是一个简单的单增计数器，针对客户端每一个事务请求，Leader 在产生新的 Proposal 事务时，都会对该计数器加 1，而高 32 位则代表了 Leader 周期的 epoch 编号 epoch 为当前集群所处的代或者周期，每次 Leader 变更后都会在 epoch 的基础上加 1，Follower 只服从 epoch 最高的 Leader 命令，所以旧的 Leader 崩溃恢复之后，其他 Follower 就不会继续追随 每次选举产生一个新的 Leader，就会从新 Leader 服务器上取出本地事务日志中最大编号 Proposal 的 zxid，从 zxid 中解析得到对应的 epoch 编号，然后再对其加 1 后作为新的 epoch 值，并将低 32 位数字归零，由 0 开始重新生成 zxid Zab 协议通过 epoch 编号来区分 Leader 变化周期，能够有效避免不同的 Leader 错误的使用了相同的 zxid 编号提出了不一样的 Proposal 的异常情况 Zab 数据同步过程：数据同步阶段要以 Leader 服务器为准 一个包含了上个 Leader 周期中尚未提交过的事务 Proposal 的服务器启动时，这台机器加入集群中会以 Follower 角色连上 Leader Leader 会根据自己服务器上最后提交的 Proposal 和 Follower 服务器的 Proposal 进行比对，让 Follower 进行一个回退或者前进操作，到一个已经被集群中过半机器 Commit 的最新 Proposal（源码解析部分详解） CAPCAP 理论指的是在一个分布式系统中，Consistency（一致性）、Availability（可用性）、Partition Tolerance（分区容错性）不能同时成立，ZooKeeper 保证的是 CP ZooKeeper 不能保证每次服务请求的可用性，在极端环境下可能会丢弃一些请求，消费者程序需要重新请求才能获得结果 进行 Leader 选举时集群都是不可用 CAP 三个基本需求，因为 P 是必须的，因此分布式系统选择就在 CP 或者 AP 中： 一致性：指数据在多个副本之间是否能够保持数据一致的特性，当一个系统在数据一致的状态下执行更新操作后，也能保证系统的数据仍然处于一致的状态 可用性：指系统提供的服务必须一直处于可用的状态，即使集群中一部分节点故障，对于用户的每一个操作请求总是能够在有限的时间内返回结果 分区容错性：分布式系统在遇到任何网络分区故障时，仍然能够保证对外提供服务，不会宕机，除非是整个网络环境都发生了故障 监听机制实现原理ZooKeeper 中引入了 Watcher 机制来实现了发布&#x2F;订阅功能，客户端注册监听目录节点，在特定事件触发时，ZooKeeper 会通知所有关注该事件的客户端，保证 ZooKeeper 保存的任何的数据的任何改变都能快速的响应到监听应用程序 监听命令：只能生效一次，接收一次通知，再次监听需要重新注册 12ls –w /path # 监听【子节点数量】的变化get –w /path # 监听【节点数据】的变化 工作流程： 在主线程中创建 Zookeeper 客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener） 通过 connect 线程将注册的监听事件发送给 Zookeeper 在 Zookeeper 的注册监听器列表中将注册的监听事件添加到列表中 Zookeeper 监听到有数据或路径变化，将消息发送给 listener 线程 listener 线程内部调用 process() 方法 Curator 框架引入了 Cache 来实现对 ZooKeeper 服务端事件的监听，三种 Watcher： NodeCache：只是监听某一个特定的节点 PathChildrenCache：监控一个 ZNode 的子节点 TreeCache：可以监控整个树上的所有节点，类似于 PathChildrenCache 和 NodeCache 的组合 监听案例整体架构客户端实时监听服务器动态上下线 代码实现客户端：先启动客户端进行监听 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class DistributeClient &#123; private String connectString = &quot;192.168.3.128:2181&quot;; private int sessionTimeout = 20000; private ZooKeeper zk; public static void main(String[] args) throws Exception &#123; DistributeClient client = new DistributeClient(); // 1 获取zk连接 client.getConnect(); // 2 监听/servers下面子节点的增加和删除 client.getServerList(); // 3 业务逻辑 client.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void getServerList() throws KeeperException, InterruptedException &#123; ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); // 获取所有子节点，true 代表触发监听操作 List&lt;String&gt; children = zk.getChildren(&quot;/servers&quot;, true); for (String child : children) &#123; // 获取子节点的数据 byte[] data = zk.getData(&quot;/servers/&quot; + child, false, null); servers.add(new String(data)); &#125; System.out.println(servers); &#125; private void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; getServerList(); &#125; &#125;); &#125;&#125; 服务端：启动时需要 Program arguments 1234567891011121314151617181920212223242526272829303132333435363738public class DistributeServer &#123; private String connectString = &quot;192.168.3.128:2181&quot;; private int sessionTimeout = 20000; private ZooKeeper zk; public static void main(String[] args) throws Exception &#123; DistributeServer server = new DistributeServer(); // 1 获取 zookeeper 连接 server.getConnect(); // 2 注册服务器到 zk 集群，注意参数 server.register(args[0]); // 3 启动业务逻辑 server.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void register(String hostname) throws KeeperException, InterruptedException &#123; // OPEN_ACL_UNSAFE: ACL 开放 // EPHEMERAL_SEQUENTIAL: 临时顺序节点 String create = zk.create(&quot;/servers/&quot; + hostname, hostname.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname + &quot; is online&quot;); &#125; private void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; &#125; &#125;); &#125;&#125; 分布式锁实现原理分布式锁可以实现在分布式系统中多个进程有序的访问该临界资源，多个进程之间不会相互干扰 核心思想：当客户端要获取锁，则创建节点，使用完锁，则删除该节点 客户端获取锁时，在 &#x2F;locks 节点下创建临时顺序节点 使用临时节点是为了防止当服务器或客户端宕机以后节点无法删除（持久节点），导致锁无法释放 使用顺序节点是为了系统自动编号排序，找最小的节点，防止客户端饥饿现象，保证公平 获取 &#x2F;locks 目录的所有子节点，判断自己的子节点序号是否最小，成立则客户端获取到锁，使用完锁后将该节点删除 反之客户端需要找到比自己小的节点，对其注册事件监听器，监听删除事件 客户端的 Watcher 收到删除事件通知，就会重新判断当前节点是否是子节点中序号最小，如果是则获取到了锁， 如果不是则重复以上步骤继续获取到比自己小的一个节点并注册监听 CuratorCurator 实现分布式锁 API，在 Curator 中有五种锁方案： InterProcessSemaphoreMutex：分布式排它锁（非可重入锁） InterProcessMutex：分布式可重入排它锁 InterProcessReadWriteLock：分布式读写锁 InterProcessMultiLock：将多个锁作为单个实体管理的容器 InterProcessSemaphoreV2：共享信号量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class CuratorLock &#123; public static CuratorFramework getCuratorFramework() &#123; // 重试策略对象 ExponentialBackoffRetry policy = new ExponentialBackoffRetry(3000, 3); // 构建客户端 CuratorFramework client = CuratorFrameworkFactory.builder() .connectString(&quot;192.168.3.128:2181&quot;) .connectionTimeoutMs(2000)\t// 连接超时时间 .sessionTimeoutMs(20000)\t// 会话超时时间 单位ms .retryPolicy(policy) // 重试策略 .build(); // 启动客户端 client.start(); System.out.println(&quot;zookeeper 启动成功&quot;); return client; &#125; public static void main(String[] args) &#123; // 创建分布式锁1 InterProcessMutex lock1 = new InterProcessMutex(getCuratorFramework(), &quot;/locks&quot;); // 创建分布式锁2 InterProcessMutex lock2 = new InterProcessMutex(getCuratorFramework(), &quot;/locks&quot;); new Thread(new Runnable() &#123; @Override public void run() &#123; lock1.acquire(); System.out.println(&quot;线程1 获取到锁&quot;); Thread.sleep(5 * 1000); lock1.release(); System.out.println(&quot;线程1 释放锁&quot;); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; lock2.acquire(); System.out.println(&quot;线程2 获取到锁&quot;); Thread.sleep(5 * 1000); lock2.release(); System.out.println(&quot;线程2 释放锁&quot;); &#125; &#125;).start(); &#125;&#125; 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;4.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;4.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-client&lt;/artifactId&gt; &lt;version&gt;4.3.0&lt;/version&gt; 源码解析服务端服务端程序的入口 QuorumPeerMain 1234public static void main(String[] args) &#123; QuorumPeerMain main = new QuorumPeerMain(); main.initializeAndRun(args);&#125; initializeAndRun 的工作： 解析启动参数 提交周期任务，定时删除过期的快照 初始化通信模型，默认是 NIO 通信 123456789// QuorumPeerMain#runFromConfigpublic void runFromConfig(QuorumPeerConfig config) &#123; // 通信信组件初始化，默认是 NIO 通信 ServerCnxnFactory cnxnFactory = ServerCnxnFactory.createFactory(); // 初始化NIO 服务端socket，绑定2181 端口，可以接收客户端请求 cnxnFactory.configure(config.getClientPortAddress(), config.getMaxClientCnxns(), false); // 启动 zk quorumPeer.start();&#125; 启动 zookeeper 1234567891011121314151617181920// QuorumPeer#startpublic synchronized void start() &#123; if (!getView().containsKey(myid)) &#123; throw new RuntimeException(&quot;My id &quot; + myid + &quot; not in the peer list&quot;); &#125; // 冷启动数据恢复，将快照中数据恢复到 DataTree loadDataBase(); // 启动通信工厂实例对象 startServerCnxnFactory(); try &#123; adminServer.start(); &#125; catch (AdminServerException e) &#123; LOG.warn(&quot;Problem starting AdminServer&quot;, e); System.out.println(e); &#125; // 准备选举环境 startLeaderElection(); // 执行选举 super.start();&#125; 选举机制环境准备QuorumPeer#startLeaderElection 初始化选举环境： 12345678910111213141516171819202122synchronized public void startLeaderElection() &#123; try &#123; // Looking 状态，需要选举 if (getPeerState() == ServerState.LOOKING) &#123; // 选票组件: myid (serverid), zxid, epoch // 开始选票时，serverid 是自己，【先投自己】 currentVote = new Vote(myid, getLastLoggedZxid(), getCurrentEpoch()); &#125; &#125; if (electionType == 0) &#123; try &#123; udpSocket = new DatagramSocket(getQuorumAddress().getPort()); // 响应投票结果线程 responder = new ResponderThread(); responder.start(); &#125; catch (SocketException e) &#123; throw new RuntimeException(e); &#125; &#125; // 创建选举算法实例 this.electionAlg = createElectionAlgorithm(electionType);&#125; 123456789101112131415// zk总的发送和接收队列准备好protected Election createElectionAlgorithm(int electionAlgorithm)&#123; // 负责选举过程中的所有网络通信，创建各种队列和集合 QuorumCnxManager qcm = createCnxnManager(); QuorumCnxManager.Listener listener = qcm.listener; if(listener != null)&#123; // 启动监听线程, 调用 client = ss.accept()阻塞，等待处理请求 listener.start(); // 准备好发送和接收队列准备 FastLeaderElection fle = new FastLeaderElection(this, qcm); // 启动选举线程，【WorkerSender 和 WorkerReceiver】 fle.start(); le = fle; &#125;&#125; 选举源码当 Zookeeper 启动后，首先都是 Looking 状态，通过选举让其中一台服务器成为 Leader 执行 super.start() 相当于执行 QuorumPeer#run() 方法 12345public void run() &#123; case LOOKING: // 进行选举，选举结束返回最终成为 Leader 胜选的那张选票 setCurrentVote(makeLEStrategy().lookForLeader());&#125; FastLeaderElection 类： lookForLeader：选举 12345678910111213141516171819public Vote lookForLeader() &#123; // 正常启动中其他服务器都会向我发送一个投票，保存每个服务器的最新合法有效的投票 HashMap&lt;Long, Vote&gt; recvset = new HashMap&lt;Long, Vote&gt;();\t// 存储合法选举之外的投票结果 HashMap&lt;Long, Vote&gt; outofelection = new HashMap&lt;Long, Vote&gt;();\t// 一次选举的最大等待时间，默认值是0.2s int notTimeout = finalizeWait;\t// 每发起一轮选举，logicalclock++,在没有合法的epoch 数据之前，都使用逻辑时钟代替 synchronized(this)&#123; // 更新逻辑时钟，每进行一次选举，都需要更新逻辑时钟 logicalclock.incrementAndGet(); // 更新选票(serverid， zxid, epoch） updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; // 广播选票，把自己的选票发给其他服务器 sendNotifications(); // 一轮一轮的选举直到选举成功 while ((self.getPeerState() == ServerState.LOOKING) &amp;&amp; (!stop))&#123; &#125;&#125; sendNotifications：广播选票 123456789private void sendNotifications() &#123; // 遍历投票参与者，给每台服务器发送选票 for (long sid : self.getCurrentAndNextConfigVoters()) &#123; // 创建发送选票 ToSend notmsg = new ToSend(...); // 把发送选票放入发送队列 sendqueue.offer(notmsg); &#125;&#125; FastLeaderElection 中有 WorkerSender 线程： ToSend m = sendqueue.poll(3000, TimeUnit.MILLISECONDS)：阻塞获取要发送的选票 process(m)：处理要发送的选票 manager.toSend(m.sid, requestBuffer)：发送选票 if (this.mySid == sid)：如果消息的接收者 sid 是自己，直接进入自己的 RecvQueue（自己投自己） else：如果接收者是其他服务器，创建对应的发送队列或者复用已经存在的发送队列，把消息放入该队列 connectOne(sid)：建立连接 sock.connect(electionAddr, cnxTO)：建立与 sid 服务器的连接 initiateConnection(sock, sid)：初始化连接 startConnection(sock, sid)：创建并启动发送器线程和接收器线程 dout = new DataOutputStream(buf)：获取 Socket 输出流，向服务器发送数据 din = new DataInputStream(new BIS(sock.getInputStream())))：通过输入流读取对方发送过来的选票 if (sid &gt; self.getId())：接收者 sid 比我的大，没有资格给对方发送连接请求的，直接关闭自己的客户端 SendWorker sw：初始化发送器，并启动发送器线程，线程 run 方法 while (running &amp;&amp; !shutdown &amp;&amp; sock != null)：连接没有断开就一直运行 ByteBuffer b = pollSendQueue()：从发送队列 SendQueue 中获取发送消息 lastMessageSent.put(sid, b)：更新对于 sid 这台服务器的最近一条消息 send(b)：执行发送 RecvWorker rw：初始化接收器，并启动接收器线程 din.readFully(msgArray, 0, length)：输入流接收消息 addToRecvQueue(new Message(messagg, sid))：将消息放入接收消息 recvQueue 队列 FastLeaderElection 中有 WorkerReceiver 线程 response = manager.pollRecvQueue()：从 RecvQueue 中阻塞获取出选举投票消息（其他服务器发送过来的） 状态同步选举结束后，每个节点都需要根据角色更新自己的状态，Leader 更新状态为 Leader，其他节点更新状态为 Follower，整体流程： Follower 需要让 Leader 知道自己的状态 (sid, epoch, zxid) Leader 接收到信息，根据信息构建新的 epoch，要返回对应的信息给 Follower，Follower 更新自己的 epoch Leader 需要根据 Follower 的状态，确定何种方式的数据同步 DIFF、TRUNC、SNAP，就是要以 Leader 服务器数据为准 DIFF：Leader 提交的 zxid 比 Follower 的 zxid 大，发送 Proposal 给 Follower 提交执行 TRUNC：Follower 的 zxid 比leader 的 zxid 大，Follower 要进行回滚 SNAP：Follower 没有任何数据，直接全量同步 执行数据同步，当 Leader 接收到超过半数 Follower 的 Ack 之后，进入正常工作状态，集群启动完成 核心函数解析： Leader 更新状态入口：Leader.lead() zk.loadData()：恢复数据到内存 cnxAcceptor = new LearnerCnxAcceptor()：启动通信组件 s = ss.accept()：等待其他 Follower 节点向 Leader 节点发送同步状态 LearnerHandler fh ：接收到 Follower 的请求，就创建 LearnerHandler 对象 fh.start()：启动线程，通过 switch-case 语法判断接收的命令，执行相应的操作 Follower 更新状态入口：Follower.followerLeader() QuorumServer leaderServer = findLeader()：查找 Leader connectToLeader(addr, hostname) ：与 Leader 建立连接 long newEpochZxid = registerWithLeader(Leader.FOLLOWERINFO)：向 Leader 注册 主从工作Leader：主服务的工作流程 Follower：从服务的工作流程，核心函数为 Follower#followLeader() readPacket(qp)：读取信息 processPacket(qp)：处理信息 1234567891011121314151617181920protected void processPacket(QuorumPacket qp) throws Exception&#123; switch (qp.getType()) &#123; case Leader.PING: break; case Leader.PROPOSAL: break; case Leader.COMMIT: break; case Leader.COMMITANDACTIVATE: break; case Leader.UPTODATE: break; case Leader.REVALIDATE: break; case Leader.SYNC: break; default: break; &#125;&#125; 客户端"},{"path":"/customize/css/ZYDark.css","content":"#ZYDark:root { --site-bg: #1c1e21; --card: #373d43; --block: #26292c; --block-border: #383d42; --block-hover: #2f3337; --text-p0: #fff; --text-p1: #ccc; --text-p2: #b3b3b3; --text-p3: #858585; --text-p4: #707070; --text-meta: #4d4d4d; --text-code: #ff6333; } @media screen and (max-width: 667px) { #ZYDark:root { --site-bg: #000; } } #ZYDark:root { --blur-bg: rgba(0,0,0,0.5); } #ZYDark .float-panel { --blur-bg: rgba(0,0,0,0.4); } #ZYDark .tag-plugin.tag { --theme: #ff6333; --theme-bg1: #3d1e14; --theme-bg2: #2f2522; --theme-border: #5c2d1f; --text-p0: #ffc4b3; --text-p1: #dfae9f; --text-p2: #f1997e; } #ZYDark .tag-plugin[color='red'] { --theme: #f44336; --theme-bg1: #3d1714; --theme-bg2: #2f2322; --theme-border: #5c231f; --text-p0: #ffb8b3; --text-p1: #dfa49f; --text-p2: #f1867e; } #ZYDark .tag-plugin[color='orange'] { --theme: #fa6400; --theme-bg1: #3d2514; --theme-bg2: #2f2722; --theme-border: #5c371f; --text-p0: #ffd1b3; --text-p1: #dfb99f; --text-p2: #f1ac7e; } #ZYDark .tag-plugin[color='yellow'] { --theme: #ffbd2b; --theme-bg1: #3d3014; --theme-bg2: #2f2b22; --theme-border: #5c491f; --text-p0: #ffe7b3; --text-p1: #dfcb9f; --text-p2: #f1cd7e; } #ZYDark .tag-plugin[color='green'] { --theme: #3dc550; --theme-bg1: #143d1a; --theme-bg2: #222f24; --theme-border: #1f5c27; --text-p0: #b3ffbd; --text-p1: #9fdfa8; --text-p2: #7ef18e; } #ZYDark .tag-plugin[color='cyan'] { --theme: #1bcdfc; --theme-bg1: #14353d; --theme-bg2: #222d2f; --theme-border: #1f4f5c; --text-p0: #b3efff; --text-p1: #9fd2df; --text-p2: #7ed9f1; } #ZYDark .tag-plugin[color='blue'] { --theme: #2196f3; --theme-bg1: #142b3d; --theme-bg2: #222a2f; --theme-border: #1f415c; --text-p0: #b3ddff; --text-p1: #9fc3df; --text-p2: #7ebef1; } #ZYDark .tag-plugin[color='purple'] { --theme: #9c27b0; --theme-bg1: #37143d; --theme-bg2: #2d222f; --theme-border: #531f5c; --text-p0: #f4b3ff; --text-p1: #d69fdf; --text-p2: #e07ef1; } #ZYDark .tag-plugin[color='light'] { --theme-border: #fff; --theme-bg1: #e0e0e0; --theme-bg2: #fff; --text-p0: #000; --text-p1: #111; --text-p2: #1f1f1f; --text-p3: #555; --text-code: #fff; } #ZYDark .tag-plugin[color='dark'] { --theme-border: #000; --theme-bg1: #1f1f1f; --theme-bg2: #111; --text-p0: #fff; --text-p1: #fff; --text-p2: #e0e0e0; --text-p3: #ddd; --text-code: #fff; } #ZYDark .tag-plugin[color='warning'], #ZYDark .tag-plugin[color='light'] { --text-p0: #000; --text-p1: #111; --text-p2: #1f1f1f; --text-p3: #555; --text-code: #fff; } #ZYDark .social-wrap a.social:hover { box-shadow: none; } /* waline评论样式 */ #ZYDark .wl-count{ padding: .375em; font-weight: bold; font-size: 1.25em; color: #fff; } #ZYDark .cmt-body.waline{ --waline-white: #000; --waline-light-grey: #666; --waline-dark-grey: #999; /* 布局颜色 */ --waline-color: #fff; --waline-bgcolor: var(--block); --waline-bgcolor-light: #272727; --waline-border-color: #333; --waline-disable-bgcolor: #444; --waline-disable-color: #272727; /* 特殊颜色 */ --waline-bq-color: #272727; /* 其他颜色 */ --waline-info-bgcolor: #272727; --waline-info-color: #666; }"},{"title":"Tool","path":"/wiki/javaNode/Tool.html","content":"GitGit概述版本系统SVN 是集中式版本控制系统，版本库是集中放在中央服务器的，而开发人员工作的时候，用的都是自己的电脑，所以首先要从中央服务器下载最新的版本，然后开发，开发完后，需要把自己开发的代码提交到中央服务器。 集中式版本控制工具缺点：服务器单点故障、容错性差 Git 是分布式版本控制系统（Distributed Version Control System，简称 DVCS） ，分为两种类型的仓库： 本地仓库和远程仓库： 本地仓库：是在开发人员自己电脑上的 Git 仓库 远程仓库：是在远程服务器上的 Git 仓库 工作流程1．从远程仓库中克隆代码到本地仓库 2．从本地仓库中 checkout 代码然后进行代码修改 3．在提交前先将代码提交到暂存区 4．提交到本地仓库。本地仓库中保存修改的各个历史版本 5．修改完成后，需要和团队成员共享代码时，将代码 push 到远程仓库 Git安装下载地址： https://git-scm.com/download 代码托管Git 中存在两种类型的仓库，即本地仓库和远程仓库。那么我们如何搭建Git远程仓库呢？我们可以借助互联网上提供的一些代码托管服务来实现，其中比较常用的有 GitHub、码云、GitLab 等。 GitHub（地址：https://github.com/）是一个面向开源及私有软件项目的托管平台，因为只支持 Git 作为唯一的版本库格式进行托管，故名 GitHub 码云（地址： https://gitee.com/）是国内的一个代码托管平台，由于服务器在国内，所以相比于 GitHub，码云速度会更快 GitLab（地址： https://about.gitlab.com/ ）是一个用于仓库管理系统的开源项目，使用 Git 作为代码管理工具，并在此基础上搭建起来的 web 服务 环境配置安装 Git 后首先要设置用户名称和 email 地址，因为每次 Git 提交都会使用该用户信息，此信息和注册的代码托管平台的信息无关 设置用户信息： git config –global user.name “Seazean” git config –global user.email “&#x69;&#x6d;&#115;&#x65;&#97;&#122;&#x65;&#97;&#110;&#x40;&#x67;&#109;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#111;&#109;“ &#x2F;&#x2F;用户名和邮箱可以随意填写，不会校对 查看配置信息： git config –list git config user.name 通过上面的命令设置的信息会保存在用户目录下 &#x2F;.gitconfig 文件中 本地仓库获取仓库 本地仓库初始化 在电脑的任意位置创建一个空目录（例如 repo1）作为本地 Git 仓库 进入这个目录中，点击右键打开 Git bash 窗口 执行命令 git init 如果在当前目录中看到 .git 文件夹（此文件夹为隐藏文件夹）则说明 Git 仓库创建成功 远程仓库克隆通过 Git 提供的命令从远程仓库进行克隆，将远程仓库克隆到本地 命令：git clone 远程 Git 仓库地址（HTTPS 或者 SSH） 生成 SSH 公钥步骤 设置账户 cd ~&#x2F;.ssh（查看是否生成过 SSH 公钥）user 目录下 生成 SSH 公钥：ssh-keygen -t rsa -C &quot;email&quot; -t 指定密钥类型，默认是 rsa ，可以省略 -C 设置注释文字，比如邮箱 -f 指定密钥文件存储文件名 查看命令：cat ~&#x2F;.ssh&#x2F;id_rsa.pub 公钥测试命令：ssh -T &#103;&#x69;&#x74;&#64;&#x67;&#x69;&#116;&#104;&#117;&#x62;&#x2e;&#x63;&#x6f;&#x6d; 工作过程 版本库：.git 隐藏文件夹就是版本库，版本库中存储了很多配置信息、日志信息和文件版本信息等 工作目录（工作区）：包含 .git 文件夹的目录就是工作目录，主要用于存放开发的代码 暂存区：.git 文件夹中有很多文件，其中有一个 index 文件就是暂存区，也可以叫做 stage，暂存区是一个临时保存修改文件的地方 文件操作常用命令 命令 作用 git status 查看 git 状态 （文件是否进行了添加、提交操作） git add filename 添加，将指定文件添加到暂存区 git commit -m ‘message’ 提交，将暂存区文件提交到本地仓库，删除暂存区的该文件 git commit –amend 修改 commit 的 message git rm filename 删除，删除工作区的文件，不是仓库，需要提交 git mv filename 移动或重命名工作区文件 git reset filename 使用当前分支上的修改覆盖暂存区，将暂存区的文件取消暂存 git checkout filename 使用暂存区的修改覆盖工作目录，用来撤销本次修改(危险) git log 查看日志（ git 提交的历史日志） git reflog 可以查看所有分支的所有操作记录（包括已经被删除的 commit 记录的操作） 其他指令：可以跳过暂存区域直接从分支中取出修改，或者直接提交修改到分支中 git commit -a 直接把所有文件的修改添加到暂存区然后执行提交 git checkout HEAD – files 取出最后一次修改，可以用来进行回滚操作 文件状态 Git 工作目录下的文件存在两种状态： untracked 未跟踪（未被纳入版本控制） tracked 已跟踪（被纳入版本控制） Unmodified 未修改状态 Modified 已修改状态 Staged 已暂存状态 查看文件状态：文件的状态会随着我们执行 Git 的命令发生变化 git status 查看文件状态 git status –s 查看更简洁的文件状态 文件忽略一般我们总会有些文件无需纳入Git 的管理，也不希望它们总出现在未跟踪文件列表。 通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。 在这种情况下，我们可以在工作目录中创建一个名为 .gitignore 的文件（文件名称固定），列出要忽略的文件模式。下面是一个示例： 123456789101112# no .a files*.a# but do track lib.a, even though you&#x27;re ignoring .a files above!lib.a# only ignore the TODO file in the current directory, not subdir/TODO/TODO# ignore all files in the build/ directorybuild/# ignore doc/notes.txt, but not doc/server/arch.txtdoc/*.txt# ignore all .pdf files in the doc/ directorydoc/**/*.pdf 远程仓库工作流程Git 有四个工作空间的概念，分别为 工作空间、暂存区、本地仓库、远程仓库。 pull &#x3D; fetch + merge fetch 是从远程仓库更新到本地仓库，pull是从远程仓库直接更新到工作空间中 查看仓库git remote：显示所有远程仓库的简写 git remote -v：显示所有远程仓库 git remote show ：显示某个远程仓库的详细信息 添加仓库git remote add ：添加一个新的远程仓库，并指定一个可以引用的简写 克隆仓库git clone (HTTPS or SSH)：克隆远程仓库 Git 克隆的是该 Git 仓库服务器上的几乎所有数据（包括日志信息、历史记录等），而不仅仅是复制工作所需要的文件，当你执行 git clone 命令的时候，默认配置下远程 Git 仓库中的每一个文件的每一个版本都将被拉取下来。 删除仓库git remote rm ：移除远程仓库，从本地移除远程仓库的记录，并不会影响到远程仓库 拉取仓库git fetch ：从远程仓库获取最新版本到本地仓库，不会自动 merge git pull ：从远程仓库获取最新版本并 merge 到本地仓库 注意：如果当前本地仓库不是从远程仓库克隆，而是本地创建的仓库，并且仓库中存在文件，此时再从远程仓库拉取文件的时候会报错（fatal: refusing to merge unrelated histories ），解决此问题可以在 git pull 命令后加入参数 –allow-unrelated-histories 推送仓库git push ：上传本地指定分支到远程仓库 版本管理 命令：git reset –hard 版本唯一索引值 分支管理查看分支git branch：列出所有本地分支 git branch -r：列出所有远程分支 git branch -a：列出所有本地分支和远程分支 创建分支git branch branch-name：新建一个分支，但依然停留在当前分支 git checkout -b branch-name：新建一个分支，并切换到该分支 推送分支git push origin branch-name：推送到远程仓库，origin 是引用名 切换分支git checkout branch-name：切换到 branch-name 分支 合并分支git merge branch-name：合并指定分支到当前分支 有时候合并操作不会如此顺利。 如果你在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git 就没办法合并它们，同时会提示文件冲突。此时需要我们打开冲突的文件并修复冲突内容，最后执行 git add 命令来标识冲突已解决 ​ 删除分支git branch -d branch-name：删除分支 git push origin –d branch-name：删除远程仓库中的分支 （origin 是引用名） 如果要删除的分支中进行了开发动作，此时执行删除命令并不会删除分支，如果坚持要删除此分支，可以将命令中的 -d 参数改为 -D：git branch -D branch-name 标签管理查看标签git tag：列出所有 tag git show tag-name：查看 tag 详细信息 标签作用：在开发的一些关键时期，使用标签来记录这些关键时刻，保存快照，例如发布版本、有重大修改、升级的时候、会使用标签记录这些时刻，来永久标记项目中的关键历史时刻 新建标签git tag tag-name：新建标签，如（git tag v1.0.1） 推送标签git push [remotename] [tagname]：推送到远程仓库 git push [remotename] –tags：推送所有的标签 切换标签git checkout tag-name：切换标签 删除标签git tag -d tag-name：删除本地标签 git push origin :refs&#x2F;tags&#x2F; tag-name：删除远程标签 IDEA操作环境配置File → Settings 打开设置窗口，找到 Version Control 下的 git 选项 选择 git 的安装目录后可以点击 Test 按钮测试是否正确配置：D:\\Program Files\\Git\\cmd\\git.exe 创建仓库1、VCS → Import into Version Control → Create Git Repository 2、选择工程所在的目录,这样就创建好本地仓库了 3、点击git后边的对勾,将当前项目代码提交到本地仓库 ​\t注意: 项目中的配置文件不需要提交到本地仓库中,提交时,忽略掉即可 文件操作右键项目名打开菜单 Git → Add → commit 版本管理 版本对比 版本切换方式一：控制台 Version Control → Log → 右键 Reset Current Branch → Reset，这种切换会抛弃原来的提交记录 版本切换方式二：控制台 Version Control → Log → Revert Commit → Merge → 处理代码 → commit，这种切换会当成一个新的提交记录，之前的提交记录也都保留 ​ 分支管理 创建分支：VCS → Git → Branches → New Branch → 给分支起名字 → ok 切换分支：idea 右下角 Git → 选择要切换的分支 → checkout 合并分支：VCS → Git → Merge changes → 选择要合并的分支 → merge 删除分支：idea 右下角 → 选中要删除的分支 → Delete 推送仓库 VCS → Git → Push → 点击 master Define remote 将远程仓库的 url 路径复制过来 → Push 克隆仓库File → Close Project → Checkout from Version Control → Git → 指定远程仓库的路径 → 指定本地存放的路径 → clone Linux操作系统操作系统（Operation System），是管理计算机硬件与软件资源的计算机程序，同时也是计算机系统的内核与基石。操作系统需要处理管理与配置内存、决定系统资源供需的优先次序、控制输入设备与输出设备、操作网络与管理文件系统等基本事务，操作系统也提供一个让用户与系统交互的操作界面 操作系统作为接口的示意图： 移动设备操作系统： Linux系统系统介绍从内到位依次是硬件 → 内核层 → Shell 层 → 应用层 → 用户 内核层：核心和基础，附着在硬件平台上，控制和管理系统内的各种资源，有效的组织进程的运行，扩展硬件的功能，提高资源利用效率，为用户提供安全可靠的应用环境。 Shell 层：与用户直接交互的界面。用户可以在提示符下输入命令行，由 Shell 解释执行并输出相应结果或者有关信息，所以我们也把 Shell 称作命令解释器，利用系统提供的丰富命令可以快捷而简便地完成许多工作。 文件系统Linux 文件系统目录结构和熟知的 windows 系统有较大区别，没有各种盘符的概念。根目录只有一个&#x2F;，采用层级式的树状目录结构。 远程连接设置IPNAT首先设置虚拟机中 NAT 模式的选项，打开 VMware，点击编辑下的虚拟网络编辑器，设置 NAT 参数 注意：VMware Network Adapter VMnet8 保证是启用状态 ​ 静态IP在普通用户下不能修改网卡的配置信息；所以我们要切换到 root 用户进行 ip 配置：su root&#x2F;su 修改网卡配置文件：vim /etc/sysconfig/network-scripts/ifcfg-ens33 修改文件内容 12345678910111213141516171819202122232425TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticIPADDR=10.2.111.62NETMASK=255.255.252.0GATEWAY=10.2.111.254DEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33UUID=2c2371f1-ef29-4514-a568-c4904bd11c82DEVICE=ens33ONBOOT=true###########################BOOTPROTO设置为静态staticIPADDR设置ip地址NETMASK设置子网掩码GATEWAY设置网关ONBOOT设置为true在系统启动时是否激活网卡执行保存 :wq! 重启网络：systemctl restart network 查看IP：ifconfig 宿主机 ping 虚拟机，虚拟机 ping 宿主机 在虚拟机中访问网络，需要增加一块 NAT 网卡 【虚拟机】–【设置】–【添加】 远程登陆服务器维护工作 都是在 远程 通过 SSH 客户端 来完成的， 并没有图形界面， 所有的维护工作都需要通过命令来完成，Linux 服务器需要安装 SSH 相关服务 首先执行 sudo apt-get install openssh-server 指令，接下来用 xshell 连接 先用普通用户登录，然后转成 root 用户管理Linux 系统是一个多用户、多任务的操作系统。多用户是指在 Linux 操作系统中可以创建多个用户，而这些多用户又可以同时执行各自不同的任务，而互不影响 在 Linux 系统中，会存在着以下几个概念： 用户名：用户的名称 用户所属的组：当前用户所属的组 用户的家目录：当前账号登录成功之后的目录，就叫做该用户的家目录 用户管理当前用户logname：用于显示目前用户的名称 –help：在线帮助 –vesion：显示版本信息 切换用户su UserName：切换用户 su -c comman root：切换用户为 root 并在执行 comman 指令后退出返回原使用者 su：切换到 root 用户 用户添加命令：useradd [options] 用户名 参数说明： -c comment 指定一段注释性描述 -d 指定用户主目录，如果此目录不存在，则同时使用 -m 选项，可以创建主目录 -m 创建用户的主目录 -g 用户组，指定用户所属的用户组 -G 用户组，用户组 指定用户所属的附加组 -s Shell 文件 指定用户的登录 Shell -u 用户号，指定用户的用户号，如果同时有 -o 选项，则可以重复使用其他用户的标识号。 如何知道添加用户成功呢？ 通过指令 cat &#x2F;etc&#x2F;passwd 查看 12seazean:x: 1000:1000:Seazean:/home/seazean:/bin/bash用户名 密码 用户ID 组ID 注释 家目录 shell程序 useradd -m Username 新建用户成功之后，会建立 home 目录，但是此时有问题没有指定 shell 的版本，不是我们熟知的 bash，功能上有很多限制，进行 sudo useradd -m -s &#x2F;bin&#x2F;bash Username 用户密码系统安装好默认的 root 用户是没有密码的，需要给 root 设置一个密码 sudo passwd root. 普通用户：sudo passwd UserName 管理员用户：passwd [options] UserName -l：锁定密码，即禁用账号 -u：密码解锁 -d：使账号无密码 -f：强迫用户下次登录时修改密码 用户权限usermod 命令通过修改系统帐户文件来修改用户账户信息 修改用户账号就是根据实际情况更改用户的有关属性，如用户号、主目录、用户组、登录 Shell 等 普通用户：sudo usermod [options] Username 管理员用户：usermod [options] Username usermod -l newName Username -l 新的登录名称 用户删除删除用户账号就是要将 &#x2F;etc&#x2F;passwd 等系统文件中的该用户记录删除，必要时还删除用户的主目录 普通用户：sudo userdel [options] Username 管理员用户：userdel [options] Username -f：强制删除用户，即使用户当前已登录 -r：删除用户的同时，删除与用户相关的所有文件 用户组管理组管理添加组：groupadd 组名 创建用户的时加入组：useradd -m -g 组名 用户名​ 添加用户组新增一个用户组（组名可见名知意，符合规范即可），然后将用户添加到组中，需要使用管理员权限 命令：groupadd [options] Groupname -g GID 指定新用户组的组标识号（GID） -o 一般与 -g 选项同时使用，表示新用户组的 GID 可以与系统已有用户组的 GID 相同 新增用户组 Seazean：groupadd Seazean 修改用户组需要使用管理员权限 命令：groupmod [options] Groupname -g GID 为用户组指定新的组标识号。 -o 与 -g 选项同时使用，用户组的新 GID 可以与系统已有用户组的 GID 相同 -n 新用户组 将用户组的名字改为新名字 修改 Seazean 组名为 zhy：groupmod -n zhy Seazean 删除用户组 普通用户：sudo groupdel Groupname 管理员用户：groupdel Groupname -f 用户的主组也继续删除 -h 显示帮助信息 用户所属组查询用户所属组：groups Username 查看用户及组信息：id Username 创建用户的时加入组：useradd -m -g Groupname Username 修改用户所属组：usermod -g Groupname Username usermod常用选项： -d 用户的新主目录 -l 新的登录名称 gpasswdgpasswd 是 Linux 工作组文件 &#x2F;etc&#x2F;group 和 &#x2F;etc&#x2F;gshadow 管理工具，用于将一个用户添加到组或从组中删除 命令：gpasswd 选项 Username Groupname -a 向组 GROUP 中添加用户 USER -d 从组 GROUP 中添加或删除用户 查看用户组下所有用户（所有用户）：grep ‘Groupname’ &#x2F;etc&#x2F;group 系统管理man在控制台输入：命令名 -h&#x2F; -help&#x2F; –h &#x2F;空 可以看到命令的帮助文档 man [指令名称]：查看帮助文档，比如 man ls，退出方式 q datedate 可以用来显示或设定系统的日期与时间 命令：date [options] -d&lt;字符串&gt;：显示字符串所指的日期与时间，字符串前后必须加上双引号； -s&lt;字符串&gt;：根据字符串来设置日期与时间，字符串前后必须加上双引号 -u：显示 GMT –version：显示版本信息 查看时间：date → 2020年 11月 30日 星期一 17:10:54 CST 查看指定格式时间：date “+%Y-%m-%d %H:%M:%S” → 2020-11-30 17:11:44 设置日期指令：date -s “2019-12-23 19:21:00” idid 会显示用户以及所属群组的实际与有效 ID，若两个 ID 相同则仅显示实际 ID；若仅指定用户名称，则显示目前用户的 ID 命令：id [-gGnru] [–help] [–version] [用户名称] &#x2F;&#x2F;参数的顺序 -g 或–group：显示用户所属群组的 ID -G 或–groups：显示用户所属附加群组的 ID -n 或–name：显示用户，所属群组或附加群组的名称。 -r 或–real：显示实际 ID -u 或–user：显示用户 ID id 命令参数虽然很多，但是常用的是不带参数的 id 命令，主要看 uid 和组信息 sudosudo：控制用户对系统命令的使用权限，通过 sudo 可以提高普通用户的操作权限 -V 显示版本编号 -h 会显示版本编号及指令的使用方式说明 -l 显示出自己（执行 sudo 的使用者）的权限 -command 要以系统管理者身份（或以 -u 更改为其他人）执行的指令 sudo -u root command -l：指定 root 用户执行指令 command toptop：用于实时显示 process 的动态 -c：command 属性进行了命令补全 -p 进程号：显示指定 pid 的进程信息 -d 秒数：表示进程界面更新时间（每几秒刷新一次） -H 表示线程模式 top -Hp 进程 id：分析该进程内各线程的 CPU 使用情况 各进程（任务）的状态监控属性解释说明： PID — 进程 id TID — 线程 id USER — 进程所有者 PR — 进程优先级 NI — nice 值，负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位 kb，VIRT&#x3D;SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位 kb，RES&#x3D;CODE+DATA SHR — 共享内存大小，单位 kb S — 进程状态，D&#x3D;不可中断的睡眠状态 R&#x3D;运行 S&#x3D;睡眠 T&#x3D;跟踪&#x2F;停止 Z&#x3D;僵尸进程 %CPU — 上次更新到现在的 CPU 时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的 CPU 时间总计，单位 1&#x2F;100 秒 COMMAND — 进程名称（命令名&#x2F;命令行） psLinux 系统中查看进程使用情况的命令是 ps 指令 命令：ps -e: 显示所有进程 -f: 全格式 a: 显示终端上的所有进程 u: 以用户的格式来显示进程信息 x: 显示后台运行的进程 -T：开启线程查看 -p：指定线程号 一般常用格式为 ps -ef 或者 ps aux 两种。显示的信息大体一致，略有区别： 如果想查看进程的 CPU 占用率和内存占用率，可以使用 aux 如果想查看进程的父进程 ID 和完整的 COMMAND 命令，可以使用 ef ps -T -p &lt;pid&gt;：显示某个进程的线程 ps 和 top 区别： ps 命令：可以查看进程的瞬间信息，是系统在过去执行的进程的静态快照 top 命令：可以持续的监视进程的动态信息 killLinux kill 命令用于删除执行中的程序或工作，并不是让进程直接停止，而是给进程发一个信号，可以进入终止逻辑 命令：kill [-s &lt;信息名称或编号&gt;] [程序]　或　kill [-l &lt;信息编号&gt;] -l &lt;信息编号&gt;：若不加&lt;信息编号&gt;选项，则-l参数会列出全部的信息名称 -s &lt;信息名称或编号&gt;：指定要送出的信息 -KILL：强制杀死进程 -9：彻底杀死进程（常用） [程序] 程序的 PID、PGID、工作编号 kill 15642 . kill -KILL 15642. kill -9 15642 杀死指定用户所有进程： 过滤出 user 用户进程 ：kill -9 $(ps -ef | grep user) 直接杀死：kill -u user shutdownshutdown 命令可以用来进行关闭系统，并且在关机以前传送讯息给所有使用者正在执行的程序，shutdown 也可以用来重开机 普通用户：sudo shutdown [-t seconds] [-rkhncfF] time [message] 管理员用户：shutdown [-t seconds] [-rkhncfF] time [message] -t seconds：设定在几秒钟之后进行关机程序 -k：并不会真的关机，只是将警告讯息传送给所有使用者 -r：关机后重新开机 -h：关机后停机 -n：不采用正常程序来关机，用强迫的方式杀掉所有执行中的程序后自行关机 -c：取消目前已经进行中的关机动作 -f：关机时，不做 fcsk 动作（检查 Linux 档系统） -F：关机时，强迫进行 fsck 动作 time：设定关机的时间 message：传送给所有使用者的警告讯息 立即关机：shutdown -h now 或者 shudown now 指定 1 分钟后关机并显示警告信息：shutdown +1 &quot;System will shutdown after 1 minutes&quot; 指定 1 分钟后重启并发出警告信息：shutdown –r +1 &quot;1分钟后关机重启&quot; rebootreboot 命令用于用来重新启动计算机 命令：reboot [-n] [-w] [-d] [-f] [-i] -n：在重开机前不做将记忆体资料写回硬盘的动作 -w：并不会真的重开机，只是把记录写到 &#x2F;var&#x2F;log&#x2F;wtmp 档案里 -d：不把记录写到 &#x2F;var&#x2F;log&#x2F;wtmp 档案里（-n 这个参数包含了 -d） -f：强迫重开机，不呼叫 shutdown 这个指令 -i：在重开机之前先把所有网络相关的装置先停止 whowho 命令用于显示系统中有哪些使用者正在上面，显示的资料包含了使用者 ID、使用的终端机、上线时间、CPU 使用量、动作等等 命令：who - [husfV] [user] -H 或 –heading：显示各栏位的标题信息列（常用 who -H） -i 或 -u 或 –idle：显示闲置时间，若该用户在前一分钟之内有进行任何动作，将标示成 . 号，如果该用户已超过 24 小时没有任何动作，则标示出 old 字符串 -m：此参数的效果和指定 am i 字符串相同 -q 或–count：只显示登入系统的帐号名称和总人数 -s：此参数将忽略不予处理，仅负责解决who指令其他版本的兼容性问题 -w 或-T或–mesg或–message或–writable：显示用户的信息状态栏 –help：在线帮助 –version：显示版本信息 systemctl命令：systemctl [command] [unit] –version 查看版本号 start：立刻启动后面接的 unit stop：立刻关闭后面接的 unit restart：立刻关闭后启动后面接的 unit，亦即执行 stop 再 start 的意思 reload：不关闭 unit 的情况下，重新载入配置文件，让设置生效 status：目前后面接的这个 unit 的状态，会列出有没有正在执行、开机时是否启动等信息 enable：设置下次开机时，后面接的 unit 会被启动 disable：设置下次开机时，后面接的 unit 不会被启动 is-active：目前有没有正在运行中 is-enable：开机时有没有默认要启用这个 unit kill ：不要被 kill 这个名字吓着了，它其实是向运行 unit 的进程发送信号 show：列出 unit 的配置 mask：注销 unit，注销后你就无法启动这个 unit 了 unmask：取消对 unit 的注销 timedatectltimedatectl用于控制系统时间和日期。可以查询和更改系统时钟于设定，同时可以设定和修改时区信息。在实际开发过程中，系统时间的显示会和实际出现不同步；我们为了校正服务器时间、时区会使用timedatectl命令 timedatectl：显示系统的时间信息 timedatectl status：显示系统的当前时间和日期 timedatectl | grep Time：查看当前时区 timedatectl list-timezones：查看所有可用的时区 timedatectl set-timezone “Asia&#x2F;Shanghai”：设置本地时区为上海 timedatectl set-ntp true&#x2F;false：启用&#x2F;禁用时间同步 timedatectl set-time “2020-12-20 20:45:00”：时间同步关闭后可以设定时间 NTP 即 Network Time Protocol（网络时间协议），是一个互联网协议，用于同步计算机之间的系统时钟，timedatectl 实用程序可以自动同步你的Linux系统时钟到使用NTP的远程服务器 clearclear 命令用于清除屏幕 通过执行 clear 命令，就可以把缓冲区的命令全部清理干净 exitexit 命令用于退出目前的 shell 执行 exit 可使 shell 以指定的状态值退出。若不设置状态值参数，则 shell 以预设值退出。状态值 0 代表执行成功，其他值代表执行失败；exit 也可用在 script，离开正在执行的 script，回到 shell 命令：exit [状态值] 0 表示成功（Zero - Success） 非 0 表示失败（Non-Zero - Failure） 2 表示用法不当（Incorrect Usage） 127 表示命令没有找到（Command Not Found） 126 表示不是可执行的（Not an executable） 大于等于 128 信号产生 文件管理常用命令lsls命令相当于我们在Windows系统中打开磁盘、或者打开文件夹看到的目录以及文件的明细。 命令：ls [options] 目录名称 -a ：全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用) -d ：仅列出目录本身，而不是列出目录内的文件数据(常用) -l ：显示不隐藏的文件与文件夹的详细信息；(常用) ls -al &#x3D; ll 命令：显示所有文件与文件夹的详细信息 pwdpwd 是 Print Working Directory 的缩写，也就是显示目前所在当前目录的命令 命令：pwd 选项 -L\t打印 $PWD 变量的值，如果它包含了当前的工作目录 -P\t打印当前的物理路径，不带有任何的符号链接 cdcd 是 Change Directory 的缩写，这是用来变换工作目录的命令 命令：cd [相对路径或绝对路径] cd ~ ：表示回到根目录 cd .. ：返回上级目录 相对路径 在输入路径时, 最前面不是以 / 开始的 , 表示相对当前目录所在的目录位置 例如： &#x2F;usr&#x2F;share&#x2F;doc 绝对路径 在输入路径时, 最前面是以 / 开始的, 表示从根目录开始的具体目录位置 由 &#x2F;usr&#x2F;share&#x2F;doc 到 &#x2F;usr&#x2F;share&#x2F;man 时，可以写成： cd ..&#x2F;man 优点：定位准确, 不会因为 工作目录变化 而变化 mkdirmkdir命令用于建立名称为 dirName 之子目录 命令：mkdir [-p] dirName -p 确保目录名称存在，不存在的就建一个，用来创建多级目录。 mkdir -p aaa/bbb：在 aaa 目录下，创建一个 bbb 的子目录。 若 aaa 目录原本不存在，则建立一个 rmdirrmdir命令删除空的目录 命令：rmdir [-p] dirName -p 是当子目录被删除后使它也成为空目录的话，则顺便一并删除 rmdir -p aaa/bbb：在 aaa 目录中，删除名为 bbb 的子目录。若 bbb 删除后，aaa 目录成为空目录，则 aaa 同时也会被删除 cpcp 命令主要用于复制文件或目录 命令：cp [options] source… directory -a：此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合 -d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式 -f：覆盖已经存在的目标文件而不给出提示 -i：与 -f 选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答 y 时目标文件将被覆盖 -p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中 -r&#x2F;R：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件 -l：不复制文件，只是生成链接文件 cp –r aaa/* ccc：复制 aaa 下的所有文件到 ccc，不加参数 -r 或者 -R，只复制文件，而略过目录 rmrm命令用于删除一个文件或者目录。 命令：rm [options] name… -i 删除前逐一询问确认。 -f 即使原档案属性设为唯读，亦直接删除，无需逐一确认 -r 将目录及以下之档案亦逐一删除，递归删除 注：文件一旦通过 rm 命令删除，则无法恢复，所以必须格外小心地使用该命令 mvmv 命令用来为文件或目录改名、或将文件或目录移入其它位置 12mv [options] source destmv [options] source... directory -i：若指定目录已有同名文件，则先询问是否覆盖旧文件 -f：在 mv 操作要覆盖某已有的目标文件时不给任何指示 命令格式 运行结果 mv 文件名 文件名 将源文件名改为目标文件名 mv 文件名 目录名 将文件移动到目标目录 mv 目录名 目录名 目标目录已存在，将源目录移动到目标目录。目标目录不存在则改名 mv 目录名 文件名 出错 文件属性基本属性Linux 系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等。 当为 d 则是目录 当为 - 则是文件 若是 l 则表示为链接文档 link file 若是 b 则表示为装置文件里面的可供储存的接口设备(可随机存取装置) 若是 c 则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置) 接下来的字符，以三个为一组，均为[rwx] 的三个参数组合。其中，[ r ]代表可读(read)、[ w ]代表可写(write)、[ x ]代表可执行(execute)。 要注意的是，这三个权限的位置不会改变，如果没有权限，就会出现[ - ]。 从左至右用 0-9 这些数字来表示： 第 0 位确定文件类型 第 1-3 位确定属主拥有该文件的权限 第 4-6 位确定属组拥有该文件的权限 第 7-9 位确定其他用户拥有该文件的权限 文件信息对于一个文件，都有一个特定的所有者，也就是对该文件具有所有权的用户（属主）；还有这个文件是属于哪个组的（属组） 文件的【属主】有一套【读写执行权限rwx】 文件的【属组】有一套【读写执行权限rwx】 ls -l 可以查看文件夹下文件的详细信息, 从左到右 依次是: 权限（A 区域）： 第一个字符如果是 d 表示目录 硬链接数（B 区域）：通俗的讲就是有多少种方式, 可以访问当前目录和文件 属主（C 区域）：文件是所有者、或是叫做属主 属组（D 区域）： 文件属于哪个组 大小（E 区域）：文件大小 时间（F 区域）：最后一次访问时间 名称（G 区域）：文件的名称 更改权限权限概述Linux 文件属性有两种设置方法，一种是数字，一种是符号 Linux 的文件调用权限分为三级 : 文件属主、属组、其他，利用 chmod 可以控制文件如何被他人所调用。 12chmod [-cfvR] [--help] [--version] mode file...mode : 权限设定字串,格式: [ugoa...][[+-=][rwxX]...][,...] u 表示档案的拥有者，g 表示与该档案拥有者属于同一个 group 者，o 表示其他的人，a 表示这三者皆是 +表示增加权限、- 表示取消权限、&#x3D; 表示唯一设定权限 r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有该档案是个子目录或者该档案已经被设定过为可执行 数字权限命令：chmod [-R] xyz 文件或目录 xyz : 就是刚刚提到的数字类型的权限属性，为 rwx 属性数值的相加 -R : 进行递归（recursive）的持续变更，亦即连同次目录下的所有文件都会变更 文件的权限字符为：[-rwxrwxrwx]， 这九个权限是三三一组的，我们使用数字来代表各个权限 各权限的数字对照表：[r]:4、[w]:2、[x]:1、[-]:0 每种身份（owner&#x2F;group&#x2F;others）的三个权限（r&#x2F;w&#x2F;x）分数是需要累加的，例如权限为：[-rwxrwx—] 分数是 owner &#x3D; rwx &#x3D; 4+2+1 &#x3D; 7 group &#x3D; rwx &#x3D; 4+2+1 &#x3D; 7 others&#x3D; — &#x3D; 0+0+0 &#x3D; 0 表示为：chmod -R 770 文件名 符号权限 user 属主权限 group 属组权限 others 其他权限 all 全部的身份 我们就可以使用 u g o a 来代表身份的权限，读写的权限可以写成 r w x chmod u=rwx,g=rx,o=r a.txt：将as.txt的权限设置为 -rwxr-xr– chmod a-r a.txt：将文件的所有权限去除 r 更改属组chgrp 命令用于变更文件或目录的所属群组 文件或目录权限的的拥有者由所属群组来管理，可以使用 chgrp 指令去变更文件与目录的所属群组 12chgrp [-cfhRv][--help][--version][所属群组][文件或目录...]chgrp [-cfhRv][--help][--reference=&lt;参考文件或目录&gt;][--version][文件或目录...] chgrp -v root aaa：将文件 aaa 的属组更改成 root（其他也可以） 更改属主利用 chown 可以将档案的拥有者加以改变。 使用权限 : 管理员账户 12chown [–R] 属主名 文件名chown [-R] 属主名:属组名 文件名 chown root aaa：将文件aaa的属主更改成root chown seazean:seazean aaa：将文件aaa的属主和属组更改为seazean 文件操作touchtouch 命令用于创建文件、修改文件或者目录的时间属性，包括存取时间和更改时间。若文件不存在，系统会建立一个新的文件 1touch [-acfm][-d&lt;日期时间&gt;][-r&lt;参考文件或目录&gt;] [-t&lt;日期时间&gt;][--help][--version][文件或目录…] -a 改变档案的读取时间记录 -m 改变档案的修改时间记录 -c 假如目的档案不存在，不会建立新的档案。与 –no-create 的效果一样 -f 不使用，是为了与其他 unix 系统的相容性而保留 -r 使用参考档的时间记录，与 –file 的效果一样 -d 设定时间与日期，可以使用各种不同的格式 -t 设定档案的时间记录，格式与 date 指令相同 –no-create 不会建立新档案 –help 列出指令格式 –version 列出版本讯息 touch t.txt：创建 t.txt 文件 touch t&#123;1..10&#125;.txt：创建10 个名为 t1.txt 到 t10.txt 的空文件 touch t.txt：更改 t.txt 的访问时间为现在 statstat 命令用于显示 inode 内容 命令：stat [文件或目录] catcat 是一个文本文件查看和连接工具，用于小文件 命令：cat [-AbeEnstTuv] [–help] [–version] Filename -n 显示文件加上行号 -b 和 -n 相似，只不过对于空白行不编号 lessless 用于查看文件，但是 less 在查看之前不会加载整个文件，用于大文件 命令：less [options] Filename -N 显示每行行号 tailtail 命令可用于查看文件的内容，有一个常用的参数 -f 常用于查阅正在改变的日志文件 命令：tail [options] Filename -f 循环读取,动态显示文档的最后内容 -n 显示文件的尾部 n 行内容 -c 显示字节数 -nf 查看最后几行日志信息 tail -f filename：动态显示最尾部的内容 tail -n +2 txtfile.txt：显示文件 txtfile.txt 的内容，从第 2 行至文件末尾 tail -n 2 txtfile.txt：显示文件 txtfile.txt 的内容，最后 2 行 headhead 命令可用于查看文件的开头部分的内容，有一个常用的参数 -n 用于显示行数，默认为 10 -q 隐藏文件名 -v 显示文件名 -c 显示的字节数 -n 显示的行数 head -n Filename：查看文件的前一部分 head -n 20 Filename：查看文件的前 20 行 grepgrep 指令用于查找内容包含指定的范本样式的文件，若不指定任何文件名称，或是所给予的文件名为 -，则 grep 指令会从标准输入设备读取数据 1grep [-abcEFGhHilLnqrsvVwxy][-A&lt;显示列数&gt;][-B&lt;显示列数&gt;][-C&lt;显示列数&gt;][-d&lt;进行动作&gt;][-e&lt;范本样式&gt;][-f&lt;范本文件&gt;][--help][范本样式][文件或目录...] -c 只输出匹配行的计数 -i 不区分大小写 -h 查询多文件时不显示文件名 -l 查询多文件时只输出包含匹配字符的文件名 -n 显示匹配行及行号 -s 不显示不存在或无匹配文本的错误信息 -v 显示不包含匹配文本的所有行 –color&#x3D;auto 可以将找到的关键词部分加上颜色的显示 **管道符 |**：表示将前一个命令处理的结果传递给后面的命令处理 grep aaaa Filename ：显示存在关键字 aaaa 的行 grep -n aaaa Filename：显示存在关键字 aaaa 的行，且显示行号 grep -i aaaa Filename：忽略大小写，显示存在关键字 aaaa 的行 grep -v aaaa Filename：显示存在关键字 aaaa 的所有行 ps -ef | grep sshd：查找包含 sshd 进程的进程信息 ps -ef | grep -c sshd：查找 sshd 相关的进程个数 echo将字符串输出到控制台 , 通常和重定向联合使用 命令：echo string，如果字符串有空格, 为了避免歧义 请增加 双引号 或者 单引号 通过 命令 &gt; 文件 将命令的成功结果覆盖指定文件内容 通过 命令 &gt;&gt; 文件 将命令的成功结果追加指定文件的后面 通过 命令 &amp;&gt;&gt; 文件 将 命令的失败结果追加指定文件的后面 echo &quot;程序员&quot; &gt;&gt; a.txt：将程序员追加到 a.txt 后面 cat 不存在的目录 &amp;&gt;&gt; error.log：将错误信息追加到 error.log 文件 awkAWK 是一种处理文本文件的语言，是一个强大的文本分析工具 12awk [options] &#x27;script&#x27; var=value file(s)awk [options] -f scriptfile var=value file(s) -F fs：指定输入文件折分隔符，fs 是一个字符串或者是一个正则表达式 -v：var&#x3D;value 赋值一个用户定义变量 -f：从脚本文件中读取 awk 命令 $n：获取第几段内容 $0：获取当前行 内容 NF：表示当前行共有多少个字段 $NF：代表最后一个字段 $(NF-1)：代表倒数第二个字段 NR：代表处理的是第几行 123命令：awk &#x27;BEGIN&#123;初始化操作&#125;&#123;每行都执行&#125; END&#123;结束时操作&#125;&#x27; 文件名BEGIN&#123; 这里面放的是执行前的语句 &#125;&#123;这里面放的是处理每一行时要执行的语句&#125;END &#123;这里面放的是处理完所有的行后要执行的语句 &#125; 1234567//准备数据zhangsan 68 99 26lisi 98 66 96wangwu 38 33 86zhaoliu 78 44 36maq 88 22 66zhouba 98 44 46 cat a.txt | awk &#39;/zhang|li/&#39;：搜索含有 zhang 和 li 的学生成绩 awk &quot;/zhang|li/&quot; a.txt ：同上一个命令，效果一样 123zhangsan 68 99 26lisi 98 66 96zhaoliu 78 44 36 cat a.txt | awk -F &#39; &#39; &#39;&#123;print $1,$2,$3&#125;&#39;：按照空格分割，打印 一二三列内容 awk -F &#39; &#39; &#39;&#123;OFS=&quot;\\t&quot;&#125;&#123;print $1,$2,$3&#125;&#39;：按照制表符 tab 进行分割，打印一二三列\\b：退格 \\f：换页 ：换行 \\r：回车 \\t：制表符 123456zhangsan\t68\t99lisi\t98\t66wangwu\t38\t33zhaoliu\t78\t44maq\t88\t22zhouba\t98\t44 awk -F &#39;,&#39; &#39;&#123;print toupper($1)&#125;&#39; a.txt：根据逗号分割，打印内容，第一段大写 函数名 含义 作用 toupper() upper 字符 转成 大写 tolower() lower 字符 转成小写 length() length 返回 字符长度 awk -F &#39; &#39; &#39;BEGIN&#123;&#125;&#123;total=total+$4&#125; END&#123;print total&#125;&#39; a.txt：计算的是第4列的总分 awk -F &#39; &#39; &#39;BEGIN&#123;&#125;&#123;total=total+$4&#125; END&#123;print total, NR&#125;&#39; a.txt ：查看总分, 总人数 awk -F &#39; &#39; &#39;BEGIN&#123;&#125;&#123;total=total+$4&#125; END&#123;print total, NR, (total/NR)&#125;&#39; a.txt：查看总分, 总人数，平均数 cat a.txt | awk -F &#39; &#39; &#39;BEGIN&#123;&#125;&#123;total=total+$4&#125; END&#123;print total&#125;&#39; ：可以这样写 findfind 命令用来在指定目录下查找文件，如果使用该命令不设置任何参数，将在当前目录下查找子目录与文件，并且将查找到的子目录和文件全部进行显示 命令：find &lt;指定目录&gt; &lt;指定条件&gt; &lt;指定内容&gt; find . -name &quot;*.gz&quot;：将目前目录及其子目录下所有延伸档名是 gz 的文件查询出来 find . -ctime -1：将目前目录及其子目录下所有最近 1 天内更新过的文件查询出来 find / -name &#39;seazean&#39;：全局搜索 seazean readread 命令用于从标准输入读取数值 1read [-ers] [-a aname] [-d delim] [-i text] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name ...] sortLinux sort 命令用于将文本文件内容加以排序 1sort [-bcdfimMnr][文件] -n 依照数值的大小排序 -r 以相反的顺序来排序（sort 默认的排序方式是升序，改成降序，加 -r） -u 去掉重复 面试题：一列数字，输出最大的 4 个不重复的数 12sort -ur a.txt | head -n 4sort -r a.txt | uniq | head -n 4 uniquniq 用于重复数据处理，使用前先 sort 排序 1uniq [OPTION]... [INPUT [OUTPUT]] -c 在数据行前出现的次数 -d 只打印重复的行，重复的行只显示一次 -D 只打印重复的行，重复的行出现多少次就显示多少次 -f 忽略行首的几个字段 -i 忽略大小写 -s 忽略行首的几个字母 -u 只打印唯一的行 -w 比较不超过 n 个字母 文件压缩tartar 的主要功能是打包、压缩和解压文件，tar 本身不具有压缩功能，是调用压缩功能实现的。 命令：tar [必要参数] [选择参数] [文件] -c 产生 .tar 文件 -v 显示详细信息 -z 打包同时压缩 -f 指定压缩后的文件名 -x 解压 .tar 文件 -t 列出 tar 文件中包含的文件的信息 -r 附加新的文件到tar文件中 tar -cvf txt.tar txtfile.txt ：将 txtfile.txt 文件打包（仅打包，不压缩） tar -zcvf combine.tar.gz 1.txt 2.txt 3.txt：将 123.txt 文件打包压缩（gzip） tar -ztvf txt.tar.gz：查看 tar 中有哪些文件 tar -zxvf Filename -C 目标路径：解压 gzipgzip命令用于压缩文件。 gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多出”.gz”的扩展名 gzip * ：压缩目录下的所有文件，删除源文件。不支持直接压缩目录 gzip -rv 目录名：递归压缩目录 gzip -dv *：解压文件并列出详细信息 gunzipgunzip命令用于解压文件。用于解开被gzip压缩过的文件 命令：gunzip [options] [文件或者目录] gunzip 001.gz ：解压001.gz文件 zipzip 命令用于压缩文件。 zip 是个使用广泛的压缩程序，文件经它压缩后会另外产生具有 .zip 扩展名的压缩文件 命令：zip [必要参数] [选择参数] [文件] -q 不显示指令执行过程 -r 递归处理，将指定目录下的所有文件和子目录一并处理 zip -q -r z.zip *：将该目录的文件全部压缩 unzipunzip 命令用于解压缩 zip 文件，unzip 为 .zip 压缩文件的解压缩程序 命令：unzip [必要参数] [选择参数] [文件] -l 查看压缩文件内所包含的文件 -d&lt;目录&gt; 指定文件解压缩后所要存储的目录。 unzip -l z.zip ：查看压缩文件中包含的文件 unzip -d ./unFiles z.zip：把文件解压到指定的目录下 bzip2bzip2 命令是 .bz2 文件的压缩程序。 bzip2 采用新的压缩演算法，压缩效果比传统的 LZ77&#x2F;LZ78 压缩演算法好，若不加任何参数，bzip2 压缩完文件后会产生 .bz2 的压缩文件，并删除原始的文件 1bzip2 [-cdfhkLstvVz][--repetitive-best][--repetitive-fast][- 压缩等级][要压缩的文件] 压缩：bzip2 a.txt bunzip2bunzip2 命令是 .bz2 文件的解压缩程序。 命令：bunzip2 [-fkLsvV] [.bz2压缩文件] -v　解压缩文件时，显示详细的信息。 解压：bunzip2 -v a.bz2 文件编辑Vimvim：是从 vi 发展出来的一个文本编辑器 命令模式：在 Linux 终端中输入vim 文件名 就进入了命令模式，但不能输入文字 编辑模式：在命令模式下按 i 就会进入编辑模式，此时可以写入程式，按 Esc 可回到命令模式 末行模式：在命令模式下按 : 进入末行模式，左下角会有一个冒号，可以敲入命令并执行 打开文件Ubuntu 默认没有安装 vim，需要先安装 vim，安装命令：sudo apt-get install vim Vim 有三种模式：命令模式（Command mode）、插入模式（Insert mode）、末行模式（Last Line mode） Vim 使用的选项 说明 常用 vim filename 打开或新建一个文件，将光标置于第一行首部 常用 vim -r filename 恢复上次vim打开时崩溃的文件 vim -R filename 把指定的文件以只读的方式放入Vim编辑器 vim + filename 打开文件，将光标置于最后一行的首部 常用 vim +n filename 打开文件，将光标置于n行的首部 常用 vim +&#x2F;pattern filename 打开文件，将光标置于第一个与pattern匹配的位置 vim -c command filename 对文件编辑前，先执行指定的命令 插入模式在命令模式下，通过按下 i、I、a、A、o、O 这 6 个字母进入插入模式 快捷键 功能描述 i 在光标所在位置插入文本，光标后的文本向右移动 I 在光标所在行的行首插入文本，行首是该行的第一个非空白字符 o 在光标所在行的下面插入新的一行，光标停在空行首 O 在光标所在行的上面插入新的一行，光标停在空行首 a 在光标所在位置之后插入文本 A 在光标所在行的行尾插入文本 按下 ESC 键，离开插入模式，进入命令模式 因为我们是一个空文件，所以使用【I】或者【i】都可以 如果里面的文本很多，要使用【A】进入编辑模式，即在行末添加文本 命令模式Vim 打开一个文件（文件可以存在，也可以不存在），默认进入命令模式。在该模式下， 输入的字符会被当做指令，而不会被当做要输入的文字 移动光标 快捷键 功能描述 w 光标移动至下一个单词的单词首 b 光标移动至上一个单词的单词首 e 光标移动至下一个单词的单词尾 0 光标移动至当前行的行首 ^ 行首, 第一个不是空白字符的位置 $ 光标移动至当前行的行尾 gg 光标移动至文件开头 G 光标移动至文件末尾 ngg 光标移动至第n行 nG 光标移动至第n行 :n 光标移动至第n行 选中文本在 vi&#x2F;vim 中要选择文本，需要显示 visual 命令切换到可视模式 vi&#x2F;vim 中提供了三种可视模式，方便程序员的选择选中文本的方式 按 ESC 可以放弃选中, 返回到命令模式 命令 模式 功能 v 可视模式 从光标位置开始按照正常模式选择文本 V 可视化模式 选中光标经过的完整行 Ctrl + v 可是块模式 垂直方向选中文本 撤销删除在学习编辑命令之前,先要知道怎样撤销之前一次错误的编辑操作 命令 英文 功能 u undo 撤销上次的命令(ctrl + z) Ctrl + r uredo 恢复撤销的命令 删除的内容此时并没有真正的被删除，在剪切板中，按下 p 键，可以将删除的内容粘贴回来 快捷键 功能描述 x 删除光标所在位置的字符 d 删除移动命令对应的内容 dd 删除光标所在行的内容 D 删除光标位置到行尾的内容 :n1,n2 删除从 a1 到 a2 行的文本内容 删除命令可以和移动命令连用, 以下是常见的组合命令(扩展): 命令 作用 dw 删除从光标位置到单词末尾 d} 删除从光标位置到段落末尾 dG 删除光标所行到文件末尾的所有内容 ndd 删除当前行（包括此行）到后 n 行内容 复制粘贴vim 中提供有一个 被复制文本的缓冲区 复制命令会将选中的文字保存在缓冲区 删除命令删除的文字会被保存在缓冲区 在需要的位置，使用粘贴命令可以将缓冲对的文字插入到光标所在的位置 vim 中的文本缓冲区只有一个，如果后续做过复制、剪切操作，之前缓冲区中的内容会被替换 快捷键 功能描述 y 复制已选中的文本到剪切板 yy 将光标所在行复制到剪切板 nyy 复制从光标所在行到向下n行 p 将剪切板中的内容粘贴到光标后 P 将剪切板中的内容粘贴到光标前 注意：vim 中的文本缓冲区和系统的剪切板不是同一个，在其他软件中使用 Ctrl + C 复制的内容，不能在 vim 中通过 p 命令粘贴，可以在编辑模式下使用鼠标右键粘贴 查找替换查找 快捷键 功能描述 &#x2F;abc 从光标所在位置向后查找字符串 abc &#x2F;^abc 查找以 abc 为行首的行 &#x2F;abc$ 查找以 abc 为行尾的行 ?abc 从光标所在位置向前查找字符串 abc * 向后查找当前光标所在单词 # 向前查找当前光标所在单词 n 查找下一个，向同一方向重复上次的查找指令 N 查找上一个，向相反方向重复上次的查找指令 替换： 命令 功能 工作模式 r 替换当前字符 命令模式 R 替换当前行光标后的字符 替换模式 光标选中要替换的字符 R 命令可以进入替换模式，替换完成后，按下 ESC 可以回到命令模式 替换命令的作用就是不用进入编辑模式，对文件进行轻量级的修改 末行模式在命令模式下，按下 : 键进入末行模式 命令 功能描述 :wq 保存并退出 Vim 编辑器 :wq! 保存并强制退出 Vim 编辑器 :q 不保存且退出 Vim 编辑器 :q! 不保存且强制退出 Vim 编辑器 :w 保存但是不退出 Vim 编辑器 :w! 强制保存但是不退出 Vim 编辑器 :w filename 另存到 filename 文件 x! 保存文本，退出保存但是不退出 Vim 编辑器，更通用的命令 ZZ 直接退出保存但是不退出 Vim 编辑器 :n 光标移动至第 n 行行首 异常处理 如果 vim 异常退出, 在磁盘上可能会保存有 交换文件 下次再使用 vim 编辑文件时，会看到以下屏幕信息： ls -a 一下，会看到隐藏的 .swp 文件，删除了此文件即可 链接1ln [-sf] source_filename dist_filename -s：默认是实体链接，加 -s 为符号链接 -f：如果目标文件存在时，先删除目标文件 实体链接： 在目录下创建一个条目，记录着文件名与 inode 编号，这个 inode 就是源文件的 inode 删除任意一个条目，文件还是存在，只要引用数量不为 0 不能跨越文件系统、不能对目录进行链接 1234ln /etc/crontab .ll34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 crontab34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab 符号链接： 符号链接文件保存着源文件所在的绝对路径，在读取时会定位到源文件上，可以理解为 Windows 的快捷方式 当源文件被删除了，链接文件就打不开了 记录的是路径，所以可以为目录建立符号链接 1234474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab53745909 lrwxrwxrwx. 1 root root 12 Jun 23 22:31 /root/crontab2 -&gt; /etc/crontab 进程管理查看进程ps 指令：查看某个时间点的进程信息 top 指令：实时显示进程信息 pstree：查看进程树 1pstree -A\t#查看所有进程树 进程 ID进程号： 进程号为 0 的进程通常是调度进程，常常被称为交换进程（swapper），该进程是内核的一部分，它并不执行任何磁盘上的程序，因此也被称为系统进程 进程号为 1 是 init 进程，是一个守护进程，在自举过程结束时由内核调用，init 进程绝不会终止，是一个普通的用户进程，但是它以超级用户特权运行 父进程 ID 为 0 的进程通常是内核进程，作为系统自举过程的一部分而启动，init 进程是个例外，它的父进程是 0，但它是用户进程 主存 &#x3D; RAM + BIOS 部分的 ROM DISK：存放 OS 和 Bootloader BIOS：基于 I&#x2F;O 处理系统 Bootloader：加载 OS，将 OS 放入内存 自举程序存储在内存中 ROM，用来加载操作系统，初始化 CPU、寄存器、内存等。CPU 的程序计数器指自举程序第一条指令，当计算机通电，CPU 开始读取并执行自举程序，将操作系统（不是全部，只是启动计算机的那部分程序）装入 RAM 中，这个过程是自举过程。装入完成后程序计数器设置为 RAM 中操作系统的第一条指令，接下来 CPU 将开始执行（启动）操作系统的指令 存储在 ROM 中保留很小的自举装入程序，完整功能的自举程序保存在磁盘的启动块上，启动块位于磁盘的固定位，拥有启动分区的磁盘称为启动磁盘或系统磁盘（C 盘） 进程状态 状态 说明 R running or runnable (on run queue) 正在执行或者可执行，此时进程位于执行队列中 D uninterruptible sleep (usually I&#x2F;O) 不可中断阻塞，通常为 IO 阻塞 S interruptible sleep (waiting for an event to complete) 可中断阻塞，此时进程正在等待某个事件完成 Z zombie (terminated but not reaped by its parent) 僵死，进程已经终止但是尚未被其父进程获取信息 T stopped (either by a job control signal or because it is being traced) 结束，进程既可以被作业控制信号结束，也可能是正在被追踪 孤儿进程： 一个父进程退出，而它的一个或多个子进程还在运行，那么这些子进程将成为孤儿进程 孤儿进程将被 init 进程所收养，并由 init 进程对它们完成状态收集工作，所以孤儿进程不会对系统造成危害 僵尸进程： 一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过 wait() 或 waitpid() 获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用 wait() 或 waitpid()，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程 僵尸进程通过 ps 命令显示出来的状态为 Z（zombie） 系统所能使用的进程号是有限的，产生大量僵尸进程，会导致系统没有可用的进程号而不能产生新的进程 要消灭系统中大量的僵尸进程，只需要将其父进程杀死，此时僵尸进程就会变成孤儿进程，从而被 init 进程所收养，这样 init 进程就会释放所有的僵尸进程所占有的资源，从而结束僵尸进程 补充： 守护进程(daemon)是一类在后台运行的特殊进程，用于执行特定的系统任务。 守护进程是脱离于终端并且在后台运行的进程，脱离终端是为了避免在执行的过程中的信息在终端上显示，并且进程也不会被任何终端所产生的终端信息所打断 很多守护进程在系统引导的时候启动，并且一直运行直到系统关闭；另一些只在需要的时候才启动，完成任务后就自动结束 状态改变SIGCHLD当一个子进程改变了它的状态时（停止运行，继续运行或者退出），有两件事会发生在父进程中： 得到 SIGCHLD 信号 waitpid() 或者 wait() 调用会返回 子进程发送的 SIGCHLD 信号包含了子进程的信息，比如进程 ID、进程状态、进程使用 CPU 的时间等；在子进程退出时进程描述符不会立即释放，父进程通过 wait() 和 waitpid() 来获得一个已经退出的子进程的信息，释放子进程的 PCB wait1pid_t wait(int *status) 参数：status 用来保存被收集的子进程退出时的状态，如果不关心子进程如何销毁，可以设置这个参数为 NULL 父进程调用 wait() 会阻塞等待，直到收到一个子进程退出的 SIGCHLD 信号，wait() 函数就会销毁子进程并返回 成功，返回被收集的子进程的进程 ID 失败，返回 -1，同时 errno 被置为 ECHILD（如果调用进程没有子进程，调用就会失败） waitpid1pid_t waitpid(pid_t pid, int *status, int options) 作用和 wait() 完全相同，只是多了两个可控制的参数 pid 和 options pid：指示一个子进程的 ID，表示只关心这个子进程退出的 SIGCHLD 信号；如果 pid&#x3D;-1 时，那么和 wait() 作用相同，都是关注所有子进程退出的 SIGCHLD 信号 options：主要有 WNOHANG 和 WUNTRACED 两个，WNOHANG 可以使 waitpid() 调用变成非阻塞的，就是会立即返回，父进程可以继续执行其它任务 网络管理network 启动：service network start 停止：service network stop 重启：service network restart ifconfigifconfig 是 Linux 中用于显示或配置网络设备的命令，英文全称是 network interfaces configuring ifconfig 命令用于显示或设置网络设备。ifconfig 可设置网络设备的状态，或是显示目前的设置 1ifconfig [网络设备][down up -allmulti -arp -promisc][add&lt;地址&gt;][del&lt;地址&gt;][&lt;hw&lt;网络设备类型&gt;&lt;硬件地址&gt;][io_addr&lt;I/O地址&gt;][irq&lt;IRQ地址&gt;][media&lt;网络媒介类型&gt;][mem_start&lt;内存地址&gt;][metric&lt;数目&gt;][mtu&lt;字节&gt;][netmask&lt;子网掩码&gt;][tunnel&lt;地址&gt;][-broadcast&lt;地址&gt;][-pointopoint&lt;地址&gt;][IP地址] ifconfig：显示激活的网卡信息 ens ens33（或 eth0）表示第一块网卡，IP地址是 192.168.0.137，广播地址 broadcast 192.168.0.255，掩码地址netmask 255.255.255.0 ，inet6 对应的是 ipv6 lo 是表示主机的回坏地址，用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口 ifconfig ens33 down：关闭网卡 ifconfig ens33 up：启用网卡 pingping 命令用于检测主机 执行 ping 指令会使用 ICMP 传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息 1ping [-dfnqrRv][-c&lt;完成次数&gt;][-i&lt;间隔秒数&gt;][-I&lt;网络界面&gt;][-l&lt;前置载入&gt;][-p&lt;范本样式&gt;][-s&lt;数据包大小&gt;][-t&lt;存活数值&gt;][主机名称或IP地址] -c&lt;完成次数&gt;：设置完成要求回应的次数； ping -c 2 www.baidu.com icmp_seq：ping 序列，从1开始 ttl：IP 生存时间值 time：响应时间,数值越小，联通速度越快 netstatnetstat 命令用于显示网络状态 1netstat [-acCeFghilMnNoprstuvVwx][-A&lt;网络类型&gt;][--ip] -a 显示所有连线中的 Socket，显示详细的连接状况 -i 显示网络界面信息表单，显示网卡列表 -p 显示正在使用 Socket 的程序识别码和程序名称 -n 显示使用 IP 地址，而不通过域名服务器 -t 显示 TCP 传输协议的连线状况。 -u 显示 UDP 传输协议的连线状况 -aptn：查看所有 TCP 开启端口 -apun：查看所有 UDP 开启端口 补充： netstat -apn | grep port：查看指定端口号 lsof -i:port ：查看指定端口号 磁盘管理挂载概念在安装 Linux 系统时设立的各个分区，如根分区、&#x2F;boot 分区等都是自动挂载的，也就是说不需要人为操作，开机就会自动挂载。但是光盘、U 盘等存储设备如果需要使用，就必须人为的进行挂载 在 Windows 下插入 U 盘也是需要挂载（分配盘符）的，只不过 Windows 下分配盘符是自动的。其实挂载可以理解为 Windows 当中的分配盘符，只不过 Windows 当中是以英文字母 ABCD 等作为盘符，而 Linux 是拿系统目录作为盘符，当然 Linux 当中也不叫盘符，而是称为挂载点，而把为分区或者光盘等存储设备分配一个挂载点的过程称为挂载 Linux 中的根目录以外的文件要想被访问，需要将其关联到根目录下的某个目录来实现，这种关联操作就是挂载，这个目录就是挂载点，解除次关联关系的过程称之为卸载 挂载点的目录需要以下几个要求： 目录要先存在，可以用 mkdir 命令新建目录 挂载点目录不可被其他进程使用到 挂载点下原有文件将被隐藏 lsblklsblk 命令的英文是 list block，即用于列出所有可用块设备的信息，而且还能显示他们之间的依赖关系，但是不会列出 RAM 盘的信息 命令：lsblk [参数] lsblk：以树状列出所有块设备 NAME：这是块设备名 MAJ：MIN : 本栏显示主要和次要设备号 RM：本栏显示设备是否可移动设备，在上面设备 sr0 的 RM 值等于 1，这说明他们是可移动设备 SIZE：本栏列出设备的容量大小信息 RO：该项表明设备是否为只读，在本案例中，所有设备的 RO 值为 0，表明他们不是只读的 TYPE：本栏显示块设备是否是磁盘或磁盘上的一个分区。在本例中，sda 和 sdb 是磁盘，而 sr0 是只读存储（rom）。 MOUNTPOINT：本栏指出设备挂载的挂载点。 lsblk -f：不会列出所有空设备 NAME表示设备名称 FSTYPE表示文件类型 LABEL表示设备标签 UUID设备编号 MOUNTPOINT表示设备的挂载点 df df 命令用于显示目前在 Linux 系统上的文件系统的磁盘使用情况统计。 命令：df [options]… [FILE]… -h 使用人类可读的格式(预设值是不加这个选项的…) –total 计算所有的数据之和 第一列指定文件系统的名称；第二列指定一个特定的文件系统，1K 是 1024 字节为单位的总容量；已用和可用列分别指定的容量；最后一个已用列指定使用的容量的百分比；最后一栏指定的文件系统的挂载点 mountmount 命令是经常会使用到的命令，它用于挂载 Linux 系统外的文件 使用者权限：所有用户，设置级别的需要管理员 1234mount [-hV]mount -a [-fFnrsvw] [-t vfstype]mount [-fnrsvw] [-o options [,...]] device | dirmount [-fnrsvw] [-t vfstype] [-o options] device dir -t：指定档案系统的型态，通常不必指定。mount 会自动选择正确的型态。 通过挂载的方式查看 Linux CD&#x2F;DVD 光驱，查看 ubuntu-20.04.1-desktop-amd64.iso 的文件 进入【虚拟机】–【设置】，设置 CD&#x2F;DVD 的内容，ubuntu-20.04.1-desktop-amd64.iso 创建挂载点（注意：一般用户无法挂载 cdrom，只有 root 用户才可以操作） mkdir -p /mnt/cdrom ：切换到 root 下创建一个挂载点（其实就是创建一个目录） 开始挂载mount -t auto /dev/cdrom /mnt/cdrom：通过挂载点的方式查看上面的【ISO文件内容】 查看挂载内容：ls -l -a ./mnt/cdrom/ 卸载 cdrom：umount /mnt/cdrom/ 防火墙概述防火墙技术是通过有机结合各类用于安全管理与筛选的软件和硬件设备，帮助计算机网络于其内、外网之间构建一道相对隔绝的保护屏障，以保护用户资料与信息安全性的一种技术。在默认情况下，Linux 系统的防火墙状态是打开的 状态启动语法：service name status 查看防火墙状态：service iptables status 临时开启：service iptables start 临时关闭：service iptables stop 开机启动：chkconfig iptables on 开机关闭：chkconfig iptables off 放行设置端口防火墙放行 修改配置文件：vim /etc/sysconfig/iptables 添加放行端口：-A INPUT -m state --state NEW -m tcp -p tcp --dport 端口号 -j ACCEPT 重新加载防火墙规则：service iptables reload 备注：默认情况下 22 端口号是放行的 Shell入门概念Shell 脚本（shell script），是一种为 shell 编写的脚本程序，又称 Shell 命令稿、程序化脚本，是一种计算机程序使用的文本文件，内容由一连串的 shell 命令组成，经由 Unix Shell 直译其内容后运作 Shell 被当成是一种脚本语言来设计，其运作方式与解释型语言相当，由 Unix shell 扮演命令行解释器的角色，在读取 shell 脚本之后，依序运行其中的 shell 命令，之后输出结果 环境Shell 编程跟 JavaScript、php 编程一样，只要有一个能编写代码的文本编辑器和一个能解释执行的脚本解释器就可以了。 cat /etc/shells：查看解释器 Linux 的 Shell 种类众多，常见的有： Bourne Shell（&#x2F;usr&#x2F;bin&#x2F;sh或&#x2F;bin&#x2F;sh） Bourne Again Shell（&#x2F;bin&#x2F;bash）：Bash 是大多数Linux 系统默认的 Shell C Shell（&#x2F;usr&#x2F;bin&#x2F;csh） K Shell（&#x2F;usr&#x2F;bin&#x2F;ksh） Shell for Root（&#x2F;sbin&#x2F;sh） 等等…… 第一个shell 新建 s.sh 文件：touch s.sh 编辑 s.sh 文件：vim s.sh 123456789101112#!/bin/bash --- 指定脚本解释器echo &quot;你好，shell !&quot; ---向窗口输入文本:&lt;&lt;!写shell的习惯 第一行指定解释器文件是sh为后缀名括号成对书写注释的时候尽量不用中文注释。不友好。[] 括号两端要要有空格。 [ neirong ]习惯代码索引，增加阅读性写语句的时候，尽量写全了，比如if。。。! 查看 s.sh文件：ls -l s.sh文件权限是【-rw-rw-r–】 chmod a+x s.sh s.sh文件权限是【-rwxrwxr-x】 执行文件：.&#x2F;s.sh 或者直接 bash s.sh 注意： #! 是一个约定的标记，告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell echo 命令用于向窗口输出文本 注释 单行注释：以 # 开头的行就是注释，会被解释器忽略 多行注释： 1234:&lt;&lt;EOF注释内容...注释内容...EOF 12345:&lt;&lt;! -----这里的符号要和结尾处的一样注释内容...注释内容...注释内容...! 变量定义变量变量名和等号之间不能有空格，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则： 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 使用变量 使用一个定义过的变量，只要在变量名前面加美元符号$即可 1234name=&quot;seazean&quot;echo $nameecho $&#123;name&#125;name=&quot;zhy&quot; 已定义的变量，可以被重新定义变量名 外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界。推荐加！！ 12比如：echo &quot;I am good at $&#123;shell-t&#125;Script&quot;通过上面的脚本我们发现，如果不给shell-t变量加花括号，写成echo &quot;I am good at $shell-tScript&quot;，解释器shell就会把$shell-tScript当成一个变量，由于我们前面没有定义shell-t变量，那么解释器执行执行的结果自然就为空了。 只读变量使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。(类似于final) 12345#!/bin/bashmyUrl=&quot;https://www.baidu.com&quot;readonly myUrlmyUrl=&quot;https://cn.bing.com/&quot; #报错 myUrl readonly 删除变量使用 unset 命令可以删除变量，变量被删除后不能再次使用。 语法：unset variable_name 1234#!/bin/shmyUrl=&quot;https://www.baidu.com&quot;unset myUrlecho $myUrl 定义myUrl变量，通过unset删除变量，然后通过echo进行输出，结果是为空，没有任何的结果输出。 字符变量 字符串是shell编程中最常用也是最有用的数据类型，字符串可以用单引号，也可以用双引号，也可以不用引号，在Java SE中我们定义一个字符串通过Stirng s&#x3D;“abc” 双引号的形式进行定义，而在shell中也是可以的。 引号 单引号 1str=&#x27;this is a string variable&#x27; 单引号字符串的限制： 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用。 双引号 123your_name=&#x27;frank&#x27;str=&quot;Hello,\\&quot;$your_name\\&quot;! &quot;echo -e $str #Hello, &quot;frank&quot;! 双引号的优点： 双引号里可以有变量 双引号里可以出现转义字符 拼接字符串123456your_name=&quot;frank&quot;# 使用双引号拼接greeting=&quot;hello, &quot;$your_name&quot; !&quot;greeting_1=&quot;hello, $&#123;your_name&#125; !&quot;echo $greeting $greeting_1#hello,frank! hello,frank 获取字符串长度命令：$&#123;#variable_name&#125; 12string=&quot;seazean&quot;echo $&#123;#string&#125; #7 提取字符串12string=&quot;abcdefghijklmn&quot;echo $&#123;string:1:4&#125; 输出为【bcde】，通过截取我们发现，它的下标和我们在java中的读取方式是一样的，下标也是从0开始。 数组bash支持一维数组（不支持多维数组），并且没有限定数组的大小。 定义数组在 Shell 中，用括号来表示数组，数组元素用”空格”符号分割开 12345678数组名=(值1 值2 ... 值n)array_name=(value0 value1 value2 value3) array_name=(value0value1value2value3) 通过下标定义数组中的其中一个元素： 123array_name[0]=value0array_name[1]=value1array_name[n]=valuen 可以不使用连续的下标，而且下标的范围没有限制 读取数组读取数组元素值的一般格式是： 1234$&#123;数组名[下标]&#125;value=$&#123;array_name[n]&#125;echo $&#123;value&#125; 使用 @ 符号可以获取数组中的所有元素，例如：echo $&#123;array_name[@]&#125; 获取长度获取数组长度的方法与获取字符串长度的方法相同，数组前加# 1234# 取得数组元素的个数length=$&#123;#array_name[@]&#125;# 或者length=$&#123;#array_name[*]&#125; 12345#! /bin/bashg=(a b c d e f)echo &quot;数组下标为2的数据为:&quot; $&#123;g[2]&#125; #cecho &quot;数组所有数据为:&quot; $&#123;#g[@]&#125; #6echo &quot;数组所有数据为:&quot; $&#123;#g[*]&#125; #6 运算符Shell 和其他编程一样，支持包括：算术、关系、布尔、字符串等运算符。原生 bash **不支持 **简单的数学运算，但是可以通过其他命令来实现，例如expr。expr 是一款表达式计算工具，使用它能完成表达式的求值操作。 规则 表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2 完整的表达式要被 &#96;&#96; 包含，注意不是单引号 条件表达式要放在方括号之间，并且要有空格，例如: [$a==$b] 是错误的，必须写成 [ $a == $b ] (())双括号里可以跟表达式，例如((i++))，((a+b)) 算术运算符 运算符 说明 举例 + 加法 expr $a + $b 结果为 30。 - 减法 expr $a - $b 结果为 -10。 * 乘法 expr $a \\* $b 结果为 200。 &#x2F; 除法 expr $b / $a 结果为 2。 % 取余 expr $b % $a 结果为 0。 &#x3D; 赋值 a&#x3D;$b 将把变量 b 的值赋给 a。 &#x3D;&#x3D; 相等。用于比较两个数字，相同则返回 true。 [ $a == $b ] 返回 false。 !&#x3D; 不相等。用于比较两个数字，不相同则返回 true。 [ $a != $b ] 返回 true。 123456789101112#! /bin/basha=4b=20echo &quot;加法运算&quot; `expr $a + $b` echo &quot;乘法运算，注意*号前面需要反斜杠&quot; ` expr $a \\* $b`echo &quot;加法运算&quot; `expr $b / $a`((a++))echo &quot;a = $a&quot;c=$((a + b)) d=$[a + b]echo &quot;c = $c&quot;echo &quot;d = $d&quot; 12345678//结果加法运算 24减法运算 -16乘法运算，注意*号前面需要反斜杠 80加法运算 5a = 5c = 25d = 25 字符运算符假定变量 a 为 “abc”，变量 b 为 “efg”，true&#x3D;0，false&#x3D;1。 运算符 说明 举例 &#x3D; 检测两个字符串是否相等，相等返回 true。 [ $a = $b ] 返回 false。 !&#x3D; 检测两个字符串是否相等，不相等返回 true。 [ $a != $b ] 返回 true。 -z 检测字符串长度是否为0，为0返回 true。 [ -z $a ] 返回 false。 -n 检测字符串长度是否为0，不为0返回 true。 [ -n &quot;$a&quot; ] 返回 true。 $ 检测字符串是否为空，不为空返回 true。 [ $a ] 返回 true。 123456789101112131415a=&quot;abc&quot;b=&quot;efg&quot;if [ $a = $b ]then echo &quot;$a = $b : a 等于 b&quot;else echo &quot;$a = $b: a 不等于 b&quot;fiif [ $a != $b ]then echo &quot;$a != $b : a 不等于 b&quot;else echo &quot;$a != $b: a 等于 b&quot;fi 关系运算符关系运算符只支持数字，不支持字符串，除非字符串的值是数字。 下表列出了常用的关系运算符，假定变量 a 为 10，变量 b 为 20： 运算符 说明 举例 -eq 检测两个数是否相等，相等返回 true。 [ $a -eq $b ] 返回 false。 -ne 检测两个数是否不相等，不相等返回 true。 [ $a -ne $b ] 返回 true。 -gt 检测左边的数是否大于右边的，如果是，则返回 true。 [ $a -gt $b ] 返回 false。 -lt 检测左边的数是否小于右边的，如果是，则返回 true。 [ $a -lt $b ] 返回 true。 -ge 检测左边的数是否大于等于右边的，如果是，则返回 true。 [ $a -ge $b ] 返回 false。 -le 检测左边的数是否小于等于右边的，如果是，则返回 true。 [ $a -le $b ] 返回 true。 123456789a=10b=20if [ $a -eq $b ]then echo &quot;$a -eq $b : a 等于 b&quot;else echo &quot;$a -eq $b: a 不等于 b&quot;fi 布尔运算符下表列出了常用的布尔运算符，假定变量 a 为 10，变量 b 为 20： 运算符 说明 举例 ! 非运算，表达式为 true 则返回 false，否则返回 true。 [ ! false ] 返回 true。 -o 或运算，有一个表达式为 true 则返回 true。 [ $a -lt 20 -o $b -gt 100 ]true -a 与运算，两个表达式都为 true 才返回 true。 [ $a -lt 20 -a $b -gt 100 ]false 逻辑运算符假定变量 a 为 10，变量 b 为 20: 运算符 说明 举例 &amp;&amp; 逻辑的 AND [[ $a -lt 100 &amp;&amp; $b -gt 100 ]] 返回 false || 逻辑的 OR [[ $a -lt 100 || $b -gt 100 ]] 返回 true 流程控制if12345678if conditionthen command1 command2 ... commandN fi#末尾的fi就是if倒过来拼写 123456789if conditionthen command1 command2 ... commandNelse commandfi 123456789if condition1then command1elif condition2 then command2else commandNfi 查找一个进程，如果进程存在就打印true 1234if [ $(ps -ef | grep -c &quot;ssh&quot;) -gt 1 ]]then echo &quot;true&quot;fi 判断两个变量是否相等 1234567891011121314a=10b=20if [ $a == $b ]then echo &quot;a 等于 b&quot;elif [ $a -gt $b ]then echo &quot;a 大于 b&quot;elif [ $a -lt $b ]then echo &quot;a 小于 b&quot;else echo &quot;没有符合的条件&quot;fi forfor循环格式为： 1234567for var in item1 item2 ... itemNdo command1 command2 ... commandNdone 顺序输出当前列表中的字母： 123456789for loop in A B C D E F G do echo &quot;顺序输出字母为: $loop&quot;done顺序输出字母为:A顺序输出字母为:B....顺序输出字母为:G whilewhile循环用于不断执行一系列命令，也用于从输入文件中读取数据 1234while conditiondo commanddone 需求：如果int小于等于10，那么条件返回真。int从0开始，每次循环处理时，int加1。 1234567891011#!/bin/basha=1while [ &quot;$&#123;a&#125;&quot; -le 10 ]do echo &quot;输出的值为：&quot; $a ((a++))done输出的值为：1输出的值为：2...输出的值为：10 case…esac与 switch … case 语句类似，是一种多分枝选择结构，每个 case 分支用右圆括号开始，用两个分号 ;; 表示 break，即执行结束，跳出整个 case … esac 语句，esac（就是 case 反过来）作为结束标记。 1234567891011121314151617case 值 in 模式1) command1 command2 command3 ;;模式2） command1 command2 command3 ;;*) command1 command2 command3 ;;esac #case反过来 case 后为取值，值可以为变量或常数。 值后为关键字 in，接下来是匹配的各种模式，每一模式最后必须以右括号结束，模式支持正则表达式。 12345678910111213v=&quot;czbk&quot;case &quot;$v&quot; in&quot;czbk&quot;) echo &quot;传智播客&quot; ;;&quot;baidu&quot;) echo &quot;baidu 搜索&quot;\t;;&quot;google&quot;) echo &quot;google 搜索&quot; ;;esac 函数输入函数语法如下： 123456[ function ] funname [()]&#123; action; [return int;]&#125; 1、可以使用function fun() 定义函数，也可以直接fun() 定义,不带任何参数。 2、函数参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值n(0-255 1234567891011121314151617181920212223242526272829#无参无返回值的方法method()&#123;\techo &quot;函数执行了!&quot;&#125;#方法的调用#method#有参无返回值的方法method2()&#123;\techo &quot;接收到的第一个参数$1&quot;\techo &quot;接收到的第二个参数$2&quot;&#125;#方法的调用#method2 1 2#有参有返回值方法的定义method3()&#123;\techo &quot;接收到的第一个参数$1&quot;\techo &quot;接收到的第二个参数$2&quot;\treturn $(($1 + $2))&#125;#方法的调用method3 10 20echo $? 读取read 变量名 — 表示把键盘录入的数据复制给这个变量 需求：在方法中键盘录入两个整数,返回这两个整数的和 1234567891011method()&#123;\techo &quot;请录入第一个数&quot;\tread number1\techo &quot;请录入第二个数&quot;\tread number2\techo &quot;两个数字分别为$&#123;number1&#125;,$&#123;number2&#125;&quot;\treturn $((number1+number2))&#125;methodecho $? Docker基本概述Docker 是一个开源的应用容器引擎，诞生于 2013 年初，基于 Go 语言实现， dotCloud 公司出品 Docker 让开发者打包开发应用以及依赖包到一个轻量级、可移植的容器中，可以发布到任何Linux机器上 容器是完全使用沙箱机制，相互隔离 容器性能开销极低。 Docker 架构： 镜像（Image）：Docker 镜像，就相当于一个 root 文件系统。比如官方镜像 ubuntu:16.04 就包含了完整的一套 Ubuntu16.04 最小系统的 root 文件系统 容器（Container）：镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的类和对象一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 仓库（Repository）：仓库可看成一个代码控制中心，用来保存镜像 安装步骤： 12345678910# step 1: 安装必要的一些系统工具sudo apt-get updatesudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common# step 2: 安装GPG证书curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# Step 3: 写入软件源信息sudo add-apt-repository &quot;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;# Step 4: 更新并安装Docker-CEsudo apt-get -y updatesudo apt-get -y install docker-ce 配置镜像加速器： 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123; &quot;registry-mirrors&quot;: [&quot;https://hicqe4pi.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 操作命令进程相关 启动docker服务： 1systemctl start docker 停止docker服务： 1systemctl stop docker 重启doker服务： 1systemctl restart docker 查看doker服务状态： 1systemctl status docker 设置开机启动docker服务： 1systemctl enable docker 镜像相关 查看镜像：查看本地所有的镜像 12docker imagesdocker images –q # 查看所用镜像的id 搜索镜像：从网络中查找需要的镜像 1docker search 镜像名称 拉取镜像：从Docker仓库下载镜像到本地，镜像名称格式为 名称:版本号，如果版本号不指定则是最新的版本。如果不知道镜像版本，可以去docker hub 搜索对应镜像查看 1docker pull 镜像名称 删除镜像：删除本地镜像 12docker rmi 镜像id # 删除指定本地镜像docker rmi `docker images -q` # 删除所有本地镜像 tab上面的键 容器相关 查看容器： 12docker ps # 查看正在运行的容器docker ps –a # 查看所有容器 创建并启动容器： 1docker run 参数 --name=... /bin/bash 参数说明： -i：保持容器运行，通常与 -t 同时使用，加入it这两个参数后，容器创建后自动进入容器中，退出容器后，容器自动关闭 -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用 -d：以守护（后台）模式运行容器。创建一个容器在后台运行，需要使用docker exec 进入容器。退出后，容器不会关闭 -it 创建的容器一般称为交互式容器，-id 创建的容器一般称为守护式容器 –name：为创建的容器命名 进入容器： 1docker exec 参数 # 退出容器，容器不会关闭 停止容器： 1docker stop 容器名称 启动容器： 1docker start 容器名称 删除容器：如果容器是运行状态则删除失败，需要停止容器才能删除 1docker rm 容器名称 查看容器信息： 1docker inspect 容器名称 数据卷 Docker 容器删除后，在容器中产生的数据也会随之销毁Docker 容器和外部机器可以直接交换文件吗？容器之间想要进行数据交互？ 数据卷：数据卷是宿主机中的一个目录或文件，当容器目录和数据卷目录绑定后，对方的修改会立即同步 一个数据卷可以被多个容器同时挂载 一个容器也可以被挂载多个数据卷 数据卷的作用： 容器数据持久化 外部机器和容器间接通信 容器之间数据交换 配置数据卷 创建启动容器时，使用-v参数设置数据卷 12docker run ... –v 宿主机目录(文件):容器内目录(文件) ... docker run -it --name=c1 -v /root(or~)/data:/root/data_container centos:7 注意事项： 目录必须是绝对路径 如果目录不存在，会自动创建 可以挂载多个数据卷 多容器进行数据交换： 多个容器挂载同一个数据卷 数据卷容器 创建启动c3数据卷容器，使用 –v 参数设置数据卷 1docker run –it --name=c3 –v /volume centos:7 /bin/bash 创建启动 c1 c2 容器，使用 –-volumes-from 参数设置数据卷 12docker run –it --name=c1 --volumes-from c3 centos:7 /bin/bashdocker run –it --name=c2 --volumes-from c3 centos:7 /bin/bash 应用部署MySQL在Docker容器中部署MySQL，通过外部mysql客户端操作MySQL Server 端口映射： 容器内的网络服务和外部机器不能直接通信，外部机器和宿主机可以直接通信，宿主机和容器可以直接通信 当容器中的网络服务需要被外部机器访问时，可以将容器中提供服务的端口映射到宿主机的端口上。外部机器访问宿主机的该端口，从而间接访问容器的服务。这种操作称为：端口映射 MySQL部署步骤：搜索mysql镜像，拉取mysql镜像，创建容器，操作容器中的mysql 搜索mysql镜像 1docker search mysql 拉取mysql镜像 1docker pull mysql:5.6 创建容器，设置端口映射、目录映射 123# 在/root目录下创建mysql目录用于存储mysql数据信息mkdir ~/mysqlcd ~/mysql 12345678docker run -id \\-p 3307:3306 \\--name=c_mysql \\-v $PWD/conf:/etc/mysql/conf.d \\-v $PWD/logs:/logs \\-v $PWD/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 \\mysql:5.6 参数说明： -p 3307:3306：将容器的 3306 端口映射到宿主机的 3307 端口 -v $PWD/conf:/etc/mysql/conf.d：将主机当前目录下的 conf&#x2F;my.cnf 挂载到容器的 &#x2F;etc&#x2F;mysql&#x2F;my.cnf，配置目录 -v $PWD/logs:/logs：将主机当前目录下的 logs目录挂载到容器的 &#x2F;logs，日志目录 -v $PWD/data:/var/lib/mysql ：将主机当前目录下的data目录挂载到容器的 &#x2F;var&#x2F;lib&#x2F;mysql 。数据目录 -e MYSQL_ROOT_PASSWORD=123456：初始化 root 用户的密码。 进入容器，操作mysql 1docker exec –it c_mysql /bin/bash 使用外部机器连接容器中的mysql Tomcat 搜索tomcat镜像 1docker search tomcat 拉取tomcat镜像 1docker pull tomcat 创建容器，设置端口映射、目录映射 123# 在/root目录下创建tomcat目录用于存储tomcat数据信息mkdir ~/tomcatcd ~/tomcat 1234docker run -id --name=c_tomcat \\-p 8080:8080 \\-v $PWD:/usr/local/tomcat/webapps \\tomcat 参数说明： -p 8080:8080：将容器的8080端口映射到主机的8080端口 -v $PWD:/usr/local/tomcat/webapps：将主机中当前目录挂载到容器的webapps 使用外部机器访问tomcat Nginx 搜索nginx镜像 1docker search nginx 拉取nginx镜像 1docker pull nginx 创建容器，设置端口映射、目录映射 1234567# 在/root目录下创建nginx目录用于存储nginx数据信息mkdir ~/nginxcd ~/nginxmkdir confcd conf# 在~/nginx/conf/下创建nginx.conf文件,粘贴下面内容vim nginx.conf 12345678910111213141516171819202122232425262728293031user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;&#125; 123456docker run -id --name=c_nginx \\-p 80:80 \\-v $PWD/conf/nginx.conf:/etc/nginx/nginx.conf \\-v $PWD/logs:/var/log/nginx \\-v $PWD/html:/usr/share/nginx/html ginx 参数说明： -p 80:80：将容器的 80端口映射到宿主机的 80 端口 -v $PWD/conf/nginx.conf:/etc/nginx/nginx.conf：将主机当前目录下的 &#x2F;conf&#x2F;nginx.conf 挂载到容器的 :&#x2F;etc&#x2F;nginx&#x2F;nginx.conf，配置目录 -v $PWD/logs:/var/log/nginx：将主机当前目录下的 logs 目录挂载到容器的&#x2F;var&#x2F;log&#x2F;nginx，日志目录 使用外部机器访问nginx Redis 搜索redis镜像 1docker search redis 拉取redis镜像 1docker pull redis:5.0 创建容器，设置端口映射 1docker run -id --name=c_redis -p 6379:6379 redis:5.0 使用外部机器连接redis 1./redis-cli.exe -h 192.168.149.135 -p 6379 镜像原理底层原理 Docker 镜像本质是什么？Docker 中一个centos镜像为什么只有200MB，而一个centos操作系统的iso文件要几个个G？Docker 中一个tomcat镜像为什么有500MB，而一个tomcat安装包只有70多MB？ 操作系统的组成部分：进程调度子系统、进程通信子系统、内存管理子系统、设备管理子系统、文件管理子系统、网络通信子系统、作业控制子系统 Linux文件系统由bootfs和rootfs两部分组成： bootfs：包含bootloader（引导加载程序）和 kernel（内核） rootfs： root文件系统，包含的就是典型 Linux 系统中的&#x2F;dev，&#x2F;proc，&#x2F;bin，&#x2F;etc等标准目录和文件 不同的linux发行版，bootfs基本一样，而rootfs不同，如ubuntu，centos Docker镜像原理： Docker镜像是一个分层文件系统，是由特殊的文件系统叠加而成，最底端是 bootfs，并复用宿主机的bootfs ，第二层是 root文件系统rootfs称为base image，然后再往上可以叠加其他的镜像文件 统一文件系统（Union File System）技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角，这样就隐藏了多层的存在，在用户的角度看来，只存在一个文件系统 一个镜像可以放在另一个镜像的上面。位于下面的镜像称为父镜像，最底部的镜像成为基础镜像。 当从一个镜像启动容器时，Docker会在最顶层加载一个读写文件系统作为容器 问题： Docker 中一个Ubuntu镜像为什么只有200MB，而一个Ubuntu操作系统的iso文件要几个个G？Ubuntu的iso镜像文件包含bootfs和rootfs，而docker的Ubuntu镜像复用操作系统的bootfs，只有rootfs和其他镜像层 Docker 中一个tomcat镜像为什么有500MB，而一个tomcat安装包只有70多MB？由于docker中镜像是分层的，tomcat虽然只有70多MB，但他需要依赖于父镜像和基础镜像，所有整个对外暴露的tomcat镜像大小500多MB 镜像制作 Dockerfile基本概述Dockerfile是一个文本文件，包含一条条的指令，每一条指令构建一层，基于基础镜像最终构建出新的镜像 对于开发人员：可以为开发团队提供一个完全一致的开发环境 对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了 对于运维人员：在部署时，可以实现应用的无缝移植 关键字 作用 备注 FROM 指定父镜像 指定dockerfile基于那个image构建 MAINTAINER 作者信息 用来标明这个dockerfile谁写的 LABEL 标签 用来标明dockerfile的标签 可以使用Label代替Maintainer 最终都是在docker image基本信息中可以查看 RUN 执行命令 执行一段命令 默认是&#x2F;bin&#x2F;sh 格式: RUN command 或者 RUN [“command” , “param1”,”param2”] CMD 容器启动命令 提供启动容器时候的默认命令 和ENTRYPOINT配合使用.格式 CMD command param1 param2 或者 CMD [“command” , “param1”,”param2”] ENTRYPOINT 入口 一般在制作一些执行就关闭的容器中会使用 COPY 复制文件 build的时候复制文件到image中 ADD 添加文件 build的时候添加文件到image中 不仅仅局限于当前build上下文 可以来源于远程服务 ENV 环境变量 指定build时候的环境变量 可以在启动的容器的时候 通过-e覆盖 格式ENV name&#x3D;value ARG 构建参数 构建参数 只在构建的时候使用的参数 如果有ENV 那么ENV的相同名字的值始终覆盖arg的参数 VOLUME 定义外部可以挂载的数据卷 指定build的image那些目录可以启动的时候挂载到文件系统中 启动容器的时候使用 -v 绑定 格式 VOLUME [“目录”] EXPOSE 暴露端口 定义容器运行的时候监听的端口 启动容器的使用-p来绑定暴露端口 格式: EXPOSE 8080 或者 EXPOSE 8080&#x2F;udp WORKDIR 工作目录 指定容器内部的工作目录 如果没有创建则自动创建 如果指定&#x2F; 使用的是绝对地址 如果不是&#x2F;开头那么是在上一条workdir的路径的相对路径 USER 指定执行用户 指定build或者启动的时候 用户 在RUN CMD ENTRYPONT执行的时候的用户 HEALTHCHECK 健康检查 指定监测当前容器的健康监测的命令 基本上没用 因为很多时候 应用本身有健康监测机制 ONBUILD 触发器 当存在ONBUILD关键字的镜像作为基础镜像的时候 当执行FROM完成之后 会执行 ONBUILD的命令 但是不影响当前镜像 用处也不怎么大 STOPSIGNAL 发送信号量到宿主机 该STOPSIGNAL指令设置将发送到容器的系统调用信号以退出。 SHELL 指定执行脚本的shell 指定RUN CMD ENTRYPOINT 执行命令的时候 使用的shell Centos自定义centos7镜像： 默认登录路径为 &#x2F;usr 可以使用vim 实现步骤： 定义父镜像：FROM centos:7 定义作者信息：MAINTAINER seazean &lt; &#x7a;&#104;&#x79;&#122;&#x68;&#121;&#x61;&#110;&#x67;&#x40;&#115;&#x69;&#110;&#97;&#x2e;&#x63;&#x6f;&#x6d;&gt; 执行安装vim命令： RUN yum install -y vim 定义默认的工作目录：WORKDIR &#x2F;usr 定义容器启动执行的命令：CMD &#x2F;bin&#x2F;bash 通过dockerfile构建镜像：docker bulid –f dockerfile文件路径 –t 镜像名称:版本 Boot定义dockerfile，发布springboot项目： 实现步骤： 定义父镜像：FROM java:8 定义作者信息：MAINTAINER seazean &lt; &#x7a;&#x68;&#121;&#x7a;&#x68;&#121;&#97;&#110;&#103;&#64;&#115;&#x69;&#x6e;&#x61;&#x2e;&#99;&#x6f;&#x6d;&gt; 将jar包添加到容器： ADD springboot.jar app.jar 定义容器启动执行的命令：CMD java–jar app.jar 通过dockerfile构建镜像：docker bulid –f dockerfile文件路径 –t 镜像名称:版本 服务编排基本介绍微服务架构的应用系统中一般包含若干个微服务，每个微服务一般都会部署多个实例，如果每个微服务都要手动启停，维护的工作量会很大。 从Dockerfile build image 或者去dockerhub拉取image； 创建多个container，管理这些container（启动停止删除） 服务编排：按照一定的业务规则批量管理容器 Docker Compose是一个编排多容器分布式部署的工具，提供命令集管理容器化应用的完整开发周期，包括服务构建，启动和停止。使用步骤： 利用 Dockerfile 定义运行环境镜像 使用 docker-compose.yml 定义组成应用的各服务 运行 docker-compose up 启动应用 功能实现使用docker compose编排nginx+springboot项目 安装Docker Compose 创建docker-compose目录 12mkdir ~/docker-composecd ~/docker-compose 编写 docker-compose.yml 文件 1234567891011121314version: &#x27;3&#x27;services: nginx: image: nginx ports: - 80:80 links: - app volumes: - ./nginx/conf.d:/etc/nginx/conf.d app: image: app expose: - &quot;8080&quot; 创建.&#x2F;nginx&#x2F;conf.d目录 1mkdir -p ./nginx/conf.d 在.&#x2F;nginx&#x2F;conf.d目录下编写***.conf文件 12345678server &#123; listen 80; access_log off; location / &#123; proxy_pass http://app:8080; &#125;&#125; 在~&#x2F;docker-compose 目录下使用docker-compose启动容器 1docker-compose up 测试访问 1http://192.168.0.137/hello 私有仓库Docker官方的Docker hub（https://hub.docker.com）是一个用于管理公共镜像的仓库，我们可以从上面拉取镜像 到本地，也可以把我们自己的镜像推送上去。但是当服务器无法访问互联网，或者不希望将自己的镜像放到公网当中，那么我们就需要搭建自己的私有仓库来存储和管理自己的镜像 私有仓库搭建 123456789101112# 1、拉取私有仓库镜像 docker pull registry# 2、启动私有仓库容器 docker run -id --name=registry -p 5000:5000 registry# 3、输入地址http://私有仓库服务器ip:5000/v2/_catalog，显示&#123;&quot;repositories&quot;:[]&#125; # 4、修改daemon.json vim /etc/docker/daemon.json # 在上述文件中添加一个key，保存退出。此步用于让 docker 信任私有仓库地址；注意将私有仓库服务器ip修改为自己私有仓库服务器真实ip &#123;&quot;insecure-registries&quot;:[&quot;192.168.0.137:5000&quot;]&#125; # 5、重启docker 服务 systemctl restart dockerdocker start registry 将镜像上传至私有仓库 12345# 1、标记镜像为私有仓库的镜像 docker tag centos:7 私有仓库服务器IP:5000/centos:7 # 2、上传标记的镜像 docker push 私有仓库服务器IP:5000/centos:7 从私有仓库拉取镜像 12#拉取镜像 docker pull 私有仓库服务器ip:5000/centos:7 虚拟机容器： 容器是将软件打包成标准化单元，以用于开发、交付和部署 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置 容器化软件在任何环境中都能够始终如一地运行。 容器赋予了软件独立性，使其免受外在环境差异的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突 容器和虚拟机对比： 相同：容器和虚拟机具有相似的资源隔离和分配优势 不同： 容器虚拟化的是操作系统，虚拟机虚拟化的是硬件。 传统虚拟机可以运行不同的操作系统，容器只能运行同一类型操作系统 特性 容器 虚拟机 启动 秒级 分钟 硬盘使用 一般为MB 一般为GB 性能 接近原生 弱于原生 系统支持量 单机支持上千个容器 一般几十个"},{"path":"/customize/js/ZYDark.js","content":"/** * 监听系统主题 * @type {MediaQueryList} */ var OSTheme = window.matchMedia('(prefers-color-scheme: dark)'); OSTheme.addListener(e => { if (window.localStorage.getItem('ZYI_Theme_Mode') === 'Moss') { ThemeChange('Moss'); } }) /** * 修改博客主题 * @param theme 亮为light,暗为dark,自动为auto * @constructor */ const ThemeChange = (theme) => { if(document.querySelector(\"#start > aside > footer > div > a:nth-child(2)\") != 'undefined' && document.querySelector(\"#start > aside > footer > div > a:nth-child(2)\") != null){ if (theme === 'light' || (theme === 'Moss' && !OSTheme.matches)) { document.querySelector(\"html\").id = \"ZYLight\"; document.querySelector(\"#start > aside > footer > div > a:nth-child(2)\").style.filter = 'grayscale(0%)'; document.querySelector(\"#start > aside > footer > div > a:nth-child(1)\").style.filter = 'grayscale(100%)'; } else { document.querySelector(\"html\").id = \"ZYDark\"; document.querySelector(\"#start > aside > footer > div > a:nth-child(1)\").style.filter = 'grayscale(0%)'; document.querySelector(\"#start > aside > footer > div > a:nth-child(2)\").style.filter = 'grayscale(100%)'; } //if (theme === 'Moss') { document.querySelector(\"#start > aside > footer > div > a:nth-child(7)\").style.filter = 'grayscale(0%)'; } //else { document.querySelector(\"#start > aside > footer > div > a:nth-child(7)\").style.filter = 'grayscale(100%)'; } window.localStorage.setItem('ZYI_Theme_Mode', theme); commentChange(theme); } } // /** // * 修改评论主题 // * @param theme // */ // const commentChange = (theme) => { // try { // let commentSrc = document.querySelector(\"#comments > section.body.cmt-body.giscus > iframe\").src; // if (theme === \"Moss\") { // theme = OSTheme.matches? 'dark' : 'light'; // } // commentSrc = commentSrc.replace(theme === 'dark' ? 'theme=light' : 'theme=dark', theme === 'dark' ? 'theme=dark' : 'theme=light'); // commentSrc = commentSrc.replace('theme=preferred_color_scheme', theme === 'dark' ? 'theme=dark' : 'theme=light'); // document.querySelector(\"#comments > section.body.cmt-body.giscus > iframe\").src = commentSrc; // } catch (e) { // } // } /** * 初始化博客主题 */ switch (window.localStorage.getItem('ZYI_Theme_Mode')) { case 'light': ThemeChange('light'); break; case 'dark': ThemeChange('dark'); break; default: ThemeChange('Moss'); } if(document.querySelector(\"#start > aside > footer > div > a:nth-child(2)\") != 'undefined' && document.querySelector(\"#start > aside > footer > div > a:nth-child(2)\") != null){ /** * 切换主题模式 */ document.querySelector(\"#start > aside > footer > div > a:nth-child(1)\").onclick = () => { ThemeChange('dark'); } document.querySelector(\"#start > aside > footer > div > a:nth-child(2)\").onclick = () => { ThemeChange('light'); } //document.querySelector(\"#start > aside > footer > div > a:nth-child(7)\").onclick = () => { // ThemeChange('Moss'); //} }"},{"title":"Web","path":"/wiki/javaNode/Web.html","content":"HTMLHTML入门概述HTML（超文本标记语言—HyperText Markup Language）是构成 Web 世界的基础，是一种用来告知浏览器如何组织页面的标记语言 超文本 Hypertext，是指连接单个或者多个网站间的网页的链接。通过链接，就能访问互联网中的内容 标记 Markup ，是用来注明文本，图片等内容，以便于在浏览器中显示，例如 &lt;head&gt;，&lt;body&gt; 等 网页的构成 HTML：通常用来定义网页内容的含义和基本结构 CSS：通常用来描述网页的表现与展示效果 JavaScript：通常用来执行网页的功能与行为 参考视频：https://www.bilibili.com/video/BV1Qf4y1T7Hx 组成标签HTML 页面由一系列的元素（elements） 组成，而元素是使用标签创建的 一对标签（tags）可以设置一段文字样式，添加一张图片或者添加超链接等等 在 HTML 中，&lt;h1&gt; 标签表示标题，我们可以使用开始标签和结束标签包围文本内容，这样其中的内容就以标题的形式显示 12&lt;h1&gt;开始学习JavaWeb&lt;/h1&gt;&lt;h2&gt;二级标题&lt;/h2&gt; 属性HTML 标签可以拥有属性 属性是属于标签的，修饰标签，让标签有更多的效果 属性一般定义在起始标签里面 属性一般以属性&#x3D;属性值的形式出现 属性值一般用 &#39;&#39; 或者 &quot;&quot; 括起来。 不加引号也是可以的(不建议使用)。比如：name&#x3D;’value’ 1&lt;h1 align=&quot;center&quot;&gt;开始学习JavaWeb&lt;/h1&gt; 在 HTML 标签中，align 属性表示水平对齐方式，我们可以赋值为 center 表示 居中 。 结构 文档结构介绍： 文档声明：用于声明当前 HTML 的版本，这里的&lt;!DOCTYPE html&gt;是 HTML5 的声明 html 根标签：除文档声明以外，其它内容全部要放在根标签 html 内部 文档头部配置：head 标签，是当前页面的配置信息，外部引入文件, 例如网页标签、字符集等 &lt;meta charset=&quot;utf-8&quot;&gt;：这个标签是页面的元数据信息，设置文档使用 utf-8 字符集编码 &lt;title&gt;：这个标签定义文档标题，位置出现在浏览器标签。在收藏页面时，它可用来描述页面 文档显示内容：body 标签，里边的内容会显示到浏览器页面上 HTML语法注释方式将一段 HTML 中的内容置为注释，你需要将其用特殊的记号 包括起来 123&lt;p&gt;我在注释外！&lt;/p&gt;&lt;!-- &lt;p&gt;我在注释内！&lt;/p&gt; --&gt; 基本元素空元素一些元素只有一个标签，叫做空元素。它是在开始标签中进行关闭的。 12第一行文档&lt;br/&gt; 第二行文档&lt;br/&gt; 嵌套元素把元素放到其它元素之中——这被称作嵌套。 1&lt;h2&gt;&lt;u&gt;二级标题&lt;/u&gt;&lt;/h2&gt; 块元素在HTML中有两种重要元素类别，块级元素和内联元素 块级元素： 独占一行。块级元素（block）在页面中以块的形式展现。相对于其前面的内容它会出现在新的一行，其后的内容也会被挤到下一行展现。比如&lt;p&gt; ，&lt;hr&gt;，&lt;li&gt; ，&lt;div&gt;等。 行内元素 行内显示。行内元素不会导致换行。通常出现在块级元素中并环绕文档内容的一小部分，而不是一整个段落或者一组内容。比如&lt;b&gt;，&lt;a&gt;，&lt;i&gt;，&lt;span&gt; 等。 注意：一个块级元素不会被嵌套进行内元素中，但可以嵌套在其它块级元素中。 常用的两个标签：（重要） &lt;div&gt; 是一个通用的内容容器，并没有任何特殊语义。它可以被用来对其它元素进行分组，一般用于样式化相关的需求。它是一个块级元素。 属性：id、style、class &lt;span&gt; 是短语内容的通用行内容器，并没有任何特殊语义。它可以被用来编组元素以达到某种样式。它是一个行内元素 基本属性标签属性，主要用于拓展标签。属性包含元素的额外信息，这些信息不会出现在实际的内容中。但是可以改变标签的一些行为或者提供数据，属性总是以name = value&quot;的格式展现。 属性名：同一个标签中，属性名不得重复。 大小写：属性和属性值对大小写不敏感。不过W3C标准中，推荐使用小写的属性&#x2F;属性值。 引号：双引号是最常用的，不过使用单引号也没有问题。 常用属性： 属性名 作用 class 定义元素类名，用来选择和访问特定的元素 id 定义元素唯一标识符，在整个文档中必须是唯一的 name 定义元素名称，可以用于提交服务器的表单字段 value 定义在元素内显示的默认值 style 定义CSS样式，这些样式会覆盖之前设置的样式 特殊字符在HTML中，字符 &lt;, &gt;,&quot;,&#39; 和 &amp; 是特殊字符 原义字符 等价字符引用 &lt; &amp;lt; &gt; &amp;gt; “ &amp;quot; ‘ &amp;apos; &amp; &amp;amp; 空格 &amp;nbsp; 文本标签使用文本内容标签设置文字基本样式 标签名 作用 p 表示文本的一个段落 h 表示文档标题，&lt;h1&gt;–&lt;h6&gt; ，呈现了六个不同的级别的标题，&lt;h1&gt; 级别最高，而 &lt;h6&gt; 级别最低 hr 表示段落级元素之间的主题转换，一般显示为水平线 li 表示列表里的条目。（常用在ul ol 中） ul 表示一个无序列表，可含多个元素，无编号显示。 ol 表示一个有序列表，通常渲染为有带编号的列表 em 表示文本着重，一般用斜体显示 strong 表示文本重要，一般用粗体显示 font 表示字体，可以设置样式（已过时） i 表示斜体 b 表示加粗文本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;文本标签演示&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;!--段落标签：&lt;p&gt;--&gt; &lt;p&gt;这些年&lt;/p&gt; &lt;p&gt;支付宝的诞生就是为了解决淘宝网的客户们的买卖问题&lt;/p&gt; &lt;!-- 标题标签：&lt;h1&gt; ~ &lt;h6&gt; --&gt; &lt;h1&gt;一级标题&lt;/h1&gt; &lt;h2&gt;二级标题&lt;/h2&gt; &lt;h3&gt;三级标题&lt;/h3&gt; &lt;h4&gt;四级标题&lt;/h4&gt; &lt;h5&gt;五级标题&lt;/h5&gt; &lt;h6&gt;六级标题&lt;/h6&gt; &lt;!--水平线标签：&lt;hr/&gt; 属性： size-大小 color-颜色\t--&gt; &lt;hr size=&quot;4&quot; color=&quot;red&quot;/&gt; &lt;!-- 无序列表：&lt;ul&gt; 属性：type-列表样式(disc实心圆、circle空心圆、square实心方块) 列表项：&lt;li&gt; --&gt; &lt;ul type=&quot;circle&quot;&gt; &lt;li&gt;javaEE&lt;/li&gt; &lt;li&gt;HTML&lt;/li&gt; &lt;/ul&gt; &lt;!-- 有序列表：&lt;ol&gt; 属性：type-列表样式(1数字、A或a字母、I或i罗马字符) start-起始位置 列表项：&lt;li&gt; --&gt; &lt;ol type=&quot;1&quot; start=&quot;10&quot;&gt; &lt;li&gt;传智播客&lt;/li&gt; &lt;li&gt;黑马程序员&lt;/li&gt; &lt;/ol&gt; &lt;!-- 斜体标签：&lt;i&gt; &lt;em&gt; --&gt; &lt;i&gt;我倾斜了&lt;/i&gt; &lt;em&gt;我倾斜了&lt;/em&gt; &lt;br/&gt; &lt;!-- 加粗标签：&lt;strong&gt; &lt;b&gt; --&gt; &lt;strong&gt;加粗文本&lt;/strong&gt; &lt;b&gt;加粗文本&lt;/b&gt; &lt;br/&gt; &lt;!-- 文字标签：&lt;font&gt; 属性： size-大小 color-颜色 --&gt; &lt;font size=&quot;5&quot; color=&quot;yellow&quot;&gt;这是一段文字&lt;/font&gt;&lt;/body&gt;&lt;/html&gt; 效果如下： 图片标签img标签中的img其实是英文image的缩写, img标签的作用, 就是告诉浏览器我们需要显示一张图片 1&lt;img src=&quot;../img/b.jpg&quot; width=&quot;400px&quot; height=&quot;200px&quot; alt=&quot;&quot; title=&quot;&quot;/&gt; 属性名 作用 src 图片路径 title 鼠标悬停（hover）时显示文本。 alt 图片描述，图形不显示时的替换文本。 height 图像的高度。 width 图像的宽度。 超链接超链接标签的作用: 就是用于控制页面与页面(服务器资源)之间跳转的 1234&lt;a href=&quot;指定需要跳转的目标路径&quot; target=&quot;打开的方式&quot;&gt;需要展现给用户的内容&lt;/a&gt;target属性取值: _blank：新起页面\t_self：当前页面（默认） 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;超链接标签演示&lt;/title&gt; &lt;style&gt; a&#123; /*去掉超链接的下划线*/ text-decoration: none; /*超链接的颜色*/ color: black; &#125; /*鼠标悬浮的样式控制*/ a:hover&#123; color: red; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 超链接标签：&lt;a&gt; 属性： href-跳转的地址 target-跳转的方式(_self当前页面、_blank新标签页) --&gt; &lt;a href=&quot;01案例二：样式演示.html&quot; target=&quot;_blank&quot;&gt;点我跳转到样式演示&lt;/a&gt; &lt;br/&gt; &lt;a href=&quot;http://www.itcast.cn&quot; target=&quot;_blank&quot;&gt;传智播客&lt;/a&gt; &lt;br/&gt; &lt;a href=&quot;http://www.itheima.com&quot; target=&quot;_self&quot;&gt;黑马程序员&lt;/a&gt; &lt;br/&gt; &lt;a href=&quot;http://www.itheima.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;../img/itheima.png&quot; width=&quot;150px&quot; height=&quot;50px&quot;/&gt;&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 效果图： 表单标签基本介绍form 表示表单，是用来收集用户输入信息并向 Web 服务器提交的一个容器 123&lt;form &gt; //表单元素&lt;/form&gt; 属性名 作用 action 处理此表单信息的Web服务器的URL地址 method 提交此表单信息到Web服务器的方式，可能的值有get和post，默认为get autocomplete 自动补全，指示表单元素是否能够拥有一个默认值，配合input标签使用 get与post区别： post：指的是 HTTP POST 方法；表单数据会包含在表单体内然后发送给服务器。 get：指的是 HTTP GET 方法；表单数据会附加在 action 属性的URI中，并以 ‘?’ 作为分隔符，然后这样得到的 URI 再发送给服务器。 地址栏可见 数据安全 数据大小 GET 可见 不安全 有限制（取决于浏览器） POST 不可见 相对安全 无限制 表单元素 标签名 作用 备注 label 表单元素的说明，配合表单元素使用 for属性值为相关表单元素id属性值 input 表单中输入控件，多种输入类型，用于接受来自用户数据 type属性值决定输入类型 button 页面中可点击的按钮，可以配合表单进行提交 type属性值决定按钮类型 select 表单的控件，下拉选项菜单 与option配合实用 optgroup option的分组标签 与option配合实用 option select的子标签，表示一个选项 textarea 表示多行纯文本编辑控件 fieldset 用来对表单中的控制元素进行分组(也包括 label 元素) legend 用于表示它的fieldset内容的标题。 fieldset 的子元素 按键控件button标签：表示按钮 type属性：表示按钮类型，submit值为提交按钮。 属性值 作用 备注 button 无行为按钮，用于结合JavaScript实现自定义动态效果 同 &lt;input type=&quot;submit&quot;/&gt; submit 提交按钮，用于提交表单数据到服务器。 同 &lt;input type=&quot;submit&quot;/&gt; reset 重置按钮，用于将表单中内容恢复为默认值。 同&lt;input type=&quot;reset&quot;&#x2F;&gt; 输入控件基本介绍 label标签：表单的说明。 for属性值：匹配input标签的id属性值 input标签：输入控件。 属性： type：表示输入类型，text值为普通文本框 id：表示标签唯一标识 name：表示标签名称，提交服务器的标识 value：表示标签的默认数据值 placeholder：默认的提示信息，仅适用于当type 属性为text, search, tel, url or email时; required：是否必须为该元素填充值，当type属性是hidden,image或者button类型时不可使用 readonly：是否只读,可以让用户不修改这个输入框的值,就使用value属性设置默认值 disabled：是否可用,如果某个输入框有disabled那么它的数据不能提交到服务器通常是使用在有的页面中，让一些按钮不能点击 autocomplete：自动补全，规定表单或输入字段是否应该自动完成。当自动完成开启，浏览器会基于用户之前的输入值自动填写值。可以设置指定的字段为off，关闭自动补全 12345678910&lt;body&gt; &lt;form action=&quot;#&quot; method=&quot;get&quot; autocomplete=&quot;off&quot;&gt; &lt;label for=&quot;username&quot;&gt;用户名：&lt;/label&gt; &lt;input type=&quot;text&quot; id=&quot;username&quot; name=&quot;username&quot; value=&quot;&quot; placeholder=&quot; 请在此处输入用户名&quot; required/&gt; &lt;button type=&quot;submit&quot;&gt;提交&lt;/button&gt; &lt;button type=&quot;reset&quot;&gt;重置&lt;/button&gt; &lt;button type=&quot;button&quot;&gt;按钮&lt;/button&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 效果图： 表单项标签 用户名： 提交 重置 按钮 n-v属性 属性名 作用 name &lt;input&gt;的名字，在提交整个表单数据时，可以用于区分属于不同&lt;input&gt;的值 value 这个&lt;input&gt;元素当前的值，允许用户通过页面输入 使用方式：以name属性值作为键，value属性值作为值，构成键值对提交到服务器，多个键值对浏览器使用&amp;进行分隔。 type属性 属性值 作用 备注 text 单行文本字段 password 单行文本字段，值被遮盖 email 用于编辑 e-mail 的字段，可以对e-mail地址进行简单校验 radio 单选按钮。 1. 在同一个”单选按钮组“中，所有单选按钮的 name 属性使用同一个值；一个单选按钮组中是，同一时间只有一个单选按钮可以被选择。 2. 必须使用 value 属性定义此控件被提交时的值。 3. 使用checked 必须指示控件是否缺省被选择。 checkbox 复选框。 1. 必须使用 value 属性定义此控件被提交时的值。 2. 使用 checked 属性指示控件是否被选择。 3. 选中多个值时，所有的值会构成一个数组而提交到Web服务器 date HTML5 用于输入日期的控件 年，月，日，不包括时间 time HTML5 用于输入时间的控件 不含时区 datetime-local HTML5 用于输入日期时间的控件 不包含时区 number HTML5 用于输入浮点数的控件 range HTML5 用于输入不精确值控件 max-规定最大值min-规定最小值 step-规定步进值 value-规定默认值 search HTML5 用于输入搜索字符串的单行文本字段 可以点击x清除内容 tel HTML5 用于输入电话号码的控件 url HTML5 用于编辑URL的字段 可以校验URL地址格式 file 此控件可以让用户选择文件，用于文件上传。 使用 accept 属性可以定义控件可以选择的文件类型。 hidden 此控件用户在页面上不可见，但它的值会被提交到服务器，用于传递隐藏值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;type属性演示&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action=&quot;#&quot; method=&quot;get&quot; autocomplete=&quot;off&quot;&gt; &lt;label for=&quot;username&quot;&gt;用户名：&lt;/label&gt; &lt;input type=&quot;text&quot; id=&quot;username&quot; name=&quot;username&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;password&quot;&gt;密码：&lt;/label&gt; &lt;input type=&quot;password&quot; id=&quot;password&quot; name=&quot;password&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;email&quot;&gt;邮箱：&lt;/label&gt; &lt;input type=&quot;email&quot; id=&quot;email&quot; name=&quot;email&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;gender&quot;&gt;性别：&lt;/label&gt; &lt;input type=&quot;radio&quot; id=&quot;gender&quot; name=&quot;gender&quot; value=&quot;men&quot;/&gt;男 &lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;women&quot;/&gt;女 &lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;other&quot;/&gt;其他&lt;br/&gt; &lt;label for=&quot;hobby&quot;&gt;爱好：&lt;/label&gt; &lt;input type=&quot;checkbox&quot; id=&quot;hobby&quot; name=&quot;hobby&quot; value=&quot;music&quot; checked/&gt;音乐 &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;game&quot;/&gt;游戏 &lt;br/&gt; &lt;label for=&quot;birthday&quot;&gt;生日：&lt;/label&gt; &lt;input type=&quot;date&quot; id=&quot;birthday&quot; name=&quot;birthday&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;time&quot;&gt;当前时间：&lt;/label&gt; &lt;input type=&quot;time&quot; id=&quot;time&quot; name=&quot;time&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;insert&quot;&gt;注册时间：&lt;/label&gt; &lt;input type=&quot;datetime-local&quot; id=&quot;insert&quot; name=&quot;insert&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;age&quot;&gt;年龄：&lt;/label&gt; &lt;input type=&quot;number&quot; id=&quot;age&quot; name=&quot;age&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;range&quot;&gt;心情值(1~10)：&lt;/label&gt; &lt;input type=&quot;range&quot; id=&quot;range&quot; name=&quot;range&quot; min=&quot;1&quot; max=&quot;10&quot; step=&quot;1&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;search&quot;&gt;可全部清除文本：&lt;/label&gt; &lt;input type=&quot;search&quot; id=&quot;search&quot; name=&quot;search&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;tel&quot;&gt;电话：&lt;/label&gt; &lt;input type=&quot;tel&quot; id=&quot;tel&quot; name=&quot;tel&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;url&quot;&gt;个人网站：&lt;/label&gt; &lt;input type=&quot;url&quot; id=&quot;url&quot; name=&quot;url&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;file&quot;&gt;文件上传：&lt;/label&gt; &lt;input type=&quot;file&quot; id=&quot;file&quot; name=&quot;file&quot;/&gt; &lt;br/&gt; &lt;label for=&quot;hidden&quot;&gt;隐藏信息：&lt;/label&gt; &lt;input type=&quot;hidden&quot; id=&quot;hidden&quot; name=&quot;hidden&quot; value=&quot;itheima&quot;/&gt; &lt;br/&gt; &lt;button type=&quot;submit&quot;&gt;提交&lt;/button&gt; &lt;button type=&quot;reset&quot;&gt;重置&lt;/button&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 选择控件下拉列表标签： 123&lt;select name=&quot;&quot;&gt;\t&lt;option value=&quot;&quot;&gt;显示的内容&lt;/option&gt;&lt;/select&gt; option：选择菜单的选项 optgroup：列表项分组标签属性：label设置分组名称 文本域控件1&lt;textarea name=&quot;textarea&quot; rows=&quot;10&quot; cols=&quot;50&quot;&gt;Write something here&lt;/textarea&gt; 属性： name-标签名称 rows-行数 cols-列数 1234567891011121314151617&lt;body&gt; &lt;form action=&quot;#&quot; method=&quot;get&quot; autocomplete=&quot;off&quot;&gt; 所在城市：&lt;select name=&quot;city&quot;&gt; &lt;option&gt;---请选择城市---&lt;/option&gt; &lt;optgroup label=&quot;直辖市&quot;&gt; &lt;option&gt;北京&lt;/option&gt; &lt;option&gt;上海&lt;/option&gt; &lt;/optgroup&gt; &lt;optgroup label=&quot;省会市&quot;&gt; &lt;option&gt;杭州&lt;/option&gt; &lt;option&gt;武汉&lt;/option&gt; &lt;/optgroup&gt; &lt;/select&gt; &lt;br/&gt; 个人介绍：&lt;textarea name=&quot;desc&quot; rows=&quot;5&quot; cols=&quot;20&quot;&gt;&lt;/textarea&gt; &lt;/form&gt;&lt;/body&gt; 分组控件123456789&lt;form action=&quot;#&quot; method=&quot;post&quot;&gt;\t&lt;fieldset&gt; &lt;legend&gt;是否同意&lt;/legend&gt; &lt;input type=&quot;radio&quot; id=&quot;radio_y&quot; name=&quot;agree&quot; value=&quot;y&quot;&gt; &lt;label for=&quot;radio_y&quot;&gt;同意&lt;/label&gt; &lt;input type=&quot;radio&quot; id=&quot;radio_n&quot; name=&quot;agree&quot; value=&quot;n&quot;&gt; &lt;label for=&quot;radio_n&quot;&gt;不同意&lt;/label&gt;\t&lt;/fieldset&gt;&lt;/form&gt; 是否同意 同意 不同意 表格标签基本属性&lt;table&gt; , 表示表格标签，表格是数据单元的行和列的两维表 tr：table row，表示表中单元的行 td：table data，表示表中一个单元格 th：table header，表格单元格的表头，通常字体样式加粗居中 代码展示： 1234567891011121314&lt;table&gt; &lt;tr&gt; &lt;th&gt;First name&lt;/th&gt; &lt;th&gt;Last name&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;John&lt;/td&gt; &lt;td&gt;Doe&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Jane&lt;/td&gt; &lt;td&gt;Doe&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; 效果图： First name Last name John Doe Jane Doe 跨行跨列123456789101112131415161718192021222324252627282930313233343536&lt;table width=&quot;400px&quot; border=&quot;1px&quot; align=&quot;center&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;性别&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;数学&lt;/th&gt; &lt;th&gt;语文&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;张三&lt;/td&gt; &lt;td rowspan=&quot;2&quot;&gt;男&lt;/td&gt; &lt;td&gt;23&lt;/td&gt; &lt;td colspan=&quot;2&quot;&gt;90&lt;/td&gt; &lt;!--&lt;td&gt;90&lt;/td&gt;--&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;李四&lt;/td&gt; &lt;!--&lt;td&gt;男&lt;/td&gt;--&gt; &lt;td&gt;24&lt;/td&gt; &lt;td&gt;95&lt;/td&gt; &lt;td&gt;98&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;tfoot&gt; &lt;tr&gt; &lt;td colspan=&quot;4&quot;&gt;总分数：&lt;/td&gt; &lt;td&gt;373&lt;/td&gt; &lt;/tr&gt; &lt;/tfoot&gt;&lt;/table&gt; 效果图： 表格结构 标签名 作用 备注 thead 定义表格的列头的行 一个表格中仅有一个 tbody 定义表格的主体 用来封装一组表行（tr元素） tfoot 定义表格的各列汇总行 一个表格中仅有一个 样式布局基本格式在head标签中，通过style标签加入样式。 基本格式：可以含有多个属性，一个属性名也可以含有多个值，同时设置多样式。 1234567&lt;style&gt; 标签名&#123; 属性名1:属性值1; 属性名2:属性值2; 属性名:属性值1 属性值2 属性值3; &#125;&lt;/style&gt; 背景格式background属性用来设置背景相关的样式。 背景色[background-color]属性定义任何元素的背景色 123body &#123; background-color: #567895;&#125; 背景图该[background-image]属性允许在元素的背景中显示图像。使用url函数指定图片路径 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;背景图片&lt;/title&gt; &lt;style&gt; body&#123; /*添加背景图片*/ background: url(&quot;../img/bg.png&quot;); &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 背景重复 [background-repeat]属性用于控制图像的平铺行为。可用值： no-repeat -停止完全重复背景 repeat-x —水平重复 repeat-y —竖直重复 repeat—默认值；双向重复 1234body &#123; background-image: url(star.png); background-repeat: repeat-x;/*水平重复*/&#125; div布局 div简单布局： broader：边界 solid：实线 blue：颜色 1234567&lt;style&gt; div&#123; border: 1px solid blue;&#125;&lt;/style&gt;&lt;div &gt;left&lt;/div&gt;&lt;div &gt;center&lt;/div&gt;&lt;div&gt;right&lt;/div&gt; class值可以设置宽度，浮动，背景 123456.class值&#123; 属性名:属性值;&#125;&lt;标签名 class=&quot;class值&quot;&gt; 提示: class是自定义的值 属性 background：背景颜色 width：宽度 (npx 或者 n%) height：长度 text-align：文本对齐方式 background-image: url(“..&#x2F;img&#x2F;bg.png”)：背景图 float：浮动 指定一个元素应沿其容器的左侧或右侧放置，允许文本或者内联元素环绕它，该元素从网页的正常流动中移除，其他部分保持正常文档流顺序。 1234567&lt;!-- 加入浮动 --&gt;float：none；不浮动float：left；左浮动float：right；右浮动&lt;!-- 清除浮动 --&gt;clear：both；清除两侧浮动，此元素不再收浮动元素布局影响。 div基本布局 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;样式演示&lt;/title&gt; &lt;style&gt; /*给div标签添加边框*/ div&#123; border: 1px solid red; &#125; /*左侧图片的div样式*/ .left&#123; width: 20%; float: left; height: 500px; &#125; /*中间正文的div样式*/ .center&#123; width: 59%; float: left; height: 500px; &#125; /*右侧广告图片的div样式*/ .right&#123; width: 20%; float: left; height: 500px; &#125; /*底部超链接的div样式*/ .footer&#123; /*清除浮动效果*/ clear: both; /*文本对齐方式*/ text-align: center; /*背景颜色*/ background: blue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;!--顶部登陆注册--&gt; &lt;div&gt;top&lt;/div&gt; &lt;!--导航条--&gt; &lt;div&gt;navibar&lt;/div&gt; &lt;!--左侧图片--&gt; &lt;div class=&quot;left&quot;&gt;left&lt;/div&gt; &lt;!--中间正文--&gt; &lt;div class=&quot;center&quot;&gt;center&lt;/div&gt; &lt;!--右侧广告图片--&gt; &lt;div class=&quot;right&quot;&gt;right&lt;/div&gt; &lt;!--底部页脚超链接--&gt; &lt;div class=&quot;footer&quot;&gt;footer&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 语义化标签为了更好的组织文档，HTML5规范中设计了几个语义元素，可以将特殊含义传达给浏览器。 标签 名称 作用 备注 header 标头元素 表示内容的介绍 块元素，文档中可以定义多个 nav 导航元素 表示导航链接 常见于网站的菜单，目录和索引等，可以嵌套在header中 article 文章元素 表示独立内容区域 标签定义的内容本身必须是有意义且必须独立于文档的其他部分 footer 页脚元素 表示页面的底部 块元素，文档中可以定义多个 HTML拓展音频标签&lt;audio&gt;：用于播放声音，比如音乐或其他音频流，是 HTML 5 的新标签。 常用属性： 属性名 取值 描述 src URL 音频资源的路径 autoplay autoplay 音频准备就绪后自动播放 controls controls 显示控件，比如播放按钮。 loop loop 表示循环播放 preload preload 音频在页面加载时进行预加载。如果使用 “autoplay”，则忽略该属性。 HTML5媒体标签-音频audio 你的浏览器不支持 audio 标签。 视频标签&lt;video&gt; 标签用于播放视频，比如电影片段或其他视频流，是 HTML 5 的新标签。 常用属性： 属性名 取值 描述 src URL 要播放的视频的 URL。 width 设置视频播放器的宽度。 height 设置视频播放器的高度。 autoplay autoplay 视频在就绪后自动播放。 control controls 显示控件，比如播放按钮。 loop loop 如果出现该属性，则当媒介文件完成播放后再次开始播放。 preload preload 视频在页面加载时进行加载。如果使用 “autoplay”，则忽略该属性。 mute muted 规定视频的音频输出应该被静音。 poste URL 视频下载时显示的图像，或者视频播放前显示的图像。 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;HTML5媒体标签-视频video&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;video src=&quot;media/movie.ogg&quot; controls&gt; 你的浏览器不支持 video 标签 &lt;/video&gt;&lt;/body&gt;&lt;/html&gt; 回到顶部在html里面锚点的作用: 通过a标签跳转到指定的位置. 1&lt;a href=&quot;#aId&quot;&gt;回到顶部&lt;/a&gt; 回到顶部 详情概要summary标签来描述概要信息, 利用details标签来描述详情信息. 默认情况下是折叠展示, 想看见详情必须点击 1234&lt;details&gt; &lt;summary&gt;概要信息&lt;/summary&gt; 详情信息&lt;/details&gt; 概要信息 详情信息 CSSCSS入门概述CSS (层叠样式表——Cascading Style Sheets，缩写为 CSS），简单的说，它是用于设置和布局网页的计算机语言。会告知浏览器如何渲染页面元素。例如，调整内容的字体，颜色，大小等样式，设置边框的样式，调整模块的间距等。 层叠：是指样式表允许以多种方式规定样式信息。可以规定在单个元素中，可以在页面头元素中，也可以在另一个CSS文件中，规定的方式会有次序的差别。 样式：是指丰富的样式外观。拿边框距离来说，允许任何设置边框，允许设置边框与框内元素的距离，允许设置边框与边框的距离等等。 组成CSS是一门基于规则的语言—你能定义用于你的网页中特定元素的一组样式规则。这里面提到了两个概念，一是特定元素，二是样式规则。对应CSS的语法，也就是选择器（selects）和声明（eclarations）。 选择器：指定要添加样式的 HTML元素的方式。可以使用标签名，class值，id值等多种方式。 声明：形式为**属性(property):值(value)**，用于设置特定元素的属性信息。 属性：指示文体特征，例如font-size，width，background-color。 值：每个指定的属性都有一个值，该值指示您如何更改这些样式。 格式： 12345选择器 &#123; 属性名:属性值; 属性名:属性值; 属性名:属性值;&#125; 实现 页面标题 h1{ font-size:40px; /* 设置字体大小为100像素*/ } 今天开始学CSS CSS语法注释方式CSS中的注释以/*和开头*/。 123456/* 设置h1的样式 */h1 &#123; color: blue; background-color: yellow; border: 1px solid black;&#125; 引入方式内联样式内联样式是CSS声明在元素的style属性中，仅影响一个元素： 格式： 1&lt;标签 style=&quot;属性名:属性值; 属性名:属性值;&quot;&gt;内容&lt;/标签&gt; 例如： 123&lt;h1 style=&quot;color: blue;background-color: yellow;border: 1px solid black;&quot;&gt; Hello World!&lt;/h1&gt; 效果： Hello World! 特点：格式简单，但是样式作用无法复用到多个元素上，不利于维护 内部样式表内部样式表是将CSS样式放在style标签中，通常style标签编写在HTML 的head标签内部。 格式： 12345678&lt;head&gt; &lt;style&gt; 选择器 &#123; 属性名: 属性值; 属性名: 属性值; &#125; &lt;/style&gt;&lt;/head&gt; 例如： 123456789&lt;head&gt; &lt;style&gt; h1 &#123; color: blue; background-color: yellow; border: 1px solid black; &#125; &lt;/style&gt; &lt;/head&gt; 特点：内部样式只能作用在当前页面上，如果是多个页面，就无法复用了 外部样式表外部样式表是CSS附加到文档中的最常见和最有用的方法，因为您可以将CSS文件链接到多个页面，从而允许您使用相同的样式表设置所有页面的样式。 外部样式表是指将CSS编写在扩展名为.css 的单独文件中，并从HTML&lt;link&gt; 元素引用它，通常link标签&#96;编写在HTML 的[head]标签内部。 格式 1&lt;link rel=&quot;stylesheet&quot; href=&quot;css文件&quot;&gt; rel：表示“关系 (relationship) ”，属性值指链接方式与包含它的文档之间的关系，引入css文件固定值为stylesheet。 href：属性需要引用某文件系统中的一个文件。 举例 创建styles.css文件 12345h1 &#123; color: blue; background-color: yellow; border: 1px solid black;&#125; link标签引入文件 12345678910&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;styles.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello World!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 效果同上 为了CSS文件的管理，在项目中创建一个css文件夹，专门保存样式文件，并调整指定的路径以匹配 12&lt;link rel=&quot;stylesheet&quot; href=&quot;../css/styles.css&quot;&gt;&lt;!--..代表上一级 相对路径--&gt; 优先级规则层叠于一个样式表中，其中数字 4 拥有最高的优先权： 浏览器缺省设置 外部样式表 内部样式表（位于 标签内部） 内联样式（在 HTML 元素内部） 选择器介绍选择器为了样式化某些元素，我们会通过选择器来选中HTML文档中的这些元素，每个CSS规则都以一个选择器或一组选择器为开始，去告诉浏览器这些规则应该应用到哪些元素上。 选择器的分类： 分类 名称 符号 作用 示例 基本选择器 元素选择器 标签名 基于标签名匹配元素 div{ } 类选择器 . 基于class属性值匹配元素 .center{ } ID选择器 # 基于id属性值匹配元素 #username{ } 通用选择器 * 匹配文档中的所有内容 *{ } 属性选择器 属性选择器 [] 基于某属性匹配元素 [type]{ } 伪类选择器 伪类选择器 : 用于向某些选择器添加特殊的效果 a:hover{ } 组合选择器 分组选择器 , 使用 , 号结合两个选择器，匹配两个选择器的元素 span,p{} 后代选择器 空格 使用空格符号结合两个选择器，基于第一个选择器，匹配第二个选择器的所有后代元素 .top li{ } 基本选择器 页面元素： 123456789&lt;body&gt; &lt;div&gt;div1&lt;/div&gt; &lt;div class=&quot;cls&quot;&gt;div2&lt;/div&gt; &lt;div class=&quot;cls&quot;&gt;div3&lt;/div&gt; &lt;div id=&quot;d1&quot;&gt;div4&lt;/div&gt; &lt;div id=&quot;d2&quot;&gt;div5&lt;/div&gt;&lt;/body&gt; 元素选择器 1234/*选择所有div标签,字体为蓝色*/div&#123; color: red;&#125; 类选择器 1234/*选择class为cls的,字体为蓝色*/.cls&#123;\tcolor: blue;&#125; ID选择器 12345678/*id选择器*/#d1&#123; color: green;/*id为d1的字体变成绿色*/&#125;#d2&#123; color: pink;/*id为d2的字体变成粉色*/&#125;/ 通用选择器 1234/*所有标签 */*&#123; background-color: aqua;&#125; 属性选择器 页面： 12345&lt;body&gt; 用户名：&lt;input type=&quot;text&quot;/&gt; &lt;br/&gt; 密码：&lt;input type=&quot;password&quot;/&gt; &lt;br&gt; 邮箱：&lt;input type=&quot;email&quot;/&gt; &lt;br&gt;&lt;/body&gt; 选择器： 12345678/*输入框中输入的字符是红色*/[type] &#123; color: red;&#125;/*输入框中输入的字符是蓝色*/[type=password] &#123; color: blue;&#125; 伪类选择器 页面元素 123&lt;body&gt; &lt;a href=&quot;https://www.baidu.com&quot; target=&quot;_blank&quot;&gt;百度一下&lt;/a&gt;&lt;/body&gt; 伪类选择器 12345678910111213141516171819/*未访问的状态*/a:link&#123;\tcolor: black;&#125;/*已访问的状态*/a:visited&#123;\tcolor: blue;&#125;/*鼠标悬浮的状态*/a:hover&#123;\tcolor: red;&#125;/*已选中的状态*/a:active&#123;\tcolor: yellow;&#125; 注意：伪类顺序 link ，visited，hover，active，否则有可能失效。 组合选择器 页面： 1234567891011121314151617&lt;body&gt; &lt;span&gt;span&lt;/span&gt; &lt;br/&gt; &lt;p&gt;段落&lt;/p&gt; &lt;div class=&quot;top&quot;&gt; &lt;ol&gt; &lt;li&gt;aa&lt;/li&gt; &lt;li&gt;bb&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt; &lt;div class=&quot;center&quot;&gt; &lt;ol&gt; &lt;li&gt;cc&lt;/li&gt; &lt;li&gt;dd&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;/body&gt; 分组选择器 1234/*span p两个标签下的字体为蓝色*/span,p&#123;\tcolor: blue;&#125; 后代选择器 1234/*class为top下的所有li标签字体颜色为红色*/.top li&#123;\tcolor: red;&#125; 优先级选择器优先级 ID选择器 &gt; 类选择器 &gt; 标签选择器 &gt; 通用选择器 如果优先级相同，那么就满足就近原则 边框样式单个边框 单个边框border：边框border-top: 上边框border-left: 左边框border-bottom: 底边框border-right: 右边框 无边框，当border值为none时，可以让边框不显示 12345div &#123;\twidth: 200px; height: 200px; border: none;&#125; 圆角 通过使用[border-radius]属性设置盒子的圆角，虽然能分别设置四个角，但是通常我们使用一个值，来设置整体效果 123456789101112131415161718192021222324#d1&#123; /*设置所有边框*/ /*border: 5px solid black;*/ /*设置上边框*/ border-top: 5px solid black; /*设置左边框*/ border-left: 5px double red; /*设置右边框*/ border-right: 5px dotted blue; /*设置下边框*/ border-bottom: 5px dashed pink; width: 150px; height: 150px;&#125;#d2&#123; border: 5px solid red; /*设置边框的弧度*/ border-radius: 25px; width: 150px; height: 150px;&#125; 12345&lt;body&gt; &lt;div id=&quot;d1&quot;&gt;&lt;/div&gt; &lt;br/&gt; &lt;div id=&quot;d2&quot;&gt;&lt;/div&gt;&lt;/body&gt; 边框轮廓轮廓outline：是绘制于元素周围的一条线，位于边框边缘的外围，可起到突出元素的作用 属性值：double：双实线 dotted：圆点 dashed：虚线 none：无 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;样式演示&lt;/title&gt; &lt;style&gt; input&#123; outline: dotted; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; 用户名：&lt;input type=&quot;text&quot;/&gt; &lt;br/&gt;&lt;/body&gt;&lt;/html&gt; 盒子模型模型介绍盒子模型是通过设置元素框与元素内容和外部元素的边距，而进行布局的方式。 element : 元素。 padding : 内边距，也有资料将其翻译为填充。 border : 边框。 margin : 外边距，也有资料将其翻译为空白或空白边。 边距内边距、边框和外边距都是可选的，默认值是零。在 CSS 中，width 和 height 指的是内容区域的宽度和高度。 外边距单独设置边框的外边距，设置上、右、下、左方向： 1234margin-topmargin-rightmargin-bottommargin-left margin: auto /*浏览器自动计算外边距，具有居中效果。*/ 123456* 一个值 ```css /* 所有 4 个外边距都是 10px */ margin:10px; 两个值 12margin:10px 5px;/* 上外边距和下外边距是 10px*/margin:10px auto;/*右外边距和左外边距是 5px */ 三个值 12/* 上外边距是 10px，右外边距和左外边距是 5px，下外边距是 15px*/margin:10px 5px 15px; 四个值 123/*上外边距是 10px，右外边距是 5px，下外边距是 15px，左外边距是 20px*//*上右下外*/margin:10px 5px 15px 20px; 内边距与外边距类似，单独设置边框的内边距，设置上、右、下、左方向： 1234padding-toppadding-rightpadding-bottompadding-left 布局 基本布局 12345678910111213141516171819&lt;style&gt; div&#123; border: 2px solid blue; &#125; .big&#123; width: 200px; height: 200px; &#125; .small&#123; width: 100px; height: 100px; margin: 30px;/* 外边距 */ &#125;&lt;/style&gt;&lt;div class=&quot;big&quot;&gt; &lt;div class=&quot;small&quot;&gt; &lt;/div&gt;&lt;/div 增加内边距会增加元素框的总尺寸 1234567891011121314 &lt;style&gt;\tdiv&#123; border: 2px solid blue;\t&#125;\t.big&#123; width: 200px; height: 200px; padding: 30px;/*内边距 */\t&#125;\t.small&#123; width: 100px; height: 100px;\t&#125;&lt;/style&gt; 文本样式基本属性 属性名 作用 属性取值 width 宽度 height 高度 color 颜色 font-family 字体样式 宋体、楷体 font-size 字体大小 px : 像素，文本高度像素绝对数值。em : 1em等于当前元素的父元素设置的字体大小，是相对数值 text-decoration 下划线 underline : 下划线 overline : 上划线 line-through : 删除线 none : 不要线条 text-align 文本水平对齐 lef : 左对齐文本right : 右对齐文本center : 使文本居中 justify : 使文本散布，改变单词间的间距，使文本所有行具有相同宽度。 line-height 行高，行间距 vertical-align 文本垂直对齐 top：居上 bottom：居下 middle：居中 或者百分比 display 元素如何显示 可以设置块级和行内元素的切换，也可以设置元素隐藏inline：内联元素(无换行、无长宽) block：块级元素(有换行) inline-block：内联元素(有长宽) none：隐藏元素 12345678910111213div&#123; color: /*red*/ #ff0000; font-family: /*宋体*/ 微软雅黑; font-size: 25px;/ text-decoration: none; text-align: center; line-height: 60px;&#125;span&#123; /*文字垂直对齐 top：居上 bottom：居下 middle：居中 百分比*/ vertical-align: 50%; /*居中对齐*/&#125; 123456789&lt;div&gt; 我是文字&lt;/div&gt;&lt;div&gt; 我是文字&lt;/div&gt;&lt;img src=&quot;../img/wx.png&quot; width=&quot;38px&quot; height=&quot;38px&quot;/&gt;&lt;span&gt;微信&lt;/span&gt; 文本显示 元素显示 1234567891011121314151617/* 把列表项显示为内联元素，无长宽*/li &#123; display:inline;&#125;/* 把span元素作为块元素，有换行*/span &#123; display:block;&#125;/* 行内块元素，结合的行内和块级的优点，既可以行内显示，又可以设置长宽，*/li &#123; display:inline-block;&#125;/*所有div在一行显示*/div&#123; display: inline-block; width: 100px;&#125; 元素隐藏 当设置为none时，可以隐藏元素。 CSS案例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/*背景图片*/body&#123; background: url(&quot;../img/bg.png&quot;);&#125;/*中间表单样式*/.center&#123; background: white; /*背景色*/ width: 40%; /*宽度*/ margin: auto; /*水平居中外边距*/ margin-top: 100px; /*上外边距*/ border-radius: 15px; /*边框弧度*/ text-align: center; /*文本水平居中*/&#125;/*表头样式*/thead th&#123; font-size: 30px; /*字体大小*/ color: orangered; /*字体颜色*/&#125;/*表体提示信息样式*/tbody label&#123; font-size: 20px; /*字体大小*/&#125;/*表体输入框样式*/tbody input&#123; border: 1px solid gray; /*边框*/ border-radius: 5px; /*边框弧度*/ width: 90%; /*输入框的宽度*/ height: 40px; /*输入框的高度*/ outline: none; /*取消轮廓的样式*/&#125;/*表底确定按钮样式*/tfoot button&#123; border: 1px solid crimson; /*边框*/ border-radius: 5px; /*边框弧度*/ width: 95%; /*宽度*/ height: 40px; /*高度*/ background: crimson; /*背景色*/ color: white; /*文字的颜色*/ font-size: 20px; /*字体大小*/&#125;/*表行高度*/tr&#123; line-height: 60px; /*行高*/&#125;/*底部页脚样式*/.footer&#123; width: 35%; /*宽度*/ margin: auto; /*水平居中外边距*/ font-size: 15px; /*字体大小*/ color: gray; /*字体颜色*/&#125;/*超链接样式*/a&#123; text-decoration: none; /*去除超链接的下划线*/ color: blue; /*超链接颜色*/&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;登录页面&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;../css/login.css&quot;/&gt;&lt;/head&gt;&lt;body&gt; &lt;!--顶部公司图标--&gt; &lt;div&gt; &lt;img src=&quot;../img/logo.png&quot;/&gt; &lt;/div&gt; &lt;!--中间表单--&gt; &lt;div class=&quot;center&quot;&gt; &lt;form action=&quot;#&quot; method=&quot;get&quot; autocomplete=&quot;off&quot;&gt; &lt;table width=&quot;100%&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th colspan=&quot;2&quot;&gt;账&amp;nbsp;密&amp;nbsp;登&amp;nbsp;录&lt;hr/&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;label for=&quot;username&quot;&gt;账号&lt;/label&gt; &lt;/td&gt; &lt;td&gt; &lt;input type=&quot;text&quot; id=&quot;username&quot; name=&quot;username&quot; placeholder=&quot; 请输入账号&quot; required/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;label for=&quot;password&quot;&gt;密码&lt;/label&gt; &lt;/td&gt; &lt;td&gt; &lt;input type=&quot;password&quot; id=&quot;password&quot; name=&quot;password&quot; placeholder=&quot; 请输入密码&quot; required/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;tfoot&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt; &lt;button type=&quot;submit&quot;&gt;确&amp;nbsp;定&lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tfoot&gt; &lt;/table&gt; &lt;/form&gt; &lt;/div&gt; &lt;!--底部页脚--&gt; &lt;div class=&quot;footer&quot;&gt; &lt;br/&gt;&lt;br/&gt; 登录/注册即表示您同意&amp;nbsp;&amp;nbsp; &lt;a href=&quot;#&quot; target=&quot;_blank&quot;&gt;用户协议&lt;/a&gt;&amp;nbsp;&amp;nbsp; 和&amp;nbsp;&amp;nbsp; &lt;a href=&quot;#&quot; target=&quot;_blank&quot;&gt;隐私条款&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&quot;#&quot; target=&quot;_blank&quot;&gt;忘记密码?&lt;/a&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; HTTP相关概念HTTP：Hyper Text Transfer Protocol，意为超文本传输协议，是建立在 TCP&#x2F;IP 协议基础上，指的是服务器和客户端之间交互必须遵循的一问一答的规则，形容这个规则：问答机制、握手机制 HTTP 协议是一个无状态的面向连接的协议，指的是协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。所以打开一个服务器上的网页和上一次打开这个服务器上的网页之间没有任何联系 注意：无状态并不是代表 HTTP 就是 UDP，面向连接也不是代表 HTTP 就是TCP HTTP 作用：用于定义 WEB 浏览器与 WEB 服务器之间交换数据的过程和数据本身的内容 浏览器和服务器交互过程：浏览器请求，服务请求响应 请求（请求行、请求头、请求体） 响应（响应行、响应头、响应体） URL 和 URI URL：统一资源定位符 格式：http://127.0.0.1:8080/request/servletDemo01 详解：http：协议；127.0.0.1：域名；8080：端口；request&#x2F;servletDemo01：请求资源路径 URI：统一资源标志符 格式：&#x2F;request&#x2F;servletDemo01 区别：URL - HOST = URI，URI 是抽象的定义，URL 用地址定位，URI 用名称定位。只要能唯一标识资源的是 URI，在 URI 的基础上给出其资源的访问方式的是 URL 从浏览器地址栏输入 URL 到请求返回发生了什么？ 进行 URL 解析，进行编码 DNS 解析，顺序是先查 hosts 文件是否有记录，有的话就会把相对应映射的 IP 返回，然后去本地 DNS 缓存中寻找，然后依次向本地域名服务器、根域名服务器、顶级域名服务器、权限域名服务器发起查询请求，最终返回 IP 地址给本地域名服务器 本地域名服务器将得到的 IP 地址返回给操作系统，同时将 IP 地址缓存起来；操作系统将 IP 地址返回给浏览器，同时自己也将 IP 地址缓存起来 查找到 IP 之后，进行 TCP 协议的三次握手建立连接 发出 HTTP 请求，取文件指令 服务器处理请求，返回响应 释放 TCP 连接 浏览器解析渲染页面 推荐阅读：https://xiaolincoding.com/network/ 版本区别版本介绍： HTTP&#x2F;0.9 仅支持 GET 请求，不支持请求头 HTTP&#x2F;1.0 默认短连接（一次请求建议一次 TCP 连接，请求完就断开），支持 GET、POST、 HEAD 请求 HTTP&#x2F;1.1 默认长连接（一次 TCP 连接可以多次请求）；支持 PUT、DELETE、PATCH 等六种请求；增加 HOST 头，支持虚拟主机；支持断点续传功能 HTTP&#x2F;2.0 多路复用，降低开销（一次 TCP 连接可以处理多个请求）；服务器主动推送（相关资源一个请求全部推送）；解析基于二进制，解析错误少，更高效（HTTP&#x2F;1.X 解析基于文本）；报头压缩，降低开销 HTTP&#x2F;3.0 QUIC (Quick UDP Internet Connections)，快速 UDP 互联网连接，基于 UDP 协议 HTTP 1.0 和 HTTP 1.1 的主要区别： 长短连接： 在HTTP&#x2F;1.0中，默认使用的是短连接，每次请求都要重新建立一次连接，比如获取 HTML 和 CSS 文件，需要两次请求。HTTP 基于 TCP&#x2F;IP 协议的，每一次建立或者断开连接都需要三次握手四次挥手，开销会比较大 HTTP 1.1起，默认使用长连接 ，默认开启 Connection: keep-alive，Keep-Alive 有一个保持时间，不会永久保持连接。持续连接有非流水线方式和流水线方式 ，流水线方式是客户端在收到 HTTP 的响应报文之前就能接着发送新的请求报文，非流水线方式是客户端在收到前一个响应后才能发送下一个请求 HTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接 错误状态响应码：在 HTTP1.1 中新增了 24 个错误状态响应码，如 409（Conflict）表示请求的资源与资源的当前状态发生冲突，410（Gone）表示服务器上的某个资源被永久性的删除 缓存处理：在 HTTP1.0 中主要使用 header 里的 If-Modified-Since，Expires 来做为缓存判断的标准，HTTP1.1 则引入了更多的缓存控制策略，例如 Entity tag，If-Unmodified-Since，If-Match，If-None-Match等 带宽优化及网络连接的使用：HTTP1.0 存在一些浪费带宽的现象，例如客户端只需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1 则在请求头引入了 range 头域，允许只请求资源的某个部分，即返回码是 206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接 HOST 头处理：在 HTTP1.0 中认为每台服务器都绑定一个唯一的 IP 地址，因此请求消息中的 URL 并没有传递主机名。HTTP1.1 时代虚拟主机技术发展迅速，在一台物理服务器上可以存在多个虚拟主机，并且共享一个 IP 地址，故 HTTP1.1 增加了 HOST 信息 HTTP 1.1 和 HTTP 2.0 的主要区别： 新的二进制格式：HTTP1.1 基于文本格式传输数据，HTTP2.0 采用二进制格式传输数据，解析更高效 多路复用：在一个连接里，允许同时发送多个请求或响应，并且这些请求或响应能够并行的传输而不被阻塞，避免 HTTP1.1 出现的队头堵塞问题 头部压缩，HTTP1.1 的 header 带有大量信息，而且每次都要重复发送；HTTP2.0 把 header 从数据中分离，并封装成头帧和数据帧，使用特定算法压缩头帧。并且 HTTP2.0 在客户端和服务器端记录了之前发送的键值对，对于相同的数据不会重复发送。比如请求 A 发送了所有的头信息字段，请求 B 则只需要发送差异数据，这样可以减少冗余数据，降低开销 服务端推送：HTTP2.0 允许服务器向客户端推送资源，无需客户端发送请求到服务器获取 安全请求HTTP 和 HTTPS 的区别： 端口 ：HTTP 默认使用端口 80，HTTPS 默认使用端口 443 安全性：HTTP 协议运行在 TCP 之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份；HTTPS 是运行在 SSL&#x2F;TLS 之上的 HTTP 协议，SSL&#x2F;TLS 运行在 TCP 之上，所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密 资源消耗：HTTP 安全性没有 HTTPS 高，但是 HTTPS 比 HTTP 耗费更多服务器资源 对称加密和非对称加密 对称加密：加密和解密使用同一个秘钥，把密钥转发给需要发送数据的客户机，中途会被拦截（类似于把带锁的箱子和钥匙给别人，对方打开箱子放入数据，上锁后发送），私钥用来解密数据，典型的对称加密算法有 DES、AES 等 优点：运算速度快 缺点：无法安全的将密钥传输给通信方 非对称加密：加密和解密使用不同的秘钥，一把作为公开的公钥，另一把作为私钥，公钥公开给任何人（类似于把锁和箱子给别人，对方打开箱子放入数据，上锁后发送），典型的非对称加密算法有 RSA、DSA 等 公钥加密，私钥解密：为了保证内容传输的安全，因为被公钥加密的内容，其他人是无法解密的，只有持有私钥的人，才能解密出实际的内容 私钥加密，公钥解密：为了保证消息不会被冒充，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的 可以更安全地将公开密钥传输给通信发送方，但是运算速度慢 使用对称加密和非对称加密的方式传送数据 使用非对称密钥加密方式，传输对称密钥加密方式所需要的 Secret Key，从而保证安全性 获取到 Secret Key 后，再使用对称密钥加密方式进行通信，从而保证效率 思想：锁上加锁 名词解释： 哈希算法：通过哈希函数计算出内容的哈希值，传输到对端后会重新计算内容的哈希，进行哈希比对来校验内容的完整性 数字签名：附加在报文上的特殊加密校验码，可以防止报文被篡改。一般是通过私钥对内容的哈希值进行加密，公钥正常解密并对比哈希值后，可以确保该内容就是对端发出的，防止出现中间人替换的问题 数字证书：由权威机构给某网站颁发的一种认可凭证 HTTPS 工作流程：服务器端的公钥和私钥，用来进行非对称加密，客户端生成的随机密钥，用来进行对称加密 客户端向服务器发起 HTTPS 请求，连接到服务器的 443 端口，请求携带了浏览器支持的加密算法和哈希算法，协商加密算法 服务器端会向数字证书认证机构注册公开密钥，认证机构用 CA 私钥对公开密钥做数字签名后绑定在数字证书（又叫公钥证书，内容有公钥，网站地址，证书颁发机构，失效日期等） 服务器将数字证书发送给客户端，私钥由服务器持有 客户端收到服务器端的数字证书后通过 CA 公钥（事先置入浏览器或操作系统）对证书进行检查，验证其合法性。如果公钥合格，那么客户端会生成一个随机值，这个随机值就是用于进行对称加密的密钥，将该密钥称之为 client key（客户端密钥、会话密钥）。用服务器的公钥对客户端密钥进行非对称加密，这样客户端密钥就变成密文，HTTPS 中的第一次 HTTP 请求结束 客户端会发起 HTTPS 中的第二个 HTTP 请求，将加密之后的客户端密钥发送给服务器 服务器接收到客户端发来的密文之后，会用自己的私钥对其进行非对称解密，解密之后的明文就是客户端密钥，然后用客户端密钥对数据进行对称加密，这样数据就变成了密文 服务器将加密后的密文发送给客户端 客户端收到服务器发送来的密文，用客户端密钥对其进行对称解密，得到服务器发送的数据，这样 HTTPS 中的第二个 HTTP 请求结束，整个 HTTPS 传输完成 参考文章：https://www.cnblogs.com/linianhui/p/security-https-workflow.html 参考文章：https://www.jianshu.com/p/14cd2c9d2cd2 请求部分请求行： 永远位于请求的第一行 请求头： 从第二行开始，到第一个空行结束 请求体： 从第一个空行后开始，到正文的结束（GET 没有） 请求方式 POST GET 123456789101112【请求行】GET /myApp/success.html?username=zs&amp;password=123456 HTTP/1.1【请求头】Accept: text/html, application/xhtml+xml, */*; X-HttpWatch-RID: 41723-10011Referer: http://localhost:8080/myApp/login.htmlAccept-Language: zh-Hans-CN,zh-Hans;q=0.5User-Agent: Mozilla/5.0 (MSIE 9.0; qdesk 2.4.1266.203; Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like GeckoAccept-Encoding: gzip, deflateHost: localhost:8080Connection: Keep-AliveCookie: Idea-b77ddca6=4bc282fe-febf-4fd1-b6c9-72e9e0f381e8 GET 和 POST 比较 作用：GET 用于获取资源，而 POST 用于传输实体主体 参数：GET 和 POST 的请求都能使用额外的参数，但是 GET 的参数是以查询字符串出现在 URL 中，而 POST 的参数存储在实体主体中（GET 也有请求体，POST 也可以通过 URL 传输参数）。不能因为 POST 参数存储在实体主体中就认为它的安全性更高，因为照样可以通过一些抓包工具（Fiddler）查看 安全：安全的 HTTP 方法不会改变服务器状态，也就是说它只是可读的。GET 方法是安全的，而 POST 不是，因为 POST 的目的是传送实体主体内容 安全的方法除了 GET 之外还有：HEAD、OPTIONS 不安全的方法除了 POST 之外还有 PUT、DELETE 幂等性：同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的，所有的安全方法也都是幂等的。在正确实现条件下，GET，HEAD，PUT 和 DELETE 等方法都是幂等的，POST 方法不是 可缓存：如果要对响应进行缓存，需要满足以下条件 请求报文的 HTTP 方法本身是可缓存的，包括 GET 和 HEAD，但是 PUT 和 DELETE 不可缓存，POST 在多数情况下不可缓存 响应报文的状态码是可缓存的，包括：200、203、204、206、300、301、404、405、410、414 and 501 响应报文的 Cache-Control 首部字段没有指定不进行缓存 PUT 和 POST 的区别 PUT 请求：如果两个请求相同，后一个请求会把第一个请求覆盖掉（幂等），所以 PUT 用来修改资源 POST 请求：后一个请求不会把第一个请求覆盖掉（非幂等），所以 POST 用来创建资源 PATCH 方法 是新引入的，是对 PUT 方法的补充，用来对已知资源进行局部更新 请求行详解 12GET /myApp/success.html?username=zs&amp;password=123456 HTTP/1.1\tPOST /myApp/success.html HTTP/1.1 内容 说明 GET&#x2F;POST 请求的方式。 &#x2F;myApp&#x2F;success.html 请求的资源。 HTTP&#x2F;1.1 使用的协议，及协议的版本。 请求头详解 从第 2 行到空行处，都叫请求头，以键值对的形式存在，但存在一个 key 对应多个值的请求头 内容 说明 Accept 告知服务器，客户浏览器支持的 MIME 类型 User-Agent 浏览器相关信息 Accept-Charset 告诉服务器，客户浏览器支持哪种字符集 Accept-Encoding 告知服务器，客户浏览器支持的压缩编码格式，常用 gzip 压缩 Accept-Language 告知服务器，客户浏览器支持的语言，zh_CN 或 en_US 等 Host 初始 URL 中的主机和端口 Referer 告知服务器，当前请求的来源。只有当前请求有来源，才有这个消息头。作用：1 投放广告 2 防盗链 Content-Type 告知服务器，请求正文的 MIME 类型，文件传输的类型，application&#x2F;x-www-form-urlencoded Content-Length 告知服务器，请求正文的长度。 Connection 表示是否需要持久连接，一般是 Keep -Alive（HTTP 1.1 默认进行持久连接 ) If-Modified-Since 告知服务器，客户浏览器缓存文件的最后修改时间 Cookie 会话管理相关（非常的重要） 请求体详解 只有 POST 请求方式，才有请求的正文，GET 方式的正文是在地址栏中的 表单的输入域有 name 属性的才会被提交，不分 GET 和 POST 的请求方式 表单的 enctype 属性取值决定了请求正文的体现形式 enctype取值 请求正文体现形式 示例 application&#x2F;x-www-form-urlencoded key&#x3D;value&amp;key&#x3D;value username&#x3D;test&amp;password&#x3D;1234 multipart&#x2F;form-data 此时变成了多部分表单数据。多部分是靠分隔符分隔的。 —————————–7df23a16c0210Content-Disposition: form-data; name&#x3D;”username”test—————————–7df23a16c0210Content-Disposition: form-data; name&#x3D;”password”1234——————————-7df23a16c0210 响应部分响应部分图： 响应行 HTTP&#x2F;1.1：使用协议的版本 200：响应状态码 OK：状态码描述 响应状态码： 状态码 说明 200 一切都 OK，与服务器连接成功，发送请求成功 302&#x2F;307 请求重定向（客户端行为，两次请求，地址栏发生改变） 304 请求资源未改变，使用缓存 400 客户端错误，请求错误，最常见的就是请求参数有问题 403 客户端错误，但 forbidden 权限不够，拒绝处理 404 客户端错误，请求资源未找到 500 服务器错误，服务器运行内部错误 转移： 301 redirect：301 代表永久性转移 (Permanently Moved) 302 redirect：302 代表暂时性转移 (Temporarily Moved ) 响应头：以 key:vaue 存在，可能多个 value 情况 消息头 说明 Location 请求重定向的地址，常与 302，307 配合使用。 Server 服务器相关信息 Content-Type 告知客户浏览器，响应正文的MIME类型 Content-Length 告知客户浏览器，响应正文的长度 Content-Encoding 告知客户浏览器，响应正文使用的压缩编码格式，常用的 gzip 压缩 Content-Language 告知客户浏览器，响应正文的语言，zh_CN 或 en_US 等 Content-Disposition 告知客户浏览器，以下载的方式打开响应正文 Refresh 客户端的刷新频率，单位是秒 Last-Modified 服务器资源的最后修改时间 Set-Cookie 服务器端发送的 Cookie，会话管理相关 Expires:-1 服务器资源到客户浏览器后的缓存时间 Catch-Control: no-catch 不要缓存，&#x2F;&#x2F;针对http协议1.1版本 Pragma:no-catch 不要缓存，&#x2F;&#x2F;针对http协议1.0版本 响应体：页面展示内容, 类似网页的源码 123456789&lt;html&gt; &lt;head&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;css.css&quot; type=&quot;text/css&quot;&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;demo.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;img src=&quot;1.jpg&quot; /&gt; &lt;/body&gt;&lt;/html&gt; ServletJavaEEJavaEE规范JavaEE 规范是 J2EE 规范的新名称，早期被称为 J2EE 规范，其全称是 Java 2 Platform Enterprise Edition，它是由 SUN 公司领导、各厂家共同制定并得到广泛认可的工业标准（JCP组织成员）。之所以改名为JavaEE，目的还是让大家清楚 J2EE 只是 Java 企业应用。在 2004 年底中国软件技术大会 Ioc 微容器（也就是 Jdon 框架的实现原理）演讲中指出：我们需要一个跨 J2SE/WEB/EJB 的微容器，保护我们的业务核心组件，以延续它的生命力，而不是依赖 J2SE/J2EE 版本。此次 J2EE 改名为 Java EE，实际也反映出业界这种共同心声 JavaEE 规范是很多 Java 开发技术的总称。这些技术规范都是沿用自 J2EE 的。一共包括了 13 个技术规范，例如：jsp/servlet，jndi，jaxp，jdbc，jni，jaxb，jmf，jta，jpa，EJB等。 其中，JCP 组织的全称是 Java Community Process，是一个开放的国际组织，主要由 Java 开发者以及被授权者组成，职能是发展和更新。成立于 1998 年。官网是：JCP JavaEE 的版本是延续了 J2EE 的版本，但是没有继续采用其命名规则。J2EE 的版本从 1.0 开始到 1.4 结束，而 JavaEE 版本是从 JavaEE 5 版本开始，目前最新的的版本是 JavaEE 8 详情请参考：JavaEE8 规范概览 Web 概述Web，在计算机领域指网络。像我们接触的 WWW，它是由 3 个单词组成的，即：World Wide Web ，中文含义是万维网。而我们前面学的 HTML 的参考文档《W3School 全套教程》中的 W3C 就是万维网联盟，他们的出现都是为了让我们在网络的世界中获取资源，这些资源的存放之处，我们称之为网站。我们通过输入网站的地址（网址），就可以访问网站中提供的资源。在网上我们能访问到的内容全是资源（不区分局域网还是广域网），只不过不同类型的资源展示的效果不一样 资源分为静态资源和动态资源 静态资源指的是，网站中提供给人们展示的资源是一成不变的，也就是说不同人或者在不同时间，看到的内容都是一样的。例如：我们看到的新闻，网站的使用手册，网站功能说明文档等等。而作为开发者，我们编写的 html、css、js 图片，多媒体等等都可以称为静态资源 动态资源它指的是，网站中提供给人们展示的资源是由程序产生的，在不同的时间或者用不同的人员由于身份的不同，所看到的内容是不一样的。例如：我们在CSDN上下载资料，只有登录成功后，且积分足够时才能下载。否则就不能下载，这就是访客身份和会员身份的区别。作为开发人员，我们编写的 JSP，servlet，php，ASP 等都是动态资源。 关于广域网和局域网的划分 广域网指的就是万维网，也就是我们说的互联网。 局域网是指的是在一定范围之内可以访问的网络，出了这个范围，就不能再使用的网络。 系统结构基础结构划分：C&#x2F;S结构，B&#x2F;S结构两类。 技术选型划分：Model1模型，Model2模型，MVC模型和三层架构+MVC模型。 部署方式划分：一体化架构，垂直拆分架构，分布式架构，流动计算架构，微服务架构。 C&#x2F;S结构：客户端—服务器的方式。其中C代表Client，S代表服务器。C&#x2F;S结构的系统设计图如下： B&#x2F;S结构是浏览器—服务器的方式。B代表Browser，S代表服务器。B&#x2F;S结构的系统设计图如下： 两种结构的区别及优劣 区别： 第一：硬件环境不同，C&#x2F;S通常是建立在专用的网络或小范围的网络环境上（即局域网），且必须要安装客户端。而B&#x2F;S是建立在广域网上的，适应范围强，通常有操作系统和浏览器就行。 第二：C&#x2F;S结构比B&#x2F;S结构更安全，因为用户群相对固定，对信息的保护更强。 第三：B&#x2F;S结构维护升级比较简单，而C&#x2F;S结构维护升级相对困难。 优劣 C&#x2F;S：能充分发挥客户端PC的处理能力，很多工作可以在客户端处理后再提交给服务器。对应的优点就是客户端响应速度快。 B&#x2F;S：总体拥有成本低、维护方便、 分布性强、开发简单，可以不用安装任何专门的软件就能实现在任何地方进行操作，客户端零维护，系统的扩展非常容易，只要有一台能上网的电脑就能使用。 我们的课程中涉及的系统结构都是是基于B&#x2F;S结构 Tomcat服务器服务器的概念非常的广泛，它可以指代一台特殊的计算机（相比普通计算机运行更快、负载更高、价格更贵），也可以指代用于部署网站的应用。我们这里说的服务器，其实是web服务器，或者应用服务器。它本质就是一个软件，一个应用。作用就是发布我们的应用（工程），让用户可以通过浏览器访问我们的应用。 常见的应用服务器，请看下表： 服务器名称 说明 weblogic 实现了 JavaEE 规范，重量级服务器，又称为 JavaEE 容器 websphereAS 实现了 JavaEE 规范，重量级服务器。 JBOSSAS 实现了 JavaEE 规范，重量级服务器，免费 Tomcat 实现了 jsp&#x2F;servlet 规范，是一个轻量级服务器，开源免费 基本介绍Windows安装下载地址：http://tomcat.apache.org/ 目录结构详解： Linux安装解压apache-tomcat-8.5.32.tar.gz。 防火墙设置 方式1：service iptables stop 关闭防火墙(不建议); 用到哪一个端口号就放行哪一个(80,8080,3306…) 方式2：放行8080 端口 修改配置文件cd /etc/sysconfig–&gt;vi iptables-A INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -j ACCEPT 重启加载防火墙或者重启防火墙service iptables reload 或者service iptables restart 启动停止Tomcat服务器的启动文件在二进制文件目录bin中：startup.bat，startup.sh Tomcat服务器的停止文件也在二进制文件目录bin中：shutdown.bat，shutdown.sh （推荐直接关闭控制台） 其中.bat文件是针对windows系统的运行程序，.sh文件是针对linux系统的运行程序。 常见问题 启动一闪而过 没有配置环境变量，配置上 JAVA_HOME 环境变量。 Tomcat 启动后控制台输出乱码 打开 /conf/logging.properties，设置 gbk java.util.logging.ConsoleHandler.encoding = gbk Address already in use : JVM_Bind：端口被占用，找到占用该端口的应用 进程不重要：使用cmd命令：netstat -a -o 查看 pid 在任务管理器中结束占用端口的进程 进程很重要：修改自己的端口号。修改的是 Tomcat 目录下\\conf\\server.xml中的配置。 IDEA集成Run -&gt; Edit Configurations -&gt; Templates -&gt; Tomcat Server -&gt; Local 发布应用虚拟目录在 server.xml 的 &lt;Host&gt; 元素中加一个 &lt;Context path=&quot;&quot; docBase=&quot;&quot;/&gt; 元素 path：访问资源URI，URI名称可以随便起，但是必须在前面加上一个&#x2F; docBase：资源所在的磁盘物理地址 虚拟主机在&lt;Engine&gt;元素中添加一个&lt;Host name=&quot;&quot; appBase=&quot;&quot; unparkWARs=&quot;&quot; autoDeploy=&quot;&quot; /&gt;，其中： name：指定主机的名称 appBase：当前主机的应用发布目录 unparkWARs：启动时是否自动解压war包 autoDeploy：是否自动发布 123&lt;Host name=&quot;www.itcast.cn&quot; appBase=&quot;D:\\itcastapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;/&gt;&lt;Host name=&quot;www.itheima.com&quot; appBase=&quot;D:\\itheimaapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;/&gt; IDEA部署 新建工程 发布工程 Run IDEA发布把资源移动到 Tomcat 工程下 web 目录中，两种访问方式 直接访问：http://localhost:8080/Tomcat/login/login.html 在 web.xml 中配置默认主页 123&lt;welcome-file-list&gt; &lt;welcome-file&gt;/默认主页&lt;/welcome-file&gt;&lt;/welcome-file-list&gt; 执行原理整体架构Tomcat 核心组件架构图如下所示： 组件介绍： GlobalNamingResources：实现 JNDI，指定一些资源的配置信息 Server：Tomcat 是一个 Servlet 容器，一个 Tomcat 对应一个 Server，一个 Server 可以包含多个 Service Service：核心服务是 Catalina，用来对请求进行处理，一个 Service 包含多个 Connector 和一个 Container Connector：连接器，负责处理客户端请求，解析不同协议及 I&#x2F;O 方式 Executor：线程池 Container：容易包含 Engine，Host，Context，Wrapper 等组件 Engine：服务交给引擎处理请求，Container 容器中顶层的容器对象，一个 Engine 可以包含多个 Host 主机 Host：Engine 容器的子容器，一个 Host 对应一个网络域名，一个 Host 包含多个 Context Context：Host 容器的子容器，表示一个 Web 应用 Wrapper：Tomcat 中的最小容器单元，表示 Web 应用中的 Servlet 核心类库： Coyote：Tomcat 连接器的名称，封装了底层的网络通信，为 Catalina 容器提供了统一的接口，使容器与具体的协议以及 I&#x2F;O 解耦 EndPoint：Coyote 通信端点，即通信监听的接口，是 Socket 接收和发送处理器，是对传输层的抽象，用来实现 TCP&#x2F;IP 协议 Processor ： Coyote 协议处理接口，用来实现 HTTP 协议，Processor 接收来自 EndPoint 的 Socket，读取字节流解析成 Tomcat 的 Request 和 Response 对象，并通过 Adapter 将其提交到容器处理，Processor 是对应用层协议的抽象 CoyoteAdapter：适配器，连接器调用 CoyoteAdapter 的 sevice 方法，传入的是 TomcatRequest 对象，CoyoteAdapter 负责将TomcatRequest 转成 ServletRequest，再调用容器的 service 方法 参考文章：https://www.jianshu.com/p/7c9401b85704 参考文章：https://www.yuque.com/yinhuidong/yu877c/ktq82e 启动过程Tomcat 的启动入口是 Bootstrap#main 函数，首先通过调用 bootstrap.init() 初始化相关组件： initClassLoaders()：初始化三个类加载器，commonLoader 的父类加载器是启动类加载器 Thread.currentThread().setContextClassLoader(catalinaLoader)：自定义类加载器加载 Catalina 类，打破双亲委派 Object startupInstance = startupClass.getConstructor().newInstance()：反射创建 Catalina 对象 method.invoke(startupInstance, paramValues)：反射调用方法，设置父类加载器是 sharedLoader catalinaDaemon = startupInstance：引用 Catalina 对象 daemon.load(args) 方法反射调用 Catalina 对象的 load 方法，对服务器的组件进行初始化，并绑定了 ServerSocket 的端口： parseServerXml(true)：解析 XML 配置文件 getServer().init()：服务器执行初始化，采用责任链的执行方式 LifecycleBase.init()：生命周期接口的初始化方法，开始链式调用 StandardServer.initInternal()：Server 的初始化，遍历所有的 Service 进行初始化 StandardService.initInternal()：Service 的初始化，对 Engine、Executor、listener、Connector 进行初始化 StandardEngine.initInternal()：Engine 的初始化 getRealm()：创建一个 Realm 对象 ContainerBase.initInternal()：容器的初始化，设置处理容器内组件的启动和停止事件的线程池 Connector.initInternal()：Connector 的初始化 123public Connector() &#123; this(&quot;HTTP/1.1&quot;); //默认无参构造方法，会创建出 Http11NioProtocol 的协议处理器&#125; adapter = new CoyoteAdapter(this)：实例化 CoyoteAdapter 对象 protocolHandler.setAdapter(adapter)：设置到 ProtocolHandler 协议处理器中 ProtocolHandler.init()：协议处理器的初始化，底层调用 AbstractProtocol#init 方法 endpoint.init()：端口的初始化，底层调用 AbstractEndpoint#init 方法 NioEndpoint.bind()：绑定方法 initServerSocket()：初始化 ServerSocket，以 NIO 的方式监听端口 serverSock = ServerSocketChannel.open()：NIO 的方式打开通道 serverSock.bind(addr, getAcceptCount())：通道绑定连接端口 serverSock.configureBlocking(true)：切换为阻塞模式（没懂，为什么阻塞） initialiseSsl()：初始化 SSL 连接 selectorPool.open(getName())：打开选择器，类似 NIO 的多路复用器 初始化完所有的组件，调用 daemon.start() 进行组件的启动，底层反射调用 Catalina 对象的 start 方法： getServer().start()：启动组件，也是责任链的模式 LifecycleBase.start()：生命周期接口的初始化方法，开始链式调用 StandardServer.startInternal()：Server 服务的启动 globalNamingResources.start()：启动 JNDI 服务 for (Service service : services)：遍历所有的 Service 进行启动 StandardService.startInternal()：Service 的启动，对所有 Executor、listener、Connector 进行启 StandardEngine.startInternal()：启动引擎，部署项目 ContainerBase.startInternal()：容器的启动 启动集群、Realm 组件，并且创建子容器，提交给线程池 ((Lifecycle) pipeline).start()：遍历所有的管道进行启动 Valve current = first：获取第一个阀门 ((Lifecycle) current).start()：启动阀门，底层 ValveBase#startInternal 中设置启动的状态 current = current.getNext()：获取下一个阀门 Connector.startInternal()：Connector 的初始化 protocolHandler.start()：协议处理器的启动 endpoint.start()：端点启动 NioEndpoint.startInternal()：启动 NIO 的端点 createExecutor()：创建 Worker 线程组，10 个线程，用来进行任务处理 initializeConnectionLatch()：用来进行连接限流，最大 8*1024 条连接 poller = new Poller()：创建 Poller 对象，开启了一个多路复用器 Selector Thread pollerThread = new Thread(poller, getName() + &quot;-ClientPoller&quot;)：创建并启动 Poller 线程，Poller 实现了 Runnable 接口，是一个任务对象，线程 start 后进入 Poller#run 方法 pollerThread.setDaemon(true)：设置为守护线程 startAcceptorThread()：启动接收者线程 acceptor = new Acceptor&lt;&gt;(this)：创建 Acceptor 对象 Thread t = new Thread(acceptor, threadName)：创建并启动 Acceptor 接受者线程 处理过程 Acceptor 监听客户端套接字，每 50ms 调用一次 **serverSocket.accept**，获取 Socket 后把封装成 NioSocketWrapper（是 SocketWrapperBase 的子类），并设置为非阻塞模式，把 NioSocketWrapper 封装成 PollerEvent 放入同步队列中 Poller 循环判断同步队列中是否有就绪的事件，如果有则通过 selector.selectedKeys() 获取就绪事件，获取 SocketChannel 中携带的 attachment（NioSocketWrapper），在 processKey 方法中根据事件类型进行 processSocket，将 Wrapper 对象封装成 SocketProcessor 对象，该对象是一个任务对象，提交到 Worker 线程池进行执行 SocketProcessorBase.run() 加锁调用 SocketProcessor#doRun，保证线程安全，从协议处理器 ProtocolHandler 中获取 AbstractProtocol，然后创建 Http11Processor 对象处理请求 Http11Processor#service 中调用 CoyoteAdapter#service ，把生成的 Tomcat 下的 Request 和 Response 对象通过方法 postParseRequest 匹配到对应的 Servlet 的请求响应，将请求传递到对应的 Engine 容器中调用 Pipeline，管道中包含若干个 Valve，执行完所有的 Valve 最后执行 StandardEngineValve，继续调用 Host 容器的 Pipeline，执行 Host 的 Valve，再传递给 Context 的 Pipeline，最后传递到 Wrapper 容器 StandardWrapperValve#invoke 中创建了 Servlet 对象并执行初始化，并为当前请求准备一个 FilterChain 过滤器链执行 doFilter 方法，ApplicationFilterChain#doFilter 是一个责任链的驱动方法，通过调用 internalDoFilter 来获取过滤器链的下一个过滤器执行 doFilter，执行完所有的过滤器后执行 servlet.service 的方法 最后调用 HttpServlet#service()，根据请求的方法来调用 doGet、doPost 等，执行到自定义的业务方法 ServletSocketSocket 是使用 TCP&#x2F;IP 或者 UDP 协议在服务器与客户端之间进行传输的技术，是网络编程的基础 Servlet 是使用 HTTP 协议在服务器与客户端之间通信的技术，是 Socket 的一种应用 HTTP 协议：是在 TCP&#x2F;IP 协议之上进一步封装的一层协议，关注数据传输的格式是否规范，底层的数据传输还是运用了 Socket 和 TCP&#x2F;IP Tomcat 和 Servlet 的关系：Servlet 的运行环境叫做 Web 容器或 Servlet 服务器，Tomcat 是 Web 应用服务器，是一个 Servlet&#x2F;JSP 容器。Tomcat 作为 Servlet 容器，负责处理客户请求，把请求传送给 Servlet，并将 Servlet 的响应传送回给客户。而 Servlet 是一种运行在支持 Java 语言的服务器上的组件，Servlet 用来扩展 Java Web 服务器功能，提供非常安全的、可移植的、易于使用的 CGI 替代品 基本介绍Servlet类Servlet是SUN公司提供的一套规范，名称就叫Servlet规范，它也是JavaEE规范之一。通过API来使用Servlet。 Servlet是一个运行在web服务端的java小程序，用于接收和响应客户端的请求。一个服务器包含多个Servlet 通过实现Servlet接口，继承GenericServlet或者HttpServlet，实现Servlet功能 每次请求都会执行service方法，在service方法中还有参数ServletRequest和ServletResponse 支持配置相关功能 执行流程创建 Web 工程 → 编写普通类继承 Servlet 相关类 → 重写方法 Servlet执行过程分析： 通过浏览器发送请求，请求首先到达Tomcat服务器，由服务器解析请求URL，然后在部署的应用列表中找到应用。然后找到web.xml配置文件，在web.xml中找到FirstServlet的配置（&#x2F;），找到后执行service方法，最后由FirstServlet响应客户浏览器。整个过程如下图所示： 实现方式实现 Servlet 功能时，可以选择以下三种方式： 第一种：实现 Servlet 接口，接口中的方法必须全部实现。使用此种方式，表示接口中的所有方法在需求方面都有重写的必要。此种方式支持最大程度的自定义。 第二种：继承 GenericServlet，service 方法必须重写，其他方可根据需求，选择性重写。使用此种方式，表示只在接收和响应客户端请求这方面有重写的需求，而其他方法可根据实际需求选择性重写，使我们的开发Servlet变得简单。但是，此种方式是和 HTTP 协议无关的。 第三种：继承 HttpServlet，它是 javax.servlet.http 包下的一个抽象类，是 GenericServlet 的子类。选择继承 HttpServlet 时，需要重写 doGet 和 doPost 方法，来接收 get 方式和 post 方式的请求，不要覆盖 service 方法。使用此种方式，表示我们的请求和响应需要和 HTTP 协议相关，我们是通过 HTTP 协议来访问。每次请求和响应都符合 HTTP 协议的规范。请求的方式就是 HTTP 协议所支持的方式（GET POST PUT DELETE TRACE OPTIONS HEAD )。 相关问题异步处理Servlet 3.0 中的异步处理指的是允许Servlet重新发起一条新线程去调用 耗时业务方法，这样就可以避免等待 生命周期servlet从创建到销毁的过程： 出生：（初始化）请求第一次到达 Servlet 时，创建对象，并且初始化成功。Only one time 活着：（服务）服务器提供服务的整个过程中，该对象一直存在，每次只是执行 service 方法 死亡：（销毁）当服务停止时，或者服务器宕机时，对象删除， serrvlet生命周期方法:init(ServletConfig config) → service(ServletRequest req, ServletResponse res) → destroy() 默认情况下, 有了第一次请求, 会调用 init() 方法进行初始化【调用一次】，任何一次请求，都会调用 service() 方法处理这个请求，服务器正常关闭或者项目从服务器移除, 调用 destory() 方法进行销毁【调用一次】 扩展：servlet 是单例多线程的，尽量不要在 servlet 里面使用全局(成员)变量，可能会导致线程不安全 单例：Servlet 对象只会创建一次，销毁一次，Servlet 对象只有一个实例。 多线程：服务器会针对每次请求, 开启一个线程调用 service() 方法处理这个请求 线程安全Servlet运用了单例模式，整个应用中只有一个实例对象，所以需要分析这个唯一的实例中的类成员是否线程安全 12345678910111213141516171819202122232425262728public class ServletDemo extends HttpServlet&#123; //1.定义用户名成员变量 //private String username = null; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String username = null; //synchronized (this) &#123; //2.获取用户名 username = req.getParameter(&quot;username&quot;); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //3.获取输出流对象 PrintWriter pw = resp.getWriter(); //4.响应给客户端浏览器 pw.print(&quot;Welcome:&quot; + username); //5.关流 pw.close(); //&#125; &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 启动两个浏览器，输入不同的参数(http://localhost:8080/ServletDemo/username=aaa 或者bbb)，访问之后发现输出的结果都是一样，所以出现线程安全问题。 在Servlet中定义了类成员之后，多个浏览器都会共享类成员的数据，其中任何一个线程修改了数据，都会影响其他线程。因此，我们可以认为Servlet它不是线程安全的。因为Servlet是单例，单例对象的类成员只会随类实例化时初始化一次，之后的操作都是改变，而不会重新初始化。 解决办法：如果类成员是共用的，只在初始化时赋值，其余时间都是获取。或者加锁synchronized 映射方式Servlet支持三种映射方式，三种映射方式的优先级为：第一种&gt;第二种&gt;第三种。 具体名称方式这种方式，只有和映射配置一模一样时，Servlet才会接收和响应来自客户端的请求。访问URL：http://localhost:8080/servlet/servletDemo 12345678&lt;servlet&gt; &lt;servlet-name&gt;servletDemo&lt;/servlet-name&gt; &lt;servlet-class&gt;com.itheima.servlet.ServletDemo&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;servletDemo&lt;/servlet-name&gt; &lt;url-pattern&gt;/servletDemo&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; &#x2F;开头+通配符的方式这种方式，只要符合目录结构即可，不用考虑结尾是什么访问URL：http://localhost:8080/servlet/ + 任何字符 12345678&lt;servlet&gt; &lt;servlet-name&gt;servletDemo&lt;/servlet-name&gt; &lt;servlet-class&gt;com.itheima.servlet.ServletDemo&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;servletDemo&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 通配符+固定格式结尾这种方式，只要符合固定结尾格式即可，其前面的访问URI无须关心（注意协议，主机和端口必须正确）访问URL：http://localhost:8080/任何字符任何目录 + .do (http://localhost:8080/seazean/i.do) 12345678&lt;servlet&gt; &lt;servlet-name&gt;servletDemo05&lt;/servlet-name&gt; &lt;servlet-class&gt;com.itheima.servlet.ServletDemo05&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;servletDemo05&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 多路径映射一个Servlet的多种路径配置的支持。给一个Servlet配置多个访问映射，从而根据不同请求的URL实现不同的功能 123456789101112131415161718192021222324252627/*多路映射*/public class ServletDemo06 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; int money = 1000; //获取访问的资源路径 String name = req.getRequestURI(); name = name.substring(name.lastIndexOf(&quot;/&quot;)); if(&quot;/vip&quot;.equals(name)) &#123; //如果访问资源路径是/vip 商品价格为9折 System.out.println(&quot;商品原价为：&quot; + money + &quot;。优惠后是：&quot; + (money*0.9)); &#125; else if(&quot;/svip&quot;.equals(name)) &#123; //如果访问资源路径是/svip 商品价格为5折 System.out.println(&quot;商品原价为：&quot; + money + &quot;。优惠后是：&quot; + (money*0.5)); &#125; else &#123; //如果访问资源路径是其他 商品价格原样显示 System.out.println(&quot;商品价格为：&quot; + money); &#125; &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 12345678910111213141516171819202122232425&lt;!--演示Servlet多路径映射--&gt;&lt;servlet&gt; &lt;servlet-name&gt;vip&lt;/servlet-name&gt; &lt;servlet-class&gt;com.itheima.servlet.ServletDemo06&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;vip&lt;/servlet-name&gt; &lt;url-pattern&gt;/vip&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;servlet&gt; &lt;servlet-name&gt;svip&lt;/servlet-name&gt; &lt;servlet-class&gt;com.itheima.servlet.ServletDemo06&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;svip&lt;/servlet-name&gt; &lt;url-pattern&gt;/svip&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;servlet&gt; &lt;servlet-name&gt;other&lt;/servlet-name&gt; &lt;servlet-class&gt;com.itheima.servlet.ServletDemo06&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;other&lt;/servlet-name&gt; &lt;url-pattern&gt;/other&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 这样就可以根据不同的网页显示不同的数据。 启动时创建 第一种：应用加载时创建Servlet，它的优势是在服务器启动时，就把需要的对象都创建完成了，从而在使用的时候减少了创建对象的时间，提高了首次执行的效率。它的弊端是在应用加载时就创建了Servlet对象，因此，导致内存中充斥着大量用不上的Servlet对象，造成了内存的浪费。 第二种：请求第一次访问是创建Servlet，它的优势就是减少了对服务器内存的浪费，因为一直没有被访问过的Servlet对象都没有创建，因此也提高了服务器的启动时间。而它的弊端就是要在应用加载时就做的初始化操作，它都没法完成，从而要考虑其他技术实现。 在web.xml中是支持对Servlet的创建时机进行配置的，配置的方式如下： 123456789101112&lt;!--配置ServletDemo3--&gt;&lt;servlet&gt; &lt;servlet-name&gt;servletDemo&lt;/servlet-name&gt; &lt;servlet-class&gt;com.itheima.web.servlet.ServletDemo&lt;/servlet-class&gt; &lt;!--配置Servlet的创建顺序，当配置此标签时，Servlet就会改为应用加载时创建 配置项的取值只能是正整数（包括0），数值越小，表明创建的优先级越高--&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;servletDemo&lt;/servlet-name&gt; &lt;url-pattern&gt;/servletDemo&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 默认Servlet默认 Servlet 是由服务器提供的一个 Servlet，它配置在 Tomcat 的 conf 目录下的 web.xml 中。 它的映射路径是&lt;url-pattern&gt;/&lt;url-pattern&gt;，我们在发送请求时，首先会在我们应用中的 web.xml 中查找映射配置。但是当找不到对应的 Servlet 路径时，就去找默认的 Servlet，由默认 Servlet 处理。 ServletConfigServletConfig 是 Servlet 的配置参数对象。在 Servlet 规范中，允许为每个 Servlet 都提供一些初始化配置，每个 Servlet 都有自己的ServletConfig，作用是在 Servlet 初始化期间，把一些配置信息传递给 Servlet 生命周期：在初始化阶段读取了 web.xml 中为 Servlet 准备的初始化配置，并把配置信息传递给 Servlet，所以生命周期与 Servlet 相同。如果 Servlet 配置了 &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;，ServletConfig 也会在应用加载时创建。 获取 ServletConfig：在 init 方法中为 ServletConfig 赋值 常用API： String getInitParameter(String name)：根据初始化参数的名称获取参数的值，根据，获取 Enumeration&lt;String&gt; getInitParameterNames() : 获取所有初始化参数名称的枚举(遍历方式看例子) ServletContext getServletContext() : 获取ServletContext对象 String getServletName() : 获取Servlet名称 代码实现： web.xml 配置：初始化参数使用 &lt;servlet&gt; 标签中的 &lt;init-param&gt; 标签来配置，并且每个 Servlet 都支持有多个初始化参数，并且初始化参数都是以键值对的形式存在的 123456789101112131415161718192021&lt;!--配置ServletDemo8--&gt;&lt;servlet&gt; &lt;servlet-name&gt;servletDemo8&lt;/servlet-name&gt; &lt;servlet-class&gt;com.itheima.web.servlet.ServletDemo8&lt;/servlet-class&gt; &lt;!--配置初始化参数--&gt; &lt;init-param&gt; &lt;!--用于获取初始化参数的key--&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;!--初始化参数的值--&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;!--每个初始化参数都需要用到init-param标签--&gt; &lt;init-param&gt; &lt;param-name&gt;servletInfo&lt;/param-name&gt; &lt;param-value&gt;This is Demo8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;servletDemo8&lt;/servlet-name&gt; &lt;url-pattern&gt;/servletDemo8&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344//演示Servlet的初始化参数对象public class ServletDemo8 extends HttpServlet &#123;\t//定义Servlet配置对象ServletConfig private ServletConfig servletConfig; //在初始化时为ServletConfig赋值 @Override public void init(ServletConfig config) throws ServletException &#123; this.servletConfig = config; &#125; /** * doGet方法输出一句话 */ @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.输出ServletConfig System.out.println(servletConfig); //2.获取Servlet的名称 String servletName= servletConfig.getServletName(); System.out.println(servletName); //3.获取字符集编码 String encoding = servletConfig.getInitParameter(&quot;encoding&quot;); System.out.println(encoding); //4.获取所有初始化参数名称的枚举 Enumeration&lt;String&gt; names = servletConfig.getInitParameterNames(); //遍历names while(names.hasMoreElements())&#123; //取出每个name String name = names.nextElement(); //根据key获取value String value = servletConfig.getInitParameter(name); System.out.println(&quot;name:&quot;+name+&quot;,value:&quot;+value); &#125; //5.获取ServletContext对象 ServletContext servletContext = servletConfig.getServletContext(); System.out.println(servletContext); &#125; //调用doGet方法 @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 效果： ServletContextServletContext 对象是应用上下文对象。服务器为每一个应用都创建了一个 ServletContext 对象，ServletContext 属于整个应用，不局限于某个 Servlet，可以实现让应用中所有 Servlet 间的数据共享。 上下文代表了程序当下所运行的环境，联系整个应用的生命周期与资源调用，是程序可以访问到的所有资源的总和，资源可以是一个变量，也可以是一个对象的引用 生命周期： 出生：应用一加载，该对象就被创建出来。一个应用只有一个实例对象（Servlet 和 ServletContext 都是单例的） 活着：只要应用一直提供服务，该对象就一直存在。 死亡：应用被卸载（或者服务器停止），该对象消亡。 域对象：指的是对象有作用域，即有作用范围，可以实现数据共享，不同作用范围的域对象，共享数据的能力不一样。 Servlet 规范中，共有4个域对象，ServletContext 是其中一个，web 应用中最大的作用域，叫 application 域，可以实现整个应用间的数据共享功能。 数据共享： 获取ServletContext： Java 项目继承 HttpServlet，HttpServlet 继承 GenericServlet，GenericServlet 中有一个方法可以直接使用 123public ServletContext getServletContext() &#123; return this.getServletConfig().getServletContext();&#125; ServletRequest 类方法： 1ServletContext getServletContext()//获取ServletContext对象 常用API： String getInitParameter(String name) : 根据名称获取全局配置的参数 String getContextPath : 获取当前应用访问的虚拟目录 String getRealPath(String path) : 根据虚拟目录获取应用部署的磁盘绝对路径 void setAttribute(String name, Object object) : 向应用域对象中存储数据 Object getAttribute(String name) : 根据名称获取域对象中的数据，没有则返回null void removeAttribute(String name) : 根据名称移除应用域对象中的数据 代码实现： web.xml配置：配置的方式，需要在&lt;web-app&gt;标签中使用&lt;context-param&gt;来配置初始化参数，它的配置是针对整个应用的配置，被称为应用的初始化参数配置。 123456789101112&lt;!--配置应用初始化参数--&gt;&lt;context-param&gt; &lt;!--用于获取初始化参数的key--&gt; &lt;param-name&gt;servletContextInfo&lt;/param-name&gt; &lt;!--初始化参数的值--&gt; &lt;param-value&gt;This is application scope&lt;/param-value&gt;&lt;/context-param&gt;&lt;!--每个应用初始化参数都需要用到context-param标签--&gt;&lt;context-param&gt; &lt;param-name&gt;globalEncoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt;&lt;/context-param&gt; 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class ServletContextDemo extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //获取ServletContext对象 ServletContext context = getServletContext(); //获取全局配置的globalEncoding String value = context.getInitParameter(&quot;globalEncoding&quot;); System.out.println(value);//UTF-8 //获取应用的访问虚拟目录 String contextPath = context.getContextPath(); System.out.println(contextPath);//servlet //根据虚拟目录获取应用部署的磁盘绝对路径 //获取b.txt文件的绝对路径 web目录下 String b = context.getRealPath(&quot;/b.txt&quot;); System.out.println(b); //获取c.txt文件的绝对路径 /WEB-INF目录下 String c = context.getRealPath(&quot;/WEB-INF/c.txt&quot;); System.out.println(c); //获取a.txt文件的绝对路径 //src目录下 String a = context.getRealPath(&quot;/WEB-INF/classes/a.txt&quot;); System.out.println(a); //向域对象中存储数据 context.setAttribute(&quot;username&quot;,&quot;zhangsan&quot;); //移除域对象中username的数据 //context.removeAttribute(&quot;username&quot;); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125;//E:\\Database\\Java\\Project\\JavaEE\\out\\artifacts\\Servlet_war_exploded\\b.txt//E:\\Database\\Java\\Project\\JavaEE\\out\\artifacts\\Servlet_war_exploded\\WEB-INF\\c.txt//E:\\Database\\Java\\Project\\JavaEE\\out\\artifacts\\Servlet_war_exploded\\WEB-INF\\classes\\a.txt 注解开发Servlet3.0 版本！不需要配置 web.xml 注解案例 123456789101112@WebServlet(&quot;/servletDemo1&quot;)public class ServletDemo1 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doPost(req,resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;Servlet Demo1 Annotation&quot;); &#125;&#125; WebServlet注解（@since Servlet 3.0 (Section 8.1.1)） 12345678910111213141516171819202122232425262728293031323334@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface WebServlet &#123; //指定Servlet的名称。相当于xml配置中&lt;servlet&gt;标签下的&lt;servlet-name&gt; String name() default &quot;&quot;; //用于映射Servlet访问的url映射，相当于xml配置时的&lt;url-pattern&gt; String[] value() default &#123;&#125;; //相当于xml配置时的&lt;url-pattern&gt; String[] urlPatterns() default &#123;&#125;;\t//用于配置Servlet的启动时机，相当于xml配置的&lt;load-on-startup&gt; int loadOnStartup() default -1; //用于配置Servlet的初始化参数，相当于xml配置的&lt;init-param&gt; WebInitParam[] initParams() default &#123;&#125;; //用于配置Servlet是否支持异步，相当于xml配置的&lt;async-supported&gt; boolean asyncSupported() default false; //用于指定Servlet的小图标 String smallIcon() default &quot;&quot;; //用于指定Servlet的大图标 String largeIcon() default &quot;&quot;; //用于指定Servlet的描述信息 String description() default &quot;&quot;; //用于指定Servlet的显示名称 String displayName() default &quot;&quot;;&#125; 手动创建容器：（了解） Request请求响应Web服务器收到客户端的http请求，会针对每一次请求，分别创建一个用于代表请求的request对象、和代表响应的response对象。 请求对象请求：客户机希望从服务器端索取一些资源，向服务器发出询问 请求对象：在 JavaEE 工程中，用于发送请求的对象，常用的对象是 ServletRequest 和 HttpServletRequest ，它们的区是是否与 HTTP 协议有关 Request 作用： 操作请求三部分(行,头,体) 请求转发 作为域对象存数据 请求路径 方法 作用 String getLocalAddr() 获取本机（服务器）地址 String getLocalName() 获取本机（服务器）名称 int getLocalPort() 获取本机（服务器）端口 String getRemoteAddr() 获取访问者IP String getRemoteHost 获取访问者主机 int getRemotePort() 获取访问者端口 String getMethod(); 获得请求方式 String getRequestURI() 获取统一资源标识符（&#x2F;request&#x2F;servletDemo01） String getRequestURL() 获取统一资源定位符（http://localhost:8080/request/servletDemo01） String getQueryString() 获取请求消息的数据（GET方式 URL中带参字符串：username&#x3D;aaa&amp;password&#x3D;123） String getContextPath() 获取虚拟目录名称（&#x2F;request） String getServletPath 获取Servlet映射路径（或@WebServlet值: &#x2F;servletDemo01） String getRealPath(String path) 根据虚拟目录获取应用部署的磁盘绝对路径 URL &#x3D; URI + HOST URL &#x3D; HOST + ContextPath + ServletPath 获取请求头 方法 作用 String getHeader(String name) 获得指定请求头的值。如果没有该请求头返回null，有多个值返回第一个 Enumeration getHeaders(String name) 获取指定请求头的多个值 Enumeration getHeaderNames() 获取所有请求头名称的枚举 1234567891011121314151617181920@WebServlet(&quot;/servletDemo02&quot;)public class ServletDemo02 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.根据请求头名称获取一个值 String connection = req.getHeader(&quot;connection&quot;); System.out.println(connection);//keep-alive //2.根据请求头名称获取多个值 Enumeration&lt;String&gt; values = req.getHeaders(&quot;accept-encoding&quot;); while(values.hasMoreElements()) &#123; String value = values.nextElement(); System.out.println(value);//gzip, deflate, br &#125; &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 请求参数请求参数请求参数是正文部分标签内容，标签属性action&#x3D;”&#x2F;request&#x2F;servletDemo08”，服务器URI 法名 作用 String getParameter(String name) 获得指定参数名的值如果没有该参数则返回null，如果有多个获得第一个 String[] getParameterValues(String name) 获得指定参数名所有的值。此方法为复选框提供的 Enumeration getParameterNames() 获得所有参数名 Map&lt;String,String[]&gt; getParameterMap() 获得所有的请求参数键值对（key&#x3D;value） 封装参数封装请求参数到类对象： 直接封装：有参构造或者set方法 1234567891011121314151617181920212223@WebServlet(&quot;/servletDemo04&quot;)public class ServletDemo04 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.获取所有的数据 String username = req.getParameter(&quot;username&quot;); String password = req.getParameter(&quot;password&quot;); String[] hobbies = req.getParameterValues(&quot;hobby&quot;); //2.封装学生对象 Student stu = new Student(username,password,hobbies); //3.输出对象 System.out.println(stu); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 123456public class Student &#123; private String username; private String password; private String[] hobby; &#125; 1234567891011121314151617&lt;!--register.html--&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;注册页面&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action=&quot;/request/servletDemo05&quot; method=&quot;get&quot; autocomplete=&quot;off&quot;&gt; 姓名：&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt; &lt;br&gt; 密码：&lt;input type=&quot;password&quot; name=&quot;password&quot;&gt; &lt;br&gt; 爱好：&lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;study&quot;&gt;学习 &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;game&quot;&gt;游戏 &lt;br&gt; &lt;button type=&quot;submit&quot;&gt;注册&lt;/button&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 反射方式： 表单&lt;input&gt;标签的name属性取值，必须和实体类中定义的属性名称一致 12345678910111213141516171819202122232425262728protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.获取请求正文的映射关系 Map&lt;String, String[]&gt; map = req.getParameterMap(); //2.封装学生对象 Student stu = new Student(); //2.1遍历集合 for(String name : map.keySet()) &#123; String[] value = map.get(name); try &#123; //2.2获取Student对象的属性描述器 //参数一：指定获取xxx属性的描述器 //参数二：指定字节码文件 PropertyDescriptor pd = new PropertyDescriptor(name,stu.getClass()); //2.3获取对应的setXxx方法 Method writeMethod = pd.getWriteMethod(); //2.4执行方法 if(value.length &gt; 1) &#123; writeMethod.invoke(stu,(Object)value); &#125;else &#123; writeMethod.invoke(stu,value); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; //3.输出对象 System.out.println(stu);&#125; commons-beanutils封装 1234567891011121314protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.获取所有的数据 Map&lt;String, String[]&gt; map = req.getParameterMap(); //2.封装学生对象 Student stu = new Student(); try &#123; BeanUtils.populate(stu,map); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; //3.输出对象 System.out.println(stu);&#125; 流获取数据ServletInputStream getInputStream() : 获取请求字节输入流对象BufferedReader getReader() : 获取请求缓冲字符输入流对象 1234567891011121314151617181920212223242526@WebServlet(&quot;/servletDemo07&quot;)public class ServletDemo07 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //字符流(必须是post方式) /*BufferedReader br = req.getReader(); String line; while((line = br.readLine()) != null) &#123; System.out.println(line); &#125;*/ //br.close(); //字节流 ServletInputStream is = req.getInputStream(); byte[] arr = new byte[1024]; int len; while((len = is.read(arr)) != -1) &#123; System.out.println(new String(arr,0,len)); &#125; //is.close(); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 12&lt;form action=&quot;/request/servletDemo07&quot; method=&quot;get&quot; autocomplete=&quot;off&quot;&gt;&lt;/form&gt; 请求域请求域request 域：可以在一次请求范围内进行共享数据 方法 作用 void setAttribute(String name, Object value) 向请求域对象中存储数据 Object getAttribute(String name) 通过名称获取请求域对象的数据 void removeAttribute(String name) 通过名称移除请求域对象的数据 请求转发请求转发：客户端的一次请求到达后，需要借助其他 Servlet 来实现功能，进行请求转发。特点： 浏览器地址栏不变 域对象中的数据不丢失 负责转发的 Servlet 转发前后响应正文会丢失 由转发目的地来响应客户端 HttpServletRequest 类方法： RequestDispatcher getRequestDispatcher(String path) : 获取任务调度对象 RequestDispatcher 类方法： void forward(ServletRequest request, ServletResponse response) : 实现转发，将请求从 Servlet 转发到服务器上的另一个资源（Servlet，JSP 文件或 HTML 文件） 过程：浏览器访问 http://localhost:8080/request/servletDemo09，/servletDemo10也会执行 12345678910111213141516@WebServlet(&quot;/servletDemo09&quot;)public class ServletDemo09 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //设置共享数据 req.setAttribute(&quot;encoding&quot;,&quot;gbk&quot;); //获取请求调度对象 RequestDispatcher rd = req.getRequestDispatcher(&quot;/servletDemo10&quot;); //实现转发功能 rd.forward(req,resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 1234567891011121314151617@WebServlet(&quot;/servletDemo10&quot;)public class ServletDemo10 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //获取共享数据 Object encoding = req.getAttribute(&quot;encoding&quot;); System.out.println(encoding);//gbk System.out.println(&quot;servletDemo10执行了...&quot;); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 请求包含请求包含：合并其他的 Servlet 中的功能一起响应给客户端。特点： 浏览器地址栏不变 域对象中的数据不丢失 被包含的 Servlet 响应头会丢失 请求转发的注意事项：负责转发的 Servlet，转发前后的响应正文丢失，由转发目的地来响应浏览器 请求包含的注意事项：被包含者的响应消息头丢失，因为它被包含者包含起来了 HttpServletRequest 类方法： RequestDispatcher getRequestDispatcher(String path) : 获取任务调度对象 RequestDispatcher 类方法： void include(ServletRequest request, ServletResponse response) : 实现包含。包括响应中资源的内容（servlet，JSP页面，HTML文件）。 12345678910111213141516171819202122232425262728@WebServlet(&quot;/servletDemo11&quot;)public class ServletDemo11 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;servletDemo11执行了...&quot;);//执行了 //获取请求调度对象 RequestDispatcher rd = req.getRequestDispatcher(&quot;/servletDemo12&quot;); //实现包含功能 rd.include(req,resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125;**********************************************************************************@WebServlet(&quot;/servletDemo12&quot;)public class ServletDemo12 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;servletDemo12执行了...&quot;);//输出了 &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 乱码问题请求体 POST：void setCharacterEncoding(String env)：设置请求体的编码 1234567891011121314151617@WebServlet(&quot;/servletDemo08&quot;)public class ServletDemo08 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //设置编码格式 req.setCharacterEncoding(&quot;UTF-8&quot;); String username = req.getParameter(&quot;username&quot;); System.out.println(username); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; GET：Tomcat8.5 版本及以后，Tomcat 服务器已经帮我们解决 Response响应对象响应，服务器把请求的处理结果告知客户端 响应对象：在 JavaEE 工程中，用于发送响应的对象 协议无关的对象标准是：ServletResponse 接口 协议相关的对象标准是：HttpServletResponse 接口 Response 的作用： 操作响应的三部分(行, 头, 体) 请求重定向 操作响应行 方法 说明 int getStatus() Gets the current status code of this response void setStatus(int sc) Sets the status code for this response 状态码：（HTTP–&gt;相应部分） 状态码 说明 1xx 消息 2xx 成功 3xx 重定向 4xx 客户端错误 5xx 服务器错误 操作响应体字节流响应响应体对应乱码问题 项目中常用的编码格式是UTF-8，而浏览器默认使用的编码是gbk。导致乱码！ 解决方式： 一：修改浏览器的编码格式(不推荐，不能让用户做修改的动作) 二：通过输出流写出一个标签：&lt;meta http-equiv&#x3D;’content-type’content&#x3D;’text&#x2F;html;charset&#x3D;UTF-8’&gt; 三：指定响应头信息：response.setHeader(“Content-Type”,”text&#x2F;html;charset&#x3D;UTF-8”) 四：response.setContentType(“text&#x2F;html;charset&#x3D;UTF-8”) 常用API： ServletOutputStream getOutputStream() : 获取响应字节输出流对象 void setContenType(&quot;text/html;charset=UTF-8&quot;) : 设置响应内容类型，解决中文乱码问题 1234567891011121314151617181920@WebServlet(&quot;/servletDemo01&quot;)public class ServletDemo01 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.设置响应内容类型 resp.setContentType(&quot;text/html;charset=UTF-8&quot;); //2.通过响应对象获取字节输出流对象 ServletOutputStream sos = resp.getOutputStream(); //3.定义消息 String str = &quot;你好&quot;; //4.通过字节流输出对象 sos.write(str.getBytes(&quot;UTF-8&quot;)); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 字符流响应response得到的字符流和字节流互斥，只能选其一，response获取的流不用关闭，由服务器关闭即可。 常用API： PrintWriter getWriter() : 获取响应字节输出流对象，可以发送标签 void setContenType(&quot;text/html;charset=UTF-8&quot;) : 设置响应内容类型，解决中文乱码问题 12345678protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String str = &quot;你好&quot;; //解决中文乱码 resp.setContentType(&quot;text/html;charset=UTF-8&quot;); //获取字符流对象 PrintWriter pw = resp.getWriter(); pw.write(str);&#125; 响应图片响应图片到浏览器 123456789101112131415161718192021222324252627@WebServlet(&quot;/servletDemo03&quot;)public class ServletDemo03 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.通过文件的相对路径来获取文件的绝对路径 String realPath = getServletContext().getRealPath(&quot;/img/hm.png&quot;); //E:\\Project\\JavaEE\\out\\artifacts\\Response_war_exploded\\img\\hm.png System.out.println(realPath); //2.创建字节输入流对象，关联图片路径 BufferedInputStream bis = new BufferedInputStream(new FileInputStream(realPath)); //3.通过响应对象获取字节输出流对象 ServletOutputStream sos = resp.getOutputStream(); //4.循环读写 byte[] arr = new byte[1024]; int len; while((len = bis.read(arr)) != -1) &#123; sos.write(arr,0,len); &#125; &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 操作响应头常用方法响应头: 是服务器指示浏览器去做什么 方法 说明 String getHeader(String name) 获取指定响应头的内容 Collection getHeaders(String name) 获取指定响应头的多个值 Collection getHeaderNames() 获取所有响应头名称的枚举 void setHeader(String name, String value) 设置响应头 void setDateHeader(String name, long date) 设置具有给定名称和日期值的响应消息头 void sendRedirect(String location) 设置重定向 setHeader常用响应头： Expires：设置缓存时间 Refresh：定时跳转 Location：重定向地址 Content-Disposition: 告诉浏览器下载 Content-Type：设置响应内容的MIME类型(服务器告诉浏览器内容的类型) 控制缓存缓存：对于不经常变化的数据，我们可以设置合理的缓存时间，防止浏览器频繁的请求服务器。 12345678910111213141516171819@WebServlet(&quot;/servletDemo04&quot;)public class ServletDemo04 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String news = &quot;设置缓存时间&quot;; //设置缓存时间，缓存一小时 resp.setDateHeader(&quot;Expires&quot;,System.currentTimeMillis()+1*60*60*1000L); //设置编码格式 resp.setContentType(&quot;text/html;charset=UTF-8&quot;); //写出数据 resp.getWriter().write(news); System.out.println(&quot;aaa&quot;);//只输出一次，不能刷新，必须从网址直接进入 &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 定时刷新定时刷新：过了指定时间后，页面进行自动跳转 格式：setHeader(&quot;Refresh&quot;, &quot;3;URL=https://www.baidu.com&quot;&quot;); Refresh设置的时间单位是秒，如果刷新到其他地址，需要在时间后面拼接上地址 12345678910111213141516171819@WebServlet(&quot;/servletDemo05&quot;)public class ServletDemo05 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String news = &quot;您的用户名或密码错误，3秒后自动跳转到登录页面...&quot;; //设置编码格式 resp.setContentType(&quot;text/html;charset=UTF-8&quot;); //写出数据 resp.getWriter().write(news); //设置响应消息头定时刷新 resp.setHeader(&quot;Refresh&quot;,&quot;3;URL=/response/login.html&quot;); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 下载文件1234567891011121314151617181920212223242526272829303132333435363738394041@WebServlet(&quot;/servletDemo06&quot;)public class ServletDemo06 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.创建字节输入流对象，关联读取的文件 String realPath = getServletContext().getRealPath(&quot;/img/hm.png&quot;);//绝对路径 BufferedInputStream bis = new BufferedInputStream(new FileInputStream(realPath)); //2.设置响应头支持的类型 应用支持的类型为字节流 /* Content-Type 消息头名称 支持的类型 application/octet-stream 消息头参数 应用类型为字节流 */ resp.setHeader(&quot;Content-Type&quot;,&quot;application/octet-stream&quot;); //3.设置响应头以下载方式打开 以附件形式处理内容 /* Content-Disposition 消息头名称 处理的形式 attachment;filename= 消息头参数 附件形式进行处理 */ resp.setHeader(&quot;Content-Disposition&quot;,&quot;attachment;filename=&quot; + System.currentTimeMillis() + &quot;.png&quot;); //4.获取字节输出流对象 ServletOutputStream sos = resp.getOutputStream(); //5.循环读写文件 byte[] arr = new byte[1024]; int len; while((len = bis.read(arr)) != -1) &#123; sos.write(arr,0,len); &#125; //6.释放资源 bis.close(); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 重定向实现重定向请求重定向：客户端的一次请求到达后，需要借助其他 Servlet 来实现功能。特点： 重定向两次请求 重定向的地址栏路径改变 重定向的路径写绝对路径（带域名 &#x2F;ip 地址，如果是同一个项目，可以省略域名 &#x2F;ip 地址） 重定向的路径可以是项目内部的,也可以是项目以外的（百度） 重定向不能重定向到 WEB-INF 下的资源 把数据存到 request 域里面，重定向不可用 实现方式： 方式一： 设置响应状态码：resp.setStatus(302) 设置重定向的路径（响应到哪里，通过响应头 location 来指定） response.setHeader(&quot;Location&quot;,&quot;http://www.baidu.com&quot;); response.setHeader(&quot;Location&quot;,&quot;/response/servletDemo08); 方式二： resp.sendRedirect(&quot;重定向的路径&quot;); 1234567891011121314151617@WebServlet(&quot;/servletDemo07&quot;)public class ServletDemo07 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //设置请求域数据 req.setAttribute(&quot;username&quot;,&quot;zhangsan&quot;); //设置重定向 resp.sendRedirect(req.getContextPath() + &quot;/servletDemo07&quot;); // resp.sendRedirect(&quot;https://www.baidu.com&quot;); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 123456789@WebServlet(&quot;/servletDemo08&quot;)public class ServletDemo08 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;servletDemo08执行了...&quot;); Object username = req.getAttribute(&quot;username&quot;); System.out.println(username); &#125;&#125; 重定向和转发请求重定向跳转的特点： 重定向是由浏览器发起的，在这个过程中浏览器会发起两次请求 重定向可以跳转到任意服务器的资源，但是无法跳转到WEB-INF中的资源 重定向不能和请求域对象共享数据，数据会丢失 重定向浏览器的地址栏中的地址会变成跳转到的路径 请求转发跳转的特点： 请求转发是由服务器发起的，在这个过程中浏览器只会发起一次请求 请求转发只能跳转到本项目的资源，但是可以跳转到WEB-INF中的资源 请求转发可以和请求域对象共享数据，数据不会丢失 请求转发浏览器地址栏不变 路径问题完整URL地址： 协议：http:&#x2F;&#x2F; 服务器主机地址：127.0.0.1 or localhost 服务器端口号：8080 项目的虚拟路径(部署路径)：&#x2F;response 具体的项目上资源路径 &#x2F;login.html or Demo 的Servlet映射路径 相对路径： 不以”&#x2F;“开头的路径写法，它是以目标路径相对当前文件的路径，其中”..”表示上一级目录。 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;hello world....&lt;/h1&gt; &lt;!-- 目标资源的url: http://localhost:8080/response/demo05 当前资源的url: http://localhost:8080/response/pages/demo.html 相对路径的优劣: 1. 优势: 无论部署的项目名怎么改变，我的路径都不需要改变 2. 劣势: 如果当前资源的位置发生改变，那么相对路径就必定要发生改变--&gt; &lt;a href=&quot;../demo05&quot;&gt;访问ServletDemo05&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 绝对路径： 绝对路径就是以”&#x2F;“开头的路径写法，项目部署的路径 Cookie会话技术会话：浏览器和服务器之间的多次请求和响应 浏览器和服务器可能产生多次的请求和响应，从浏览器访问服务器开始，到访问服务器结束（关闭浏览器、到了过期时间），这期间产生的多次请求和响应加在一起称为浏览器和服务器之间的一次对话 作用：保存用户各自的数据（以浏览器为单位），在多次请求间实现数据共享 常用的会话管理技术： Cookie：客户端会话管理技术，用户浏览的信息以键值对（key&#x3D;value）的形式保存在浏览器上。如果没有关闭浏览器，再次访问服务器，会把 cookie 带到服务端，服务端就可以做相应的处理 Session：服务端会话管理技术。当客户端第一次请求 session 对象时，服务器为每一个浏览器开辟一块内存空间，并将通过特殊算法算出一个 session 的 ID，用来标识该 session 对象。由于内存空间是每一个浏览器独享的，所有用户在访问的时候，可以把信息保存在 session 对象中，同时服务器会把 sessionId 写到 cookie 中，再次访问的时候，浏览器会把 cookie(sessionId) 带过来，找到对应的 session 对象即可 tomcat 生成的 sessionID 叫做 jsessionID 两者区别： Cookie 存储在客户端中，而 Session 存储在服务器上，相对来说 Session 安全性更高。如果要在 Cookie 中存储一些敏感信息，不要直接写入 Cookie，应该将 Cookie 信息加密然后使用到的时候再去服务器端解密 Cookie 一般用来保存用户信息，在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候就不需要重新登录，因为用户登录的时候可以存放一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可（为了安全考虑，重新登录一般要将 Token 重写），所以登录一次网站后访问网站其他页面不需要重新登录 Session 通过服务端记录用户的状态，服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户 Cookie 只能存储 ASCII 码，而 Session 可以存储任何类型的数据 参考文章：https://blog.csdn.net/weixin_43625577/article/details/92393581 基本介绍Cookie：客户端会话管理技术，把要共享的数据保存到了客户端（也就是浏览器端）。每次请求时，把会话信息带到服务器，从而实现多次请求的数据共享。 作用：保存客户浏览器访问网站的相关内容（需要客户端不禁用 Cookie），从而在每次访问同一个内容时，先从本地缓存获取，使资源共享，提高效率。 基本使用常用API Cookie属性： 属性名称 属性作用 是否重要 name cookie的名称 必要属性 value cookie的值（不能是中文） 必要属性 path cookie的路径 重要 domain cookie的域名 重要 maxAge cookie的生存时间 重要 version cookie的版本号 不重要 comment cookie的说明 不重要 注意：Cookie 有大小，个数限制。每个网站最多只能存20个 Cookie，且大小不能超过 4kb。同时所有网站的 Cookie 总数不超过300个。 Cookie类API： Cookie(String name, String value) : 构造方法创建 Cookie 对象 Cookie 属性对应的 set 和 get 方法，name 属性被 final 修饰，没有 set 方法 HttpServletResponse 类 API： void addCookie(Cookie cookie)：向客户端添加 Cookie，Adds cookie to the response HttpServletRequest类API： Cookie[] getCookies()：获取所有的 Cookie 对象，client sent with this request 有效期如果不设置过期时间，表示这个 Cookie 生命周期为浏览器会话期间，只要关闭浏览器窗口 Cookie 就消失，这种生命期为浏览会话期的 Cookie 被称为会话 Cookie，会话 Cookie 一般不保存在硬盘上而是保存在内存里。 如果设置过期时间，浏览器就会把 Cookie 保存到硬盘上，关闭后再次打开浏览器，这些 Cookie 依然有效直到超过设定的过期时间。存储在硬盘上的 Cookie 可以在不同的浏览器进程间共享，比如两个 IE 窗口，而对于保存在内存的 Cookie，不同的浏览器有不同的处理方式 设置 Cookie 存活时间 API：void setMaxAge(int expiry) -1：默认，代表 Cookie 数据存到浏览器关闭（保存在浏览器文件中） 0：代表删除 Cookie，如果要删除 Cookie 要确保路径一致 正整数：以秒为单位保存数据有有效时间（把缓存数据保存到磁盘中） 12345678910111213141516171819202122232425262728293031323334353637@WebServlet(&quot;/servletDemo01&quot;)public class ServletDemo01 extends HttpServlet&#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.通过响应对象写出提示信息 resp.setContentType(&quot;text/html;charset=UTF-8&quot;); PrintWriter pw = resp.getWriter(); pw.write(&quot;欢迎访问本网站，您的最后访问时间为：&lt;br&gt;&quot;); //2.创建Cookie对象，用于记录最后访问时间 Cookie cookie = new Cookie(&quot;time&quot;,System.currentTimeMillis()+&quot;&quot;); //3.设置最大存活时间 cookie.setMaxAge(3600); //cookie.setMaxAge(0); // 立即清除 //4.将cookie对象添加到客户端 resp.addCookie(cookie); //5.获取cookie Cookie[] cookies = req.getCookies(); for(Cookie c : cookies) &#123; if(&quot;time&quot;.equals(c.getName())) &#123; //6.获取cookie对象中的value，进行写出 String value = c.getValue(); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); pw.write(sdf.format(Long.parseLong(value))); &#125; &#125; &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 有效路径setPath(String url) : Cookie 设置有效路径 有效路径作用 : 保证不会携带别的网站&#x2F;项目里面的 Cookie 到我们自己的项目 路径不一样，Cookie 的 key 可以相同 保证自己的项目可以合理的利用自己项目的 Cookie 判断路径是否携带 Cookie：请求资源 URI.startWith(cookie的path)，返回 true 就带 访问URL URI部分 Cookie的Path 是否携带Cookie 能否取到Cookie servletDemo02 &#x2F;servlet&#x2F;servletDemo02 &#x2F;servlet&#x2F; 带 能取到 servletDemo03 &#x2F;servlet&#x2F;servletDemo03 &#x2F;servlet&#x2F; 带 能取到 servletDemo04 &#x2F;servlet&#x2F;aaa&#x2F;servletDemo04 &#x2F;servlet&#x2F; 带 能取到 servletDemo05 &#x2F;bbb&#x2F;servletDemo04 &#x2F;servlet&#x2F; 不带 不能取到 只有当访问资源的 url 包含此 cookie 的有效 path 的时候，才会携带这个 cookie 想要当前项目下的 Servlet 可以使用该 cookie，一般设置：cookie.setPath(request.getContextPath()) 安全性如果 Cookie 中设置了 HttpOnly 属性，通过 js 脚本将无法读取到 cookie 信息，这样能有效的防止 XSS 攻击，窃取 cookie 内容，这样就增加了安全性，即便是这样，也不要将重要信息存入cookie。 XSS 全称 Cross SiteScript，跨站脚本攻击，是Web程序中常见的漏洞，XSS 属于被动式且用于客户端的攻击方式，所以容易被忽略其危害性。其原理是攻击者向有 XSS 漏洞的网站中输入(传入)恶意的 HTML 代码，当其它用户浏览该网站时，这段HTML代码会自动执行，从而达到攻击的目的。如盗取用户 Cookie、破坏页面结构、重定向到其它网站等。 Session基本介绍Session：服务器端会话管理技术，本质也是采用客户端会话管理技术，不过在客户端保存的是一个特殊标识，共享的数据保存到了服务器的内存对象中。每次请求时，会将特殊标识带到服务器端，根据标识来找到对应的内存空间，从而实现数据共享。简单说它就是一个服务端会话对象，用于存储用户的会话数据 Session 域（会话域）对象是 Servlet 规范中四大域对象之一，并且它也是用于实现数据共享的 域对象 功能 创建 销毁 使用场景 ServletContext 应用域 服务器启动 服务器关闭 在整个应用之间实现数据共享（记录网站访问次数，聊天室） ServletRequest 请求域 请求到来 响应了这个请求 在当前请求或者请求转发之间实现数据共享 HttpSession 会话域 getSession() session过期，调用invalidate()，服务器关闭 在当前会话范围中实现数据共享，可以在多次请求中实现数据共享。（验证码校验, 保存用户登录状态等） 基本使用获取会话HttpServletRequest类获取Session： 方法 说明 HttpSession getSession() 获取HttpSession对象 HttpSession getSession(boolean creat) 获取HttpSession对象，未获取到是否自动创建 常用API 方法 说明 void setAttribute(String name, Object value) 设置会话域中的数据 Object getAttribute(String name) 获取指定名称的会话域数据 Enumeration getAttributeNames() 获取所有会话域所有属性的名称 void removeAttribute(String name) 移除会话域中指定名称的数据 String getId() 获取唯一标识名称，Jsessionid的值 void invalidate() 立即失效session 实现会话通过第一个Servlet设置共享的数据用户名，并在第二个Servlet获取到 项目执行完以后，去浏览器抓包，Request Headers 中的 Cookie JSESSIONID的值是一样的 12345678910111213141516171819@WebServlet(&quot;/servletDemo01&quot;)public class ServletDemo01 extends HttpServlet&#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.获取请求的用户名 String username = req.getParameter(&quot;username&quot;); //2.获取HttpSession的对象 HttpSession session = req.getSession(); System.out.println(session); System.out.println(session.getId()); //3.将用户名信息添加到共享数据中 session.setAttribute(&quot;username&quot;,username); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 1234567891011121314151617@WebServlet(&quot;/servletDemo02&quot;)public class ServletDemo02 extends HttpServlet&#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.获取HttpSession对象 HttpSession session = req.getSession(); //2.获取共享数据 Object username = session.getAttribute(&quot;username&quot;); //3.将数据响应给浏览器 resp.getWriter().write(username+&quot;&quot;); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 生命周期Session 的创建：一个常见的错误是以为 Session 在有客户端访问时就被创建，事实是直到某 server 端程序（如 Servlet）调用 HttpServletRequest.getSession(true) 这样的语句时才会被创建 Session 在以下情况会被删除： 程序调用 HttpSession.invalidate() 距离上一次收到客户端发送的 session id 时间间隔超过了 session 的最大有效时间 服务器进程被停止 注意事项： 客户端只保存 sessionID 到 cookie 中，而不会保存 session 关闭浏览器只会使存储在客户端浏览器内存中的 cookie 失效，不会使服务器端的 session 对象失效，同样也不会使已经保存到硬盘上的持久化cookie消失 打开两个浏览器窗口访问应用程序会使用的是不同的session，通常 session cookie 是不能跨窗口使用，当新开了一个浏览器窗口进入相同页面时，系统会赋予一个新的 session id，实现跨窗口信息共享： 先把 session id 保存在 persistent cookie 中（通过设置session的最大有效时间） 在新窗口中读出来，就可以得到上一个窗口的 session id，这样通过 session cookie 和 persistent cookie 的结合就可以实现跨窗口的会话跟踪 会话问题禁用Cookie浏览器禁用Cookie解决办法： 方式一：通过提示信息告知用户 123456789101112131415161718@WebServlet(&quot;/servletDemo03&quot;)public class ServletDemo03 extends HttpServlet&#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1.获取HttpSession对象 HttpSession session = req.getSession(false); System.out.println(session); if(session == null) &#123; resp.setContentType(&quot;text/html;charset=UTF-8&quot;); resp.getWriter().write(&quot;为了不影响正常的使用，请不要禁用浏览器的Cookie~&quot;); &#125; &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 方式二：访问时拼接 jsessionid 标识，通过 encodeURL() 方法重写地址 123456789@Overrideprotected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; HttpSession session = req.getSession(); //实现url重写 相当于在地址栏后面拼接了一个jsessionid resp.getWriter().write(&quot;&lt;a href=&#x27;&quot;+ resp.encodeURL (&quot;http://localhost:8080/session/servletDemo03&quot;) + &quot;&#x27;&gt;go servletDemo03&lt;/a&gt;&quot;);&#125; 钝化活化Session 存放在服务器端的内存中，可以做持久化管理。 钝化：序列化，持久态。把长时间不用，但还不到过期时间的 HttpSession 进行序列化写到磁盘上。 活化：相反的状态 何时钝化： 当访问量很大时，服务器会根据getLastAccessTime来进行排序，对长时间不用，但是还没到过期时间的HttpSession进行序列化（持久化） 当服务器进行重启的时候，为了保持客户HttpSession中的数据，也要对HttpSession进行序列化（持久化） 注意： HttpSession的持久化由服务器来负责管理，我们不用关心 只有实现了序列化接口的类才能被序列化 JSPJSP概述JSP(Java Server Page)：是一种动态网页技术标准。（页面技术） JSP是基于Java语言的，它的本质就是Servlet，一个特殊的Servlet。 JSP部署在服务器上，可以处理客户端发送的请求，并根据请求内容动态的生成HTML、XML或其他格式文档的Web网页，然后响应给客户端。 类别 适用场景 HTML 开发静态资源，不能包含java代码，无法添加动态数据。 CSS 美化页面 JavaScript 给网页添加动态效果 Servlet 编写java代码，实现后台功能处理，但是很不方便，开发效率低。 JSP 包括了显示页面技术，同时具备Servlet输出动态资源的能力。但是不适合作为控制器来用。 执行原理 新建JavaEE工程，编写index.jsp文件 123456789&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;JSP的入门&lt;/title&gt; &lt;/head&gt; &lt;body&gt; 这是第一个JSP页面 &lt;/body&gt;&lt;/html&gt; 执行过程： 客户端提交请求——Tomcat服务器解析请求地址——找到JSP页面——Tomcat将JSP页面翻译成Servlet的java文件——将翻译好的.java文件编译成.class文件——返回到客户浏览器上 溯源，打开JSP翻译后的Java文件 public final class index_jsp extends org.apache.jasper.runtime.HttpJspBase，public abstract class HttpJspBase extends HttpServlet implements HttpJspPage，HttpJspBase是个抽象类继承HttpServlet，所以JSP本质上继承HttpServlet 在文件中找到了输出页面的代码，本质都是用out.write()输出的JSP语句 总结：JSP它是一个特殊的Servlet，主要是用于展示动态数据。它展示的方式是用流把数据输出出来，而我们在使用JSP时，涉及HTML的部分，都与HTML的用法一致，这部分称为jsp中的模板元素，决定了页面的外观。 JSP语法 JSP注释： 注释类型 方法 作用 JSP注释 &lt;%–注释内容–%&gt; 被jsp注释的部分不会被翻译成.java文件，不会在浏览器上显示 HTML注释 在Jsp中可以使用html的注释，但是只能注释html元素被html注释部分会参与翻译，并且会在浏览器上显示 Java注释 &#x2F;&#x2F;; &#x2F;* *&#x2F; Java代码块 12&lt;% 此处写java代码 %&gt;&lt;%--由tomcat负责翻译，翻译之后是service方法的成员变量--%&gt; JSP表达式 12&lt;%=表达式%&gt;&lt;%--翻译成Service()方法里面的内容,相当于调用out.print()--%&gt; JSP声明 12&lt;%! 声明的变量或方法 %&gt;&lt;%--翻译成Servlet类里面的内容--%&gt; 语法示例： 12345678910111213141516171819202122232425262728293031323334353637&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;jsp语法&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;%--1. 这是注释--%&gt; &lt;%-- 2.java代码块 System.out.println(&quot;Hello JSP&quot;); 普通输出语句，输出在控制台!! out.println(&quot;Hello JSP&quot;);out是JspWriter对象，输出在页面上 --%&gt; &lt;% System.out.println(&quot;Hello JSP&quot;); out.println(&quot;Hello JSP&lt;br&gt;&quot;); String str = &quot;hello&lt;br&gt;&quot;; out.println(str); %&gt; &lt;%-- 3.jsp表达式,相当于 out.println(&quot;Hello&quot;); --%&gt; &lt;%=&quot;Hello&lt;br&gt;&quot;%&gt; &lt;%-- 4.jsp中的声明(变量或方法) 如果加! 代表的是声明的是成员变量 如果不加! 代表的是声明的是局部变量,页面显示abc --%&gt; &lt;%! String s = &quot;abc&quot;;%&gt; &lt;% String s = &quot;def&quot;;%&gt; &lt;%=s%&gt; &lt;%! public void getSum()&#123;&#125;%&gt;&lt;/body&gt;&lt;/html&gt; 123456控制台输出：Hello JSP页面输出：\tHello JSP\thello\tHello\tdef JSP指令 page指令： 1&lt;%@ page 属性名=属性值 属性名=属性值... %&gt; 属性名 作用 contentType 设置响应正文支持的MIME类型和编码格式：contentType&#x3D;”text&#x2F;html;charset&#x3D;UTF-8” language 告知引擎，脚本使用的语言，默认为Java errorPage 当前页面出现异常后跳转的页面 isErrorPage 是否抓住异常。值为true页面中就能使用exception对象，打印异常信息。默认值false import 导入哪些包（类）&lt;%@ page import&#x3D;”java.util.ArrayList” %&gt; session 是否创建HttpSession对象，默认是true buffer 设定JspWriter用s输出jsp内容的缓存大小。默认8kb pageEncoding 翻译jsp时所用的编码格式，pageEncoding&#x3D;”UTF-8”相当于用UTF-8读取JSP isELIgnored 是否忽略EL表达式，默认值是false Note：当使用全局错误页面，就无须配置errorPage实现跳转错误页面，而是由服务器负责跳转到错误页面 配置全局错误页面：web.xml 12345678&lt;error-page&gt; &lt;exception-type&gt;java.lang.Exception&lt;/exception-type&gt; &lt;location&gt;/error.jsp&lt;/location&gt;&lt;/error-page&gt;&lt;error-page&gt; &lt;error-code&gt;404&lt;/error-code&gt; &lt;location&gt;/404.html&lt;/location&gt;&lt;/error-page&gt; include指令：包含其他页面 1&lt;%@include file=&quot;被包含的页面&quot; %&gt; 属性：file，以&#x2F;开头，就代表当前应用 taglib指令：引入外部标签库 1&lt;%taglib uri=&quot;标签库的地址&quot; prefix=&quot;前缀名称&quot;%&gt; html标签和jsp标签不用引入 隐式对象九大隐式对象隐式对象：在jsp中可以不声明就直接使用的对象。它只存在于jsp中，因为java类中的变量必须要先声明再使用。jsp中的隐式对象也并不是未声明，它是在翻译成.java文件时声明的，所以我们在jsp中可以直接使用。 隐式对象名称 类型 备注 request javax.servlet.http.HttpServletRequest response javax.servlet.http.HttpServletResponse session javax.servlet.http.HttpSession Page指令可以控制开关 application javax.servlet.ServletContext page Java.lang.Object 当前jsp对应的servlet引用实例 config javax.servlet.ServletConfig exception java.lang.Throwable page指令有开关 out javax.servlet.jsp.JspWriter 字符输出流，相当于printwriter pageContext javax.servlet.jsp.PageContext 很重要，页面域 PageContext PageContext对象特点： PageContextd对象是JSP独有的对象，Servlet中没有 PageContextd对象是一个页面域（作用范围）对象，还可以操作其他三个域对象中的属性 PageContextd对象可以获取其他八个隐式对象 PageContextd对象是一个局部变量，它的生命周期随着JSP的创建而诞生，随着JSP的结束而消失。每个JSP页面都有一个独立的PageContext PageContext方法如下，页面域操作的方法定义在了PageContext的父类JspContext中 四大域对象 域对象名称 范围 级别 备注 PageContext 页面范围 最小，只能在当前页面用 因范围太小，开发中用的很少 ServletRequest 请求范围 一次请求或当期请求转发用 当请求转发之后，再次转发时请求域丢失 HttpSession 会话范围 多次请求数据共享时使用 多次请求共享数据，但不同的客户端不能共享 ServletContext 应用范围 最大，整个应用都可以使用 尽量少用，如果对数据有修改需要做同步处理 MVC模型M : model， 通常用于封装数据，封装的是数据模型V : view，通常用于展示数据。动态展示用jsp页面，静态数据展示用htmlC : controller，通常用于处理请求和响应，一般指的是Servlet ELEL概述EL表达式：Expression Language，意为表达式语言。它是Servlet规范中的一部分，是JSP2.0规范加入的内容。 EL表达式作用：在JSP页面中获取数据，让JSP脱离java代码块和JSP表达式 EL表达式格式： $&#123;表达式内容&#125; EL表达式特点： 有明确的返回值 把内容输出到页面上 只能在四大域对象中获取数据，不在四大域对象中的数据取不到。 EL用法多种类型EL表达式可以获取不同类型数据，前提是数据放入四大域对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;%@ page import=&quot;bean.Student&quot; %&gt;&lt;%@ page import=&quot;java.util.ArrayList&quot; %&gt;&lt;%@ page import=&quot;java.util.HashMap&quot; %&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;EL表达式获取不同类型数据&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;%--1.获取基本数据类型--%&gt; &lt;% pageContext.setAttribute(&quot;num&quot;,10); %&gt; 基本数据类型：$&#123;num&#125; &lt;br&gt; &lt;%--2.获取自定义对象类型--%&gt; &lt;% Student stu = new Student(&quot;张三&quot;,23); pageContext.setAttribute(&quot;stu&quot;,stu); %&gt; 自定义对象：$&#123;stu&#125; &lt;br&gt; &lt;%--stu.name 实现原理 getName()--%&gt; 学生姓名：$&#123;stu.name&#125; &lt;br&gt; 学生年龄：$&#123;stu.age&#125; &lt;br&gt; &lt;%--3.获取数组类型--%&gt; &lt;% String[] arr = &#123;&quot;hello&quot;,&quot;world&quot;&#125;; pageContext.setAttribute(&quot;arr&quot;,arr); %&gt; 数组：$&#123;arr&#125; &lt;br&gt; 0索引元素：$&#123;arr[0]&#125; &lt;br&gt; 1索引元素：$&#123;arr[1]&#125; &lt;br&gt; &lt;%--4.获取List集合--%&gt; &lt;% ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;aaa&quot;); list.add(&quot;bbb&quot;); pageContext.setAttribute(&quot;list&quot;,list); %&gt; List集合：$&#123;list&#125; &lt;br&gt; 0索引元素：$&#123;list[0]&#125; &lt;br&gt; &lt;%--5.获取Map集合--%&gt; &lt;% HashMap&lt;String,Student&gt; map = new HashMap&lt;&gt;(); map.put(&quot;hm01&quot;,new Student(&quot;张三&quot;,23)); map.put(&quot;hm02&quot;,new Student(&quot;李四&quot;,24)); pageContext.setAttribute(&quot;map&quot;,map); %&gt; Map集合：$&#123;map&#125; &lt;br&gt; 第一个学生对象：$&#123;map.hm01&#125; &lt;br&gt; 第一个学生对象的姓名：$&#123;map.hm01.name&#125;&lt;/body&gt;&lt;/html&gt;&lt;--页面输出效果基本数据类型：10自定义对象：bean.Student@5f8da92c (地址)学生姓名：张三学生年龄：23数组：[Ljava.lang.String;@4b3bd5200索引元素：hello1索引元素：worldList集合：[aaa, bbb]0索引元素：aaaMap集合：&#123;hm01=bean.Student@4768d250, hm02=bean.Student@67f237d9&#125;第一个学生对象：bean.Student@4768d250第一个学生对象的姓名：张三--&gt; 异常问题EL表达式的注意事项： EL表达式没有空指针异常 EL表达式没有数组下标越界 EL表达式没有字符串拼接 12345678910111213141516171819202122232425262728293031323334&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;EL表达式的注意事项&lt;/title&gt; &lt;/head&gt; &lt;body&gt; 第一个：没有空指针异常&lt;br/&gt; &lt;% String str = null; request.setAttribute(&quot;testNull&quot;,str); %&gt; str：$&#123;testNull&#125; &lt;hr/&gt; 第二个：没有数组下标越界&lt;br/&gt; &lt;% String[] strs = new String[]&#123;&quot;a&quot;,&quot;b&quot;,&quot;c&quot;&#125;; request.setAttribute(&quot;strs&quot;,strs); %&gt; 取第一个元素：$&#123;strs[0]&#125;&lt;br/&gt; 取第六个元素：$&#123;strs[5]&#125;&lt;br/&gt; &lt;hr/&gt; 第三个：没有字符串拼接&lt;br/&gt; &lt;%--$&#123;strs[0]+strs[1]&#125;--%&gt; 拼接：$&#123;strs[0]&#125;+$&#123;strs[1]&#125; &lt;%--注意拼接--%&gt; &lt;/body&gt;&lt;/html&gt;&lt;--页面输出效果第一个：没有空指针异常str：第二个：没有数组下标越界取第一个元素：a取第六个元素：第三个：没有字符串拼接拼接：a+b--&gt; 运算符EL表达式中运算符： 关系运算符： 逻辑运算符： 逻辑运算符 说明 &amp;&amp; 或 and 交集 || 或 or 并集 ! 或 not 非 ​ 其他运算符 运算符 作用 empty 1. 判断对象是否为null2. 判断字符串是否为空字符串3. 判断容器元素是否为0 条件 ? 表达式1 : 表达式2 三元运算符，条件?真:假 1234567891011121314151617181920212223&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;EL表达式运算符&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;%--empty--%&gt; &lt;% String str1 = null; String str2 = &quot;&quot;; int[] arr = &#123;&#125;; %&gt; $&#123;empty str1&#125; &lt;br&gt; $&#123;empty str2&#125; &lt;br&gt; $&#123;empty arr&#125; &lt;br&gt; &lt;%--三元运算符。获取性别的数据，在对应的按钮上进行勾选--%&gt; &lt;% pageContext.setAttribute(&quot;gender&quot;,&quot;women&quot;); %&gt; &lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;men&quot; $&#123;gender==&quot;men&quot;?&quot;checked&quot;:&quot;&quot;&#125;&gt;男 &lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;women&quot; $&#123;gender==&quot;women&quot;?&quot;checked&quot;:&quot;&quot;&#125;&gt;女&lt;/body&gt;&lt;/html&gt; 四大域数据EL表达式只能从从四大域中获取数据，调用的就是findAttribute(name,value);方法，根据名称由小到大在域对象中查找，找到就返回，找不到就什么都不显示。 1234567891011121314151617181920&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;EL表达式使用细节&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;%--获取四大域对象中的数据--%&gt; &lt;% //pageContext.setAttribute(&quot;username&quot;,&quot;zhangsan&quot;); request.setAttribute(&quot;username&quot;,&quot;zhangsan&quot;); //session.setAttribute(&quot;username&quot;,&quot;zhangsan&quot;); //application.setAttribute(&quot;username&quot;,&quot;zhangsan&quot;); %&gt; $&#123;username&#125; &lt;br&gt; &lt;%--获取JSP中其他八个隐式对象 获取虚拟目录名称--%&gt; &lt;%= request.getContextPath()%&gt; $&#123;pageContext.request.contextPath&#125;&lt;/body&gt;&lt;/html&gt; EL隐式对象EL表达式隐式对象EL表达式也为我们提供隐式对象，可以让我们不声明直接来使用，需要注意的是，它和JSP的隐式对象不是同一种事物。 EL中的隐式对象 类型 对应JSP隐式对象 备注 PageContext Javax.serlvet.jsp.PageContext PageContext 完全一样 ApplicationScope Java.util.Map 没有 应用层范围 SessionScope Java.util.Map 没有 会话范围 RequestScope Java.util.Map 没有 请求范围 PageScope Java.util.Map 没有 页面层范围 Header Java.util.Map 没有 请求消息头key，值是value（一个） HeaderValues Java.util.Map 没有 请求消息头key，值是数组（一个头多个值） Param Java.util.Map 没有 请求参数key，值是value（一个） ParamValues Java.util.Map 没有 请求参数key，值是数组（一个名称多个值） InitParam Java.util.Map 没有 全局参数，key是参数名称，value是参数值 Cookie Java.util.Map 没有 Key是cookie的名称，value是cookie对象 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;EL表达式11个隐式对象&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;%--pageContext对象 可以获取其他三个域对象和JSP中八个隐式对象--%&gt; $&#123;pageContext.request.contextPath&#125; &lt;br&gt; &lt;%--applicationScope sessionScope requestScope pageScope 操作四大域对象中的数据--%&gt; &lt;% request.setAttribute(&quot;username&quot;,&quot;zhangsan&quot;); %&gt; $&#123;username&#125; &lt;br&gt; $&#123;requestScope.username&#125; &lt;br&gt; &lt;%--header headerValues 获取请求头数据--%&gt; $&#123;header[&quot;connection&quot;]&#125; &lt;br&gt; $&#123;headerValues[&quot;connection&quot;][0]&#125; &lt;br&gt; &lt;%--param paramValues 获取请求参数数据--%&gt; $&#123;param.username&#125; &lt;br&gt; $&#123;paramValues.hobby[0]&#125; &lt;br&gt; $&#123;paramValues.hobby[1]&#125; &lt;br&gt; &lt;%--initParam 获取全局配置参数--%&gt; $&#123;initParam[&quot;pname&quot;]&#125; &lt;br&gt; &lt;%--cookie 获取cookie信息--%&gt; $&#123;cookie&#125; &lt;br&gt; &lt;%--获取Map集合--%&gt; $&#123;cookie.JSESSIONID&#125; &lt;br&gt; &lt;%--获取map集合中第二个元素--%&gt; $&#123;cookie.JSESSIONID.name&#125; &lt;br&gt; &lt;%--获取cookie对象的名称--%&gt; $&#123;cookie.JSESSIONID.value&#125; &lt;%--获取cookie对象的值--%&gt;&lt;/body&gt;&lt;/html&gt;&lt;--页面显示/elzhangsanzhangsankeep-alivekeep-alivebbb&#123;JSESSIONID=javax.servlet.http.Cookie@435c8431, Idea-5a5d203e=javax.servlet.http.Cookie@46be0b58, Idea-be3279e7=javax.servlet.http.Cookie@4ef6e8e8&#125;javax.servlet.http.Cookie@435c8431JSESSIONIDE481B2A845A448AD88A71FD43611FF02 --&gt; 在web.xml配置全局参数 12345678&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app ******&gt; &lt;!--配置全局参数--&gt; &lt;context-param&gt; &lt;param-name&gt;pname&lt;/param-name&gt; &lt;param-value&gt;bbb&lt;/param-value&gt; &lt;/context-param&gt;&lt;/web-app&gt; 获取JSP隐式对象通过获取页面域对象，获取其他JSP八个隐式对象 1234567891011121314&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;EL表达式使用细节&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;%--获取虚拟目录名称--%&gt; &lt;%= request.getContextPath()%&gt; $&#123;pageContext.request.contextPath&#125;&lt;/body&gt;&lt;/html&gt;&lt;--页面显示/el /el--&gt; JSTLJSTL：Java Server Pages Standarded Tag Library，JSP中标准标签库。 作用：提供给开发人员一个标准的标签库，开发人员可以利用这些标签取代JSP页面上的Java代码，从而提高程序的可读性，降低程序的维护难度。 组成 作用 说明 Core 核心标签库 通用逻辑处理 Fmt 国际化有关 需要不同地域显示不同语言时使用 Functions EL函数 EL表达式可以使用的方法 SQL 操作数据库 XML 操作XML 使用：添加jar包，通过taglib导入，prefix属性表示程序调用标签使用的引用名 标签名称 功能分类 分类 作用 &#96;&lt;c:if test&#x3D;”${A&#x3D;&#x3D;B C&#x3D;&#x3D;D}”&gt;&#96; 流程控制 &lt;c:choose&gt; ,&lt;c:when&gt;,&lt;c:otherwise&gt; 流程控制 核心标签库 用于多个条件判断 &lt;c:foreache&gt; 迭代操作 核心标签库 用于循环遍历 流程控制 12345678910111213141516171819202122232425&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;%@taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot;%&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;流程控制&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;%--向域对象中添加成绩数据--%&gt; $&#123;pageContext.setAttribute(&quot;score&quot;,&quot;T&quot;)&#125; &lt;%--对成绩进行判断--%&gt; &lt;c:if test=&quot;$&#123;score eq &#x27;A&#x27;&#125;&quot;&gt; 优秀 &lt;/c:if&gt; &lt;%--对成绩进行多条件判断--%&gt; &lt;c:choose&gt; &lt;c:when test=&quot;$&#123;score eq &#x27;A&#x27;&#125;&quot;&gt;优秀&lt;/c:when&gt; &lt;c:when test=&quot;$&#123;score eq &#x27;B&#x27;&#125;&quot;&gt;良好&lt;/c:when&gt; &lt;c:when test=&quot;$&#123;score eq &#x27;C&#x27;&#125;&quot;&gt;及格&lt;/c:when&gt; &lt;c:when test=&quot;$&#123;score eq &#x27;D&#x27;&#125;&quot;&gt;较差&lt;/c:when&gt; &lt;c:otherwise&gt;成绩非法&lt;/c:otherwise&gt; &lt;/c:choose&gt;&lt;/body&gt;&lt;/html&gt; 迭代操作c:forEach：用来遍历集合，属性： 属性 作用 items 指定要遍历的集合，它可以是用EL表达式取出来的元素 var 把当前遍历的元素放入指定的page域中。var的值是key，遍历的元素是value注意：var不支持EL表达式，只能是字符串常量 begin 开始遍历的索引 end 结束遍历的索引 step 步长，i+&#x3D;step varStatus 它是一个计数器对象，有两个属性，一个是用于记录索引，一个是用于计数。索引是从0开始，计数是从1开始 1234567891011121314151617181920212223&lt;%@ page import=&quot;java.util.ArrayList&quot; %&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;%@taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot;%&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;循环&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;%--向域对象中添加集合--%&gt; &lt;% ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;aa&quot;); list.add(&quot;bb&quot;); list.add(&quot;cc&quot;); list.add(&quot;dd&quot;); pageContext.setAttribute(&quot;list&quot;,list); %&gt; &lt;%--遍历集合--%&gt; &lt;c:forEach items=&quot;$&#123;list&#125;&quot; var=&quot;str&quot;&gt; $&#123;str&#125; &lt;br&gt; &lt;/c:forEach&gt;&lt;/body&gt;&lt;/html&gt; Filter过滤器Filter：过滤器，是 JavaWeb 三大组件之一，另外两个是 Servlet 和 Listener 工作流程：在程序访问服务器资源时，当一个请求到来，服务器首先判断是否有过滤器与去请求资源相关联，如果有过滤器可以将请求拦截下来，完成一些特定的功能，再由过滤器决定是否交给请求资源，如果没有就直接请求资源，响应同理 作用：过滤器一般用于完成通用的操作，例如：登录验证、统一编码处理、敏感字符过滤等 相关类FilterFilter是一个接口，如果想实现过滤器的功能，必须实现该接口 核心方法 方法 说明 void init(FilterConfig filterConfig) 初始化，开启过滤器 void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) 对请求资源和响应资源过滤 void destroy() 销毁过滤器 配置方式 注解方式 12@WebFilter(&quot;/*&quot;)()内填拦截路径，/*代表全部路径 配置文件 12345678&lt;filter&gt; &lt;filter-name&gt;filterDemo01&lt;/filter-name&gt; &lt;filter-class&gt;filter.FilterDemo01&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;filterDemo01&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; FilterChain FilterChain 是一个接口，代表过滤器对象。由Servlet容器提供实现类对象，直接使用即可。 过滤器可以定义多个，就会组成过滤器链 核心方法：void doFilter(ServletRequest request, ServletResponse response) 用来放行方法 如果有多个过滤器，在第一个过滤器中调用下一个过滤器，以此类推，直到到达最终访问资源。如果只有一个过滤器，放行时就会直接到达最终访问资源。 FilterConfigFilterConfig 是一个接口，代表过滤器的配置对象，可以加载一些初始化参数 方法 作用 String getFilterName() 获取过滤器对象名称 String getInitParameter(String name) 获取指定名称的初始化参数的值，不存在返回null Enumeration getInitParameterNames() 获取所有参数的名称 ServletContext getServletContext() 获取应用上下文对象 Filter使用设置页面编码请求先被过滤器拦截进行相关操作 过滤器放行之后执行完目标资源，仍会回到过滤器中 Filter 代码： 123456789101112@WebFilter(&quot;/*&quot;)public class FilterDemo01 implements Filter&#123; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(&quot;filterDemo01拦截到请求...&quot;); //处理乱码 servletResponse.setContentType(&quot;text/html;charset=UTF-8&quot;); //过滤器放行 filterChain.doFilter(servletRequest,servletResponse); System.out.println(&quot;filterDemo1放行之后，又回到了doFilter方法&quot;); &#125;&#125; Servlet 代码： 123456789101112@WebServlet(&quot;/servletDemo01&quot;)public class ServletDemo01 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;servletDemo01执行了...&quot;); resp.getWriter().write(&quot;servletDemo01执行了...&quot;); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doGet(req,resp); &#125;&#125; 控制台输出： 123filterDemo01拦截到请求...servletDemo01执行了...filterDemo1放行之后，又回到了doFilter方法 多过滤器顺序多个过滤器使用的顺序，取决于过滤器映射的顺序。 两个 Filter 代码： 1234567891011121314public class FilterDemo01 implements Filter&#123; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(&quot;filterDemo01执行了...&quot;); filterChain.doFilter(servletRequest,servletResponse); &#125;&#125;public class FilterDemo02 implements Filter&#123; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(&quot;filterDemo02执行了...&quot;); filterChain.doFilter(servletRequest,servletResponse); &#125;&#125; Servlet代码：System.out.println(&quot;servletDemo02执行了...&quot;); web.xml配置： 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;filterDemo01&lt;/filter-name&gt; &lt;filter-class&gt;filter.FilterDemo01&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;filterDemo01&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter&gt; &lt;filter-name&gt;filterDemo02&lt;/filter-name&gt; &lt;filter-class&gt;filter.FilterDemo02&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;filterDemo02&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 控制台输出： 123filterDemo01执行了filterDemo02执行了servletDemo02执行了... 在过滤器的配置中，有过滤器的声明和过滤器的映射两部分，到底是声明决定顺序，还是映射决定顺序呢？ 答案是：&lt;filter-mapping&gt;的配置前后顺序决定过滤器的调用顺序，也就是由映射配置顺序决定。 Filter生命周期创建：当应用加载时实例化对象并执行init()初始化方法 服务：对象提供服务的过程，执行doFilter()方法 销毁：当应用卸载时或服务器停止时对象销毁，执行destroy()方法 Filter代码： 123456789101112131415161718192021222324252627@WebFilter(&quot;/*&quot;)public class FilterDemo03 implements Filter&#123; /* 初始化方法 */ @Override public void init(FilterConfig filterConfig) &#123; System.out.println(&quot;对象初始化成功了...&quot;); &#125; /* 提供服务方法 */ @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(&quot;filterDemo03执行了...&quot;); //过滤器放行 filterChain.doFilter(servletRequest,servletResponse); &#125; /* 对象销毁方法，关闭Tomcat服务器 */ @Override public void destroy() &#123; System.out.println(&quot;对象销毁了...&quot;); &#125;&#125; Servlet 代码：System.out.println(&quot;servletDemo03执行了...&quot;); 控制台输出： 1234对象初始化成功了...filterDemo03执行了...servletDemo03执行了...对象销毁了 FilterConfig使用Filter初始化函数init的参数是FilterConfig 对象 Filter代码： 1234567891011121314151617181920212223public class FilterDemo04 implements Filter&#123;\t//初始化方法 @Override public void init(FilterConfig filterConfig) &#123; System.out.println(&quot;对象初始化成功了...&quot;); //获取过滤器名称 String filterName = filterConfig.getFilterName(); System.out.println(filterName); //根据name获取value String username = filterConfig.getInitParameter(&quot;username&quot;); System.out.println(username); &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(&quot;filterDemo04执行了...&quot;); filterChain.doFilter(servletRequest,servletResponse); &#125; @Override public void destroy() &#123;&#125;&#125; web.xml配置 123456789101112&lt;filter&gt; &lt;filter-name&gt;filterDemo04&lt;/filter-name&gt; &lt;filter-class&gt;filter.FilterDemo04&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;username&lt;/param-name&gt; &lt;param-value&gt;zhangsan&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;filterDemo04&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 控制台输出： 123对象初始化成功了...filterDemo04zhangsan Filter案例在访问html，js，image时，不需要每次都重新发送请求读取资源，就可以通过设置响应消息头的方式，设置缓存时间。但是如果每个Servlet都编写相同的代码，显然不符合我们统一调用和维护的理念。 静态资源设置缓存时间：html设置为1小时，js设置为2小时，css设置为3小时 配置过滤器 12345678910111213141516171819202122232425262728&lt;filter&gt; &lt;filter-name&gt;StaticResourceNeedCacheFilter&lt;/filter-name&gt; &lt;filter-class&gt;filter.StaticResourceNeedCacheFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;html&lt;/param-name&gt; &lt;param-value&gt;3&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;js&lt;/param-name&gt; &lt;param-value&gt;4&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;css&lt;/param-name&gt; &lt;param-value&gt;5&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;StaticResourceNeedCacheFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*.html&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;StaticResourceNeedCacheFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*.js&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;StaticResourceNeedCacheFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*.css&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 编写过滤器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class StaticResourceNeedCacheFilter implements Filter &#123;\tprivate FilterConfig filterConfig;//获取初始化参数 @Override\tpublic void init(FilterConfig filterConfig) throws ServletException &#123; this.filterConfig = filterConfig; &#125; @Override public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; //1.把doFilter的请求和响应对象转换成跟http协议有关的对象 HttpServletRequest request; HttpServletResponse response; try &#123; request = (HttpServletRequest) req; response = (HttpServletResponse) res; &#125; catch (ClassCastException e) &#123; throw new ServletException(&quot;non-HTTP request or response&quot;); &#125; //2.获取请求资源URI String uri = request.getRequestURI(); //3.得到请求资源到底是什么类型 String extend = uri.substring(uri.lastIndexOf(&quot;.&quot;)+1);//我们只需要判断它是不是html,css,js。其他的不管 //4.判断到底是什么类型的资源 long time = 60*60*1000; if(&quot;html&quot;.equals(extend))&#123; //html 缓存1小时 String html = filterConfig.getInitParameter(&quot;html&quot;); time = time*Long.parseLong(html); &#125;else if(&quot;js&quot;.equals(extend))&#123; //js 缓存2小时 String js = filterConfig.getInitParameter(&quot;js&quot;); time = time*Long.parseLong(js); &#125;else if(&quot;css&quot;.equals(extend))&#123; //css 缓存3小时 String css = filterConfig.getInitParameter(&quot;css&quot;); time = time*Long.parseLong(css); &#125; //5.设置响应消息头 response.setDateHeader(&quot;Expires&quot;, System.currentTimeMillis()+time); //6.放行 chain.doFilter(request, response); &#125; @Override public void destroy() &#123;&#125;&#125; 拦截行为Filter过滤器默认拦截的是请求，但是在实际开发中，我们还有请求转发和请求包含，以及由服务器触发调用的全局错误页面。默认情况下过滤器是不参与过滤的，需要配置web.xml 开启功能后，当访问页面发生相关行为后，会执行过滤器的操作 五种拦截行为： 1234567891011121314151617181920212223&lt;!--配置过滤器--&gt;&lt;filter&gt; &lt;filter-name&gt;FilterDemo5&lt;/filter-name&gt; &lt;filter-class&gt;filter.FilterDemo5&lt;/filter-class&gt; &lt;!--配置开启异步支持，当dispatcher配置ASYNC时，需要配置此行--&gt; &lt;async-supported&gt;true&lt;/async-supported&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;FilterDemo5&lt;/filter-name&gt; &lt;url-pattern&gt;/error.jsp&lt;/url-pattern&gt; &lt;!--&lt;url-pattern&gt;/index.jsp&lt;/url-pattern&gt;--&gt; &lt;!--过滤请求：默认值。--&gt; &lt;dispatcher&gt;REQUEST&lt;/dispatcher&gt; &lt;!--过滤全局错误页面：开启后，当由服务器调用全局错误页面时，过滤器工作--&gt; &lt;dispatcher&gt;ERROR&lt;/dispatcher&gt; &lt;!--过滤请求转发：开启后，当请求转发时，过滤器工作。--&gt; &lt;dispatcher&gt;FORWARD&lt;/dispatcher&gt; &lt;!--过滤请求包含：当请求包含时，过滤器工作。它只能过滤动态包含，jsp的include指令是静态包含--&gt; &lt;dispatcher&gt;INCLUDE&lt;/dispatcher&gt; &lt;!--过滤异步类型，它要求我们在filter标签中配置开启异步支持--&gt; &lt;dispatcher&gt;ASYNC&lt;/dispatcher&gt;&lt;/filter-mapping&gt; web.xml： 1234567891011&lt;filter&gt; &lt;filter-name&gt;FilterDemo5&lt;/filter-name&gt; &lt;filter-class&gt;filter.FilterDemo5&lt;/filter-class&gt; &lt;!--配置开启异步支持，当dispatcher配置ASYNC时，需要配置此行--&gt; &lt;async-supported&gt;true&lt;/async-supported&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;FilterDemo5&lt;/filter-name&gt; &lt;url-pattern&gt;/error.jsp&lt;/url-pattern&gt; &lt;dispatcher&gt;ERROR&lt;/dispatcher&gt;&lt;filter-mapping&gt; ServletDemo03： 1234protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;servletDemo03执行了...&quot;); int i = 1/ 0; &#125; FilterDemo05： 12345678public class FilterDemo05 implements Filter&#123; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(&quot;filterDemo05执行了...&quot;); //放行 filterChain.doFilter(servletRequest,servletResponse); &#125;&#125; 访问URL：http://localhost:8080/filter/servletDemo03 控制台输出（注意输出顺序）： 12servletDemo03执行了...filterDemo05执行了... 对比Servlet 方法&#x2F;类型 Servlet Filter 备注 初始化 方法 void init(ServletConfig); void init(FilterConfig); 几乎一样，都是在web.xml中配置参数，用该对象的方法可以获取到。 提供服务方法 void service(request,response); void dofilter(request,response,FilterChain) Filter比Servlet多了一个FilterChain，它不仅能完成Servlet的功能，而且还可以决定程序是否能继续执行。所以过滤器比Servlet更为强大。 在Struts2中，核心控制器就是一个过滤器。 销毁方法 void destroy(); void destroy(); 方法&#x2F;类型 Listener观察者设计者所有的监听器都是基于观察者设计模式的。 观察者模式通常由以下三部分组成： 事件源：触发事件的对象。 事件：触发的动作，里面封装了事件源。 监听器：当事件源触发事件后，可以完成的功能。一般是一个接口，由使用者来实现。（此处的思想还涉及了一个策略模式） 监听器分类在程序当中，我们可以对：对象的创建销毁、域对象中属性的变化、会话相关内容进行监听。 Servlet规范中共计8个监听器，监听器都是以接口形式提供，具体功能需要我们自己完成 监听对象 ServletContextListener：用于监听ServletContext对象的创建和销毁 方法 作用 void contextInitialized(ServletContextEvent sce) 对象创建时执行该方法 void contextDestroyed(ServletContextEvent sce) 对象销毁时执行该方法 参数ServletContextEvent 代表事件对象，事件对象中封装了事件源ServletContext，真正的事件指的是创建或者销毁ServletContext对象的操作 HttpSessionListener：用于监听HttpSession对象的创建和销毁 方法 作用 void sessionCreated(HttpSessionEvent se) 对象创建时执行该方法 void sessionDestroyed(HttpSessionEvent se) 对象销毁时执行该方法 参数HttpSessionEvent 代表事件对象，事件对象中封装了事件源HttpSession，真正的事件指的是创建或者销毁HttpSession对象的操作 ServletRequestListener：用于监听ServletRequest对象的创建和销毁 方法 作用 void requestInitialized(ServletRequestEvent sre) 对象创建时执行该方法 void requestDestroyed(ServletRequestEvent sre) 对象销毁时执行该方法 参数ServletRequestEvent 代表事件对象，事件对象中封装了事件源ServletRequest，真正的事件指的是创建或者销毁ServletRequest对象的操作 监听域对象属性 ServletContextAttributeListener：用于监听ServletContext应用域中属性的变化 方法 作用 void attributeAdded(ServletContextAttributeEvent event) 域中添加属性时执行该方法 void attributeRemoved(ServletContextAttributeEvent event) 域中移除属性时执行该方法 void attributeReplaced(ServletContextAttributeEvent event) 域中替换属性时执行该方法 参数ServletContextAttributeEvent 代表事件对象，事件对象中封装了事件源ServletContext，真正的事件指的是添加、移除、替换应用域中属性的操作 HttpSessionAttributeListener：用于监听HttpSession会话域中属性的变化 方法 作用 void attributeAdded(HttpSessionBindingEvent event) 域中添加属性时执行该方法 void attributeRemoved(HttpSessionBindingEvent event) 域中移除属性时执行该方法 void attributeReplaced(HttpSessionBindingEvent event) 域中替换属性时执行该方法 参数HttpSessionBindingEvent 代表事件对象，事件对象中封装了事件源HttpSession，真正的事件指的是添加、移除、替换应用域中属性的操作 ServletRequestAttributeListener：用于监听ServletRequest请求域中属性的变化 方法 作用 void attributeAdded(ServletRequestAttributeEvent srae) 域中添加属性时执行该方法 void attributeRemoved(ServletRequestAttributeEvent srae) 域中移除属性时执行该方法 void attributeReplaced(ServletRequestAttributeEvent srae) 域中替换属性时执行该方法 参数ServletRequestAttributeEvent 代表事件对象，事件对象中封装了事件源ServletRequest，真正的事件指的是添加、移除、替换应用域中属性的操作 页面域对象没有监听器 感知型监听器监听会话相关的感知型监听器，和会话域相关的两个感知型监听器是无需配置（注解）的，可以直接编写代码 HttpSessionBindingListener：用于感知对象和会话域绑定的监听器 方法 作用 void valueBound(HttpSessionBindingEvent event) 数据添加到会话域中(绑定)时执行该方法 void valueUnbound(HttpSessionBindingEvent event) 数据从会话域中移除(解绑)时执行该方法 参数HttpSessionBindingEvent 代表事件对象，事件对象中封装了事件源HttpSession，真正的事件指的是添加、移除、替换应用域中属性的操作 HttpSessionActivationListener：用于感知会话域中对象和钝化和活化的监听器 方法 作用 void sessionWillPassivate(HttpSessionEvent se) 会话域中数据钝化时执行该方法 void sessionDidActivate(HttpSessionEvent se) 会话域中数据活化时执行该方法 监听器使用ServletContextListenerServletContext对象的创建和销毁的监听器 注解方式： 12345678910111213141516@WebListenerpublic class ServletContextListenerDemo implements ServletContextListener &#123; //创建时执行此方法 @Override public void contextInitialized(ServletContextEvent sce) &#123; System.out.println(&quot;监听到对象的创建....&quot;);//启动服务器就创建 ServletContext servletContext = sce.getServletContext(); System.out.println(servletContext); &#125; //销毁时执行的方法 @Override public void contextDestroyed(ServletContextEvent sce) &#123; System.out.println(&quot;监听到对象的销毁...&quot;);//关闭服务器就销毁 &#125;&#125; 配置web.xml 123456&lt;web-app&gt;&lt;!--配置监听器--&gt; &lt;listener&gt; &lt;listener-class&gt;listener.ServletContextAttributeListenerDemo&lt;/listener-class&gt; &lt;/listener&gt;&lt;/web-app&gt; ServletContextAttributeListener应用域对象中的属性变化的监听器 12345678910111213141516171819202122232425262728293031323334353637383940414243public class ServletContextAttributeListenerDemo implements ServletContextAttributeListener&#123; /* 向应用域对象中添加属性时执行此方法 */ @Override public void attributeAdded(ServletContextAttributeEvent scae) &#123; System.out.println(&quot;监听到了属性的添加...&quot;); //获取应用域对象 ServletContext servletContext = scae.getServletContext(); //获取属性 Object value = servletContext.getAttribute(&quot;username&quot;); System.out.println(value);//zhangsan &#125; /* 向应用域对象中替换属性时执行此方法 */ @Override public void attributeReplaced(ServletContextAttributeEvent scae) &#123; System.out.println(&quot;监听到了属性的替换...&quot;); //获取应用域对象 ServletContext servletContext = scae.getServletContext(); //获取属性 Object value = servletContext.getAttribute(&quot;username&quot;); System.out.println(value);//lisi &#125; /* 向应用域对象中移除属性时执行此方法 */ @Override public void attributeRemoved(ServletContextAttributeEvent scae) &#123; System.out.println(&quot;监听到了属性的移除...&quot;); //获取应用域对象 ServletContext servletContext = scae.getServletContext(); //获取属性 Object value = servletContext.getAttribute(&quot;username&quot;); System.out.println(value);//null &#125;&#125; 123456789101112131415161718192021222324public class ServletContextListenerDemo implements ServletContextListener&#123; //ServletContext对象创建的时候执行此方法 @Override public void contextInitialized(ServletContextEvent sce) &#123; System.out.println(&quot;监听到了对象的创建...&quot;); //获取对象 ServletContext servletContext = sce.getServletContext(); //添加属性 servletContext.setAttribute(&quot;username&quot;,&quot;zhangsan&quot;); //替换属性 servletContext.setAttribute(&quot;username&quot;,&quot;lisi&quot;); //移除属性 servletContext.removeAttribute(&quot;username&quot;); &#125; //ServletContext对象销毁的时候执行此方法 @Override public void contextDestroyed(ServletContextEvent sce) &#123; System.out.println(&quot;监听到了对象的销毁...&quot;); &#125;&#125; 控制台输出： 1234567监听到了对象的创建...监听到了属性的添加...zhangsan监听到了属性的替换lisi监听到属性的移除null JS概述JavaScript 是一种客户端脚本语言。运行在客户端浏览器中，每一个浏览器都具备解析 JavaScript 的引擎。 脚本语言：不需要编译，就可以被浏览器直接解析执行了。 作用：增强用户和 HTML 页面的交互过程，让页面产生动态效果，增强用户的体验。 组成部分：ECMAScript、DOM、BOM 开发环境搭建：安装Node.js，是JavaScript运行环境 语法引入引入HTML文件 内部方式：标签 12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;JS快速入门&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;!--html语句--&gt;&lt;/body&gt;&lt;script&gt; // JS语句&lt;/script&gt; &lt;/html&gt; 外部方式 创建js文件：my.js 1alert(&quot;Hello&quot;);//js语句 在html中引用外部js文件 12345&lt;body&gt; &lt;!--html语句--&gt;&lt;/body&gt;&lt;script src=&quot;js/my.js&quot;&gt;&lt;/script&gt;&lt;html&gt; 注释 单行注释 1// 注释的内容 多行注释 123/*注释的内容*/ 输入输出 输入框：prompt(“提示内容”); 弹出警告框：alert(“提示内容”); 控制台输出：console.log(“显示内容”); 页面内容输出：document.write(“显示内容”); 注：document.write(&quot;&lt;br/&gt;&quot;)换行，通常输出数据后跟br标签 变量常量JavaScript 属于弱类型的语言，定义变量时不区分具体的数据类型 定义局部变量：let 变量名 = 值; 123let name = &quot;张三&quot;;let age = 23;document.write(name + &quot;,&quot; + age +&quot;&lt;br&gt;&quot;); 定义全局变量：变量名 = 值; 1234&#123; l2 = &quot;bb&quot;;&#125;document.write(l2 + &quot;&lt;br&gt;&quot;); 定义常量：const 常量名 = 值;常量不能被重新赋值 123const PI = 3.1415926;//PI = 3.15; document.write(PI); 数据类型 数据类型 说明 boolean 布尔类型，true或false null 声明null值的特殊关键字 undefined 代表变量未定义 number 整数或浮点数 string 字符串 bigint 大整数，例如：let num = 10n; typeof 用于判断变量的数据类型 12let age = 18; document.write(typeof(age)); // number 运算符 算术运算符 算术运算符 说明 + 加法运算 - 减法运算 * 乘法运算 / 除法运算 % 取余数 ++ 自增 -- 自减 赋值运算符 赋值运算符 说明 = 加法运算 += 减法运算 -= 乘法运算 *= 除法运算 /= 取余数 %= 自增 比较运算符 比较运算符 说明 == 判断值是否相等 === 判断数据类型和值是否相等 > 大于 >= 大于等于"},{"title":"java","path":"/wiki/javaNode/Java.html","content":"SE基础数据变量类型静态变量只有一个，成员变量是类中的变量，局部变量是方法中的变量 初学时笔记内容参考视频：https://www.bilibili.com/video/BV1TE41177mP，随着学习的深入又增加很多知识 数据类型基本类型Java 语言提供了八种基本类型。六种数字类型（四个整数型，两个浮点型），一种字符类型，还有一种布尔型 byte： byte 数据类型是 8 位、有符号的，以二进制补码表示的整数，8 位一个字节，首位是符号位 最小值是 -128（-2^7）、最大值是 127（2^7-1） 默认值是 0 byte 类型用在大型数组中节约空间，主要代替整数，byte 变量占用的空间只有 int 类型的四分之一 例子：byte a = 100，byte b = -50 short： short 数据类型是 16 位、有符号的以二进制补码表示的整数 最小值是 -32768（-2^15）、最大值是 32767（2^15 - 1） short 数据类型也可以像 byte 那样节省空间，一个 short 变量是 int 型变量所占空间的二分之一 默认值是 0 例子：short s = 1000，short r = -20000 int： int 数据类型是 32 位 4 字节、有符号的以二进制补码表示的整数 最小值是 -2,147,483,648（-2^31）、最大值是 2,147,483,647（2^31 - 1） 一般地整型变量默认为 int 类型 默认值是 0 例子：int a = 100000, int b = -200000 long： long 数据类型是 64 位 8 字节、有符号的以二进制补码表示的整数 最小值是 -9,223,372,036,854,775,808（-2^63）、最大值是 9,223,372,036,854,775,807（2^63 -1） 这种类型主要使用在需要比较大整数的系统上 默认值是 0L 例子： long a = 100000L，Long b = -200000L，L 理论上不分大小写，但是若写成 I 容易与数字 1 混淆，不容易分辩 float： float 数据类型是单精度、32 位、符合 IEEE 754 标准的浮点数 float 在储存大型浮点数组的时候可节省内存空间 默认值是 0.0f 浮点数不能用来表示精确的值，如货币 例子：float f1 = 234.5F double： double 数据类型是双精度、64 位、符合 IEEE 754 标准的浮点数 浮点数的默认类型为 double 类型 double 类型同样不能表示精确的值，如货币 默认值是 0.0d 例子：double d1 = 123.4 boolean： boolean 数据类型表示一位的信息 只有两个取值：true 和 false JVM 规范指出 boolean 当做 int 处理，boolean 数组当做 byte 数组处理，这样可以得出 boolean 类型单独使用占了 4 个字节，在数组中是 1 个字节 默认值是 false 例子：boolean one = true char： char 类型是一个单一的 16 位两个字节的 Unicode 字符 最小值是 \\u0000（即为 0） 最大值是 \\uffff（即为 65535） char 数据类型可以存储任何字符 例子：char c = &#39;A&#39;，char c = &#39;张&#39; 上下转型 float 与 double： Java 不能隐式执行向下转型，因为这会使得精度降低，但是可以向上转型 1234//1.1字面量属于double类型，不能直接将1.1直接赋值给 float 变量，因为这是向下转型float f = 1.1;//报错//1.1f 字面量才是 float 类型float f = 1.1f; 12345float f1 = 1.234f;double d1 = f1;double d2 = 1.23;float f2 = (float) d2;//向下转型需要强转 12345int i1 = 1245;long l1 = i1;long l2 = 1234;int i2 = (int) l2; 隐式类型转换： 字面量 1 是 int 类型，比 short 类型精度要高，因此不能隐式地将 int 类型向下转型为 short 类型 使用 +&#x3D; 或者 ++ 运算符会执行类型转换： 1234short s1 = 1;s1 += 1;\t//s1++;//上面的语句相当于将 s1 + 1 的计算结果进行了向下转型s1 = (short) (s1 + 1); 引用类型引用数据类型：类，接口，数组都是引用数据类型，又叫包装类 包装类的作用： 包装类作为类首先拥有了 Object 类的方法 包装类作为引用类型的变量可以存储 null 值 12345678910基本数据类型 包装类（引用数据类型）byte Byteshort Shortint Integerlong Longfloat Floatdouble Doublechar Characterboolean Boolean Java 为包装类做了一些特殊功能，具体来看特殊功能主要有： 可以把基本数据类型的值转换成字符串类型的值 调用 toString() 方法 调用 Integer.toString(基本数据类型的值) 得到字符串 直接把基本数据类型 + 空字符串就得到了字符串（推荐使用） 把字符串类型的数值转换成对应的基本数据类型的值（重要） Xxx.parseXxx(“字符串类型的数值”) → Integer.parseInt(numStr) Xxx.valueOf(“字符串类型的数值”) → Integer.valueOf(numStr) （推荐使用） 123456789101112131415161718192021222324public class PackageClass02 &#123; public static void main(String[] args) &#123; // 1.把基本数据类型的值转成字符串 Integer it = 100 ; // a.调用toString()方法。 String itStr = it.toString(); System.out.println(itStr+1);//1001 // b.调用Integer.toString(基本数据类型的值)得到字符串。 String itStr1 = Integer.toString(it); System.out.println(itStr1+1);//1001 // c.直接把基本数据类型+空字符串就得到了字符串。 String itStr2 = it + &quot;&quot;; System.out.println(itStr2+1);// 1001 // 2.把字符串类型的数值转换成对应的基本数据类型的值 String numStr = &quot;23&quot;; int numInt = Integer.valueOf(numStr); System.out.println(numInt+1);//24 String doubleStr = &quot;99.9&quot;; double doubleDb = Double.valueOf(doubleStr); System.out.println(doubleDb+0.1);//100.0 &#125;&#125; 类型对比 有了基本数据类型，为什么还要引用数据类型？ 引用数据类型封装了数据和处理该数据的方法，比如 Integer.parseInt(String) 就是将 String 字符类型数据转换为 Integer 整型 Java 中大部分类和方法都是针对引用数据类型，包括泛型和集合 引用数据类型那么好，为什么还用基本数据类型？ 引用类型的对象要多储存对象头，对基本数据类型来说空间浪费率太高。逻辑上来讲，Java 只有包装类就够了，为了运行速度，需要用到基本数据类型；优先考虑运行效率的问题，所以二者同时存在是合乎情理的 Java 集合不能存放基本数据类型，只存放对象的引用？ 不能放基本数据类型是因为不是 Object 的子类。泛型思想，如果不用泛型要写很多参数类型不同的但功能相同的函数（方法重载） &#x3D;&#x3D; &#x3D;&#x3D; 比较基本数据类型：比较的是具体的值&#x3D;&#x3D; 比较引用数据类型：比较的是对象地址值 装箱拆箱自动装箱：可以直接把基本数据类型的值或者变量赋值给包装类 自动拆箱：可以把包装类的变量直接赋值给基本数据类型 123456789101112131415161718public class PackegeClass &#123; public static void main(String[] args) &#123; int a = 12 ; Integer a1 = 12 ; // 自动装箱 Integer a2 = a ; // 自动装箱 Integer a3 = null; // 引用数据类型的默认值可以为null Integer c = 100 ; int c1 = c ; // 自动拆箱 Integer it = Integer.valueOf(12); // 手工装箱！ // Integer it1 = new Integer(12); // 手工装箱！ Integer it2 = 12; Integer it3 = 111 ; int it33 = it3.intValue(); // 手工拆箱 &#125;&#125; 自动装箱反编译后底层调用 Integer.valueOf() 实现，源码： 123456public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) // 【缓存池】，本质上是一个数组 return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 自动拆箱调用 java.lang.Integer#intValue，源码： 123public int intValue() &#123; return value;&#125; 缓存池new Integer(123) 与 Integer.valueOf(123) 的区别在于： new Integer(123)：每次都会新建一个对象 Integer.valueOf(123)：会使用缓存池中的对象，多次调用取得同一个对象的引用 123456Integer x = new Integer(123);Integer y = new Integer(123);System.out.println(x == y); // falseInteger z = Integer.valueOf(123);Integer k = Integer.valueOf(123);System.out.println(z == k); // true valueOf() 方法的实现比较简单，就是先判断值是否在缓存池中，如果在的话就直接返回缓存池的内容。编译器会在自动装箱过程调用 valueOf() 方法，因此多个值相同且值在缓存池范围内的 Integer 实例使用自动装箱来创建，那么就会引用相同的对象 基本类型对应的缓存池如下： Boolean values true and false all byte values Short values between -128 and 127 Long values between -128 and 127 Integer values between -128 and 127 Character in the range \\u0000 to \\u007F (0 and 127) 在 jdk 1.8 所有的数值类缓冲池中，Integer 的缓存池 IntegerCache 很特殊，这个缓冲池的下界是 -128，上界默认是 127，但是上界是可调的，在启动 JVM 时通过 AutoBoxCacheMax=&lt;size&gt; 来指定这个缓冲池的大小，该选项在 JVM 初始化的时候会设定一个名为 java.lang.Integer.IntegerCache 系统属性，然后 IntegerCache 初始化的时候就会读取该系统属性来决定上界 1234567891011Integer x = 100; // 自动装箱，底层调用 Integer.valueOf(1)Integer y = 100;System.out.println(x == y); // trueInteger x = 1000;Integer y = 1000;System.out.println(x == y); // false，因为缓存池最大127int x = 1000;Integer y = 1000;System.out.println(x == y); // true，因为 y 会调用 intValue 【自动拆箱】返回 int 原始值进行比较 输入数据语法：Scanner sc = new Scanner(System.in) next()：遇到了空格，就不再录入数据了，结束标记：空格、tab 键 nextLine()：可以将数据完整的接收过来，结束标记：回车换行符 一般使用 sc.nextInt() 或者 sc.nextLine() 接受整型和字符串，然后转成需要的数据类型 Scanner：BufferedReader br = new BufferedReader(new InputStreamReader(System.in)) print：PrintStream.write() 使用引用数据类型的API 123456public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); while (sc.hasNextLine()) &#123; String msg = sc.nextLine(); &#125;&#125; 数组初始化数组就是存储数据长度固定的容器，存储多个数据的数据类型要一致，数组也是一个对象 创建数组： 数据类型[] 数组名：int[] arr （常用） 数据类型 数组名[]：int arr[] 静态初始化： 数据类型[] 数组名 &#x3D; new 数据类型[]{元素1,元素2,…}：int[] arr = new int[]&#123;11,22,33&#125; 数据类型[] 数组名 &#x3D; {元素1,元素2,…}：int[] arr = &#123;44,55,66&#125; 动态初始化 数据类型[] 数组名 &#x3D; new 数据类型[数组长度]：int[] arr = new int[3] 元素访问 索引：每一个存储到数组的元素，都会自动的拥有一个编号，从 0 开始。这个自动编号称为数组索引（index），可以通过数组的索引访问到数组中的元素 访问格式：数组名[索引]，arr[0] 赋值：arr[0] = 10 内存分配内存是计算机中的重要器件，临时存储区域，作用是运行程序。编写的程序是存放在硬盘中，在硬盘中的程序是不会运行的，必须放进内存中才能运行，运行完毕后会清空内存，Java 虚拟机要运行程序，必须要对内存进行空间的分配和管理 区域名称 作用 寄存器 给 CPU 使用 本地方法栈 JVM 在使用操作系统功能的时候使用 方法区 存储可以运行的 class 文件 堆内存 存储对象或者数组，new 来创建的，都存储在堆内存 方法栈 方法运行时使用的内存，比如 main 方法运行，进入方法栈中执行 内存分配图：Java 数组分配在堆内存 一个数组内存图 两个数组内存图 多个数组指向相同内存图 数组异常 索引越界异常：ArrayIndexOutOfBoundsException 空指针异常：NullPointerException 12345678public class ArrayDemo &#123; public static void main(String[] args) &#123; int[] arr = new int[3]; //把null赋值给数组 arr = null; System.out.println(arr[0]); &#125;&#125; arr &#x3D; null，表示变量 arr 将不再保存数组的内存地址，也就不允许再操作数组，因此运行的时候会抛出空指针异常。在开发中，空指针异常是不能出现的，一旦出现了，就必须要修改编写的代码 解决方案：给数组一个真正的堆内存空间引用即可 二维数组二维数组也是一种容器，不同于一维数组，该容器存储的都是一维数组容器 初始化： 遍历： 12345678910111213141516171819public class Test1 &#123; /* 步骤: 1. 遍历二维数组，取出里面每一个一维数组 2. 在遍历的过程中，对每一个一维数组继续完成遍历，获取内部存储的每一个元素 */ public static void main(String[] args) &#123; int[][] arr = &#123;&#123;11, 22, 33&#125;, &#123;33, 44, 55&#125;&#125;; // 1. 遍历二维数组，取出里面每一个一维数组 for (int i = 0; i &lt; arr.length; i++) &#123; //System.out.println(arr[i]); // 2. 在遍历的过程中，对每一个一维数组继续完成遍历，获取内部存储的每一个元素 //int[] temp = arr[i]; for (int j = 0; j &lt; arr[i].length; j++) &#123; System.out.println(arr[i][j]); &#125; &#125; &#125;&#125; 运算 i++ 与 ++i 的区别？ i++ 表示先将 i 放在表达式中运算，然后再加 1，++i 表示先将 i 加 1，然后再放在表达式中运算 || 和 |，&amp;&amp; 和&amp; 的区别，逻辑运算符 &amp; 和| 称为布尔运算符，位运算符；&amp;&amp; 和 || 称为条件布尔运算符，也叫短路运算符 如果 &amp;&amp; 运算符的第一个操作数是 false，就不需要考虑第二个操作数的值了，因为无论第二个操作数的值是什么，其结果都是 false；同样，如果第一个操作数是 true，|| 运算符就返回 true，无需考虑第二个操作数的值；但 &amp; 和 | 却不是这样，它们总是要计算两个操作数。为了提高性能，尽可能使用 &amp;&amp; 和 || 运算符 异或 ^：两位相异为 1，相同为 0，又叫不进位加法 同或：两位相同为 1，相异为 0 switch：从 Java 7 开始，可以在 switch 条件判断语句中使用 String 对象 1234567891011String s = &quot;a&quot;;switch (s) &#123; case &quot;a&quot;: System.out.println(&quot;aaa&quot;); break; case &quot;b&quot;: System.out.println(&quot;bbb&quot;); break; default: break;&#125; switch 不支持 long、float、double，switch 的设计初衷是对那些只有少数几个值的类型进行等值判断，如果值过于复杂，那么用 if 比较合适 break：跳出一层循环 移位运算：计算机里一般用补码表示数字，正数、负数的表示区别就是最高位是 0 还是 1 正数的原码反码补码相同，最高位为 0 1100:\t00000000 00000000 00000000 01100100 负数：原码：最高位为 1，其余位置和正数相同反码：保证符号位不变，其余位置取反补码：保证符号位不变，其余位置取反后加 1，即反码 +1 123-100 原码:\t10000000 00000000 00000000 01100100\t//32位-100 反码:\t11111111 11111111 11111111 10011011-100 补码:\t11111111 11111111 11111111 10011100 补码 → 原码：符号位不变，其余位置取反加 1 运算符： &gt;&gt; 运算符：将二进制位进行右移操作，相当于除 2 &lt;&lt; 运算符：将二进制位进行左移操作，相当于乘 2 &gt;&gt;&gt; 运算符：无符号右移，忽略符号位，空位都以 0 补齐 运算规则： 正数的左移与右移，空位补 0 负数原码的左移与右移，空位补 0 负数反码的左移与右移，空位补 1 负数补码，左移低位补 0（会导致负数变为正数的问题，因为移动了符号位），右移高位补 1 无符号移位，空位补 0 参数形参实参形参： 形式参数，用于定义方法的时候使用的参数，只能是变量 形参只有在方法被调用的时候，虚拟机才分配内存单元，方法调用结束之后便会释放所分配的内存单元 实参：调用方法时传递的数据可以是常量，也可以是变量 可变参数可变参数用在形参中可以接收多个数据，在方法内部本质上就是一个数组 格式：数据类型… 参数名称 作用：传输参数非常灵活，可以不传输参数、传输一个参数、或者传输一个数组 可变参数的注意事项： 一个形参列表中可变参数只能有一个 可变参数必须放在形参列表的最后面 1234567891011121314public static void main(String[] args) &#123;\tsum(); // 可以不传输参数。\tsum(10); // 可以传输一个参数。\tsum(10,20,30); // 可以传输多个参数。\tsum(new int[]&#123;10,30,50,70,90&#125;); // 可以传输一个数组。&#125;public static void sum(int... nums)&#123;\tint sum = 0;\tfor(int i : a) &#123; sum += i;\t&#125;\treturn sum;&#125; 方法方法概述方法（method）是将具有独立功能的代码块组织成为一个整体，使其具有特殊功能的代码集 注意：方法必须先创建才可以使用，该过程成为方法定义，方法创建后并不是直接可以运行的，需要手动使用后才执行，该过程成为方法调用 在方法内部定义的叫局部变量，局部变量不能加 static，包括 protected、private、public 这些也不能加 原因：局部变量是保存在栈中的，而静态变量保存于方法区（JDK8 在堆中），局部变量出了方法就被栈回收了，而静态变量不会，所以在局部变量前不能加 static 关键字，静态变量是定义在类中，又叫类变量 定义调用定义格式： 1234public static 返回值类型 方法名(参数) &#123;\t//方法体;\treturn 数据 ;&#125; 调用格式： 1数据类型 变量名 = 方法名 (参数) ; 方法名：调用方法时候使用的标识 参数：由数据类型和变量名组成，多个参数之间用逗号隔开 方法体：完成功能的代码块 return：如果方法操作完毕，有数据返回，用于把数据返回给调用者 如果方法操作完毕 void 类型的方法，直接调用即可，而且方法体中一般不写 return 非 void 类型的方法，推荐用变量接收调用 原理：每个方法在被调用执行的时候，都会进入栈内存，并且拥有自己独立的内存空间，方法内部代码调用完毕之后，会从栈内存中弹栈消失 注意事项 方法不能嵌套定义 123456789public class MethodDemo &#123;\tpublic static void main(String[] args) &#123;\t&#125;\tpublic static void methodOne() &#123; public static void methodTwo() &#123; // 这里会引发编译错误!!! &#125;\t&#125;&#125; void 表示无返回值，可以省略 return，也可以单独的书写 return，后面不加数据 12345public static void methodTwo() &#123;\t//return 100; 编译错误，因为没有具体返回值类型\treturn;\t//System.out.println(100); return语句后面不能跟数据或代码&#125; 方法重载重载介绍方法重载指同一个类中定义的多个方法之间的关系，满足下列条件的多个方法相互构成重载： 多个方法在同一个类中 多个方法具有相同的方法名 多个方法的参数不相同，类型不同或者数量不同 重载仅对应方法的定义，与方法的调用无关，调用方式参照标准格式 重载仅针对同一个类中方法的名称与参数进行识别，与返回值无关，不能通过返回值来判定两个方法是否构成重载 原理：JVM → 运行机制 → 方法调用 → 多态原理 12345678910111213public class MethodDemo &#123;\tpublic static void fn(int a) &#123; //方法体\t&#125; public static int fn(int a) &#123; /*错误原因：重载与返回值无关*/ //方法体\t&#125; public static void fn(int a, int b) &#123;/*正确格式*/ //方法体\t&#125;&#125; 方法选取重载的方法在编译过程中即可完成识别，方法调用时 Java 编译器会根据所传入参数的声明类型（注意与实际类型区分）来选取重载方法。选取的过程共分为三个阶段： 一阶段：在不考虑对基本类型自动装拆箱 (auto-boxing，auto-unboxing)，以及可变长参数的情况下选取重载方法 二阶段：如果第一阶段中没有找到适配的方法，那么在允许自动装拆箱，但不允许可变长参数的情况下选取重载方法 三阶段：如果第二阶段中没有找到适配的方法，那么在允许自动装拆箱以及可变长参数的情况下选取重载方法 如果 Java 编译器在同一个阶段中找到了多个适配的方法，那么会选择一个最为贴切的，而决定贴切程度的一个关键就是形式参数类型的继承关系，一般会选择形参为参数类型的子类的方法，因为子类时更具体的实现： 12345678910public class MethodDemo &#123; void invoke(Object obj, Object... args) &#123; ... &#125; void invoke(String s, Object obj, Object... args) &#123; ... &#125; invoke(null, 1); // 调用第二个invoke方法，选取的第二阶段 invoke(null, 1, 2); // 调用第二个invoke方法，匹配第一个和第二个，但String是Object的子类 invoke(null, new Object[]&#123;1&#125;); // 只有手动绕开可变长参数的语法糖，才能调用第一个invoke方法 // 可变参数底层是数组，JVM-&gt;运行机制-&gt;代码优化&#125; 因此不提倡可变长参数方法的重载 继承重载除了同一个类中的方法，重载也可以作用于这个类所继承而来的方法。如果子类定义了与父类中非私有方法同名的方法，而且这两个方法的参数类型不同，那么在子类中，这两个方法同样构成了重载 如果这两个方法都是静态的，那么子类中的方法隐藏了父类中的方法 如果这两个方法都不是静态的，且都不是私有的，那么子类的方法重写了父类中的方法，也就是多态 参数传递Java 的参数是以值传递的形式传入方法中 值传递和引用传递的区别在于传递后会不会影响实参的值：值传递会创建副本，引用传递不会创建副本 基本数据类型：形式参数的改变，不影响实际参数 每个方法在栈内存中，都会有独立的栈空间，方法运行结束后就会弹栈消失 1234567891011public class ArgsDemo01 &#123;\tpublic static void main(String[] args) &#123; int number = 100; System.out.println(&quot;调用change方法前：&quot; + number);//100 change(number); System.out.println(&quot;调用change方法后：&quot; + number);//100\t&#125;\tpublic static void change(int number) &#123; number = 200;\t&#125;&#125; 引用类型：形式参数的改变，影响实际参数的值 引用数据类型的传参，本质上是将对象的地址以值的方式传递到形参中，内存中会造成两个引用指向同一个内存的效果，所以即使方法弹栈，堆内存中的数据也已经是改变后的结果 12345678910111213public class PassByValueExample &#123; public static void main(String[] args) &#123; Dog dog = new Dog(&quot;A&quot;); func(dog); System.out.println(dog.getName());\t// B &#125; private static void func(Dog dog) &#123; dog.setName(&quot;B&quot;); &#125;&#125;class Dog &#123; String name;//.....&#125; 枚举枚举是 Java 中的一种特殊类型，为了做信息的标志和信息的分类 定义枚举的格式： 123修饰符 enum 枚举名称&#123;\t第一行都是罗列枚举实例的名称。&#125; 枚举的特点： 枚举类是用 final 修饰的，枚举类不能被继承 枚举类默认继承了 java.lang.Enum 枚举类 枚举类的第一行都是常量，必须是罗列枚举类的实例名称 枚举类相当于是多例设计模式 每个枚举项都是一个实例，是一个静态成员变量 方法名 说明 String name() 获取枚举项的名称 int ordinal() 返回枚举项在枚举类中的索引值 int compareTo(E o) 比较两个枚举项，返回的是索引值的差值 String toString() 返回枚举常量的名称 static T valueOf(Class type,String name) 获取指定枚举类中的指定名称的枚举值 values() 获得所有的枚举项 源码分析： 12345678910111213enum Season &#123; SPRING , SUMMER , AUTUMN , WINTER;&#125;// 枚举类的编译以后源代码：public final class Season extends java.lang.Enum&lt;Season&gt; &#123;\tpublic static final Season SPRING = new Season();\tpublic static final Season SUMMER = new Season();\tpublic static final Season AUTUMN = new Season();\tpublic static final Season WINTER = new Season();\tpublic static Season[] values();\tpublic static Season valueOf(java.lang.String);&#125; API 使用 123456789101112131415161718192021222324public class EnumDemo &#123; public static void main(String[] args)&#123; // 获取索引 Season s = Season.SPRING; System.out.println(s);\t//SPRING System.out.println(s.ordinal()); // 0，该值代表索引，summer 就是 1 s.s.doSomething(); // 获取全部枚举 Season[] ss = Season.values(); for(int i = 0; i &lt; ss.length; i++)&#123; System.out.println(ss[i]); &#125; int result = Season.SPRING.compareTo(Season.WINTER); System.out.println(result);//-3 &#125;&#125;enum Season &#123; SPRING , SUMMER , AUTUMN , WINTER; public void doSomething() &#123; System.out.println(&quot;hello &quot;); &#125;&#125; DebugDebug 是供程序员使用的程序调试工具，它可以用于查看程序的执行流程，也可以用于追踪程序执行过程来调试程序。 加断点 → Debug 运行 → 单步运行 → 看 Debugger 窗口 → 看 Console 窗口 对象概述Java 是一种面向对象的高级编程语言 面向对象三大特征：封装，继承，多态 两个概念：类和对象 类：相同事物共同特征的描述，类只是学术上的一个概念并非真实存在的，只能描述一类事物 对象：是真实存在的实例， 实例 &#x3D;&#x3D; 对象，对象是类的实例化 结论：有了类和对象就可以描述万千世界所有的事物，必须先有类才能有对象 类定义定义格式 12修饰符 class 类名&#123;&#125; 类名的首字母建议大写，满足驼峰模式，比如 StudentNameCode 一个 Java 代码中可以定义多个类，按照规范一个 Java 文件一个类 一个 Java 代码文件中，只能有一个类是 public 修饰，public 修饰的类名必须成为当前 Java 代码的文件名称 123456789101112类中的成分:有且仅有五大成分修饰符 class 类名&#123; 1.成员变量(Field): 描述类或者对象的属性信息的。 2.成员方法(Method): 描述类或者对象的行为信息的。 3.构造器(Constructor): 初始化一个对象返回。 4.代码块 5.内部类 &#125;类中有且仅有这五种成分，否则代码报错！public class ClassDemo &#123; System.out.println(1);//报错&#125; 构造器构造器格式： 123修饰符 类名(形参列表)&#123;&#125; 作用：初始化类的一个对象返回 分类：无参数构造器，有参数构造器 注意：一个类默认自带一个无参数构造器，写了有参数构造器默认的无参数构造器就消失，还需要用无参数构造器就要重新写 构造器初始化对象的格式：类名 对象名称 &#x3D; new 构造器 无参数构造器的作用：初始化一个类的对象（使用对象的默认值初始化）返回 有参数构造器的作用：初始化一个类的对象（可以在初始化对象的时候为对象赋值）返回 包包：分门别类的管理各种不同的技术，便于管理技术，扩展技术，阅读技术 定义包的格式：package 包名，必须放在类名的最上面 导包格式：import 包名.类名 相同包下的类可以直接访问；不同包下的类必须导包才可以使用 封装封装的哲学思维：合理隐藏，合理暴露 封装最初的目的：提高代码的安全性和复用性，组件化 封装的步骤： 成员变量应该私有，用 private 修饰，只能在本类中直接访问 提供成套的 getter 和 setter 方法暴露成员变量的取值和赋值 使用 private 修饰成员变量的原因：实现数据封装，不想让别人使用修改你的数据，比较安全 thisthis 关键字的作用： this 关键字代表了当前对象的引用 this 出现在方法中：哪个对象调用这个方法 this 就代表谁 this 可以出现在构造器中：代表构造器正在初始化的那个对象 this 可以区分变量是访问的成员变量还是局部变量 static基本介绍Java 是通过成员变量是否有 static 修饰来区分是类的还是属于对象的 按照有无 static 修饰，成员变量和方法可以分为： 成员变量： 静态成员变量（类变量）：static 修饰的成员变量，属于类本身，与类一起加载一次，只有一个，直接用类名访问即可 实例成员变量：无 static 修饰的成员变量，属于类的每个对象的，与类的对象一起加载，对象有多少个，实例成员变量就加载多少个，必须用类的对象来访问 成员方法： 静态方法：有 static 修饰的成员方法称为静态方法也叫类方法，属于类本身的，直接用类名访问即可 实例方法：无 static 修饰的成员方法称为实例方法，属于类的每个对象的，必须用类的对象来访问 static 用法成员变量的访问语法： 静态成员变量：只有一份可以被类和类的对象共享访问 类名.静态成员变量（同一个类中访问静态成员变量可以省略类名不写） 对象.静态成员变量（不推荐） 实例成员变量： 对象.实例成员变量（先创建对象） 成员方法的访问语法： 静态方法：有 static 修饰，属于类 类名.静态方法（同一个类中访问静态成员可以省略类名不写） 对象.静态方法（不推荐，参考 JVM → 运行机制 → 方法调用） 实例方法：无 static 修饰，属于对象 对象.实例方法 12345678910111213141516public class Student &#123; // 1.静态方法：有static修饰，属于类，直接用类名访问即可！ public static void inAddr()&#123; &#125; // 2.实例方法：无static修饰，属于对象，必须用对象访问！ public void eat()&#123;&#125; public static void main(String[] args) &#123; // a.类名.静态方法 Student.inAddr(); inAddr(); // b.对象.实例方法 // Student.eat(); // 报错了！ Student sea = new Student(); sea.eat(); &#125;&#125; 两个问题内存问题： 栈内存存放 main 方法和地址 堆内存存放对象和变量 方法区存放 class 和静态变量（jdk8 以后移入堆） 访问问题： 实例方法是否可以直接访问实例成员变量？可以，因为它们都属于对象 实例方法是否可以直接访问静态成员变量？可以，静态成员变量可以被共享访问 实例方法是否可以直接访问实例方法? 可以，实例方法和实例方法都属于对象 实例方法是否可以直接访问静态方法？可以，静态方法可以被共享访问 静态方法是否可以直接访问实例变量？ 不可以，实例变量必须用对象访问！！ 静态方法是否可以直接访问静态变量？ 可以，静态成员变量可以被共享访问。 静态方法是否可以直接访问实例方法? 不可以，实例方法必须用对象访问！！ 静态方法是否可以直接访问静态方法？可以，静态方法可以被共享访问！！ 继承基本介绍继承是 Java 中一般到特殊的关系，是一种子类到父类的关系 被继承的类称为：父类&#x2F;超类 继承父类的类称为：子类 继承的作用： 提高代码的复用，相同代码可以定义在父类中 子类继承父类，可以直接使用父类这些代码（相同代码重复利用） 子类得到父类的属性（成员变量）和行为（方法），还可以定义自己的功能，子类更强大 继承的特点： 子类的全部构造器默认先访问父类的无参数构造器，再执行自己的构造器 单继承：一个类只能继承一个直接父类 多层继承：一个类可以间接继承多个父类（家谱） 一个类可以有多个子类 一个类要么默认继承了 Object 类，要么间接继承了 Object 类，Object 类是 Java 中的祖宗类 继承的格式： 123子类 extends 父类&#123;&#125; 子类不能继承父类的东西： 子类不能继承父类的构造器，子类有自己的构造器 子类是不能可以继承父类的私有成员的，可以反射暴力去访问继承自父类的私有成员 子类是不能继承父类的静态成员，父类静态成员只有一份可以被子类共享访问，共享并非继承 1234567891011121314151617public class ExtendsDemo &#123; public static void main(String[] args) &#123; Cat c = new Cat(); // c.run(); Cat.test(); System.out.println(Cat.schoolName); &#125;&#125;class Cat extends Animal&#123;&#125;class Animal&#123; public static String schoolName =&quot;seazean&quot;; public static void test()&#123;&#125; private void run()&#123;&#125;&#125; 变量访问继承后成员变量的访问特点：就近原则，子类有找子类，子类没有找父类，父类没有就报错 如果要申明访问父类的成员变量可以使用：super.父类成员变量，super指父类引用 1234567891011121314151617181920212223public class ExtendsDemo &#123; public static void wmain(String[] args) &#123; Wolf w = new Wolf();w w.showName(); &#125;&#125;class Wolf extends Animal&#123; private String name = &quot;子类狼&quot;; public void showName()&#123; String name = &quot;局部名称&quot;; System.out.println(name); // 局部name System.out.println(this.name); // 子类对象的name System.out.println(super.name); // 父类的 System.out.println(name1); // 父类的 //System.out.println(name2); // 报错。子类父类都没有 &#125;&#125;class Animal&#123; public String name = &quot;父类动物名称&quot;; public String name1 = &quot;父类&quot;;&#125; 方法访问子类继承了父类就得到了父类的方法，可以直接调用，受权限修饰符的限制，也可以重写方法 方法重写：子类重写一个与父类申明一样的方法来覆盖父类的该方法 方法重写的校验注解：@Override 方法加了这个注解，那就必须是成功重写父类的方法，否则报错 @Override 优势：可读性好，安全，优雅 子类可以扩展父类的功能，但不能改变父类原有的功能，重写有以下三个限制： 子类方法的访问权限必须大于等于父类方法 子类方法的返回类型必须是父类方法返回类型或为其子类型 子类方法抛出的异常类型必须是父类抛出异常类型或为其子类型 继承中的隐藏问题： 子类和父类方法都是静态的，那么子类中的方法会隐藏父类中的方法 在子类中可以定义和父类成员变量同名的成员变量，此时子类的成员变量隐藏了父类的成员变量，在创建对象为对象分配内存的过程中，隐藏变量依然会被分配内存 12345678910111213public class ExtendsDemo &#123; public static void main(String[] args) &#123; Wolf w = new Wolf(); w.run(); &#125;&#125;class Wolf extends Animal&#123; @Override public void run()&#123;&#125;//&#125;class Animal&#123; public void run()&#123;&#125;&#125; 常见问题 为什么子类构造器会先调用父类构造器？ 子类的构造器的第一行默认 super() 调用父类的无参数构造器，写不写都存在 子类继承父类，子类就得到了父类的属性和行为。调用子类构造器初始化子类对象数据时，必须先调用父类构造器初始化继承自父类的属性和行为 参考 JVM → 类加载 → 对象创建 12345678910111213141516class Animal &#123; public Animal() &#123; System.out.println(&quot;==父类Animal的无参数构造器==&quot;); &#125;&#125;class Tiger extends Animal &#123; public Tiger() &#123; super(); // 默认存在的，根据参数去匹配调用父类的构造器。 System.out.println(&quot;==子类Tiger的无参数构造器==&quot;); &#125; public Tiger(String name) &#123; //super(); 默认存在的，根据参数去匹配调用父类的构造器。 System.out.println(&quot;==子类Tiger的有参数构造器==&quot;); &#125;&#125; 为什么 Java 是单继承的？ 答：反证法，假如 Java 可以多继承，请看如下代码： 1234567891011121314151617class A&#123;\tpublic void test()&#123; System.out.println(&quot;A&quot;);\t&#125;&#125;class B&#123;\tpublic void test()&#123; System.out.println(&quot;B&quot;);\t&#125;&#125;class C extends A , B &#123;\tpublic static void main(String[] args)&#123; C c = new C(); c.test(); // 出现了类的二义性！所以Java不能多继承！！\t&#125;&#125; super继承后 super 调用父类构造器，父类构造器初始化继承自父类的数据。 总结与拓展： this 代表了当前对象的引用（继承中指代子类对象）：this.子类成员变量、this.子类成员方法、this(…) 可以根据参数匹配访问本类其他构造器 super 代表了父类对象的引用（继承中指代了父类对象空间）：super.父类成员变量、super.父类的成员方法、super(…) 可以根据参数匹配访问父类的构造器 注意： this(…) 借用本类其他构造器，super(…) 调用父类的构造器 this(…) 或 super(…) 必须放在构造器的第一行，否则报错 this(…) 和 super(…) 不能同时出现在构造器中，因为构造函数必须出现在第一行上，只能选择一个 1234567891011121314151617181920212223242526272829303132public class ThisDemo &#123; public static void main(String[] args) &#123; // 需求：希望如果不写学校默认就是”张三“！ Student s1 = new Student(&quot;天蓬元帅&quot;, 1000 ); Student s2 = new Student(&quot;齐天大圣&quot;, 2000, &quot;清华大学&quot; ); &#125;&#125;class Study extends Student &#123; public Study(String name, int age, String schoolName) &#123; super(name , age , schoolName) ; // 根据参数匹配调用父类构造器 &#125;&#125;class Student&#123; private String name ; private int age ; private String schoolName ; public Student() &#123; &#125; public Student(String name , int age)&#123; // 借用兄弟构造器的功能！ this(name , age , &quot;张三&quot;); &#125;\tpublic Student(String name, int age, String schoolName) &#123; this.name = name; this.age = age; this.schoolName = schoolName; &#125;\t// .......get + set&#125; final基本介绍final 用于修饰：类，方法，变量 final 修饰类，类不能被继承了，类中的方法和变量可以使用 final 可以修饰方法，方法就不能被重写 final 修饰变量总规则：变量有且仅能被赋值一次 final 和 abstract 的关系是互斥关系，不能同时修饰类或者同时修饰方法 修饰变量静态变量final 修饰静态成员变量，变量变成了常量 常量：有 public static final 修饰，名称字母全部大写，多个单词用下划线连接 final 修饰静态成员变量可以在哪些地方赋值： 定义的时候赋值一次 可以在静态代码块中赋值一次 12345678910public class FinalDemo &#123;\t//常量：public static final修饰，名称字母全部大写，下划线连接。 public static final String SCHOOL_NAME = &quot;张三&quot; ; public static final String SCHOOL_NAME1; static&#123; //SCHOOL_NAME = &quot;java&quot;;//报错 SCHOOL_NAME1 = &quot;张三1&quot;; &#125;&#125; 实例变量final 修饰变量的总规则：有且仅能被赋值一次 final 修饰实例成员变量可以在哪些地方赋值 1 次： 定义的时候赋值一次 可以在实例代码块中赋值一次 可以在每个构造器中赋值一次 123456789101112131415161718192021public class FinalDemo &#123; private final String name = &quot;张三&quot; ; private final String name1; private final String name2; &#123; // 可以在实例代码块中赋值一次。 name1 = &quot;张三1&quot;; &#125;\t//构造器赋值一次 public FinalDemo()&#123; name2 = &quot;张三2&quot;; &#125; public FinalDemo(String a)&#123; name2 = &quot;张三2&quot;; &#125; public static void main(String[] args) &#123; FinalDemo f1 = new FinalDemo(); //f1.name = &quot;张三1&quot;; // 第二次赋值 报错！ &#125;&#125; 抽象类基本介绍 父类知道子类要完成某个功能，但是每个子类实现情况不一样 抽象方法：没有方法体，只有方法签名，必须用 abstract 修饰的方法就是抽象方法 抽象类：拥有抽象方法的类必须定义成抽象类，必须用 abstract 修饰，抽象类是为了被继承 一个类继承抽象类，必须重写抽象类的全部抽象方法，否则这个类必须定义成抽象类 1234567891011121314151617public class AbstractDemo &#123; public static void main(String[] args) &#123; Dog d = new Dog(); d.run(); &#125;&#125;class Dog extends Animal&#123; @Override public void run() &#123; System.out.println(&quot;🐕跑&quot;); &#125;&#125;abstract class Animal&#123; public abstract void run();&#125; 常见问题一、抽象类是否有构造器，是否可以创建对象? 抽象类有构造器，但是抽象类不能创建对象，类的其他成分它都具备，构造器提供给子类继承后调用父类构造器使用 抽象类中存在抽象方法，但不能执行，抽象类中也可没有抽象方法 抽象在学术上本身意味着不能实例化 123456789101112131415public class AbstractDemo &#123; public static void main(String[] args) &#123; //Animal a = new Animal(); 抽象类不能创建对象！ //a.run(); // 抽象方法不能执行 &#125;&#125;abstract class Animal&#123; private String name; public static String schoolName = &quot;张三&quot;; public Animal()&#123; &#125; public abstract void run(); //普通方法 public void go()&#123; &#125;&#125; 二、static 与 abstract 能同时使用吗？ 答：不能，被 static 修饰的方法属于类，是类自己的东西，不是给子类来继承的，而抽象方法本身没有实现，就是用来给子类继承 存在意义被继承，抽象类就是为了被子类继承，否则抽象类将毫无意义（核心） 抽象类体现的是”模板思想”：部分实现，部分抽象，可以使用抽象类设计一个模板模式 123456789101112131415161718192021222324//作文模板public class ExtendsDemo &#123; public static void main(String[] args) &#123; Student xiaoMa = new Student(); xiaoMa.write(); &#125;&#125;class Student extends Template&#123; @Override public String writeText() &#123;return &quot;\\t内容&quot;&#125;&#125;// 1.写一个模板类：代表了作文模板。abstract class Template&#123; private String title = &quot;\\t\\t\\t\\t\\t标题&quot;; private String start = &quot;\\t开头&quot;; private String last = &quot;\\t结尾&quot;; public void write()&#123; System.out.println(title+&quot; &quot;+start); System.out.println(writeText()); System.out.println(last); &#125; // 正文部分定义成抽象方法，交给子类重写！！ public abstract String writeText();&#125; 接口基本介绍接口是 Java 语言中一种引用类型，是方法的集合。 接口是更加彻底的抽象，接口中只有抽象方法和常量，没有其他成分 123456 修饰符 interface 接口名称&#123;\t// 抽象方法\t// 默认方法\t// 静态方法\t// 私有方法&#125; 抽象方法：接口中的抽象方法默认会加上 public abstract 修饰，所以可以省略不写 静态方法：静态方法必须有方法体 常量：是 public static final 修饰的成员变量，仅能被赋值一次，值不能改变。常量的名称规范上要求全部大写，多个单词下划线连接，public static final 可以省略不写 1234567public interface InterfaceDemo&#123; //public static final String SCHOOL_NAME = &quot;张三&quot;;\tString SCHOOL_NAME = &quot;张三&quot;; //public abstract void run(); void run();//默认补充&#125; 实现接口接口是用来被类实现的。 类与类是继承关系：一个类只能直接继承一个父类，单继承 类与接口是实现关系：一个类可以实现多个接口，多实现，接口不能继承类 接口与接口继承关系：多继承 123456修饰符 class 实现类名称 implements 接口1,接口2,接口3,....&#123;&#125;修饰符 interface 接口名 extend 接口1,接口2,接口3,....&#123; &#125; 实现多个接口的使用注意事项： 当一个类实现多个接口时，多个接口中存在同名的静态方法并不会冲突，只能通过各自接口名访问静态方法 当一个类实现多个接口时，多个接口中存在同名的默认方法，实现类必须重写这个方法 当一个类既继承一个父类，又实现若干个接口时，父类中成员方法与接口中默认方法重名，子类就近选择执行父类的成员方法 接口中，没有构造器，不能创建对象，接口是更彻底的抽象，连构造器都没有，自然不能创建对象 12345678910111213141516171819202122public class InterfaceDemo &#123; public static void main(String[] args) &#123; Student s = new Student(); s.run(); s.rule(); &#125;&#125;class Student implements Food, Person&#123; @Override public void eat() &#123;&#125; @Override public void run() &#123;&#125;&#125;interface Food&#123; void eat();&#125;interface Person&#123; void run();&#125;//可以直接 interface Person extend Food,//然后 class Student implements Person 效果一样 新增功能jdk1.8 以后新增的功能： 默认方法（就是普通实例方法） 必须用 default 修饰，默认会 public 修饰 必须用接口的实现类的对象来调用 必须有默认实现 静态方法 默认会 public 修饰 接口的静态方法必须用接口的类名本身来调用 调用格式：ClassName.method() 必须有默认实现 私有方法：JDK 1.9 才开始有的，只能在本类中被其他的默认方法或者私有方法访问 12345678910111213141516171819202122232425262728293031323334353637383940public class InterfaceDemo &#123; public static void main(String[] args) &#123; // 1.默认方法调用：必须用接口的实现类对象调用。 Man m = new Man(); m.run(); m.work(); // 2.接口的静态方法必须用接口的类名本身来调用。 InterfaceJDK8.inAddr(); &#125;&#125;class Man implements InterfaceJDK8 &#123; @Override public void work() &#123; System.out.println(&quot;工作中。。。&quot;); &#125;&#125;interface InterfaceJDK8 &#123; //抽象方法！！ void work(); // a.默认方法（就是之前写的普通实例方法） // 必须用接口的实现类的对象来调用。 default void run() &#123; go(); System.out.println(&quot;开始跑步🏃‍&quot;); &#125; // b.静态方法 // 注意：接口的静态方法必须用接口的类名本身来调用 static void inAddr() &#123; System.out.println(&quot;我们在武汉&quot;); &#125; // c.私有方法（就是私有的实例方法）: JDK 1.9才开始有的。 // 只能在本接口中被其他的默认方法或者私有方法访问。 private void go() &#123; System.out.println(&quot;开始。。&quot;); &#125;&#125; 对比抽象类 参数 抽象类 接口 默认的方法实现 可以有默认的方法实现 接口完全是抽象的，jdk8 以后有默认的实现 实现 子类使用 extends 关键字来继承抽象类。如果子类不是抽象类的话，它需要提供抽象类中所有声明的方法的实现。 子类使用关键字 implements 来实现接口。它需要提供接口中所有声明的方法的实现 构造器 抽象类可以有构造器 接口不能有构造器 与正常Java类的区别 除了不能实例化抽象类之外，和普通 Java 类没有任何区别 接口是完全不同的类型 访问修饰符 抽象方法有 public、protected 和 default 这些修饰符 接口默认修饰符是 public，别的修饰符需要有方法体 main方法 抽象方法可以有 main 方法并且我们可以运行它 jdk8 以前接口没有 main 方法，不能运行；jdk8 以后接口可以有 default 和 static 方法，可以运行 main 方法 多继承 抽象方法可以继承一个类和实现多个接口 接口可以继承一个或多个其它接口，接口不可继承类 速度 比接口速度要快 接口是稍微有点慢的，因为它需要时间去寻找在类中实现的方法 添加新方法 如果往抽象类中添加新的方法，可以给它提供默认的实现，因此不需要改变现在的代码 如果往接口中添加方法，那么必须改变实现该接口的类 多态基本介绍多态的概念：同一个实体同时具有多种形式同一个类型的对象，执行同一个行为，在不同的状态下会表现出不同的行为特征 多态的格式： 父类类型范围 &gt; 子类类型范围 12父类类型 对象名称 = new 子类构造器;接口 对象名称 = new 实现类构造器; 多态的执行： 对于方法的调用：编译看左边，运行看右边（分派机制） 对于变量的调用：编译看左边，运行看左边 多态的使用规则： 必须存在继承或者实现关系 必须存在父类类型的变量引用子类类型的对象 存在方法重写 多态的优势： 在多态形式下，右边对象可以实现组件化切换，便于扩展和维护，也可以实现类与类之间的解耦 父类类型作为方法形式参数，传递子类对象给方法，可以传入一切子类对象进行方法的调用，更能体现出多态的扩展性与便利性 多态的劣势： 多态形式下，不能直接调用子类特有的功能，因为编译看左边，父类中没有子类独有的功能，所以代码在编译阶段就直接报错了 1234567891011121314151617181920212223public class PolymorphicDemo &#123; public static void main(String[] args) &#123; Animal c = new Cat(); c.run(); //c.eat();//报错 编译看左边 需要强转 go(c); go(new Dog); &#125; //用 Dog或者Cat 都没办法让所有动物参与进来，只能用Anima public static void go(Animal d)&#123;&#125; &#125;class Dog extends Animal&#123;&#125;class Cat extends Animal&#123; public void eat(); @Override public void run()&#123;&#125;&#125;class Animal&#123; public void run()&#123;&#125;&#125; 上下转型 基本数据类型的转换： 小范围类型的变量或者值可以直接赋值给大范围类型的变量 大范围类型的变量或者值必须强制类型转换给小范围类型的变量 引用数据类型的自动类型转换语法：子类类型的对象或者变量可以自动类型转换赋值给父类类型的变量 父类引用指向子类对象 **向上转型 (upcasting)**：通过子类对象（小范围）实例化父类对象（大范围），这种属于自动转换 **向下转型 (downcasting)**：通过父类对象（大范围）实例化子类对象（小范围），这种属于强制转换 12345678public class PolymorphicDemo &#123; public static void main(String[] args)&#123; Animal a = new Cat();\t// 向上转型 Cat c = (Cat)a; // 向下转型 &#125;&#125;class Animal&#123;&#125;class Cat extends Animal&#123;&#125; instanceofinstanceof：判断左边的对象是否是右边的类的实例，或者是其直接或间接子类，或者是其接口的实现类 引用类型强制类型转换：父类类型的变量或者对象强制类型转换成子类类型的变量，否则报错 强制类型转换的格式：类型 变量名称 &#x3D; (类型)(对象或者变量) 有继承&#x2F;实现关系的两个类型就可以进行强制类型转换，编译阶段一定不报错，但是运行阶段可能出现类型转换异常 ClassCastException 123456789101112131415public class Demo&#123; public static void main(String[] args)&#123; Aniaml a = new Dog(); //Dog d = (Dog)a; //Cat c = (Cat)a; 编译不报错，运行报ClassCastException错误 if(a instanceof Cat)&#123; Cat c = (Cat)a; &#125; else if(a instanceof Dog) &#123; Dog d = (Dog)a; &#125; &#125;&#125;class Dog extends Animal&#123;&#125;class Cat extends Animal&#123;&#125;class Animal&#123;&#125; 内部类概述内部类是类的五大成分之一：成员变量，方法，构造器，代码块，内部类 概念：定义在一个类里面的类就是内部类 作用：提供更好的封装性，体现出组件思想，间接解决类无法多继承引起的一系列问题 分类：静态内部类、实例内部类（成员内部类）、局部内部类、匿名内部类（重点） 静态内部类定义：有 static 修饰，属于外部类本身，会加载一次 静态内部类中的成分研究： 类有的成分它都有，静态内部类属于外部类本身，只会加载一次 特点与外部类是完全一样的，只是位置在别人里面 可以定义静态成员 静态内部类的访问格式：外部类名称.内部类名称 静态内部类创建对象的格式：外部类名称.内部类名称 对象名称 &#x3D; new 外部类名称.内部类构造器 静态内部类的访问拓展： 静态内部类中是否可以直接访问外部类的静态成员?\t可以，外部类的静态成员只有一份，可以被共享 静态内部类中是否可以直接访问外部类的实例成员?\t不可以，外部类的成员必须用外部类对象访问 123456789101112131415public class Demo&#123; public static void main(String[] args)&#123; Outter.Inner in = new Outter.Inner(); &#125;&#125;static class Outter&#123; public static int age; private double salary; public static class Inner&#123; //拥有类的所有功能 构造器 方法 成员变量 System.out.println(age); //System.out.println(salary);报错\t&#125;&#125; 实例内部类定义：无 static 修饰的内部类，属于外部类的每个对象，跟着外部类对象一起加载 实例内部类的成分特点：实例内部类中不能定义静态成员，其他都可以定义 实例内部类的访问格式：外部类名称.内部类名称 创建对象的格式：外部类名称.内部类名称 对象名称 &#x3D; new 外部类构造器.new 内部构造器 Outter.Inner in = new Outter().new Inner() 实例内部类可以访问外部类的全部成员 实例内部类中可以直接访问外部类的静态成员，外部类的静态成员可以被共享访问 实例内部类中可以访问外部类的实例成员，实例内部类属于外部类对象，可以直接访问外部类对象的实例成员 局部内部类局部内部类：定义在方法中，在构造器中，代码块中，for 循环中定义的内部类 局部内部类中的成分特点：只能定义实例成员，不能定义静态成员 12345678910public class InnerClass&#123;\tpublic static void main(String[] args)&#123; String name; class&#123;&#125; &#125; public static void test()&#123; class Animal&#123;&#125; class Cat extends Animal&#123;&#125; &#125;&#125; 匿名内部类匿名内部类：没有名字的局部内部类 匿名内部类的格式： 123new 类名|抽象类|接口(形参)&#123;\t//方法重写。&#125; 匿名内部类的特点： 匿名内部类不能定义静态成员 匿名内部类一旦写出来，就会立即创建一个匿名内部类的对象返回 匿名内部类的对象的类型相当于是当前 new 的那个的类型的子类类型 匿名内部类引用局部变量必须是常量，底层创建为内部类的成员变量（原因：JVM → 运行机制 → 代码优化） 1234567891011121314151617181920public class Anonymity &#123; public static void main(String[] args) &#123; Animal a = new Animal()&#123; @Override public void run() &#123; System.out.println(&quot;猫跑的贼溜~~&quot;); //System.out.println(n); &#125; &#125;; a.run(); a.go(); &#125;&#125;abstract class Animal&#123; public abstract void run(); public void go()&#123; System.out.println(&quot;开始go~~~&quot;); &#125;&#125; 权限符权限修饰符：有四种（private -&gt; 缺省 -&gt; protected - &gt; public ）可以修饰成员变量，修饰方法，修饰构造器，内部类，不同修饰符修饰的成员能够被访问的权限将受到限制 四种修饰符访问权限 private 缺省 protected public 本类中 √ √ √ √ 本包下的子类中 X √ √ √ 本包下其他类中 X √ √ √ 其他包下的子类中 X X √ √ 其他包下的其他类中 X X X √ protected 用于修饰成员，表示在继承体系中成员对于子类可见 基类的 protected 成员是包内可见的，并且对子类可见 若子类与基类不在同一包中，那么子类实例可以访问其从基类继承而来的 protected 方法（重写），而不能访问基类实例的 protected 方法 代码块静态代码块静态代码块的格式： 12static &#123;&#125; 静态代码块特点： 必须有 static 修饰，只能访问静态资源 会与类一起优先加载，且自动触发执行一次 静态代码块作用： 可以在执行类的方法等操作之前先在静态代码块中进行静态资源的初始化 先执行静态代码块，在执行 main 函数里的操作 1234567891011121314151617181920212223public class CodeDemo &#123; public static String schoolName ; public static ArrayList&lt;String&gt; lists = new ArrayList&lt;&gt;(); // 静态代码块,属于类，与类一起加载一次! static &#123; System.out.println(&quot;静态代码块被触发执行~~~~~~~&quot;); // 在静态代码块中进行静态资源的初始化操作 schoolName = &quot;张三&quot;; lists.add(&quot;3&quot;); lists.add(&quot;4&quot;); lists.add(&quot;5&quot;); &#125; public static void main(String[] args) &#123; System.out.println(&quot;main方法被执行&quot;); System.out.println(schoolName); System.out.println(lists); &#125;&#125;/*静态代码块被触发执行~~~~~~~main方法被执行张三[3, 4, 5] */ 实例代码块实例代码块的格式： 123&#123;&#125; 实例代码块的特点： 无 static 修饰，属于对象 会与类的对象一起加载，每次创建类的对象的时候，实例代码块都会被加载且自动触发执行一次 实例代码块的代码在底层实际上是提取到每个构造器中去执行的 实例代码块的作用：实例代码块可以在创建对象之前进行实例资源的初始化操作 123456789101112131415161718public class CodeDemo &#123; private String name; private ArrayList&lt;String&gt; lists = new ArrayList&lt;&gt;(); &#123; name = &quot;代码块&quot;; lists.add(&quot;java&quot;); System.out.println(&quot;实例代码块被触发执行一次~~~~~~~~&quot;); &#125; public CodeDemo02()&#123; &#125;//构造方法 public CodeDemo02(String name)&#123;&#125; public static void main(String[] args) &#123; CodeDemo c = new CodeDemo();//实例代码块被触发执行一次 System.out.println(c.name); System.out.println(c.lists); new CodeDemo02();//实例代码块被触发执行一次 &#125;&#125; APIObject基本介绍Object 类是 Java 中的祖宗类，一个类或者默认继承 Object 类，或者间接继承 Object 类，Object 类的方法是一切子类都可以直接使用 Object 类常用方法： public String toString()：默认是返回当前对象在堆内存中的地址信息：类的全限名@内存地址，例：Student@735b478； 直接输出对象名称，默认会调用 toString() 方法，所以省略 toString() 不写； 如果输出对象的内容，需要重写 toString() 方法，toString 方法存在的意义是为了被子类重写 public boolean equals(Object o)：默认是比较两个对象的引用是否相同 protected Object clone()：创建并返回此对象的副本 只要两个对象的内容一样，就认为是相等的： 1234567891011public boolean equals(Object o) &#123;\t// 1.判断是否自己和自己比较，如果是同一个对象比较直接返回true\tif (this == o) return true;\t// 2.判断被比较者是否为null ,以及是否是学生类型。\tif (o == null || this.getClass() != o.getClass()) return false;\t// 3.o一定是学生类型，强制转换成学生，开始比较内容！\tStudent student = (Student) o;\treturn age == student.age &amp;&amp; sex == student.sex &amp;&amp; Objects.equals(name, student.name);&#125; 面试题：&#x3D;&#x3D; 和 equals 的区别 &#x3D;&#x3D; 比较的是变量（栈）内存中存放的对象的（堆）内存地址，用来判断两个对象的地址是否相同，即是否是指相同一个对象，比较的是真正意义上的指针操作 Object 类中的方法，默认比较两个对象的引用，重写 equals 方法比较的是两个对象的内容是否相等，所有的类都是继承自 java.lang.Object 类，所以适用于所有对象 hashCode 的作用： hashCode 的存在主要是用于查找的快捷性，如 Hashtable，HashMap 等，可以在散列存储结构中确定对象的存储地址 如果两个对象相同，就是适用于 equals(java.lang.Object) 方法，那么这两个对象的 hashCode 一定要相同 哈希值相同的数据不一定内容相同，内容相同的数据哈希值一定相同 深浅克隆Object 的 clone() 是 protected 方法，一个类不显式去重写 clone()，就不能直接去调用该类实例的 clone() 方法 深浅拷贝（克隆）的概念： 浅拷贝 (shallowCopy)：对基本数据类型进行值传递，对引用数据类型只是复制了引用，被复制对象属性的所有的引用仍然指向原来的对象，简而言之就是增加了一个指针指向原来对象的内存地址 Java 中的复制方法基本都是浅拷贝：Object.clone()、System.arraycopy()、Arrays.copyOf() 深拷贝 (deepCopy)：对基本数据类型进行值传递，对引用数据类型是一个整个独立的对象拷贝，会拷贝所有的属性并指向的动态分配的内存，简而言之就是把所有属性复制到一个新的内存，增加一个指针指向新内存。所以使用深拷贝的情况下，释放内存的时候不会出现使用浅拷贝时释放同一块内存的错误 Cloneable 接口是一个标识性接口，即该接口不包含任何方法（包括 clone），但是如果一个类想合法的进行克隆，那么就必须实现这个接口，在使用 clone() 方法时，若该类未实现 Cloneable 接口，则抛出异常 Clone &amp; Copy：Student s = new Student Student s1 = s：只是 copy 了一下 reference，s 和 s1 指向内存中同一个 Object，对对象的修改会影响对方 Student s2 = s.clone()：会生成一个新的 Student 对象，并且和 s 具有相同的属性值和方法 Shallow Clone &amp; Deep Clone： 浅克隆：Object 中的 clone() 方法在对某个对象克隆时对其仅仅是简单地执行域对域的 copy 对基本数据类型和包装类的克隆是没有问题的。String、Integer 等包装类型在内存中是不可以被改变的对象，所以在使用克隆时可以视为基本类型，只需浅克隆引用即可 如果对一个引用类型进行克隆时只是克隆了它的引用，和原始对象共享对象成员变量 深克隆：在对整个对象浅克隆后，对其引用变量进行克隆，并将其更新到浅克隆对象中去 12345678910111213public class Student implements Cloneable&#123; private String name; private Integer age; private Date date; @Override protected Object clone() throws CloneNotSupportedException &#123; Student s = (Student) super.clone(); s.date = (Date) date.clone(); return s; &#125; //.....&#125; SDP → 创建型 → 原型模式 ObjectsObjects 类与 Object 是继承关系 Objects 的方法： public static boolean equals(Object a, Object b)：比较两个对象是否相同 1234public static boolean equals(Object a, Object b) &#123; // 进行非空判断，从而可以避免空指针异常 return a == b || a != null &amp;&amp; a.equals(b);&#125; public static boolean isNull(Object obj)：判断变量是否为 null ，为 null 返回 true public static String toString(对象)：返回参数中对象的字符串表示形式 public static String toString(对象, 默认字符串)：返回对象的字符串表示形式 1234567891011121314public class ObjectsDemo &#123; public static void main(String[] args) &#123; Student s1 = null; Student s2 = new Student(); System.out.println(Objects.equals(s1 , s2));//推荐使用 // System.out.println(s1.equals(s2)); // 空指针异常 System.out.println(Objects.isNull(s1)); System.out.println(s1 == null);//直接判断比较好 &#125;&#125;public class Student &#123;&#125; String基本介绍String 被声明为 final，因此不可被继承 （Integer 等包装类也不能被继承） 123456public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0&#125; 在 Java 9 之后，String 类的实现改用 byte 数组存储字符串，同时使用 coder 来标识使用了哪种编码 value 数组被声明为 final，这意味着 value 数组初始化之后就不能再引用其它数组，并且 String 内部没有改变 value 数组的方法，因此可以保证 String 不可变，也保证线程安全 注意：不能改变的意思是每次更改字符串都会产生新的对象，并不是对原始对象进行改变 12String s = &quot;abc&quot;;s = s + &quot;cd&quot;; //s = abccd 新对象 常用方法常用 API： public boolean equals(String s)：比较两个字符串内容是否相同、区分大小写 public boolean equalsIgnoreCase(String anotherString)：比较字符串的内容，忽略大小写 public int length()：返回此字符串的长度 public String trim()：返回一个字符串，其值为此字符串，并删除任何前导和尾随空格 public String[] split(String regex)：将字符串按给定的正则表达式分割成字符串数组 public char charAt(int index)：取索引处的值 public char[] toCharArray()：将字符串拆分为字符数组后返回 public boolean startsWith(String prefix)：测试此字符串是否以指定的前缀开头 public int indexOf(String str)：返回指定子字符串第一次出现的字符串内的索引，没有返回 -1 public int lastIndexOf(String str)：返回字符串最后一次出现 str 的索引，没有返回 -1 public String substring(int beginIndex)：返回子字符串，以原字符串指定索引处到结尾 public String substring(int i, int j)：指定索引处扩展到 j - 1 的位置，字符串长度为 j - i public String toLowerCase()：将此 String 所有字符转换为小写，使用默认语言环境的规则 public String toUpperCase()：使用默认语言环境的规则将此 String 所有字符转换为大写 public String replace(CharSequence target, CharSequence replacement)：使用新值，将字符串中的旧值替换，得到新的字符串 12String s = 123-78;s.replace(&quot;-&quot;,&quot;&quot;);//12378 构造方式构造方法： public String()：创建一个空白字符串对象，不含有任何内容 public String(char[] chs)：根据字符数组的内容，来创建字符串对象 public String(String original)：根据传入的字符串内容，来创建字符串对象 直接赋值：String s = &quot;abc&quot; 直接赋值的方式创建字符串对象，内容就是 abc 通过构造方法创建：通过 new 创建的字符串对象，每一次 new 都会申请一个内存空间，虽然内容相同，但是地址值不同，返回堆内存中对象的引用 直接赋值方式创建：以 &quot; &quot; 方式给出的字符串，只要字符序列相同（顺序和大小写），无论在程序代码中出现几次，JVM 都只会在 String Pool 中创建一个字符串对象，并在字符串池中维护 String str = new String(&quot;abc&quot;) 创建字符串对象： 创建一个对象：字符串池中已经存在 abc 对象，那么直接在创建一个对象放入堆中，返回堆内引用 创建两个对象：字符串池中未找到 abc 对象，那么分别在堆中和字符串池中创建一个对象，字符串池中的比较都是采用 equals() new String(&quot;a&quot;) + new String(&quot;b&quot;) 创建字符串对象： 对象 1：new StringBuilder() 对象 2：new String(“a”)、对象 3：常量池中的 a 对象 4：new String(“b”)、对象 5：常量池中的 b StringBuilder 的 toString()： 1234@Overridepublic String toString() &#123; return new String(value, 0, count);&#125; 对象 6：new String(“ab”) StringBuilder 的 toString() 调用，在字符串常量池中没有生成 ab，new String(“ab”) 会创建两个对象因为传参数的时候使用字面量创建了对象 ab，当使用数组构造 String 对象时，没有加入常量池的操作 String Pool基本介绍字符串常量池（String Pool &#x2F; StringTable &#x2F; 串池）保存着所有字符串字面量（literal strings），这些字面量在编译时期就确定，常量池类似于 Java 系统级别提供的缓存，存放对象和引用 StringTable，类似 HashTable 结构，通过 -XX:StringTableSize 设置大小，JDK 1.8 中默认 60013 常量池中的字符串仅是符号，第一次使用时才变为对象，可以避免重复创建字符串对象 字符串变量的拼接的原理是 StringBuilder#append，append 方法比字符串拼接效率高（JDK 1.8） 字符串常量拼接的原理是编译期优化，拼接结果放入常量池 可以使用 String 的 intern() 方法在运行过程将字符串添加到 String Pool 中 intern()JDK 1.8：当一个字符串调用 intern() 方法时，如果 String Pool 中： 存在一个字符串和该字符串值相等，就会返回 String Pool 中字符串的引用（需要变量接收） 不存在，会把对象的引用地址复制一份放入串池，并返回串池中的引用地址，前提是堆内存有该对象，因为 Pool 在堆中，为了节省内存不再创建新对象 JDK 1.6：将这个字符串对象尝试放入串池，如果有就不放入，返回已有的串池中的对象的引用；如果没有会把此对象复制一份，放入串池，把串池中的对象返回 1234567891011121314151617181920212223242526public class Demo &#123; // 常量池中的信息都加载到运行时常量池，这时a b ab是常量池中的符号，还不是java字符串对象，是懒惰的 // ldc #2 会把 a 符号变为 &quot;a&quot; 字符串对象 ldc:反编译后的指令 // ldc #3 会把 b 符号变为 &quot;b&quot; 字符串对象 // ldc #4 会把 ab 符号变为 &quot;ab&quot; 字符串对象 public static void main(String[] args) &#123; String s1 = &quot;a&quot;; // 懒惰的 String s2 = &quot;b&quot;; String s3 = &quot;ab&quot;;\t// 串池 String s4 = s1 + s2;\t// 返回的是堆内地址 // 原理：new StringBuilder().append(&quot;a&quot;).append(&quot;b&quot;).toString() new String(&quot;ab&quot;) String s5 = &quot;a&quot; + &quot;b&quot;; // javac 在编译期间的优化，结果已经在编译期确定为ab System.out.println(s3 == s4); // false System.out.println(s3 == s5); // true String x2 = new String(&quot;c&quot;) + new String(&quot;d&quot;); // new String(&quot;cd&quot;) // 虽然 new，但是在字符串常量池没有 cd 对象，因为 toString() 方法 x2.intern(); String x1 = &quot;cd&quot;; System.out.println(x1 == x2); //true &#125;&#125; &#x3D;&#x3D; 比较基本数据类型：比较的是具体的值 &#x3D;&#x3D; 比较引用数据类型：比较的是对象地址值 结论： 1234String s1 = &quot;ab&quot;; // 仅放入串池String s2 = new String(&quot;a&quot;) + new String(&quot;b&quot;);\t// 仅放入堆// 上面两条指令的结果和下面的 效果 相同String s = new String(&quot;ab&quot;); 常见问题问题一： 1234567891011public static void main(String[] args) &#123; String s = new String(&quot;a&quot;) + new String(&quot;b&quot;);//new String(&quot;ab&quot;) //在上一行代码执行完以后，字符串常量池中并没有&quot;ab&quot; String s2 = s.intern(); //jdk6：串池中创建一个字符串&quot;ab&quot; //jdk8：串池中没有创建字符串&quot;ab&quot;,而是创建一个引用指向 new String(&quot;ab&quot;)，将此引用返回 System.out.println(s2 == &quot;ab&quot;);//jdk6:true jdk8:true System.out.println(s == &quot;ab&quot;);//jdk6:false jdk8:true&#125; 问题二： 1234567public static void main(String[] args) &#123; String str1 = new StringBuilder(&quot;58&quot;).append(&quot;tongcheng&quot;).toString(); System.out.println(str1 == str1.intern());//true，字符串池中不存在，把堆中的引用复制一份放入串池 String str2 = new StringBuilder(&quot;ja&quot;).append(&quot;va&quot;).toString(); System.out.println(str2 == str2.intern());//false，字符串池中存在，直接返回已经存在的引用&#125; 原因： System 类当调用 Version 的静态方法，导致 Version 初始化： 123private static void initializeSystemClass() &#123; sun.misc.Version.init();&#125; Version 类初始化时需要对静态常量字段初始化，被 launcher_name 静态常量字段所引用的 &quot;java&quot; 字符串字面量就被放入的字符串常量池： 12345678910package sun.misc;public class Version &#123; private static final String launcher_name = &quot;java&quot;; private static final String java_version = &quot;1.8.0_221&quot;; private static final String java_runtime_name = &quot;Java(TM) SE Runtime Environment&quot;; private static final String java_profile_name = &quot;&quot;; private static final String java_runtime_version = &quot;1.8.0_221-b11&quot;; //...&#125; 内存位置Java 7 之前，String Pool 被放在运行时常量池中，属于永久代；Java 7 以后，String Pool 被移到堆中，这是因为永久代的空间有限，在大量使用字符串的场景下会导致 OutOfMemoryError 错误 演示 StringTable 位置： -Xmx10m 设置堆内存 10m 在 JDK8 下设置： -Xmx10m -XX:-UseGCOverheadLimit（运行参数在 Run Configurations VM options） 在 JDK6 下设置： -XX:MaxPermSize=10m 1234567891011121314public static void main(String[] args) throws InterruptedException &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); int i = 0; try &#123; for (int j = 0; j &lt; 260000; j++) &#123; list.add(String.valueOf(j).intern()); i++; &#125; &#125; catch (Throwable e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(i); &#125;&#125; 优化常量池两种方式： 调整 -XX:StringTableSize&#x3D;桶个数，数量越少，性能越差 intern 将字符串对象放入常量池，通过复用字符串的引用，减少内存占用 123456789101112131415161718192021222324252627/** * 演示 intern 减少内存占用 * -XX:StringTableSize=200000 -XX:+PrintStringTableStatistics * -Xsx500m -Xmx500m -XX:+PrintStringTableStatistics -XX:StringTableSize=200000 */public class Demo1_25 &#123; public static void main(String[] args) throws IOException &#123; List&lt;String&gt; address = new ArrayList&lt;&gt;(); System.in.read(); for (int i = 0; i &lt; 10; i++) &#123; //很多数据 try (BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;linux.words&quot;), &quot;utf-8&quot;))) &#123; String line = null; long start = System.nanoTime(); while (true) &#123; line = reader.readLine(); if(line == null) &#123; break; &#125; address.add(line.intern()); &#125; System.out.println(&quot;cost:&quot; +(System.nanoTime()-start)/1000000); &#125; &#125; System.in.read(); &#125;&#125; 不可变好处 可以缓存 hash 值，例如 String 用做 HashMap 的 key，不可变的特性可以使得 hash 值也不可变，只要进行一次计算 String Pool 的需要，如果一个 String 对象已经被创建过了，就会从 String Pool 中取得引用，只有 String 是不可变的，才可能使用 String Pool 安全性，String 经常作为参数，String 不可变性可以保证参数不可变。例如在作为网络连接参数的情况下如果 String 是可变的，那么在网络连接过程中，String 被改变，改变 String 的那一方以为现在连接的是其它主机，而实际情况却不一定是 String 不可变性天生具备线程安全，可以在多个线程中安全地使用 防止子类继承，破坏 String 的 API 的使用 StringBuilderString StringBuffer 和 StringBuilder 区别： String : 不可变的字符序列，线程安全 StringBuffer : 可变的字符序列，线程安全，底层方法加 synchronized，效率低 StringBuilder : 可变的字符序列，JDK5.0 新增；线程不安全，效率高 相同点：底层使用 char[] 存储 构造方法： public StringBuilder()：创建一个空白可变字符串对象，不含有任何内容 public StringBuilder(String str)：根据字符串的内容，来创建可变字符串对象 常用API : public StringBuilder append(任意类型)：添加数据，并返回对象本身 public StringBuilder reverse()：返回相反的字符序列 public String toString()：通过 toString() 就可以实现把 StringBuilder 转换为 String 存储原理： 1234String str = &quot;abc&quot;;char data[] = &#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;&#125;;StringBuffer sb1 = new StringBuffer();//new byte[16] sb1.append(&#x27;a&#x27;); //value[0] = &#x27;a&#x27;; append 源码：扩容为二倍 1234567891011121314151617181920public AbstractStringBuilder append(String str) &#123; if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this;&#125;private void ensureCapacityInternal(int minimumCapacity) &#123; // 创建超过数组长度就新的char数组，把数据拷贝过去 if (minimumCapacity - value.length &gt; 0) &#123; //int newCapacity = (value.length &lt;&lt; 1) + 2;每次扩容2倍+2 value = Arrays.copyOf(value, newCapacity(minimumCapacity)); &#125;&#125;public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) &#123; // 将字符串中的字符复制到目标字符数组中\t// 字符串调用该方法，此时value是字符串的值，dst是目标字符数组 System.arraycopy(value, srcBegin, dst, dstBegin, srcEnd - srcBegin);&#125; ArraysArray 的工具类 Arrays 常用API： public static String toString(int[] a)：返回指定数组的内容的字符串表示形式 public static void sort(int[] a)：按照数字顺序排列指定的数组 public static int binarySearch(int[] a, int key)：利用二分查找返回指定元素的索引 public static &lt;T&gt; List&lt;T&gt; asList(T... a)：返回由指定数组支持的列表 12345678910111213141516public class MyArraysDemo &#123; public static void main(String[] args) &#123; //按照数字顺序排列指定的数组 int [] arr = &#123;3,2,4,6,7&#125;; Arrays.sort(arr); System.out.println(Arrays.toString(arr)); int [] arr = &#123;1,2,3,4,5,6,7,8,9,10&#125;; int index = Arrays.binarySearch(arr, 0); System.out.println(index); //1,数组必须有序 //2.如果要查找的元素存在,那么返回的是这个元素实际的索引 //3.如果要查找的元素不存在,那么返回的是 (-插入点-1) //插入点:如果这个元素在数组中,他应该在哪个索引上. &#125; &#125; Random用于生成伪随机数。 使用步骤： 导入包：import java.util.Random 创建对象：Random r = new Random() 随机整数：int num = r.nextInt(10) 解释：10 代表的是一个范围，如果括号写 10，产生的随机数就是 0 - 9，括号写 20 的随机数则是 0 - 19 获取 0 - 10：int num = r.nextInt(10 + 1) 随机小数：public double nextDouble() 从范围 0.0d 至 1.0d （左闭右开），伪随机地生成并返回 SystemSystem 代表当前系统 静态方法： public static void exit(int status)：终止 JVM 虚拟机，非 0 是异常终止 public static long currentTimeMillis()：获取当前系统此刻时间毫秒值 static void arraycopy(Object var0, int var1, Object var2, int var3, int var4)：数组拷贝 参数一：原数组 参数二：从原数组的哪个位置开始赋值 参数三：目标数组 参数四：从目标数组的哪个位置开始赋值 参数五：赋值几个 1234567891011121314public class SystemDemo &#123; public static void main(String[] args) &#123; //System.exit(0); // 0代表正常终止!! long startTime = System.currentTimeMillis();//定义sdf 按照格式输出 for(int i = 0; i &lt; 10000; i++)&#123;输出i&#125; long endTime = new Date().getTime(); System.out.println( (endTime - startTime)/1000.0 +&quot;s&quot;);//程序用时 int[] arr1 = new int[]&#123;10 ,20 ,30 ,40 ,50 ,60 ,70&#125;; int[] arr2 = new int[6]; // [ 0 , 0 , 0 , 0 , 0 , 0] // 变成arrs2 = [0 , 30 , 40 , 50 , 0 , 0 ] System.arraycopy(arr1, 2, arr2, 1, 3); &#125;&#125; Date构造器： public Date()：创建当前系统的此刻日期时间对象。 public Date(long time)：把时间毫秒值转换成日期对象 方法： public long getTime()：返回自 1970 年 1 月 1 日 00:00:00 GMT 以来总的毫秒数。 时间记录的两种方式： Date 日期对象 时间毫秒值：从 1970-01-01 00:00:00 开始走到此刻的总的毫秒值，1s &#x3D; 1000ms 1234567891011public class DateDemo &#123; public static void main(String[] args) &#123; Date d = new Date(); System.out.println(d);//Fri Oct 16 21:58:44 CST 2020 long time = d.getTime() + 121*1000;//过121s是什么时间 System.out.println(time);//1602856875485 Date d1 = new Date(time); System.out.println(d1);//Fri Oct 16 22:01:15 CST 2020 &#125;&#125; 12345678public static void main(String[] args)&#123; Date d = new Date(); long startTime = d.getTime(); for(int i = 0; i &lt; 10000; i++)&#123;输出i&#125; long endTime = new Date().getTime(); System.out.println( (endTime - startTime) / 1000.0 +&quot;s&quot;); //运行一万次输出需要多长时间&#125; DateFormatDateFormat 作用： 可以把“日期对象”或者“时间毫秒值”格式化成我们喜欢的时间形式（格式化时间） 可以把字符串的时间形式解析成日期对象（解析字符串时间） DateFormat 是一个抽象类，不能直接使用，使用它的子类：SimpleDateFormat SimpleDateFormat 简单日期格式化类： public SimpleDateFormat(String pattern)：指定时间的格式创建简单日期对象 public String format(Date date) ：把日期对象格式化成我们喜欢的时间形式，返回字符串 public String format(Object time)：把时间毫秒值格式化成设定的时间形式，返回字符串! public Date parse(String date)：把字符串的时间解析成日期对象 yyyy年MM月dd日 HH:mm:ss EEE a” 周几 上午下午 12345678910111213public static void main(String[] args)&#123;\tDate date = new Date(); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss); String time = sdf.format(date); System.out.println(time);//2020-10-18 19:58:34 //过121s后是什么时间 long time = date.getTime(); time+=121; System.out.println(sdf.formate(time)); String d = &quot;2020-10-18 20:20:20&quot;;//格式一致 Date newDate = sdf.parse(d); System.out.println(sdf.format(newDate)); //按照前面的方法输出&#125; CalendarCalendar 代表了系统此刻日期对应的日历对象，是一个抽象类，不能直接创建对象 Calendar 日历类创建日历对象：Calendar rightNow = Calendar.getInstance()（饿汉单例模式） Calendar 的方法： public static Calendar getInstance()：返回一个日历类的对象 public int get(int field)：取日期中的某个字段信息 public void set(int field,int value)：修改日历的某个字段信息 public void add(int field,int amount)：为某个字段增加&#x2F;减少指定的值 public final Date getTime()：拿到此刻日期对象 public long getTimeInMillis()：拿到此刻时间毫秒值 123456789101112131415public static void main(String[] args)&#123;\tCalendar rightNow = Calendar.getInsance(); int year = rightNow.get(Calendar.YEAR);//获取年 int month = rightNow.get(Calendar.MONTH) + 1;//月要+1 int days = rightNow.get(Calendar.DAY_OF_YEAR); rightNow.set(Calendar.YEAR , 2099);//修改某个字段 rightNow.add(Calendar.HOUR , 15);//加15小时 -15就是减去15小时 Date date = rightNow.getTime();//日历对象 long time = rightNow.getTimeInMillis();//时间毫秒值 //700天后是什么日子 rightNow.add(Calendar.DAY_OF_YEAR , 701); Date date d = rightNow.getTime(); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); System.out.println(sdf.format(d));//输出700天后的日期&#125; LocalDateTimeJDK1.8 新增，线程安全 LocalDate 表示日期（年月日） LocalTime 表示时间（时分秒） LocalDateTime 表示时间+ 日期 （年月日时分秒） 构造方法： public static LocalDateTime now()：获取当前系统时间 public static LocalDateTime of(年, 月 , 日, 时, 分, 秒)：使用指定年月日和时分秒初始化一个对象 常用API： 方法名 说明 public int getYear() 获取年 public int getMonthValue() 获取月份（1-12） public int getDayOfMonth() 获取月份中的第几天（1-31） public int getDayOfYear() 获取一年中的第几天（1-366） public DayOfWeek getDayOfWeek() 获取星期 public int getMinute() 获取分钟 public int getHour() 获取小时 public LocalDate toLocalDate() 转换成为一个 LocalDate 对象（年月日） public LocalTime toLocalTime() 转换成为一个 LocalTime 对象（时分秒） public String format(指定格式) 把一个 LocalDateTime 格式化成为一个字符串 public LocalDateTime parse(准备解析的字符串, 解析格式) 把一个日期字符串解析成为一个 LocalDateTime 对象 public static DateTimeFormatter ofPattern(String pattern) 使用指定的日期模板获取一个日期格式化器 DateTimeFormatter 对象 123456789101112public class JDK8DateDemo2 &#123; public static void main(String[] args) &#123; LocalDateTime now = LocalDateTime.now(); System.out.println(now); LocalDateTime localDateTime = LocalDateTime.of(2020, 11, 11, 11, 11, 11); System.out.println(localDateTime); DateTimeFormatter pattern = DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日 HH:mm:ss&quot;); String s = localDateTime.format(pattern); LocalDateTime parse = LocalDateTime.parse(s, pattern); &#125;&#125; 方法名 说明 public LocalDateTime plusYears (long years) 添加或者减去年 public LocalDateTime withYear(int year) 直接修改年 时间间隔 Duration 类API： 方法名 说明 public static Period between(开始时间,结束时间) 计算两个“时间”的间隔 public int getYears() 获得这段时间的年数 public int getMonths() 获得此期间的总月数 public int getDays() 获得此期间的天数 public long toTotalMonths() 获取此期间的总月数 public static Durationbetween(开始时间,结束时间) 计算两个“时间”的间隔 public long toSeconds() 获得此时间间隔的秒 public long toMillis() 获得此时间间隔的毫秒 public long toNanos() 获得此时间间隔的纳秒 12345678910public class JDK8DateDemo9 &#123; public static void main(String[] args) &#123; LocalDate localDate1 = LocalDate.of(2020, 1, 1); LocalDate localDate2 = LocalDate.of(2048, 12, 12); Period period = Period.between(localDate1, localDate2); System.out.println(period);//P28Y11M11D Duration duration = Duration.between(localDateTime1, localDateTime2); System.out.println(duration);//PT21H57M58S &#125;&#125; MathMath 用于做数学运算 Math 类中的方法全部是静态方法，直接用类名调用即可： 方法 说明 public static int abs(int a) 获取参数a的绝对值 public static double ceil(double a) 向上取整 public static double floor(double a) 向下取整 public static double pow(double a, double b) 获取 a 的 b 次幂 public static long round(double a) 四舍五入取整 public static int max(int a,int b) 返回较大值 public static int min(int a,int b) 返回较小值 public static double random() 返回值为 double 的正值，[0.0,1.0) 12345678910111213141516171819public class MathDemo &#123; public static void main(String[] args) &#123; // 1.取绝对值:返回正数。 System.out.println(Math.abs(10)); System.out.println(Math.abs(-10.3)); // 2.向上取整: 5 System.out.println(Math.ceil(4.00000001)); // 5.0 System.out.println(Math.ceil(-4.00000001));//4.0 // 3.向下取整：4 System.out.println(Math.floor(4.99999999)); // 4.0 System.out.println(Math.floor(-4.99999999)); // 5.0 // 4.求指数次方 System.out.println(Math.pow(2 , 3)); // 2^3 = 8.0 // 5.四舍五入 10 System.out.println(Math.round(4.49999)); // 4 System.out.println(Math.round(4.500001)); // 5 System.out.println(Math.round(5.5));//6 &#125;&#125; DecimalFormat使任何形式的数字解析和格式化 123456789101112131415161718192021222324public static void main(String[]args)&#123; double pi = 3.1415927;　//圆周率 //取一位整数 System.out.println(new DecimalFormat(&quot;0&quot;).format(pi)); //3 //取一位整数和两位小数 System.out.println(new DecimalFormat(&quot;0.00&quot;).format(pi));　//3.14 //取两位整数和三位小数，整数不足部分以0填补。 System.out.println(new DecimalFormat(&quot;00.000&quot;).format(pi));// 03.142 //取所有整数部分 System.out.println(new DecimalFormat(&quot;#&quot;).format(pi)); //3 //以百分比方式计数，并取两位小数 System.out.println(new DecimalFormat(&quot;#.##%&quot;).format(pi));　//314.16% long c =299792458; //光速 //显示为科学计数法，并取五位小数 System.out.println(new DecimalFormat(&quot;#.#####E0&quot;).format(c));//2.99792E8 //显示为两位整数的科学计数法，并取四位小数 System.out.println(new DecimalFormat(&quot;00.####E0&quot;).format(c));//29.9792E7 //每三位以逗号进行分隔。 System.out.println(new DecimalFormat(&quot;,###&quot;).format(c));//299,792,458 //将格式嵌入文本 System.out.println(new DecimalFormat(&quot;光速大小为每秒,###米。&quot;).format(c));&#125; BigDecimalJava 在 java.math 包中提供的 API 类，用来对超过16位有效位的数进行精确的运算 构造方法： public static BigDecimal valueOf(double val)：包装浮点数成为大数据对象。 public BigDecimal(double val) public BigDecimal(String val) 常用API： public BigDecimal add(BigDecimal value)：加法运算 public BigDecimal subtract(BigDecimal value)：减法运算 public BigDecimal multiply(BigDecimal value)：乘法运算 public BigDecimal divide(BigDecimal value)：除法运算 public double doubleValue()：把 BigDecimal 转换成 double 类型 public int intValue()：转为 int 其他类型相同 public BigDecimal divide (BigDecimal value，精确几位，舍入模式)：除法 123456789101112131415161718192021public class BigDecimalDemo &#123; public static void main(String[] args) &#123; // 浮点型运算的时候直接+ - * / 可能会出现数据失真（精度问题）。 System.out.println(0.1 + 0.2); System.out.println(1.301 / 100); double a = 0.1 ; double b = 0.2 ; double c = a + b ; System.out.println(c);//0.30000000000000004 // 1.把浮点数转换成大数据对象运算 BigDecimal a1 = BigDecimal.valueOf(a); BigDecimal b1 = BigDecimal.valueOf(b); BigDecimal c1 = a1.add(b1);//a1.divide(b1);也可以 System.out.println(c1); // BigDecimal只是解决精度问题的手段，double数据才是我们的目的！！ double d = c1.doubleValue(); &#125;&#125; 总结： BigDecimal 是用来进行精确计算的 创建 BigDecimal 的对象，构造方法使用参数类型为字符串的 四则运算中的除法，如果除不尽请使用 divide 的三个参数的方法 1234567BigDecimal divide = bd1.divide(参与运算的对象,小数点后精确到多少位,舍入模式);//参数1：表示参与运算的BigDecimal 对象。//参数2：表示小数点后面精确到多少位//参数3：舍入模式 // BigDecimal.ROUND_UP 进一法// BigDecimal.ROUND_FLOOR 去尾法// BigDecimal.ROUND_HALF_UP 四舍五入 Regex概述正则表达式的作用：是一些特殊字符组成的校验规则，可以校验信息的正确性，校验邮箱、电话号码、金额等。 比如检验 qq 号： 123public static boolean checkQQRegex(String qq)&#123; return qq!=null &amp;&amp; qq.matches(&quot;\\\\d&#123;4,&#125;&quot;);//即是数字 必须大于4位数&#125;// 用\\\\d 是因为\\用来告诉它是一个校验类，不是普通的字符 比如 \\t java.util.regex 包主要包括以下三个类： Pattern 类： Pattern 对象是一个正则表达式的编译表示。Pattern 类没有公共构造方法，要创建一个 Pattern 对象，必须首先调用其公共静态编译方法，返回一个 Pattern 对象。该方法接受一个正则表达式作为它的第一个参数 Matcher 类： Matcher 对象是对输入字符串进行解释和匹配操作的引擎。与Pattern 类一样，Matcher 也没有公共构造方法，需要调用 Pattern 对象的 matcher 方法来获得一个 Matcher 对象 PatternSyntaxException： PatternSyntaxException 是一个非强制异常类，它表示一个正则表达式模式中的语法错误。 字符匹配普通字符字母、数字、汉字、下划线、以及没有特殊定义的标点符号，都是“普通字符”。表达式中的普通字符，在匹配一个字符串的时候，匹配与之相同的一个字符。其他统称元字符 特殊字符\\r 是 Windows 中的文本行结束标签，在 Unix&#x2F;Linux 则是 元字符 说明 \\ 将下一个字符标记为一个特殊字符或原义字符，告诉它是一个校验类，不是普通字符 \\f 换页符 换行符 \\r 回车符 \\t 制表符 \\ 代表 \\ 本身 () 使用 () 定义一个子表达式。子表达式的内容可以当成一个独立元素 标准字符能够与多种字符匹配的表达式，注意区分大小写，大写是相反的意思，只能校验单个字符。 元字符 说明 . 匹配任意一个字符（除了换行符），如果要匹配包括 在内的所有字符，一般用 [\\s\\S] \\d 数字字符，0~9 中的任意一个，等价于 [0-9] \\D 非数字字符，等价于 [ ^0-9] \\w 大小写字母或数字或下划线，等价于[a-zA-Z_0-9_] \\W 对\\w取非，等价于[ ^\\w] \\s 空格、制表符、换行符等空白字符的其中任意一个，等价于[\\f \\r\\t\\v] \\S 对 \\s 取非 \\x 匹配十六进制字符，\\0 匹配八进制，例如 \\xA 对应值为 10 的 ASCII 字符 ，即 自定义符自定义符号集合，[ ] 方括号匹配方式，能够匹配方括号中任意一个字符 元字符 说明 [ab5@] 匹配 “a” 或 “b” 或 “5” 或 “@” [^abc] 匹配 “a”,”b”,”c” 之外的任意一个字符 [f-k] 匹配 “f”~”k” 之间的任意一个字母 [^A-F0-3] 匹配 “A”,”F”,”0”~”3” 之外的任意一个字符 [a-d[m-p]] 匹配 a 到 d 或者 m 到 p：[a-dm-p]（并集） [a-z&amp;&amp;[m-p]] 匹配 a 到 z 并且 m 到 p：[a-dm-p]（交集） [^] 取反 正则表达式的特殊符号，被包含到中括号中，则失去特殊意义，除了 ^,- 之外，需要在前面加 \\ 标准字符集合，除小数点外，如果被包含于中括号，自定义字符集合将包含该集合。比如：[\\d. \\ -+] 将匹配：数字、小数点、+、- 量词字符修饰匹配次数的特殊符号。 匹配次数中的贪婪模式(匹配字符越多越好，默认 ！)，* 和 + 都是贪婪型元字符。 匹配次数中的非贪婪模式（匹配字符越少越好，修饰匹配次数的特殊符号后再加上一个 ? 号） 元字符 说明 X? X 一次或一次也没，有相当于 {0,1} X* X 不出现或出现任意次，相当于 {0,} X+ X 至少一次，相当于 {1,} X{n} X 恰好 n 次 {n,} X 至少 n 次 {n,m} X 至少 n 次，但是不超过 m 次 位置匹配字符边界本组标记匹配的不是字符而是位置，符合某种条件的位置 元字符 说明 ^ 与字符串开始的地方匹配（在字符集合中用来求非，在字符集合外用作匹配字符串的开头） $ 与字符串结束的地方匹配 \\b 匹配一个单词边界 捕获组捕获组是把多个字符当一个单独单元进行处理的方法，它通过对括号内的字符分组来创建。 在表达式 ((A)(B(C)))，有四个这样的组：((A)(B(C)))、(A)、(B(C))、(C)（按照括号从左到右依次为 group(1)…） 调用 matcher 对象的 groupCount 方法返回一个 int 值，表示 matcher 对象当前有多个捕获组。 特殊的组 group(0)、group()，代表整个表达式，该组不包括在 groupCount 的返回值中。 表达式 说明 | (分支结构) 左右两边表达式之间 “或” 关系，匹配左边或者右边 () (捕获组) (1) 在被修饰匹配次数的时候，括号中的表达式可以作为整体被修饰(2) 取匹配结果的时候，括号中的表达式匹配到的内容可以被单独得到(3) 每一对括号分配一个编号,()的捕获根据左括号的顺序从 1 开始自动编号。捕获元素编号为零的第一个捕获是由整个正则表达式模式匹配的文本 (?:Expression) 非捕获组 一些表达式中，不得不使用( )，但又不需要保存 () 中子表达式匹配的内容，这时可以用非捕获组来抵消使用( )带来的副作用。 反向引用反向引用（ umber），又叫回溯引用： 每一对()会分配一个编号，使用 () 的捕获根据左括号的顺序从1开始自动编号 通过反向引用，可以对分组已捕获的字符串进行引用，继续匹配 把匹配到的字符重复一遍在进行匹配 应用 1： 1String regex = &quot;((\\d)3)\\1[0-9](\\w)\\2&#123;2&#125;&quot;; 首先匹配 ((\\d)3)，其次 \\1 匹配 ((\\d)3) 已经匹配到的内容，\\2 匹配 (\\d)， {2} 指的是 \\2 的值出现两次 实例：23238n22（匹配到 2 未来就继续匹配 2） 实例：43438n44 应用 2：爬虫 1String regex = &quot;&lt;(h[1-6])&gt;\\w*?&lt;\\/\\1&gt;&quot;; 匹配结果 123&lt;h1&gt;x&lt;/h1&gt;//匹配&lt;h2&gt;x&lt;/h2&gt;//匹配&lt;h3&gt;x&lt;/h1&gt;//不匹配 零宽断言预搜索（零宽断言）（环视） 只进行子表达式的匹配，匹配内容不计入最终的匹配结果，是零宽度 判断当前位置的前后字符，是否符合指定的条件，但不匹配前后的字符，是对位置的匹配 正则表达式匹配过程中，如果子表达式匹配到的是字符内容，而非位置，并被保存到最终的匹配结果中，那么就认为这个子表达式是占有字符的；如果子表达式匹配的仅仅是位置，或者匹配的内容并不保存到最终的匹配结果中，那么就认为这个子表达式是零宽度的。占有字符还是零宽度，是针对匹配的内容是否保存到最终的匹配结果中而言的 表达式 说明 (?&#x3D;exp) 断言自身出现的位置的后面能匹配表达式exp (?&lt;&#x3D;exp) 断言自身出现的位置的前面能匹配表达式exp (?!exp) 断言此位置的后面不能匹配表达式exp (?&lt;!exp) 断言此位置的前面不能匹配表达式exp 匹配模式正则表达式的匹配模式： IGNORECASE 忽略大小写模式 匹配时忽略大小写。 默认情况下，正则表达式是要区分大小写的。 SINGLELINE 单行模式 整个文本看作一个字符串，只有一个开头，一个结尾。 使小数点 “.” 可以匹配包含换行符（ ）在内的任意字符。 MULTILINE 多行模式 每行都是一个字符串，都有开头和结尾。 在指定了 MULTILINE 之后，如果需要仅匹配字符串开始和结束位置，可以使用 \\A 和 \\Z 分组匹配Pattern 类： static Pattern compile(String regex)：将给定的正则表达式编译为模式 Matcher matcher(CharSequence input)：创建一个匹配器，匹配给定的输入与此模式 static boolean matches(String regex, CharSequence input)：编译正则表达式，并匹配输入 Matcher 类： boolean find()：扫描输入的序列，查找与该模式匹配的下一个子序列 String group()：返回与上一个匹配的输入子序列，同 group(0)，匹配整个表达式的子字符串 String group(int group)：返回在上一次匹配操作期间由给定组捕获的输入子序列 int groupCount()：返回此匹配器模式中捕获组的数量 1234567891011121314151617181920212223public class Demo01&#123;\tpublic static void main(String[] args) &#123; //表达式对象 Pattern p = Pattern.compile(&quot;\\\\w+&quot;); //创建Matcher对象 Matcher m = p.matcher(&quot;asfsdf2&amp;&amp;3323&quot;); //boolean b = m.matches();//尝试将整个字符序列与该模式匹配 //System.out.println(b);//false //boolean b2 = m.find();//该方法扫描输入的序列，查找与该模式匹配的下一个子序列 //System.out.println(b2);//true //System.out.println(m.find()); //System.out.println(m.group());//asfsdf2 //System.out.println(m.find()); //System.out.println(m.group());//3323 while(m.find())&#123; System.out.println(m.group());\t//group(),group(0)匹配整个表达式的子字符串 System.out.println(m.group(0)); &#125; &#125;&#125; 12345678910111213141516public class Demo02 &#123;\tpublic static void main(String[] args) &#123; //在这个字符串：asfsdf23323，是否符合指定的正则表达式：\\w+ //表达式对象 Pattern p = Pattern.compile(&quot;(([a-z]+)([0-9]+))&quot;);//不需要加多余的括号 //创建Matcher对象 Matcher m = p.matcher(&quot;aa232**ssd445&quot;); while(m.find())&#123; System.out.println(m.group());//aa232 ssd445 System.out.println(m.group(1));//aa232 ssd445 System.out.println(m.group(2));//aa ssd System.out.println(m.group(3));//232 445 &#125;\t&#125;&#125; 正则表达式改为 &quot;(([a-z]+)(?:[0-9]+))&quot; 没有 group(3) 因为是非捕获组 正则表达式改为 &quot;([a-z]+)([0-9]+)&quot; 没有 group(3) aa232 - aa –232 应用基本验证1234567891011121314public static void main(String[] args)&#123;\tSystem.out.println(&quot;a&quot;.matches(&quot;[abc]&quot;));//true判断a是否在abc System.out.println(&quot;a&quot;.matches(&quot;[^abc]&quot;));//false 判断a是否在abc之外的 System.out.println(&quot;a&quot;.matches(&quot;\\\\d&quot;)); //false 是否a是整数 System.out.println(&quot;a&quot;.matches(&quot;\\\\w&quot;)); //true 是否是字符 System.out.println(&quot;你&quot;.matches(&quot;\\\\w&quot;)); // false System.out.println(&quot;aa&quot;.matches(&quot;\\\\w&quot;));//false 只能检验单个字符 // 密码 必须是数字 字母 下划线 至少 6位\tSystem.out.println(&quot;ssds3c&quot;.matches(&quot;\\\\w&#123;6,&#125;&quot;)); // true // 验证。必须是数字和字符 必须是4位 System.out.println(&quot;dsd22&quot;.matches(&quot;[a-zA-Z0-9]&#123;4&#125;&quot;)); // false System.out.println(&quot;A3dy&quot;.matches(&quot;[a-zA-Z0-9]&#123;4&#125;&quot;)); // true&#125; 验证号码123456789101112//1开头 第二位是2-9的数字public static void checkPhone(String phone)&#123; if(phone.matches(&quot;1[3-9]\\\\d&#123;9&#125;&quot;))&#123; System.out.println(&quot;手机号码格式正确！&quot;); &#125; else &#123;.......&#125;&#125;//1111@qq.com zhy@pic.com.cnpublic static void checkEmail(String email)&#123; if(email.matches(&quot;\\\\w&#123;1,&#125;@\\\\w&#123;1,&#125;(\\\\.\\\\w&#123;2,5&#125;)&#123;1,2&#125;&quot;))&#123; System.out.println(&quot;邮箱格式正确！&quot;); &#125;// .是任意字符 \\\\.就是点&#125; 查找替换 public String[] split(String regex)：按照正则表达式匹配的内容进行分割字符串，反回一个字符串数组 public String replaceAll(String regex,String newStr)：按照正则表达式匹配的内容进行替换 123456789101112131415161718//数组分割public static void main(String[] args) &#123;\t// 1.split的基础用法\tString names = &quot;风清扬,张无忌,周芷若&quot;;\t// 以“，”分割成字符串数组 String[] nameArrs = names.split(&quot;,&quot;); // 2.split集合正则表达式做分割 String names1 = &quot;风清扬lv434fda324张无忌87632fad2342423周芷若&quot;; // 以匹配正则表达式的内容为分割点分割成字符串数组\tString[] nameArrs1 = names1.split(&quot;\\\\w+&quot;); // 使用正则表达式定位出内容，替换成/\tSystem.out.println(names1.replaceAll(&quot;\\\\w+&quot;,&quot;/&quot;));//风清扬/张无忌/周芷若\tString names3 = &quot;风清扬,张无忌,周芷若&quot;;\tSystem.out.println(names3.replaceAll(&quot;,&quot;,&quot;-&quot;));//风清扬-张无忌-周芷若&#125; 搜索号码找出所有 189 和 132 开头的手机号 1234567891011public class RegexDemo &#123; public static void main(String[] args) &#123; String rs = &quot;189asjk65as1891898777745gkkkk189745612318936457894&quot;; String regex = &quot;(?=((189|132)\\\\d&#123;8&#125;))&quot;; Pattern pattern = Pattern.compile(regex); Matcher matcher = pattern.matcher(rs); while (matcher.find()) &#123; System.out.println(matcher.group(1)); &#125; &#125;&#125; 集合集合概述集合是一个大小可变的容器，容器中的每个数据称为一个元素 集合特点：类型可以不确定，大小不固定；集合有很多，不同的集合特点和使用场景不同 数组：类型和长度一旦定义出来就都固定 作用： 在开发中，很多时候元素的个数是不确定的 而且经常要进行元素的增删该查操作，集合都是非常合适的，开发中集合用的更多 存储结构数据结构指的是数据以什么方式组织在一起，不同的数据结构，增删查的性能是不一样的 数据存储的常用结构有：栈、队列、数组、链表和红黑树 队列（queue）：先进先出，后进后出。(FIFO first in first out) 栈（stack）：后进先出，先进后出 （LIFO） 数组：数组是内存中的连续存储区域，分成若干等分的小区域（每个区域大小是一样的）元素存在索引 特点：查询元素快（根据索引快速计算出元素的地址，然后立即去定位），增删元素慢（创建新数组，迁移元素） 链表：元素不是内存中的连续区域存储，元素是游离存储的，每个元素会记录下个元素的地址特点：查询元素慢，增删元素快（针对于首尾元素，速度极快，一般是双链表） 树： 二叉树：binary tree 永远只有一个根节点，是每个结点不超过2个节点的树（tree） 特点：二叉排序树：小的左边，大的右边，但是可能树很高，性能变差，为了做排序和搜索会进行左旋和右旋实现平衡查找二叉树，让树的高度差不大于1 红黑树（基于红黑规则实现自平衡的排序二叉树）：树保证到了很矮小，但是又排好序，性能最高的 特点：红黑树的增删查改性能都好 各数据结构时间复杂度对比： 图片来源：https://www.bigocheatsheet.com/ Collection概述Java 中集合的代表是 Collection，Collection 集合是 Java 中集合的祖宗类 Collection 集合底层为数组：[value1, value2, ....] 12345678Collection集合的体系: Collection&lt;E&gt;(接口) / \\ Set&lt;E&gt;(接口) List&lt;E&gt;(接口) / \\ / \\ HashSet&lt;E&gt;(实现类) TreeSet&lt;&gt;(实现类) ArrayList&lt;E&gt;(实现类) LinekdList&lt;&gt;(实现类) /LinkedHashSet&lt;&gt;(实现类) 集合的特点： Set 系列集合：添加的元素是无序，不重复，无索引的 HashSet：添加的元素是无序，不重复，无索引的 LinkedHashSet：添加的元素是有序，不重复，无索引的 TreeSet：不重复，无索引，按照大小默认升序排序 List 系列集合：添加的元素是有序，可重复，有索引 ArrayList：添加的元素是有序，可重复，有索引 LinekdList：添加的元素是有序，可重复，有索引 APICollection 是集合的祖宗类，它的功能是全部集合都可以继承使用的，所以要学习它。 Collection 子类的构造器都有可以包装其他子类的构造方法，如： public ArrayList(Collection&lt;? extends E&gt; c)：构造新集合，元素按照由集合的迭代器返回的顺序 public HashSet(Collection&lt;? extends E&gt; c)：构造一个包含指定集合中的元素的新集合 Collection API 如下： public boolean add(E e)：把给定的对象添加到当前集合中 。 public void clear()：清空集合中所有的元素。 public boolean remove(E e)：把给定的对象在当前集合中删除。 public boolean contains(Object obj)：判断当前集合中是否包含给定的对象。 public boolean isEmpty()：判断当前集合是否为空。 public int size()：返回集合中元素的个数。 public Object[] toArray()：把集合中的元素，存储到数组中 public boolean addAll(Collection&lt;? extends E&gt; c)：将指定集合中的所有元素添加到此集合 1234567891011121314151617181920public class CollectionDemo &#123; public static void main(String[] args) &#123; Collection&lt;String&gt; sets = new HashSet&lt;&gt;(); sets.add(&quot;MyBatis&quot;); System.out.println(sets.add(&quot;Java&quot;));//true System.out.println(sets.add(&quot;Java&quot;));//false sets.add(&quot;Spring&quot;); sets.add(&quot;MySQL&quot;); System.out.println(sets)//[]无序的; System.out.println(sets.contains(&quot;java&quot;));//true 存在 Object[] arrs = sets.toArray(); System.out.println(&quot;数组：&quot;+ Arrays.toString(arrs)); Collection&lt;String&gt; c1 = new ArrayList&lt;&gt;(); c1.add(&quot;java&quot;); Collection&lt;String&gt; c2 = new ArrayList&lt;&gt;(); c2.add(&quot;ee&quot;); c1.addAll(c2);// c1:[java,ee] c2:[ee]; &#125;&#125; 遍历Collection 集合的遍历方式有三种: 集合可以直接输出内容，因为底层重写了 toString() 方法 迭代器 public Iterator iterator()：获取集合对应的迭代器，用来遍历集合中的元素的 E next()：获取下一个元素值 boolean hasNext()：判断是否有下一个元素，有返回 true ，反之返回 false default void remove()：从底层集合中删除此迭代器返回的最后一个元素，这种方法只能在每次调用 next() 时调用一次 增强 for 循环：可以遍历集合或者数组，遍历集合实际上是迭代器遍历的简化写法 123for(被遍历集合或者数组中元素的类型 变量名称 : 被遍历集合或者数组)&#123;&#125; 缺点：遍历无法知道遍历到了哪个元素了，因为没有索引 JDK 1.8 开始之后的新技术 Lambda 表达式 1234567891011121314151617181920212223242526public class CollectionDemo &#123; public static void main(String[] args) &#123; Collection&lt;String&gt; lists = new ArrayList&lt;&gt;(); lists.add(&quot;aa&quot;); lists.add(&quot;bb&quot;); lists.add(&quot;cc&quot;); System.out.println(lists); // lists = [aa, bb, cc] //迭代器流程 // 1.得到集合的迭代器对象。 Iterator&lt;String&gt; it = lists.iterator(); // 2.使用while循环遍历。 while(it.hasNext())&#123; String ele = it.next(); System.out.println(ele); &#125; //增强for for (String ele : lists) &#123; System.out.println(ele); &#125; //lambda表达式 lists.forEach(s -&gt; &#123; System.out.println(s); &#125;); &#125;&#125; List概述List 集合继承了 Collection 集合全部的功能。 List 系列集合有索引，所以多了很多按照索引操作元素的功能：for 循环遍历（4 种遍历） List 系列集合： ArrayList：添加的元素是有序，可重复，有索引 LinekdList：添加的元素是有序，可重复，有索引 ArrayList介绍ArrayList 添加的元素，是有序，可重复，有索引的 public boolean add(E e)：将指定的元素追加到此集合的末尾 public void add(int index, E element)：将指定的元素，添加到该集合中的指定位置上 public E get(int index)：返回集合中指定位置的元素 public E remove(int index)：移除列表中指定位置的元素，返回的是被移除的元素 public E set(int index, E element)：用指定元素替换集合中指定位置的元素，返回更新前的元素值 int indexOf(Object o)：返回列表中指定元素第一次出现的索引，如果不包含此元素，则返回 -1 12345678910public static void main(String[] args)&#123; List&lt;String&gt; lists = new ArrayList&lt;&gt;();//多态 lists.add(&quot;java1&quot;); lists.add(&quot;java1&quot;);//可以重复 lists.add(&quot;java2&quot;); for(int i = 0 ; i &lt; lists.size() ; i++ ) &#123; String ele = lists.get(i); System.out.println(ele); &#125;&#125; 源码ArrayList 实现类集合底层基于数组存储数据的，查询快，增删慢，支持快速随机访问 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123;&#125; RandomAccess 是一个标志接口，表明实现这个这个接口的 List 集合是支持快速随机访问的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。 ArrayList 实现了 Cloneable 接口 ，即覆盖了函数 clone()，能被克隆 ArrayList 实现了 Serializable 接口，这意味着 ArrayList 支持序列化，能通过序列化去传输 核心方法： 构造函数：以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量（惰性初始化），即向数组中添加第一个元素时，数组容量扩为 10 添加元素： 123456// e 插入的元素 elementData底层数组 size 插入的位置public boolean add(E e) &#123; ensureCapacityInternal(size + 1);\t// Increments modCount!! elementData[size++] = e; // 插入size位置，然后加一 return true;&#125; 当 add 第 1 个元素到 ArrayList，size 是 0，进入 ensureCapacityInternal 方法， 123private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125; 12345678private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; // 判断elementData是不是空数组 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 返回默认值和最小需求容量最大的一个 return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125; 如果需要的容量大于数组长度，进行扩容： 12345678// 判断是否需要扩容private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 索引越界 if (minCapacity - elementData.length &gt; 0) // 调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity);&#125; 指定索引插入，在旧数组上操作： 12345678public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! // 将指定索引后的数据后移 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 扩容：新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1)，oldCapacity &gt;&gt; 1 需要取整，所以新容量大约是旧容量的 1.5 倍左右，即 oldCapacity+oldCapacity&#x2F;2 扩容操作需要调用 Arrays.copyOf()（底层 System.arraycopy()）把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数 12345678910111213private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //检查新容量是否大于最小需要容量，若小于最小需要容量，就把最小需要容量当作数组的新容量 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity;//不需要扩容计算 //检查新容量是否大于最大数组容量 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE` //否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8` newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; MAX_ARRAY_SIZE：要分配的数组的最大大小，分配更大的可能会导致 OutOfMemoryError:Requested array size exceeds VM limit（请求的数组大小超出 VM 限制） OutOfMemoryError: Java heap space（堆区内存不足，可以通过设置 JVM 参数 -Xmx 来调节） 删除元素：需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，在旧数组上操作，该操作的时间复杂度为 O(N)，可以看到 ArrayList 删除元素的代价是非常高的 123456789101112public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 序列化：ArrayList 基于数组并且具有动态扩容特性，因此保存元素的数组不一定都会被使用，就没必要全部进行序列化。保存元素的数组 elementData 使用 transient 修饰，该关键字声明数组默认不会被序列化 1transient Object[] elementData; ensureCapacity：增加此实例的容量，以确保它至少可以容纳最小容量参数指定的元素数，减少增量重新分配的次数 12345678public void ensureCapacity(int minCapacity) &#123; if (minCapacity &gt; elementData.length &amp;&amp; !(elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA &amp;&amp; minCapacity &lt;= DEFAULT_CAPACITY)) &#123; modCount++; grow(minCapacity); &#125;&#125; Fail-Fast：快速失败，modCount 用来记录 ArrayList 结构发生变化的次数，结构发生变化是指添加或者删除至少一个元素的操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，改变了抛出 ConcurrentModificationException 异常 123public Iterator&lt;E&gt; iterator() &#123; return new Itr();&#125; 123456789101112131415161718192021222324252627282930private class Itr implements Iterator&lt;E&gt; &#123; int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; Itr() &#123;&#125; public boolean hasNext() &#123; return cursor != size; &#125; // 获取下一个元素时首先判断结构是否发生变化 public E next() &#123; checkForComodification(); // ..... &#125; // modCount 被其他线程改变抛出并发修改异常 final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125;\t// 【允许删除操作】 public void remove() &#123; // ... checkForComodification(); // ... // 删除后重置 expectedModCount expectedModCount = modCount; &#125;&#125; Vector同步：Vector 的实现与 ArrayList 类似，但是方法上使用了 synchronized 进行同步 构造：默认长度为 10 的数组 扩容：Vector 的构造函数可以传入 capacityIncrement 参数，作用是在扩容时使容量 capacity 增长 capacityIncrement，如果这个参数的值小于等于 0（默认0），扩容时每次都令 capacity 为原来的两倍 对比 ArrayList Vector 是同步的，开销比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序来控制 Vector 每次扩容请求其大小的 2 倍（也可以通过构造函数设置增长的容量），而 ArrayList 是 1.5 倍 底层都是 Object[] 数组存储 LinkedList介绍LinkedList 也是 List 的实现类：基于双向链表实现，使用 Node 存储链表节点信息，增删比较快，查询慢 LinkedList 除了拥有 List 集合的全部功能还多了很多操作首尾元素的特殊功能： public boolean add(E e)：将指定元素添加到此列表的结尾 public E poll()：检索并删除此列表的头（第一个元素） public void addFirst(E e)：将指定元素插入此列表的开头 public void addLast(E e)：将指定元素添加到此列表的结尾 public E pop()：从此列表所表示的堆栈处弹出一个元素 public void push(E e)：将元素推入此列表所表示的堆栈 public int indexOf(Object o)：返回此列表中指定元素的第一次出现的索引，如果不包含返回 -1 public int lastIndexOf(Object o)：从尾遍历找 public boolean remove(Object o)：一次只删除一个匹配的对象，如果删除了匹配对象返回 true public E remove(int index)：删除指定位置的元素 123456789101112131415161718192021222324252627public class ListDemo &#123; public static void main(String[] args) &#123; // 1.用LinkedList做一个队列:先进先出，后进后出。 LinkedList&lt;String&gt; queue = new LinkedList&lt;&gt;(); // 入队 queue.addLast(&quot;1号&quot;); queue.addLast(&quot;2号&quot;); queue.addLast(&quot;3号&quot;); System.out.println(queue); // [1号, 2号, 3号] // 出队 System.out.println(queue.removeFirst());//1号 System.out.println(queue.removeFirst());//2号 System.out.println(queue);//[3号] // 做一个栈 先进后出 LinkedList&lt;String&gt; stack = new LinkedList&lt;&gt;(); // 压栈 stack.push(&quot;第1颗子弹&quot;);//addFirst(e); stack.push(&quot;第2颗子弹&quot;); stack.push(&quot;第3颗子弹&quot;); System.out.println(stack); // [ 第3颗子弹, 第2颗子弹, 第1颗子弹] // 弹栈 System.out.println(stack.pop());//removeFirst(); 第3颗子弹 System.out.println(stack.pop()); System.out.println(stack);// [第1颗子弹] &#125;&#125; 源码LinkedList 是一个实现了 List 接口的双端链表，支持高效的插入和删除操作，另外也实现了 Deque 接口，使得 LinkedList 类也具有队列的特性 核心方法： 使 LinkedList 变成线程安全的，可以调用静态类 Collections 类中的 synchronizedList 方法： 1List list = Collections.synchronizedList(new LinkedList(...)); 私有内部类 Node：这个类代表双端链表的节点 Node 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 构造方法：只有无参构造和用已有的集合创建链表的构造方法 添加元素：默认加到尾部 1234public boolean add(E e) &#123; linkLast(e); return true;&#125; 获取元素：get(int index) 根据指定索引返回数据 获取头节点 (index&#x3D;0)：getFirst()、element()、peek()、peekFirst() 这四个获取头结点方法的区别在于对链表为空时的处理方式，是抛出异常还是返回NULL，其中 getFirst() element() 方法将会在链表为空时，抛出异常 获取尾节点 (index&#x3D;-1)：getLast() 方法在链表为空时，抛出 NoSuchElementException，而 peekLast() 不会，只会返回 null 删除元素： remove()、removeFirst()、pop()：删除头节点 removeLast()、pollLast()：删除尾节点，removeLast()在链表为空时抛出NoSuchElementException，而pollLast()方法返回null 对比 ArrayList 是否保证线程安全：ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全 底层数据结构： Arraylist 底层使用的是 Object 数组 LinkedList 底层使用的是双向链表数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环） 插入和删除是否受元素位置的影响： ArrayList 采用数组存储，所以插入和删除元素受元素位置的影响 LinkedList采 用链表存储，所以对于add(E e)方法的插入，删除元素不受元素位置的影响 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，ArrayList 支持 快速随机访问就是通过元素的序号快速获取元素对象(对应于 get(int index) 方法) 内存空间占用： ArrayList 的空间浪费主要体现在在 list 列表的结尾会预留一定的容量空间 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList更多的空间（因为要存放直接后继和直接前驱以及数据） Set概述Set 系列集合： HashSet：添加的元素是无序，不重复，无索引的 LinkedHashSet：添加的元素是有序，不重复，无索引的 TreeSet：不重复，无索引，按照大小默认升序排序 注意：没有索引，不能使用普通 for 循环遍历 HashSet哈希值： 哈希值：JDK 根据对象的地址或者字符串或者数字计算出来的数值 获取哈希值：Object 类中的 public int hashCode() 哈希值的特点 同一个对象多次调用 hashCode() 方法返回的哈希值是相同的 默认情况下，不同对象的哈希值是不同的，而重写 hashCode() 方法，可以实现让不同对象的哈希值相同 HashSet 底层就是基于 HashMap 实现，值是 PRESENT &#x3D; new Object() Set 集合添加的元素是无序，不重复的。 是如何去重复的？ 123456789101112131415161.对于有值特性的，Set集合可以直接判断进行去重复。2.对于引用数据类型的类对象，Set集合是按照如下流程进行是否重复的判断。 Set集合会让两两对象，先调用自己的hashCode()方法得到彼此的哈希值（所谓的内存地址） 然后比较两个对象的哈希值是否相同，如果不相同则直接认为两个对象不重复。 如果哈希值相同，会继续让两个对象进行equals比较内容是否相同，如果相同认为真的重复了 如果不相同认为不重复。 Set集合会先让对象调用hashCode()方法获取两个对象的哈希值比较 / \\ false true / \\ 不重复 继续让两个对象进行equals比较 / \\ false true / \\ 不重复 重复了 Set 系列集合元素无序的根本原因 Set 系列集合添加元素无序的根本原因是因为底层采用了哈希表存储元素。 JDK 1.8 之前：哈希表 &#x3D; 数组（初始容量16) + 链表 + （哈希算法） JDK 1.8 之后：哈希表 &#x3D; 数组（初始容量16) + 链表 + 红黑树 + （哈希算法） 当链表长度超过阈值 8 且当前数组的长度 &gt; 64时，将链表转换为红黑树，减少了查找时间 当链表长度超过阈值 8 且当前数组的长度 &lt; 64时，扩容 每个元素的 hashcode() 的值进行响应的算法运算，计算出的值相同的存入一个数组块中，以链表的形式存储，如果链表长度超过8就采取红黑树存储，所以输出的元素是无序的。 如何设置只要对象内容一样，就希望集合认为重复：重写 hashCode 和 equals 方法 LinkedLinkedHashSet 为什么是有序的？ LinkedHashSet 底层依然是使用哈希表存储元素的，但是每个元素都额外带一个链来维护添加顺序，不光增删查快，还有顺序，缺点是多了一个存储顺序的链会占内存空间，而且不允许重复，无索引 TreeSetTreeSet 集合自排序的方式： 有值特性的元素直接可以升序排序（浮点型，整型） 字符串类型的元素会按照首字符的编号排序 对于自定义的引用数据类型，TreeSet 默认无法排序，执行的时候报错，因为不知道排序规则 自定义的引用数据类型，TreeSet 默认无法排序，需要定制排序的规则，方案有 2 种： 直接为对象的类实现比较器规则接口 Comparable，重写比较方法： 方法：public int compareTo(Employee o): this 是比较者, o 是被比较者 * 比较者大于被比较者，返回正数 * 比较者小于被比较者，返回负数 * 比较者等于被比较者，返回 0 直接为集合设置比较器 Comparator 对象，重写比较方法： 方法：public int compare(Employee o1, Employee o2): o1 比较者, o2 被比较者 比较者大于被比较者，返回正数 比较者小于被比较者，返回负数 比较者等于被比较者，返回 0 注意：如果类和集合都带有比较规则，优先使用集合自带的比较规则 123456789101112131415161718192021222324252627282930public class TreeSetDemo&#123; public static void main(String[] args)&#123; Set&lt;Student&gt; students = new TreeSet&lt;&gt;(); Collections.add(students,s1,s2,s3); System.out.println(students);//按照年龄比较 升序 Set&lt;Student&gt; s = new TreeSet&lt;&gt;(new Comparator&lt;Student&gt;()&#123; @Override public int compare(Student o1, Student o2) &#123; // o1比较者 o2被比较者 return o2.getAge() - o1.getAge();//降序 &#125; &#125;); &#125;&#125;public class Student implements Comparable&lt;Student&gt;&#123; private String name; private int age; // 重写了比较方法。 // e1.compareTo(o) // 比较者：this // 被比较者：o // 需求：按照年龄比较 升序，年龄相同按照姓名 @Override public int compareTo(Student o) &#123; int result = this.age - o.age; return result == 0 ? this.getName().compareTo(o.getName):result; &#125;&#125; 比较器原理：底层是以第一个元素为基准，加一个新元素，就会和第一个元素比，如果大于，就继续和大于的元素进行比较，直到遇到比新元素大的元素为止，放在该位置的左边（红黑树） QueueQueue：队列，先进先出的特性 PriorityQueue 是优先级队列，底层存储结构为 Object[]，默认实现为小顶堆，每次出队最小的元素 构造方法： public PriorityQueue()：构造默认长度为 11 的队列（数组） public PriorityQueue(Comparator&lt;? super E&gt; comparator)：利用比较器自定义堆排序的规则 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 Queue&lt;Integer&gt; pq = new PriorityQueue&lt;&gt;((v1, v2) -&gt; v2 - v1);//实现大顶堆常用 API：* `public boolean offer(E e)`：将指定的元素插入到此优先级队列的**尾部*** `public E poll() `：检索并删除此队列的**头元素**，如果此队列为空，则返回 null * `public E peek()`：检索但不删除此队列的头，如果此队列为空，则返回 null* `public boolean remove(Object o)`：从该队列中删除指定元素（如果存在），删除元素 e 使用 o.equals(e) 比较，如果队列包含多个这样的元素，删除第一个****#### Collectionsjava.utils.Collections：集合**工具类**，Collections 并不属于集合，是用来操作集合的工具类Collections 有几个常用的API：* `public static &lt;T&gt; boolean addAll(Collection&lt;? super T&gt; c, T... e)`：给集合对象批量添加元素* `public static void shuffle(List&lt;?&gt; list)`：打乱集合顺序* `public static &lt;T&gt; void sort(List&lt;T&gt; list)`：将集合中元素按照默认规则排序* `public static &lt;T&gt; void sort(List&lt;T&gt; list,Comparator&lt;? super T&gt; )`：集合中元素按照指定规则排序* `public static &lt;T&gt; List&lt;T&gt; synchronizedList(List&lt;T&gt; list)`：返回由指定 list 支持的线程安全 list* `public static &lt;T&gt; Set&lt;T&gt; singleton(T o)`：返回一个只包含指定对象的不可变组```javapublic class CollectionsDemo &#123; public static void main(String[] args) &#123; Collection&lt;String&gt; names = new ArrayList&lt;&gt;(); Collections.addAll(names,&quot;张&quot;,&quot;王&quot;,&quot;李&quot;,&quot;赵&quot;); List&lt;Double&gt; scores = new ArrayList&lt;&gt;(); Collections.addAll(scores, 98.5, 66.5 , 59.5 , 66.5 , 99.5 ); Collections.shuffle(scores); Collections.sort(scores); // 默认升序排序！ System.out.println(scores); List&lt;Student&gt; students = new ArrayList&lt;&gt;(); Collections.addAll(students,s1,s2,s3,s4); Collections.sort(students,new Comparator&lt;Student&gt;()&#123; &#125;) &#125;&#125;public class Student&#123; private String name; private int age;&#125; Map概述Collection 是单值集合体系，Map集合是一种双列集合，每个元素包含两个值。 Map集合的每个元素的格式：key&#x3D;value（键值对元素），Map集合也被称为键值对集合 Map集合的完整格式：&#123;key1=value1, key2=value2, key3=value3, ...&#125; 123456Map集合的体系： Map&lt;K , V&gt;(接口,Map集合的祖宗类) / \\ TreeMap&lt;K , V&gt; HashMap&lt;K , V&gt;(实现类,经典的，用的最多) \\ LinkedHashMap&lt;K, V&gt;(实现类) Map 集合的特点： Map 集合的特点都是由键决定的 Map 集合的键是无序，不重复的，无索引的（Set） Map 集合的值无要求（List） Map 集合的键值对都可以为 null Map 集合后面重复的键对应元素会覆盖前面的元素 HashMap：元素按照键是无序，不重复，无索引，值不做要求 LinkedHashMap：元素按照键是有序，不重复，无索引，值不做要求 常用APIMap 集合的常用 API public V put(K key, V value)：把指定的键与值添加到 Map 集合中，重复的键会覆盖前面的值元素 public V remove(Object key)：把指定的键对应的键值对元素在集合中删除，返回被删除元素的值 public V get(Object key)：根据指定的键，在 Map 集合中获取对应的值 public Set&lt;K&gt; keySet()：获取 Map 集合中所有的键，存储到 Set 集合中 public Collection&lt;V&gt; values()：获取全部值的集合，存储到 Collection 集合 public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet()：获取Map集合中所有的键值对对象的集合 public boolean containsKey(Object key)：判断该集合中是否有此键 123456789101112public class MapDemo &#123; public static void main(String[] args) &#123; Map&lt;String , Integer&gt; maps = new HashMap&lt;&gt;(); maps.put(.....); System.out.println(maps.isEmpty());//false Integer value = maps.get(&quot;....&quot;);//返回键值对象 Set&lt;String&gt; keys = maps.keySet();//获取Map集合中所有的键， //Map集合的键是无序不重复的，所以返回的是一个Set集合 Collection&lt;Integer&gt; values = maps.values(); //Map集合的值是不做要求的，可能重复，所以值要用Collection集合接收! &#125;&#125; 遍历方式Map集合的遍历方式有：3种。 “键找值”的方式遍历：先获取 Map 集合全部的键，再根据遍历键找值。 “键值对”的方式遍历：难度较大，采用增强 for 或者迭代器 JDK 1.8 开始之后的新技术：foreach，采用 Lambda 表达式 集合可以直接输出内容，因为底层重写了 toString() 方法 123456789101112131415161718192021222324252627public static void main(String[] args)&#123; Map&lt;String , Integer&gt; maps = new HashMap&lt;&gt;();\t//(1)键找值 Set&lt;String&gt; keys = maps.keySet(); for(String key : keys) &#123; System.out.println(key + &quot;=&quot; + maps.get(key)); &#125; //Iterator&lt;String&gt; iterator = hm.keySet().iterator(); //(2)键值对 //(2.1)普通方式 Set&lt;Map.Entry&lt;String,Integer&gt;&gt; entries = maps.entrySet(); for (Map.Entry&lt;String, Integer&gt; entry : entries) &#123; System.out.println(entry.getKey() + &quot;=&gt;&quot; + entry.getValue()); &#125; //(2.2)迭代器方式 Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; iterator = maps.entrySet().iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; entry = iterator.next(); System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue()); &#125; //(3) Lamda maps.forEach((k,v) -&gt; &#123; System.out.println(k + &quot;==&gt;&quot; + v); &#125;)&#125; HashMap基本介绍HashMap 基于哈希表的 Map 接口实现，是以 key-value 存储形式存在，主要用来存放键值对 特点： HashMap 的实现不是同步的，这意味着它不是线程安全的 key 是唯一不重复的，底层的哈希表结构，依赖 hashCode 方法和 equals 方法保证键的唯一 key、value 都可以为null，但是 key 位置只能是一个null HashMap 中的映射不是有序的，即存取是无序的 key 要存储的是自定义对象，需要重写 hashCode 和 equals 方法，防止出现地址不同内容相同的 key JDK7 对比 JDK8： 7 &#x3D; 数组 + 链表，8 &#x3D; 数组 + 链表 + 红黑树 7 中是头插法，多线程容易造成环，8 中是尾插法 7 的扩容是全部数据重新定位，8 中是位置不变或者当前位置 + 旧 size 大小来实现 7 是先判断是否要扩容再插入，8 中是先插入再看是否要扩容 底层数据结构： 哈希表（Hash table，也叫散列表），根据关键码值而直接访问的数据结构。通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度，这个映射函数叫做散列函数，存放记录的数组叫做散列表 JDK1.8 之前 HashMap 由数组+链表组成 数组是 HashMap 的主体 链表则是为了解决哈希冲突而存在的（拉链法解决冲突），拉链法就是头插法，两个对象调用的 hashCode 方法计算的哈希码值（键的哈希）一致导致计算的数组索引值相同 JDK1.8 以后 HashMap 由数组+链表 +红黑树数据结构组成 解决哈希冲突时有了较大的变化 当链表长度超过（大于）阈值（或者红黑树的边界值，默认为 8）并且当前数组的长度大于等于 64 时，此索引位置上的所有数据改为红黑树存储 即使哈希函数取得再好，也很难达到元素百分百均匀分布。当 HashMap 中有大量的元素都存放到同一个桶中时，就相当于一个长的单链表，假如单链表有 n 个元素，遍历的**时间复杂度是 O(n)，所以 JDK1.8 中引入了 红黑树（查找时间复杂度为 O(logn)**）来优化这个问题，使得查找效率更高 参考视频：https://www.bilibili.com/video/BV1nJ411J7AA 继承关系HashMap 继承关系如下图所示： 说明： Cloneable 空接口，表示可以克隆， 创建并返回 HashMap 对象的一个副本。 Serializable 序列化接口，属于标记性接口，HashMap 对象可以被序列化和反序列化。 AbstractMap 父类提供了 Map 实现接口，以最大限度地减少实现此接口所需的工作 成员属性 序列化版本号 1private static final long serialVersionUID = 362498820763181265L; 集合的初始化容量（必须是二的 n 次幂 ） 12// 默认的初始容量是16 -- 1&lt;&lt;4相当于1*2的4次方---1*16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; HashMap 构造方法指定集合的初始化容量大小： 1HashMap(int initialCapacity)// 构造一个带指定初始容量和默认加载因子 (0.75) 的空 HashMap 为什么必须是 2 的 n 次幂？用位运算替代取余计算，减少 rehash 的代价（移动的节点少） HashMap 中添加元素时，需要根据 key 的 hash 值确定在数组中的具体位置。为了减少碰撞，把数据分配均匀，每个链表长度大致相同，实现该方法就是取模 hash%length，计算机中直接求余效率不如位移运算， hash % length == hash &amp; (length-1) 的前提是 length 是 2 的 n 次幂 散列平均分布：2 的 n 次方是 1 后面 n 个 0，2 的 n 次方 -1 是 n 个 1，可以保证散列的均匀性，减少碰撞 12例如长度为8时候，3&amp;(8-1)=3 2&amp;(8-1)=2 ，不同位置上，不碰撞；例如长度为9时候，3&amp;(9-1)=0 2&amp;(9-1)=0 ，都在0上，碰撞了； 如果输入值不是 2 的幂会怎么样？ 创建 HashMap 对象时，HashMap 通过位移运算和或运算得到的肯定是 2 的幂次数，并且是大于那个数的最近的数字，底层采用 tableSizeFor() 方法 默认的负载因子，默认值是 0.75 1static final float DEFAULT_LOAD_FACTOR = 0.75f; 集合最大容量 12// 集合最大容量的上限是：2的30次幂static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 0100 0000 0000 0000 0000 0000 0000 0000 = 2 ^ 30 当链表的值超过 8 则会转红黑树（JDK1.8 新增） 12// 当桶(bucket)上的结点数大于这个值时会转成红黑树static final int TREEIFY_THRESHOLD = 8; 为什么 Map 桶中节点个数大于 8 才转为红黑树？ 在 HashMap 中有一段注释说明：空间和时间的权衡 123456789101112TreeNodes占用空间大约是普通节点的两倍，所以我们只在箱子包含足够的节点时才使用树节点。当节点变少(由于删除或调整大小)时，就会被转换回普通的桶。在使用分布良好的用户hashcode时，很少使用树箱。理想情况下，在随机哈希码下，箱子中节点的频率服从&quot;泊松分布&quot;，默认调整阈值为0.75，平均参数约为0.5，尽管由于调整粒度的差异很大。忽略方差，列表大小k的预期出现次数是(exp(-0.5)*pow(0.5, k)/factorial(k))0: 0.606530661: 0.303265332: 0.075816333: 0.012636064: 0.001579525: 0.000157956: 0.000013167: 0.000000948: 0.00000006more: less than 1 in ten million一个bin中链表长度达到8个元素的概率为0.00000006，几乎是不可能事件，所以我们选择8这个数字 其他说法红黑树的平均查找长度是 log(n)，如果长度为 8，平均查找长度为 log(8)&#x3D;3，链表的平均查找长度为 n&#x2F;2，当长度为 8 时，平均查找长度为 8&#x2F;2&#x3D;4，这才有转换成树的必要；链表长度如果是小于等于 6，6&#x2F;2&#x3D;3，而 log(6)&#x3D;2.6，虽然速度也很快的，但转化为树结构和生成树的时间并不短 当链表的值小于 6 则会从红黑树转回链表 12// 当桶(bucket)上的结点数小于这个值时树转链表static final int UNTREEIFY_THRESHOLD = 6; 当 Map 里面的数量大于等于这个阈值时，表中的桶才能进行树形化 ，否则桶内元素超过 8 时会扩容，而不是树形化。为了避免进行扩容、树形化选择的冲突，这个值不能小于 4 * TREEIFY_THRESHOLD (8) 12// 桶中结构转化为红黑树对应的数组长度最小的值 static final int MIN_TREEIFY_CAPACITY = 64; 原因：数组比较小的情况下变为红黑树结构，反而会降低效率，红黑树需要进行左旋，右旋，变色这些操作来保持平衡 table 用来初始化（必须是二的 n 次幂） 12// 存储元素的数组 transient Node&lt;K,V&gt;[] table; HashMap 中存放元素的个数 12// 存放元素的个数，HashMap中K-V的实时数量，不是table数组的长度transient int size; 记录 HashMap 的修改次数 12// 每次扩容和更改map结构的计数器 transient int modCount; 调整大小下一个容量的值计算方式为：容量 * 负载因子，容量是数组的长度 12// 临界值，当实际大小(容量*负载因子)超过临界值时，会进行扩容int threshold; 哈希表的加载因子 1final float loadFactor; 加载因子的概述 loadFactor 加载因子，是用来衡量 HashMap 满的程度，表示 HashMap 的疏密程度，影响 hash 操作到同一个数组位置的概率，计算 HashMap 的实时加载因子的方法为 size&#x2F;capacity，而不是占用桶的数量去除以 capacity，capacity 是桶的数量，也就是 table 的长度 length 当 HashMap 容纳的元素已经达到数组长度的 75% 时，表示 HashMap 拥挤需要扩容，而扩容这个过程涉及到 rehash、复制数据等操作，非常消耗性能，所以开发中尽量减少扩容的次数，通过创建 HashMap 集合对象时指定初始容量来避免 1HashMap(int initialCapacity, float loadFactor)//构造指定初始容量和加载因子的空HashMap 为什么加载因子设置为 0.75，初始化临界值是 12？ loadFactor 太大导致查找元素效率低，存放的数据拥挤，太小导致数组的利用率低，存放的数据会很分散。loadFactor 的默认值为 0.75f 是官方给出的一个比较好的临界值 threshold 计算公式：capacity（数组长度默认16） * loadFactor（默认 0.75）。当 size &gt;&#x3D; threshold 的时候，那么就要考虑对数组的 resize（扩容），这就是衡量数组是否需要扩增的一个标准， 扩容后的 HashMap 容量是之前容量的两倍 构造方法 构造一个空的 HashMap ，默认初始容量（16）和默认负载因子（0.75） 1234public HashMap() &#123;\tthis.loadFactor = DEFAULT_LOAD_FACTOR; // 将默认的加载因子0.75赋值给loadFactor，并没有创建数组&#125; 构造一个具有指定的初始容量和默认负载因子（0.75）HashMap 1234// 指定“容量大小”的构造函数public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125; 构造一个具有指定的初始容量和负载因子的 HashMap 1234567public HashMap(int initialCapacity, float loadFactor) &#123; // 进行判断 // 将指定的加载因子赋值给HashMap成员变量的负载因子loadFactor this.loadFactor = loadFactor; // 最后调用了tableSizeFor this.threshold = tableSizeFor(initialCapacity);&#125; 对于 this.threshold = tableSizeFor(initialCapacity) JDK8 以后的构造方法中，并没有对 table 这个成员变量进行初始化，table 的初始化被推迟到了 put 方法中，在 put 方法中会对 threshold 重新计算 包含另一个 Map 的构造函数 123456// 构造一个映射关系与指定 Map 相同的新 HashMappublic HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; // 负载因子loadFactor变为默认的负载因子0.75 this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125; putMapEntries 源码分析： 1234567891011121314151617181920212223242526final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; //获取参数集合的长度 int s = m.size(); if (s &gt; 0) &#123; //判断参数集合的长度是否大于0 if (table == null) &#123; // 判断table是否已经初始化 // pre-size // 未初始化，s为m的实际元素个数 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 计算得到的t大于阈值，则初始化阈值 if (t &gt; threshold) threshold = tableSizeFor(t); &#125; // 已初始化，并且m元素个数大于阈值，进行扩容处理 else if (s &gt; threshold) resize(); // 将m中的所有元素添加至HashMap中 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; float ft = ((float)s / loadFactor) + 1.0F 这一行代码中为什么要加 1.0F ？ s &#x2F; loadFactor 的结果是小数，加 1.0F 相当于是对小数做一个向上取整以尽可能的保证更大容量，更大的容量能够减少 resize 的调用次数，这样可以减少数组的扩容 成员方法 hash()：HashMap 是支持 Key 为空的；HashTable 是直接用 Key 来获取 HashCode，key 为空会抛异常 &amp;（按位与运算）：相同的二进制数位上，都是 1 的时候，结果为 1，否则为零 ^（按位异或运算）：相同的二进制数位上，数字相同，结果为 0，不同为 1，不进位加法 0 1 相互做 &amp; | ^ 运算，结果出现 0 和 1 的数量分别是 3:1、1:3、1:1，所以异或是最平均的 123456static final int hash(Object key) &#123; int h; // 1）如果key等于null：可以看到当key等于null的时候也是有哈希值的，返回的是0 // 2）如果key不等于null：首先计算出key的hashCode赋值给h,然后与h无符号右移16位后的二进制进行按位异或 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 计算 hash 的方法：将 hashCode 无符号右移 16 位，高 16bit 和低 16bit 做异或，扰动运算 原因：当数组长度很小，假设是 16，那么 n-1 即为 1111 ，这样的值和 hashCode() 直接做按位与操作，实际上只使用了哈希值的后 4 位。如果当哈希值的高位变化很大，低位变化很小，就很容易造成哈希冲突了，所以这里把高低位都利用起来，让高16 位也参与运算，从而解决了这个问题 哈希冲突的处理方式： 开放定址法：线性探查法（ThreadLocalMap 使用），平方探查法（i + 1^2、i - 1^2、i + 2^2……）、双重散列（多个哈希函数） 链地址法：拉链法 put()：jdk1.8 前是头插法 (链地址法)，多线程下扩容出现循环链表，jdk1.8 以后引入红黑树，插入方法变成尾插法 第一次调用 put 方法时创建数组 Node[] table，因为散列表耗费内存，为了防止内存浪费，所以延迟初始化 存储数据步骤（存储过程）： 先通过 hash 值计算出 key 映射到哪个桶，哈希寻址 如果桶上没有碰撞冲突，则直接插入 如果出现碰撞冲突：如果该桶使用红黑树处理冲突，则调用红黑树的方法插入数据；否则采用传统的链式方法插入，如果链的长度达到临界值，则把链转变为红黑树 如果数组位置相同，通过 equals 比较内容是否相同：相同则新的 value 覆盖旧 value，不相同则将新的键值对添加到哈希表中 最后判断 size 是否大于阈值 threshold，则进行扩容 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; putVal() 方法中 key 在这里执行了一下 hash()，在 putVal 函数中使用到了上述 hash 函数计算的哈希值： 1234567891011121314151617181920212223final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //。。。。。。。。。。。。。。 if ((p = tab[i = (n - 1) &amp; hash]) == null)&#123;//这里的n表示数组长度16 //..... &#125; else &#123; if (e != null) &#123; // existing mapping for key V oldValue = e.value; //onlyIfAbsent默认为false，所以可以覆盖已经存在的数据，如果为true说明不能覆盖 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); // 如果这里允许覆盖，就直接返回了 return oldValue; &#125; &#125; // 如果是添加操作，modCount ++，如果不是替换，不会走这里的逻辑，modCount用来记录逻辑的变化 ++modCount; // 数量大于扩容阈值 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; (n - 1) &amp; hash：计算下标位置 余数本质是不断做除法，把剩余的数减去，运算效率要比位运算低 treeifyBin() 节点添加完成之后判断此时节点个数是否大于 TREEIFY_THRESHOLD 临界值 8，如果大于则将链表转换为红黑树，转换红黑树的方法 treeifyBin，整体代码如下： 123if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st //转换为红黑树 tab表示数组名 hash表示哈希值 treeifyBin(tab, hash); 如果当前数组为空或者数组的长度小于进行树形化的阈 MIN_TREEIFY_CAPACITY &#x3D; 64 就去扩容，而不是将节点变为红黑树 如果是树形化遍历桶中的元素，创建相同个数的树形节点，复制内容，建立起联系，类似单向链表转换为双向链表 让桶中的第一个元素即数组中的元素指向新建的红黑树的节点，以后这个桶里的元素就是红黑树而不是链表数据结构了 tableSizeFor()：创建 HashMap 指定容量时，HashMap 通过位移运算和或运算得到比指定初始化容量大的最小的 2 的 n 次幂 123456789static final int tableSizeFor(int cap) &#123;//int cap = 10 int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 分析算法： int n = cap - 1：防止 cap 已经是 2 的幂。如果 cap 已经是 2 的幂， 不执行减 1 操作，则执行完后面的无符号右移操作之后，返回的 capacity 将是这个 cap 的 2 倍 n&#x3D;0 （cap-1 之后），则经过后面的几次无符号右移依然是 0，返回的 capacity 是 1，最后有 n+1 |（按位或运算）：相同的二进制数位上，都是 0 的时候，结果为 0，否则为 1 核心思想：把最高位是 1 的位以及右边的位全部置 1，结果加 1 后就是大于指定容量的最小的 2 的 n 次幂 例如初始化的值为 10： 第一次右移 1234567int n = cap - 1;//cap=10 n=9n |= n &gt;&gt;&gt; 1;00000000 00000000 00000000 00001001 //900000000 00000000 00000000 00000100 //9右移之后变为4--------------------------------------------------00000000 00000000 00000000 00001101 //按位或之后是13//使得n的二进制表示中与最高位的1紧邻的右边一位为1 第二次右移 123456n |= n &gt;&gt;&gt; 2;//n通过第一次右移变为了：n=1300000000 00000000 00000000 00001101 // 1300000000 00000000 00000000 00000011 // 13右移之后变为3-------------------------------------------------00000000 00000000 00000000 00001111 //按位或之后是15//无符号右移两位，会将最高位两个连续的1右移两位，然后再与原来的n做或操作，这样n的二进制表示的高位中会有4个连续的1 注意：容量最大是 32bit 的正数，因此最后 n |= n &gt;&gt;&gt; 16，最多是 32 个 1（但是这已经是负数了）。在执行 tableSizeFor 之前，对 initialCapacity 做了判断，如果大于 MAXIMUM_CAPACITY(2 ^ 30)，则取 MAXIMUM_CAPACITY；如果小于 MAXIMUM_CAPACITY(2 ^ 30)，会执行移位操作，所以移位操作之后，最大 30 个 1，加 1 之后得 2 ^ 30 得到的 capacity 被赋值给了 threshold 1this.threshold = tableSizeFor(initialCapacity);//initialCapacity=10 JDK 11 12345678910111213141516171819static final int tableSizeFor(int cap) &#123; //无符号右移，高位补0\t//-1补码: 11111111 11111111 11111111 11111111 int n = -1 &gt;&gt;&gt; Integer.numberOfLeadingZeros(cap - 1); return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125;//返回最高位之前的0的位数public static int numberOfLeadingZeros(int i) &#123; if (i &lt;= 0) return i == 0 ? 32 : 0; // 如果i&gt;0，那么就表明在二进制表示中其至少有一位为1 int n = 31; // i的最高位1在高16位，把i右移16位，让最高位1进入低16位继续递进判断 if (i &gt;= 1 &lt;&lt; 16) &#123; n -= 16; i &gt;&gt;&gt;= 16; &#125; if (i &gt;= 1 &lt;&lt; 8) &#123; n -= 8; i &gt;&gt;&gt;= 8; &#125; if (i &gt;= 1 &lt;&lt; 4) &#123; n -= 4; i &gt;&gt;&gt;= 4; &#125; if (i &gt;= 1 &lt;&lt; 2) &#123; n -= 2; i &gt;&gt;&gt;= 2; &#125; return n - (i &gt;&gt;&gt; 1);&#125; resize()： 当 HashMap 中的元素个数超过 (数组长度)*loadFactor(负载因子) 或者链表过长时（链表长度 &gt; 8，数组长度 &lt; 64），就会进行数组扩容，创建新的数组，伴随一次重新 hash 分配，并且遍历 hash 表中所有的元素非常耗时，所以要尽量避免 resize 扩容机制为扩容为原来容量的 2 倍： 12345678910111213141516if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; // 以前的容量已经是最大容量了，这时调大 扩容阈值 threshold threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold&#125;else if (oldThr &gt; 0) // 初始化的threshold赋值给newCap newCap = oldThr;else &#123; newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);&#125; HashMap 在进行扩容后，节点要么就在原来的位置，要么就被分配到”原位置+旧容量”的位置 判断：e.hash 与 oldCap 对应的有效高位上的值是 1，即当前数组长度 n 二进制为 1 的位为 x 位，如果 key 的哈希值 x 位也为 1，则扩容后的索引为 now + n 注意：这里要求数组长度 2 的幂 普通节点：把所有节点分成高低位两个链表，转移到数组 123456789101112131415161718192021222324// 遍历所有的节点do &#123; next = e.next; // oldCap 旧数组大小，2 的 n 次幂 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e;\t//指向低位链表头节点 else loTail.next = e; loTail = e; //指向低位链表尾节点 &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125;&#125; while ((e = next) != null);if (loTail != null) &#123; loTail.next = null;\t// 低位链表的最后一个节点可能在原哈希表中指向其他节点，需要断开 newTab[j] = loHead;&#125; 红黑树节点：扩容时 split 方法会将树拆成高位和低位两个链表，判断长度是否小于等于 6 123456789101112131415//如果低位链表首节点不为null，说明有这个链表存在if (loHead != null) &#123; //如果链表下的元素小于等于6 if (lc &lt;= UNTREEIFY_THRESHOLD) //那就从红黑树转链表了，低位链表，迁移到新数组中下标不变，还是等于原数组到下标 tab[index] = loHead.untreeify(map); else &#123; //低位链表，迁移到新数组中下标不变，把低位链表整个赋值到这个下标下 tab[index] = loHead; //如果高位首节点不为空，说明原来的红黑树已经被拆分成两个链表了 if (hiHead != null) //需要构建新的红黑树了 loHead.treeify(tab); &#125;&#125; ​ remove()：删除是首先先找到元素的位置，如果是链表就遍历链表找到元素之后删除。如果是用红黑树就遍历树然后找到之后做删除，树小于 6 的时候退化为链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; // 节点数组tab不为空、数组长度n大于0、根据hash定位到的节点对象p， // 该节点为树的根节点或链表的首节点）不为空，从该节点p向下遍历，找到那个和key匹配的节点对象 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v;//临时变量，储存要返回的节点信息 //key和value都相等，直接返回该节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; //如果是树节点，调用getTreeNode方法从树结构中查找满足条件的节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); //遍历链表 else &#123; do &#123; //e节点的键是否和key相等，e节点就是要删除的节点，赋值给node变量 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; //跳出循环 break; &#125; p = e;//把当前节点p指向e 继续遍历 &#125; while ((e = e.next) != null); &#125; &#125; //如果node不为空，说明根据key匹配到了要删除的节点 //如果不需要对比value值或者对比value值但是value值也相等，可以直接删除 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p)//node是首节点 tab[index] = node.next; else\t//node不是首节点 p.next = node.next; ++modCount; --size; //LinkedHashMap afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; get() 通过 hash 值获取该 key 映射到的桶 桶上的 key 就是要查找的 key，则直接找到并返回 桶上的 key 不是要找的 key，则查看后续的节点： 如果后续节点是红黑树节点，通过调用红黑树的方法根据 key 获取 value 如果后续节点是链表节点，则通过循环遍历链表根据 key 获取 value 红黑树节点调用的是 getTreeNode 方法通过树形节点的 find 方法进行查 查找红黑树，之前添加时已经保证这个树是有序的，因此查找时就是折半查找，效率更高。 这里和插入时一样，如果对比节点的哈希值相等并且通过 equals 判断值也相等，就会判断 key 相等，直接返回，不相等就从子树中递归查找 时间复杂度 O(1) 若为树，则在树中通过 key.equals(k) 查找，O(logn) 若为链表，则在链表中通过 key.equals(k) 查找，O(n) 并发异常HashMap 和 ArrayList 一样，内部采用 modCount 用来记录集合结构发生变化的次数，结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果其他线程此时修改了集合内部的结构，就会直接抛出 ConcurrentModificationException 异常 12HashMap map = new HashMap();Iterator iterator = map.keySet().iterator(); 123456789101112final class KeySet extends AbstractSet&lt;K&gt; &#123; // 底层获取的是 KeyIterator\tpublic final Iterator&lt;K&gt; iterator() &#123; return new KeyIterator(); &#125;&#125;final class KeyIterator extends HashIterator implements Iterator&lt;K&gt; &#123; // 回调 HashMap.HashIterator#nextNode public final K next() &#123; return nextNode().key; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748abstract class HashIterator &#123; Node&lt;K,V&gt; next; // next entry to return Node&lt;K,V&gt; current; // current entry int expectedModCount; // for 【fast-fail】，快速失败 int index; // current slot HashIterator() &#123; // 把当前 map 的数量赋值给 expectedModCount，迭代时判断 expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; index = 0; if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125;\t// iterator.next() 会调用这个函数 final Node&lt;K,V&gt; nextNode() &#123; Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; // 这里会判断 集合的结构是否发生了变化，变化后 modCount 会改变，直接抛出并发异常 if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123; do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; return e; &#125;\t// 迭代器允许删除集合的元素，【删除后会重置 expectedModCount = modCount】 public final void remove() &#123; Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); // 同步expectedModCount expectedModCount = modCount; &#125;&#125; LinkedMap原理分析LinkedHashMap 是 HashMap 的子类 优点：添加的元素按照键有序不重复的，有序的原因是底层维护了一个双向链表 缺点：会占用一些内存空间 对比 Set： HashSet 集合相当于是 HashMap 集合的键，不带值 LinkedHashSet 集合相当于是 LinkedHashMap 集合的键，不带值 底层原理完全一样，都是基于哈希表按照键存储数据的，只是 Map 多了一个键的值 源码解析： 内部维护了一个双向链表，用来维护插入顺序或者 LRU 顺序 12transient LinkedHashMap.Entry&lt;K,V&gt; head;transient LinkedHashMap.Entry&lt;K,V&gt; tail; accessOrder 决定了顺序，默认为 false 维护的是插入顺序（先进先出），true 为访问顺序（LRU 顺序） 1final boolean accessOrder; 维护顺序的函数 12void afterNodeAccess(Node&lt;K,V&gt; p) &#123;&#125;void afterNodeInsertion(boolean evict) &#123;&#125; put() 123456// 调用父类HashMap的put方法public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict)→ afterNodeInsertion(evict);// evict为true afterNodeInsertion方法，当 removeEldestEntry() 方法返回 true 时会移除最近最久未使用的节点，也就是链表首部节点 first 12345678void afterNodeInsertion(boolean evict) &#123; LinkedHashMap.Entry&lt;K,V&gt; first; // evict 只有在构建 Map 的时候才为 false，这里为 true if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true);//移除头节点 &#125;&#125; removeEldestEntry() 默认为 false，如果需要让它为 true，需要继承 LinkedHashMap 并且覆盖这个方法的实现，在实现 LRU 的缓存中特别有用，通过移除最近最久未使用的节点，从而保证缓存空间足够，并且缓存的数据都是热点数据 123protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; get() 当一个节点被访问时，如果 accessOrder 为 true，则会将该节点移到链表尾部。也就是说指定为 LRU 顺序之后，在每次访问一个节点时会将这个节点移到链表尾部，那么链表首部就是最近最久未使用的节点 12345678public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;&#125; 12345678910111213141516171819202122232425262728293031323334void afterNodeAccess(Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; // 向下转型 LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; // 判断 p 是否是首节点 if (b == null) //是头节点 让p后继节点成为头节点 head = a; else //不是头节点 让p的前驱节点的next指向p的后继节点，维护链表的连接 b.after = a; // 判断p是否是尾节点 if (a != null) // 不是尾节点 让p后继节点指向p的前驱节点 a.before = b; else // 是尾节点 让last指向p的前驱节点 last = b; // 判断last是否是空 if (last == null) // last为空说明p是尾节点或者只有p一个节点 head = p; else &#123; // last和p相互连接 p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; remove() 123//调用HashMap的remove方法final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value,boolean matchValue, boolean movable)→ afterNodeRemoval(node); 当 HashMap 删除一个键值对时调用，会把在 HashMap 中删除的那个键值对一并从链表中删除 123456789101112131415161718void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; // 让p节点与前驱节点和后继节点断开链接 p.before = p.after = null; // 判断p是否是头节点 if (b == null) // p是头节点 让head指向p的后继节点 head = a; else // p不是头节点 让p的前驱节点的next指向p的后继节点，维护链表的连接 b.after = a; // 判断p是否是尾节点，是就让tail指向p的前驱节点，不是就让p.after指向前驱节点，双向 if (a == null) tail = b; else a.before = b;&#125; LRU使用 LinkedHashMap 实现的一个 LRU 缓存： 设定最大缓存空间 MAX_ENTRIES 为 3 使用 LinkedHashMap 的构造函数将 accessOrder 设置为 true，开启 LRU 顺序 覆盖 removeEldestEntry() 方法实现，在节点多于 MAX_ENTRIES 就会将最近最久未使用的数据移除 123456789101112131415161718192021public static void main(String[] args) &#123; LRUCache&lt;Integer, String&gt; cache = new LRUCache&lt;&gt;(); cache.put(1, &quot;a&quot;); cache.put(2, &quot;b&quot;); cache.put(3, &quot;c&quot;); cache.get(1);//把1放入尾部 cache.put(4, &quot;d&quot;); System.out.println(cache.keySet());//[3, 1, 4]只能存3个，移除2&#125;class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private static final int MAX_ENTRIES = 3; protected boolean removeEldestEntry(Map.Entry eldest) &#123; return size() &gt; MAX_ENTRIES; &#125; LRUCache() &#123; super(MAX_ENTRIES, 0.75f, true); &#125;&#125; TreeMapTreeMap 实现了 SotredMap 接口，是有序不可重复的键值对集合，基于红黑树（Red-Black tree）实现，每个 key-value 都作为一个红黑树的节点，如果构造 TreeMap 没有指定比较器，则根据 key 执行自然排序（默认升序），如果指定了比较器则按照比较器来进行排序 TreeMap 集合指定大小规则有 2 种方式： 直接为对象的类实现比较器规则接口 Comparable，重写比较方法 直接为集合设置比较器 Comparator 对象，重写比较方法 说明：TreeSet 集合的底层是基于 TreeMap，只是键的附属值为空对象而已 成员属性： Entry 节点 12345678static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; K key; V value; Entry&lt;K,V&gt; left; //左孩子节点 Entry&lt;K,V&gt; right; //右孩子节点 Entry&lt;K,V&gt; parent; //父节点 boolean color = BLACK;\t//节点的颜色，在红黑树中只有两种颜色，红色和黑色&#125; compare() 12345//如果comparator为null，采用comparable.compartTo进行比较，否则采用指定比较器比较大小final int compare(Object k1, Object k2) &#123; return comparator == null ? ((Comparable&lt;? super K&gt;)k1).compareTo((K)k2) : comparator.compare((K)k1, (K)k2);&#125; 参考文章：https://blog.csdn.net/weixin_33991727/article/details/91518677 WeakMapWeakHashMap 是基于弱引用的，内部的 Entry 继承 WeakReference，被弱引用关联的对象在下一次垃圾回收时会被回收，并且构造方法传入引用队列，用来在清理对象完成以后清理引用 12345678private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; &#123; Entry(Object key, V value, ReferenceQueue&lt;Object&gt; queue, int hash, Entry&lt;K,V&gt; next) &#123; super(key, queue); this.value = value; this.hash = hash; this.next = next; &#125;&#125; WeakHashMap 主要用来实现缓存，使用 WeakHashMap 来引用缓存对象，由 JVM 对这部分缓存进行回收 Tomcat 中的 ConcurrentCache 使用了 WeakHashMap 来实现缓存功能，ConcurrentCache 采取分代缓存： 经常使用的对象放入 eden 中，eden 使用 ConcurrentHashMap 实现，不用担心会被回收（伊甸园） 不常用的对象放入 longterm，longterm 使用 WeakHashMap 实现，这些老对象会被垃圾收集器回收 当调用 get() 方法时，会先从 eden 区获取，如果没有找到的话再到 longterm 获取，当从 longterm 获取到就把对象放入 eden 中，从而保证经常被访问的节点不容易被回收 当调用 put() 方法时，如果 eden 的大小超过了 size，那么就将 eden 中的所有对象都放入 longterm 中，利用虚拟机回收掉一部分不经常使用的对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960 public final class ConcurrentCache&lt;K, V&gt; &#123; private final int size; private final Map&lt;K, V&gt; eden; private final Map&lt;K, V&gt; longterm; public ConcurrentCache(int size) &#123; this.size = size; this.eden = new ConcurrentHashMap&lt;&gt;(size); this.longterm = new WeakHashMap&lt;&gt;(size); &#125; public V get(K k) &#123; V v = this.eden.get(k); if (v == null) &#123; v = this.longterm.get(k); if (v != null) this.eden.put(k, v); &#125; return v; &#125; public void put(K k, V v) &#123; if (this.eden.size() &gt;= size) &#123; this.longterm.putAll(this.eden); this.eden.clear(); &#125; this.eden.put(k, v); &#125; &#125;***### 泛型#### 概述泛型（Generic）：* 泛型就是一个标签：&lt;数据类型&gt;* 泛型可以在编译阶段约束只能操作某种数据类型。注意：* JDK 1.7 开始之后，泛型后面的申明可以省略不写* **泛型和集合都只能支持引用数据类型，不支持基本数据类型**```javaArrayList&lt;Object&gt; lists = new ArrayList&lt;&gt;(); lists.add(99.9);lists.add(&#x27;a&#x27;);lists.add(&quot;Java&quot;);ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();lists1.add(10);lists1.add(20); 优点：泛型在编译阶段约束了操作的数据类型，从而不会出现类型转换异常，体现的是 Java 的严谨性和规范性 自定义泛型类泛型类：使用了泛型定义的类就是泛型类 泛型类格式： 1234修饰符 class 类名&lt;泛型变量&gt;&#123;&#125;泛型变量建议使用 E , T , K , V 1234567891011public class GenericDemo &#123; public static void main(String[] args) &#123; MyArrayList&lt;String&gt; list = new MyArrayList&lt;String&gt;(); MyArrayList&lt;Integer&gt; list1 = new MyArrayList&lt;Integer&gt;(); list.add(&quot;自定义泛型类&quot;); &#125;&#125;class MyArrayList&lt;E&gt;&#123;\tpublic void add(E e)&#123;&#125; public void remove(E e)&#123;&#125;&#125; 泛型方法泛型方法：定义了泛型的方法就是泛型方法 泛型方法的定义格式： 123修饰符 &lt;泛型变量&gt; 返回值类型 方法名称(形参列表)&#123;&#125; 方法定义了是什么泛型变量，后面就只能用什么泛型变量。 泛型类的核心思想：把出现泛型变量的地方全部替换成传输的真实数据类型 12345678910111213public class GenericDemo &#123; public static void main(String[] args) &#123; Integer[] num = &#123;10 , 20 , 30 , 40 , 50&#125;; String s1 = arrToString(nums); String[] name = &#123;&quot;张三&quot;,&quot;李四&quot;,&quot;王五&quot;&#125;; String s2 = arrToString(names); &#125; public static &lt;T&gt; String arrToString(T[] arr)&#123; -------------- &#125;&#125; 自定义泛型接口 泛型接口：使用了泛型定义的接口就是泛型接口。 泛型接口的格式： 123修饰符 interface 接口名称&lt;泛型变量&gt;&#123;&#125; 12345678910111213141516public class GenericDemo &#123; public static void main(String[] args) &#123; Data d = new StudentData(); d.add(new Student()); ................ &#125;&#125;public interface Data&lt;E&gt;&#123; void add(E e); void delete(E e); void update(E e); E query(int index);&#125;class Student&#123;&#125;class StudentData implements Data&lt;Student&gt;&#123;重写所有方法&#125; 通配符通配符：？ ? 可以用在使用泛型的时候代表一切类型 E、T、K、V 是在定义泛型的时候使用代表一切类型 泛型的上下限： ? extends Car：那么 ? 必须是 Car 或者其子类（泛型的上限） ? super Car：那么 ? 必须是 Car 或者其父类（泛型的下限，不是很常见） 1234567891011121314151617//需求：开发一个极品飞车的游戏，所有的汽车都能一起参与比赛。public class GenericDemo &#123; public static void main(String[] args) &#123; ArrayList&lt;BMW&gt; bmws = new ArrayList&lt;&gt;(); ArrayList&lt;AD&gt; ads = new ArrayList&lt;&gt;(); ArrayList&lt;Dog&gt; dogs = new ArrayList&lt;&gt;(); run(bmws); //run(dogs); &#125; //public static void run(ArrayList&lt;?&gt; car)&#123;&#125;//这样 dou对象也能进入 public static void run(ArrayList&lt;? extends Car&gt; car)&#123;&#125;&#125;class Car&#123;&#125;class BMW extends Car&#123;&#125;class AD extends Car&#123;&#125;class Dog&#123;&#125; 异常基本介绍异常：程序在编译或者执行的过程中可能出现的问题，Java 为常见的代码异常都设计一个类来代表 错误：Error ，程序员无法处理的错误，只能重启系统，比如内存奔溃，JVM 本身的奔溃 Java 中异常继承的根类是：Throwable 123456异常的体系: Throwable(根类，不是异常类) / \\ Error Exception（异常，需要研究和处理） / \\ 编译时异常 RuntimeException(运行时异常) Exception 异常的分类: 编译时异常：继承自 Exception 的异常或者其子类，编译阶段就会报错 运行时异常：继承自 RuntimeException 的异常或者其子类，编译阶段是不会出错的，在运行阶段出错 处理过程异常的产生默认的处理过程解析：（自动处理的过程） 默认会在出现异常的代码那里自动的创建一个异常对象：ArithmeticException（算术异常） 异常会从方法中出现的点这里抛出给调用者，调用者最终抛出给 JVM 虚拟机 虚拟机接收到异常对象后，先在控制台直接输出异常栈信息数据 直接从当前执行的异常点终止当前程序 后续代码没有机会执行了，因为程序已经死亡 1234567891011public class ExceptionDemo &#123; public static void main(String[] args) &#123; System.out.println(&quot;程序开始。。。。。。。。。。&quot;); chu( 10 ,0 ); System.out.println(&quot;程序结束。。。。。。。。。。&quot;);//不执行 &#125; public static void chu(int a , int b)&#123; int c = a / b ;// 出现了运行时异常,自动创建异常对象：ArithmeticException System.out.println(&quot;结果是：&quot;+c); &#125;&#125; 编译异常基本介绍编译时异常：继承自 Exception 的异常或者其子类，没有继承 RuntimeException，编译时异常是编译阶段就会报错 编译时异常的作用是什么：在编译阶段就爆出一个错误，目的在于提醒，请检查并注意不要出 BUG 123456public static void main(String[] args) throws ParseException &#123;\tString date = &quot;2015-01-12 10:23:21&quot;;\tSimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\tDate d = sdf.parse(date);\tSystem.out.println(d);&#125; 处理机制throws在出现编译时异常的地方层层把异常抛出去给调用者，调用者最终抛出给 JVM 虚拟机，JVM 虚拟机输出异常信息，直接终止掉程序，这种方式与默认方式是一样的 Exception 是异常最高类型可以抛出一切异常 1234567public static void main(String[] args) throws Exception &#123; System.out.println(&quot;程序开始。。。。&quot;); String s = &quot;2013-03-23 10:19:23&quot;; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); Date date = sdf.parse(s); System.out.println(&quot;程序结束。。。。。&quot;);&#125; try&#x2F;catch可以处理异常，并且出现异常后代码也不会死亡 捕获异常和处理异常的格式：捕获处理 123456789try&#123; // 监视可能出现异常的代码！&#125;catch(异常类型1 变量)&#123; // 处理异常&#125;catch(异常类型2 变量)&#123; // 处理异常&#125;...finall&#123;//资源释放&#125; 监视捕获处理异常写法：Exception 可以捕获处理一切异常类型 12345try&#123; // 可能出现异常的代码！&#125;catch (Exception e)&#123; e.printStackTrace(); // **直接打印异常栈信息**&#125; Throwable成员方法: public String getMessage()：返回此 throwable 的详细消息字符串 public String toString()：返回此可抛出的简短描述 public void printStackTrace()：把异常的错误信息输出在控制台 123456789101112public static void main(String[] args) &#123; System.out.println(&quot;程序开始。。。。&quot;); try &#123; String s = &quot;2013-03-23 10:19:23&quot;; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); Date date = sdf.parse(s); InputStream is = new FileInputStream(&quot;D:/meinv.png&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;程序结束。。。。。&quot;);&#125; 规范做法在出现异常的地方把异常一层一层的抛出给最外层调用者，最外层调用者集中捕获处理 123456789101112public class ExceptionDemo&#123;\tpublic static void main(String[] args)&#123; System.out.println(&quot;程序开始。。。。&quot;); try &#123; parseDate(&quot;2013-03-23 10:19:23&quot;); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; System.out.println(&quot;程序结束。。。。&quot;); &#125; public static void parseDate(String time) throws Exception&#123;...&#125;&#125; 运行异常基本介绍继承自 RuntimeException 的异常或者其子类，编译阶段是不会出错的，是在运行时阶段可能出现的错误，运行时异常编译阶段可以处理也可以不处理，代码编译都能通过 常见的运行时异常： 数组索引越界异常：ArrayIndexOutOfBoundsException 空指针异常：NullPointerException，直接输出没问题，调用空指针的变量的功能就会报错 类型转换异常：ClassCastException 迭代器遍历没有此元素异常：NoSuchElementException 算术异常（数学操作异常）：ArithmeticException 数字转换异常：NumberFormatException 处理机制运行时异常在编译阶段是不会报错，在运行阶段才会出错，运行时出错了程序还是会停止，运行时异常也建议要处理，运行时异常是自动往外抛出的，不需要手工抛出 运行时异常的处理规范：直接在最外层捕获处理即可，底层会自动抛出 123456789101112131415public class ExceptionDemo&#123; public static void main(String[] args)&#123; System.out.println(&quot;程序开始。。。。&quot;); try&#123; chu(10 / 0);//ArithmeticException: / by zero System.out.println(&quot;操作成功！&quot;);//没输出 &#125;catch (Exception e)&#123; e.printStackTrace(); System.out.println(&quot;操作失败！&quot;);//输出了 &#125; System.out.println(&quot;程序结束。。。。&quot;);//输出了 &#125; public static void chu(int a , int b) &#123; System.out.println( a / b );&#125;&#125; Finally用在捕获处理的异常格式中的，放在最后面 12345678910try&#123; // 可能出现异常的代码！&#125;catch(Exception e)&#123; e.printStackTrace();&#125;finally&#123; // 无论代码是出现异常还是正常执行，最终一定要执行这里的代码！！&#125;try: 1次。catch：0-N次 (如果有finally那么catch可以没有!!)finally: 0-1次 finally 的作用：可以在代码执行完毕以后进行资源的释放操作 资源：资源都是实现了 Closeable 接口的，都自带 close() 关闭方法 注意：如果在 finally 中出现了 return，会吞掉异常 12345678910111213141516171819202122232425262728293031323334public class FinallyDemo &#123; public static void main(String[] args) &#123; System.out.println(chu());//一定会输出 finally,优先级比return高 &#125; public static int chu()&#123; try&#123; int a = 10 / 2 ; return a ; &#125;catch (Exception e)&#123; e.printStackTrace(); return -1; &#125;finally &#123; System.out.println(&quot;=====finally被执行&quot;); //return 111; // 不建议在finally中写return，会覆盖前面所有的return值! &#125; &#125; public static void test()&#123; InputStream is = null; try&#123; is = new FileInputStream(&quot;D:/cang.png&quot;); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;finally &#123; System.out.println(&quot;==finally被执行===&quot;); // 回收资源。用于在代码执行完毕以后进行资源的回收操作！ try &#123; if(is!=null)is.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 自定义自定义异常: 自定义编译时异常：定义一个异常类继承 Exception，重写构造器，在出现异常的地方用 throw new 自定义对象抛出 自定义运行时异常：定义一个异常类继承 RuntimeException，重写构造器，在出现异常的地方用 throw new 自定义对象抛出 throws：用在方法上，用于抛出方法中的异常 throw: 用在出现异常的地方，创建异常对象且立即从此处抛出 12345678910111213141516171819202122232425262728293031//需求：认为年龄小于0岁，大于200岁就是一个异常。public class ExceptionDemo &#123; public static void main(String[] args) &#123; try &#123; checkAge(101); &#125; catch (AgeIllegalException e) &#123; e.printStackTrace(); &#125; &#125; public static void checkAge(int age) throws ItheimaAgeIllegalException &#123; if(age &lt; 0 || age &gt; 200)&#123;//年龄在0-200之间 throw new AgeIllegalException(&quot;/ age is illegal!&quot;); //throw new AgeIllegalRuntimeException(&quot;/ age is illegal!&quot;); &#125;else&#123; System.out.println(&quot;年龄是：&quot; + age); &#125; &#125;&#125;public class AgeIllegalException extends Exception&#123; Alt + Insert-&gt;Constructor &#125;//编译时异常public class AgeIllegalRuntimeException extends RuntimeException&#123;\tpublic AgeIllegalRuntimeException() &#123; &#125; public AgeIllegalRuntimeException(String message) &#123; super(message); &#125;&#125;//运行时异常 处理规范异常的语法注意： 运行时异常被抛出可以不处理，可以自动抛出；编译时异常必须处理；按照规范都应该处理 重写方法申明抛出的异常，子类方法抛出的异常类型必须是父类抛出异常类型或为其子类型 方法默认都可以自动抛出运行时异常， throws RuntimeException 可以省略不写 当多异常处理时，捕获处理，前面的异常类不能是后面异常类的父类 在 try&#x2F;catch 后可以追加 finally 代码块，其中的代码一定会被执行，通常用于资源回收操作 异常的作用： 可以处理代码问题，防止程序出现异常后的死亡 提高了程序的健壮性和安全性 12345678910111213141516public class Demo&#123; public static void main(String[] args)&#123; //请输入一个合法的年龄 while(true)&#123; try&#123; Scanner sc = new Scanner(System.in); System.out.println(&quot;请您输入您的年年龄：&quot;); int age = sc.nextInt(); System.out.println(&quot;年龄：&quot;+age); break; &#125;catch(Exception e)&#123; System.err.println(&quot;您的年龄是瞎输入的！&quot;); &#125; &#125; &#125;&#125; λlambda基本介绍Lambda 表达式是 JDK1.8 开始之后的新技术，是一种代码的新语法，一种特殊写法 作用：为了简化匿名内部类的代码写法 Lambda 表达式的格式： 123(匿名内部类被重写方法的形参列表) -&gt; &#123;\t//被重写方法的方法体代码&#125; Lambda 表达式并不能简化所有匿名内部类的写法，只能简化函数式接口的匿名内部类 简化条件：首先必须是接口，接口中只能有一个抽象方法 @FunctionalInterface 函数式接口注解：一旦某个接口加上了这个注解，这个接口只能有且仅有一个抽象方法 简化方法Lambda 表达式的省略写法（进一步在 Lambda 表达式的基础上继续简化） 如果 Lambda 表达式的方法体代码只有一行代码，可以省略大括号不写，同时要省略分号；如果这行代码是 return 语句，必须省略 return 不写 参数类型可以省略不写 如果只有一个参数，参数类型可以省略，同时 () 也可以省略 12345678910111213141516171819202122232425List&lt;String&gt; names = new ArrayList&lt;&gt;();names.add(&quot;a&quot;);names.add(&quot;b&quot;);names.add(&quot;c&quot;);names.forEach(new Consumer&lt;String&gt;() &#123; @Override public void accept(String s) &#123; System.out.println(s); &#125;&#125;);names.forEach((String s) -&gt; &#123; System.out.println(s);&#125;);names.forEach((s) -&gt; &#123; System.out.println(s);&#125;);names.forEach(s -&gt; &#123; System.out.println(s);&#125;);names.forEach(s -&gt; System.out.println(s) ); 常用简化Comparator 12345678910111213141516171819public class CollectionsDemo &#123; public static void main(String[] args) &#123; List&lt;Student&gt; lists = new ArrayList&lt;&gt;();//...s1 s2 s3 Collections.addAll(lists , s1 , s2 , s3); Collections.sort(lists, new Comparator&lt;Student&gt;() &#123; @Override public int compare(Student s1, Student s2) &#123; return s1.getAge() - s2.getAge(); &#125; &#125;); // 简化写法 Collections.sort(lists ,(Student t1, Student t2) -&gt; &#123; return t1.getAge() - t2.getAge(); &#125;); // 参数类型可以省略,最简单的 Collections.sort(lists ,(t1,t2) -&gt; t1.getAge()-t2.getAge()); &#125;&#125; 方法引用基本介绍方法引用：方法引用是为了进一步简化 Lambda 表达式的写法 方法引用的格式：类型或者对象::引用的方法 关键语法是：:: 123lists.forEach( s -&gt; System.out.println(s));// 方法引用！lists.forEach(System.out::println); 静态方法引用格式：类名::静态方法 简化步骤：定义一个静态方法，把需要简化的代码放到一个静态方法中去 静态方法引用的注意事项：被引用的方法的参数列表要和函数式接口中的抽象方法的参数列表一致,才能引用简化 1234567891011121314//定义集合加入几个Student元素// 使用静态方法进行简化！Collections.sort(lists, (o1, o2) -&gt; Student.compareByAge(o1 , o2));// 如果前后参数是一样的，而且方法是静态方法，既可以使用静态方法引用Collections.sort(lists, Student::compareByAge);public class Student &#123; private String name ; private int age ; public static int compareByAge(Student o1 , Student o2)&#123; return o1.getAge() - o2.getAge(); &#125;&#125; 实例方法引用格式：对象::实例方法 简化步骤：定义一个实例方法，把需要的代码放到实例方法中去 实例方法引用的注意事项：被引用的方法的参数列表要和函数式接口中的抽象方法的参数列表一致。 12345678910111213public class MethodDemo &#123; public static void main(String[] args) &#123; List&lt;String&gt; lists = new ArrayList&lt;&gt;(); lists.add(&quot;java1&quot;); lists.add(&quot;java2&quot;); lists.add(&quot;java3&quot;); // 对象是 System.out = new PrintStream(); // 实例方法：println() // 前后参数正好都是一个 lists.forEach(s -&gt; System.out.println(s)); lists.forEach(System.out::println); &#125;&#125; 特定类型特定类型：String，任何类型 引用格式：特定类型::方法 注意事项：如果第一个参数列表中的形参中的第一个参数作为了后面的方法的调用者，并且其余参数作为后面方法的形参，那么就可以用特定类型方法引用了 12345678910111213141516171819202122public class MethodDemo&#123; public static void main(String[] args) &#123; String[] strs = new String[]&#123;&quot;James&quot;, &quot;AA&quot;, &quot;John&quot;, &quot;Patricia&quot;,&quot;Dlei&quot; , &quot;Robert&quot;,&quot;Boom&quot;, &quot;Cao&quot; ,&quot;black&quot; , &quot;Michael&quot;, &quot;Linda&quot;,&quot;cao&quot;,&quot;after&quot;,&quot;sa&quot;&#125;; // public static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c) // 需求：按照元素的首字符(忽略大小写)升序排序！！！ Arrays.sort(strs, new Comparator&lt;String&gt;() &#123; @Override public int compare(String s1, String s2) &#123; return s1.compareToIgnoreCase(s2);//按照元素的首字符(忽略大小写) &#125; &#125;); Arrays.sort(strs, ( s1, s2 ) -&gt; s1.compareToIgnoreCase(s2)); // 特定类型的方法引用： Arrays.sort(strs, String::compareToIgnoreCase); System.out.println(Arrays.toString(strs)); &#125;&#125; 构造器格式：类名::new 注意事项：前后参数一致的情况下，又在创建对象，就可以使用构造器引用 1234567891011121314151617181920212223public class ConstructorDemo &#123; public static void main(String[] args) &#123; List&lt;String&gt; lists = new ArrayList&lt;&gt;(); lists.add(&quot;java1&quot;); lists.add(&quot;java2&quot;); lists.add(&quot;java3&quot;); // 集合默认只能转成Object类型的数组。 Object[] objs = lists.toArray(); // 我们想指定转换成字符串类型的数组！最新的写法可以结合构造器引用实现 String[] strs = lists.toArray(new IntFunction&lt;String[]&gt;() &#123; @Override public String[] apply(int value) &#123; return new String[value]; &#125; &#125;); String[] strs1 = lists.toArray(s -&gt; new String[s]); String[] strs2 = lists.toArray(String[]::new); System.out.println(&quot;String类型的数组：&quot;+ Arrays.toString(strs2)); &#125;&#125; I&#x2F;OStream概述Stream 流其实就是一根传送带，元素在上面可以被 Stream 流操作 可以解决已有集合类库或者数组 API 的弊端 Stream 流简化集合和数组的操作 链式编程 12345678list.stream().filter(new Predicate&lt;String&gt;() &#123; @Override public boolean test(String s) &#123; return s.startsWith(&quot;张&quot;); &#125; &#125;);list.stream().filter(s -&gt; s.startsWith(&quot;张&quot;)); 获取流集合获取 Stream 流用：default Stream&lt;E&gt; stream() 数组：Arrays.stream(数组) &#x2F; Stream.of(数组); 12345678910111213141516// Collection集合获取Stream流。Collection&lt;String&gt; c = new ArrayList&lt;&gt;();Stream&lt;String&gt; listStream = c.stream();// Map集合获取流// 先获取键的Stream流。Stream&lt;String&gt; keysStream = map.keySet().stream();// 在获取值的Stream流Stream&lt;Integer&gt; valuesStream = map.values().stream();// 获取键值对的Stream流（key=value： Map.Entry&lt;String,Integer&gt;）Stream&lt;Map.Entry&lt;String,Integer&gt;&gt; keyAndValues = map.entrySet().stream();//数组获取流String[] arr = new String[]&#123;&quot;Java&quot;, &quot;JavaEE&quot; ,&quot;Spring Boot&quot;&#125;;Stream&lt;String&gt; arrStream1 = Arrays.stream(arr);Stream&lt;String&gt; arrStream2 = Stream.of(arr); 常用API 方法名 说明 void forEach(Consumer&lt;? super T&gt; action) 逐一处理（遍历） long count 返回流中的元素数 Stream filter(Predicate&lt;? super T&gt; predicate) 用于对流中的数据进行过滤 Stream limit(long maxSize) 返回此流中的元素组成的流，截取前指定参数个数的数据 Stream skip(long n) 跳过指定参数个数的数据，返回由该流的剩余元素组成的流 Stream map(Function&lt;? super T,? extends R&gt; mapper) 加工方法，将当前流中的 T 类型数据转换为另一种 R 类型的流 static Stream concat(Stream a, Stream b) 合并 a 和 b 两个流为一个，调用 Stream.concat(s1,s2) Stream distinct() 返回由该流的不同元素组成的流 1234567891011121314151617181920212223242526272829303132333435public class StreamDemo &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;张无忌&quot;); list.add(&quot;周芷若&quot;); list.add(&quot;赵敏&quot;); list.add(&quot;张三&quot;); list.add(&quot;张三丰&quot;); list.add(&quot;张&quot;); //取以张开头并且名字是三位数的 list.stream().filter(s -&gt; s.startsWith(&quot;张&quot;) .filter(s -&gt; s.length == 3).forEach(System.out::println); //统计数量 long count = list.stream().filter(s -&gt; s.startsWith(&quot;张&quot;) .filter(s -&gt; s.length == 3).count(); //取前两个 list.stream().filter(s -&gt; s.length == 3).limit(2).forEach(...); //跳过前两个 list.stream().filter(s -&gt; s.length == 3).skip(2).forEach(...); // 需求：把名称都加上“张三的:+xxx” list.stream().map(s -&gt; &quot;张三的&quot; + s).forEach(System.out::println); // 需求：把名称都加工厂学生对象放上去!! // list.stream().map(name -&gt; new Student(name)); list.stream.map(Student::new).forEach(System.out::println); //数组流 Stream&lt;Integer&gt; s1 = Stream.of(10,20,30,40,50); //集合流 Stream&lt;String&gt; s2 = list.stream(); //合并流 Stream&lt;Object&gt; s3 = Stream.concat(s1,s2); s3.forEach(System.out::println); &#125;&#125;class Student&#123; private String name; //......&#125; 终结方法终结方法：Stream 调用了终结方法，流的操作就全部终结，不能继续使用，如 foreach，count 方法等 非终结方法：每次调用完成以后返回一个新的流对象，可以继续使用，支持链式编程 123// foreach终结方法list.stream().filter(s -&gt; s.startsWith(&quot;张&quot;)) .filter(s -&gt; s.length() == 3).forEach(System.out::println); 收集流收集 Stream：把 Stream 流的数据转回到集合中去 Stream 流：工具 集合：目的 Stream 收集方法：R collect(Collector collector) 把结果收集到集合中 Collectors 方法： public static &lt;T&gt; Collector toList()：把元素收集到 List 集合中 public static &lt;T&gt; Collector toSet()：把元素收集到 Set 集合中 public static Collector toMap(Function keyMapper,Function valueMapper)：把元素收集到 Map 集合中 Object[] toArray()：把元素收集数组中 public static Collector groupingBy(Function&lt;? super T, ? extends K&gt; classifier)：分组 1234567891011121314151617public static void main(String[] args) &#123;\tList&lt;String&gt; list = new ArrayList&lt;&gt;();\tStream&lt;String&gt; stream = list.stream().filter(s -&gt; s.startsWith(&quot;张&quot;)); //把stream流转换成Set集合。 Set&lt;String&gt; set = stream.collect(Collectors.toSet()); //把stream流转换成List集合。 //重新定义，因为资源已经被关闭了 Stream&lt;String&gt; stream1 = list.stream().filter(s -&gt; s.startsWith(&quot;张&quot;)); List&lt;String&gt; list = stream.collect(Collectors.toList()); //把stream流转换成数组。 Stream&lt;String&gt; stream2 = list.stream().filter(s -&gt; s.startsWith(&quot;张&quot;)); Object[] arr = stream2.toArray(); // 可以借用构造器引用申明转换成的数组类型！！！ String[] arr1 = stream2.toArray(String[]::new);&#125; File文件类File 类：代表操作系统的文件对象，是用来操作操作系统的文件对象的，删除文件，获取文件信息，创建文件（文件夹），广义来说操作系统认为文件包含（文件和文件夹） File 类构造器： public File(String pathname)：根据路径获取文件对象 public File(String parent , String child)：根据父路径和文件名称获取文件对象 File 类创建文件对象的格式： File f = new File(&quot;绝对路径/相对路径&quot;); 绝对路径：从磁盘的的盘符一路走到目的位置的路径 绝对路径依赖具体的环境，一旦脱离环境，代码可能出错 一般是定位某个操作系统中的某个文件对象 相对路径：不带盘符的（重点） 默认是直接相对到工程目录下寻找文件的。 相对路径只能用于寻找工程下的文件，可以跨平台 File f = new File(&quot;文件对象/文件夹对象&quot;) 广义来说：文件是包含文件和文件夹的 123456789101112131415161718192021public class FileDemo&#123; public static void main(String[] args) &#123; // 1.创建文件对象：使用绝对路径 // 文件路径分隔符： // -- a.使用正斜杠： / // -- b.使用反斜杠： \\\\ // -- c.使用分隔符API:File.separator //File f1 = new File(&quot;D:&quot;+File.separator+&quot;it&quot;+File.separator //+&quot;图片资源&quot;+File.separator+&quot;beautiful.jpg&quot;); File f1 = new File(&quot;D:\\\\seazean\\\\图片资源\\\\beautiful.jpg&quot;); System.out.println(f1.length()); // 获取文件的大小，字节大小 // 2.创建文件对象：使用相对路径 File f2 = new File(&quot;Day09Demo/src/dlei.txt&quot;); System.out.println(f2.length()); // 3.创建文件对象：代表文件夹。 File f3 = new File(&quot;D:\\\\it\\\\图片资源&quot;); System.out.println(f3.exists());// 判断路径是否存在！！ &#125;&#125; 常用API常用方法 方法 说明 String getAbsolutePath() 返回此 File 的绝对路径名字符串 String getPath() 获取创建文件对象的时候用的路径 String getName() 返回由此 File 表示的文件或目录的名称 long length() 返回由此 File 表示的文件的长度（大小） long length(FileFilter filter) 文件过滤器 123456789101112131415161718192021222324252627public class FileDemo &#123; public static void main(String[] args) &#123; // 1.绝对路径创建一个文件对象 File f1 = new File(&quot;E:/图片/test.jpg&quot;); // a.获取它的绝对路径。 System.out.println(f1.getAbsolutePath()); // b.获取文件定义的时候使用的路径。 System.out.println(f1.getPath()); // c.获取文件的名称：带后缀。 System.out.println(f1.getName()); // d.获取文件的大小：字节个数。 System.out.println(f1.length()); System.out.println(&quot;------------------------&quot;); // 2.相对路径 File f2 = new File(&quot;Demo/src/test.txt&quot;); // a.获取它的绝对路径。 System.out.println(f2.getAbsolutePath()); // b.获取文件定义的时候使用的路径。 System.out.println(f2.getPath()); // c.获取文件的名称：带后缀。 System.out.println(f2.getName()); // d.获取文件的大小：字节个数。 System.out.println(f2.length()); &#125;&#125; 判断方法方法列表： boolean exists()：此 File 表示的文件或目录是否实际存在 boolean isDirectory()：此 File 表示的是否为目录 boolean isFile()：此 File 表示的是否为文件 1234567File f = new File(&quot;Demo/src/test.txt&quot;);// a.判断文件路径是否存在System.out.println(f.exists()); // true// b.判断文件对象是否是文件,是文件返回true ,反之System.out.println(f.isFile()); // true// c.判断文件对象是否是文件夹,是文件夹返回true ,反之System.out.println(f.isDirectory()); // false 创建删除方法列表： boolean createNewFile()：当且仅当具有该名称的文件尚不存在时， 创建一个新的空文件 boolean delete()：删除由此 File 表示的文件或目录（只能删除空目录） boolean mkdir()：创建由此 File 表示的目录（只能创建一级目录） boolean mkdirs()：可以创建多级目录（建议使用） 123456789101112131415161718192021public class FileDemo &#123; public static void main(String[] args) throws IOException &#123; File f = new File(&quot;Demo/src/test.txt&quot;); // a.创建新文件，创建成功返回true ,反之 System.out.println(f.createNewFile()); // b.删除文件或者空文件夹 System.out.println(f.delete()); // 不能删除非空文件夹，只能删除空文件夹 File f1 = new File(&quot;E:/it/aaaaa&quot;); System.out.println(f1.delete()); // c.创建一级目录 File f2 = new File(&quot;E:/bbbb&quot;); System.out.println(f2.mkdir()); // d.创建多级目录 File f3 = new File(&quot;D:/it/e/a/d/ds/fas/fas/fas/fas/fas/fas&quot;); System.out.println(f3.mkdirs()); &#125;&#125; 遍历目录 public String[] list()：获取当前目录下所有的一级文件名称到一个字符串数组中去返回 public File[] listFiles()：获取当前目录下所有的一级文件对象到一个文件对象数组中去返回（重点） public long lastModified：返回此抽象路径名表示的文件上次修改的时间 123456789101112131415161718192021public class FileDemo &#123; public static void main(String[] args) &#123; File dir = new File(&quot;D:\\\\seazean&quot;); // a.获取当前目录对象下的全部一级文件名称到一个字符串数组返回。 String[] names = dir.list(); for (String name : names) &#123; System.out.println(name); &#125; // b.获取当前目录对象下的全部一级文件对象到一个File类型的数组返回。 File[] files = dir.listFiles(); for (File file : files) &#123; System.out.println(file.getAbsolutePath()); &#125; // c File f1 = new File(&quot;D:\\\\图片资源\\\\beautiful.jpg&quot;); long time = f1.lastModified(); // 最后修改时间！ SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); System.out.println(sdf.format(time)); &#125;&#125; 文件搜索递归实现文件搜索（非规律递归） 定义一个方法用于做搜索 进入方法中进行业务搜索分析 1234567891011121314151617181920212223242526272829303132333435/** * 去某个目录下搜索某个文件 * @param dir 搜索文件的目录。 * @param fileName 搜索文件的名称。 */public static void searchFiles(File dir , String fileName)&#123; // 1.判断是否存在该路径，是否是文件夹 if(dir.exists() &amp;&amp; dir.isDirectory())&#123; // 2.提取当前目录下的全部一级文件对象 File files = dir.listFiles();// 可能是null/也可能是空集合[] // 3.判断是否存在一级文件对象,判断是否不为空目录 if(files != null &amp;&amp; files.length &gt; 0)&#123; // 4.判断一级文件对象 for(File file : files)&#123; // 5.判断file是文件还是文件夹 if(file.isFile())&#123; // 6.判断该文件是否为我要找的文件对象 if(f.getName().contains(fileName))&#123;//模糊查找 sout(f.getAbsolutePath()); try &#123; // 启动它（拓展） Runtime r = Runtime.getRuntime(); r.exec(f.getAbsolutePath()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; else &#123; // 7.该文件是文件夹，文件夹要递归进入继续寻找 searchFiles(file,fileName) &#125; &#125; &#125; &#125;&#125; Character字符集：为字符编制的一套编号规则 计算机的底层是不能直接存储字符的，只能存储二进制 010101 ASCII 编码：8 个开关一组就可以编码字符，1 个字节 2^8 &#x3D; 256， 一个字节存储一个字符完全够用，英文和数字在底层存储都是采用 1 个字节存储的 12345678a 97b 98A 65B 660 481 49 中国人：中国人有 9 万左右字符，2 个字节编码一个中文字符，1 个字节编码一个英文字符，这套编码叫：GBK 编码，兼容 ASCII 编码表 美国人：收集全球所有的字符，统一编号，这套编码叫 Unicode 编码（万国码），一个英文等于两个字节，一个中文（含繁体）等于两个字节，中文标点占两个字节，英文标点占两个字节 UTF-8 是变种形式，也必须兼容 ASCII 编码表 UTF-8 一个中文一般占 3 个字节，中文标点占 3 个，英文字母和数字 1 个字节 编码前与编码后的编码集必须一致才不会乱码 IOStream概述IO 输入输出流：输入&#x2F;输出流 Input：输入 Output：输出 引入：File 类只能操作文件对象本身，不能读写文件对象的内容，读写数据内容，应该使用 IO 流 IO 流是一个水流模型：IO 理解成水管，把数据理解成水流 IO 流的分类： 按照流的方向分为：输入流，输出流。 输出流：以内存为基准，把内存中的数据写出到磁盘文件或者网络介质中去的流称为输出流 输入流：以内存为基准，把磁盘文件中的数据或者网络中的数据读入到内存中的流称为输入流 按照流的内容分为：字节流，字符流 字节流：流中的数据的最小单位是一个一个的字节，这个流就是字节流 字符流：流中的数据的最小单位是一个一个的字符，这个流就是字符流（针对于文本内容） 流大体分为四大类：字节输入流、字节输出流、字符输入流、字符输出流 12345678IO 流的体系： 字节流 字符流 字节输入流 字节输出流 字符输入流 字符输出流InputStream OutputStream Reader Writer (抽象类)FileInputStream FileOutputStream FileReader FileWriter(实现类)BufferedInputStream BufferedOutputStream BufferedReader BufferedWriter(实现类缓冲流) InputStreamReader OutputStreamWriterObjectInputStream ObjectOutputStream 字节流字节输入FileInputStream 文件字节输入流：以内存为基准，把磁盘文件中的数据按照字节的形式读入到内存中的流 构造方法： public FileInputStream(File path)：创建一个字节输入流管道与源文件对象接通 public FileInputStream(String pathName)：创建一个字节输入流管道与文件路径对接，底层实质上创建 File 对象 方法： public int read()：每次读取一个字节返回，读取完毕会返回 -1 public int read(byte[] buffer)：从字节输入流中读取字节到字节数组中去，返回读取的字节数量，没有字节可读返回 -1，byte 中新读取的数据默认是覆盖原数据，构造 String 需要设定长度 public String(byte[] bytes,int offset,int length)：构造新的 String public long transferTo(OutputStream out) ：从输入流中读取所有字节，并按读取的顺序，将字节写入给定的输出流 123456789101112131415161718public class FileInputStreamDemo01 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建文件对象定位dlei01.txt File file = new File(&quot;Demo/src/dlei01.txt&quot;); // 2.创建一个字节输入流管道与源文件接通 InputStream is = new FileInputStream(file); // 3.读取一个字节的编号返回，读取完毕返回-1 //int code1 = is.read(); // 读取一滴水，一个字节 //System.out.println((char)code1); // 4.使用while读取字节数 // 定义一个整数变量存储字节 int ch = 0 ; while((ch = is.read())!= -1)&#123; System.out.print((char) ch); &#125; &#125;&#125; 一个一个字节读取英文和数字没有问题，但是读取中文输出无法避免乱码，因为会截断中文的字节。一个一个字节的读取数据，性能也较差，所以禁止使用上面的方案 采取下面的方案： 1234567891011public static void main(String[] args) throws Exception &#123; //简化写法，底层实质上创建了File对象 InputStream is = new FileInputStream(&quot;Demo/src/test.txt&quot;); byte[] buffer = new byte[3];//开发中使用byte[1024] int len; while((len = is.read(buffer)) !=-1)&#123; // 读取了多少就倒出多少！ String rs = new String(buffer, 0, len); System.out.print(rs); &#125;&#125; 123456File f = new File(&quot;Demo/src/test.txt&quot;);InputStream is = new FileInputStream(f);// 读取全部的byte[] buffer = is.readAllBytes();String rs = new String(buffer);System.out.println(rs); 字节输出FileOutputStream 文件字节输出流：以内存为基准，把内存中的数据，按照字节的形式写出到磁盘文件中去 构造方法： public FileOutputStream(File file)：创建一个字节输出流管道通向目标文件对象 public FileOutputStream(String file) ：创建一个字节输出流管道通向目标文件路径 public FileOutputStream(File file, boolean append) : 创建一个追加数据的字节输出流管道到目标文件对象 public FileOutputStream(String file, boolean append) : 创建一个追加数据的字节输出流管道通向目标文件路径 API： public void write(int a)：写一个字节出去 public void write(byte[] buffer)：写一个字节数组出去 public void write(byte[] buffer , int pos , int len)：写一个字节数组的一部分出去，从 pos 位置，写出 len 长度 FileOutputStream 字节输出流每次启动写数据的时候都会先清空之前的全部数据，重新写入： OutputStream os = new FileOutputStream(&quot;Demo/out05&quot;)：覆盖数据管道 OutputStream os = new FileOutputStream(&quot;Demo/out05&quot; , true)：追加数据的管道 说明： 字节输出流只能写字节出去，字节输出流默认是覆盖数据管道 换行用：os.write(“\\r ”.getBytes()) 关闭和刷新：刷新流可以继续使用，关闭包含刷新数据但是流就不能使用了 123456OutputStream os = new FileOutputStream(&quot;Demo/out05&quot;);os.write(97);//aos.write(&#x27;b&#x27;);os.write(&quot;\\r &quot;.getBytes());os.write(&quot;我爱Java&quot;.getBytes());os.close(); 文件复制字节是计算机中一切文件的组成，所以字节流适合做一切文件的复制 123456789101112131415161718192021222324252627282930public class CopyDemo01 &#123; public static void main(String[] args) &#123; InputStream is = null ; OutputStream os = null ; try&#123; //（1）创建一个字节输入流管道与源文件接通。 is = new FileInputStream(&quot;D:\\\\seazean\\\\图片资源\\\\test.jpg&quot;); //（2）创建一个字节输出流与目标文件接通。 os = new FileOutputStream(&quot;D:\\\\seazean\\\\test.jpg&quot;); //（3）创建一个字节数组作为桶 byte buffer = new byte[1024]; //（4）从字节输入流管道中读取数据，写出到字节输出流管道即可 int len = 0; while((len = is.read(buffer)) != -1)&#123; os.write(buffer,0,len); &#125; System.out.println(&quot;复制完成！&quot;); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; finally &#123; /**（5）关闭资源！ */ try&#123; if(os!=null)os.close(); if(is!=null)is.close(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 字符流字符输入FileReader：文件字符输入流，以内存为基准，把磁盘文件的数据以字符的形式读入到内存，读取文本文件内容到内存中去 构造器： public FileReader(File file)：创建一个字符输入流与源文件对象接通。 public FileReader(String filePath)：创建一个字符输入流与源文件路径接通。 方法： public int read()：读取一个字符的编号返回，读取完毕返回 -1 public int read(char[] buffer)：读取一个字符数组，读取多少个就返回多少个，读取完毕返回 -1 结论： 字符流一个一个字符的读取文本内容输出，可以解决中文读取输出乱码的问题，适合操作文本文件，但是一个一个字符的读取文本内容性能较差 字符流按照字符数组循环读取数据，可以解决中文读取输出乱码的问题，而且性能也较好 字符流不能复制图片，视频等类型的文件。字符流在读取完了字节数据后并没有直接往目的地写，而是先查编码表，查到对应的数据就将该数据写入目的地。如果查不到，则码表会将一些未知区域中的数据去 map 这些字节数据，然后写到目的地，这样的话就造成了源数据和目的数据的不一致。 123456789101112131415161718192021public class FileReaderDemo01&#123;//字符 public static void main(String[] args) throws Exception &#123; // 创建一个字符输入流管道与源文件路径接通 Reader fr = new FileReader(&quot;Demo/src/test.txt&quot;); int ch; while((ch = fr.read()) != -1)&#123; System.out.print((char)ch); &#125; &#125;&#125;public class FileReaderDemo02 &#123;//字符数组 public static void main(String[] args) throws Exception &#123; Reader fr = new FileReader(&quot;Demo/src/test.txt&quot;); char[] buffer = new char[1024]; int len; while((len = fr.read(buffer)) != -1) &#123; System.out.print(new String(buffer, 0 , len)); &#125; &#125;&#125; 字符输出FileWriter：文件字符输出流，以内存为基准，把内存中的数据按照字符的形式写出到磁盘文件中去 构造器： public FileWriter(File file)：创建一个字符输出流管道通向目标文件对象（覆盖数据管道） public FileWriter(String filePath)：创建一个字符输出流管道通向目标文件路径 public FileWriter(File file, boolean append)：创建一个追加数据的字符输出流管道通向文件对象（追加数据管道） public FileWriter(String filePath, boolean append)：创建一个追加数据的字符输出流管道通向目标文件路径 方法： public void write(int c)：写一个字符出去 public void write(char[] buffer)：写一个字符数组出去 public void write(String c, int pos, int len)：写字符串的一部分出去 public void write(char[] buffer, int pos, int len)：写字符数组的一部分出去 fw.write(&quot;\\r &quot;)：换行 读写字符文件数据建议使用字符流 123456Writer fw = new FileWriter(&quot;Demo/src/test.txt&quot;);fw.write(97); // 字符afw.write(&#x27;b&#x27;); // 字符bfw.write(&quot;Java是最优美的语言！&quot;);fw.write(&quot;\\r &quot;);fw.close; 缓冲流基本介绍缓冲流可以提高字节流和字符流的读写数据的性能 缓冲流分为四类： BufferedInputStream：字节缓冲输入流，可以提高字节输入流读数据的性能 BufferedOutStream：字节缓冲输出流，可以提高字节输出流写数据的性能 BufferedReader：字符缓冲输入流，可以提高字符输入流读数据的性能 BufferedWriter：字符缓冲输出流，可以提高字符输出流写数据的性能 字节缓冲输入字节缓冲输入流：BufferedInputStream 作用：可以把低级的字节输入流包装成一个高级的缓冲字节输入流管道，提高字节输入流读数据的性能 构造器：public BufferedInputStream(InputStream in) 原理：缓冲字节输入流管道自带了一个 8KB 的缓冲池，每次可以直接借用操作系统的功能最多提取 8KB 的数据到缓冲池中去，以后我们直接从缓冲池读取数据，所以性能较好 123456789101112131415public class BufferedInputStreamDemo01 &#123; public static void main(String[] args) throws Exception &#123; // 1.定义一个低级的字节输入流与源文件接通 InputStream is = new FileInputStream(&quot;Demo/src/test.txt&quot;); // 2.把低级的字节输入流包装成一个高级的缓冲字节输入流。 BufferInputStream bis = new BufferInputStream(is); // 3.定义一个字节数组按照循环读取。 byte[] buffer = new byte[1024]; int len; while((len = bis.read(buffer)) != -1)&#123; String rs = new String(buffer, 0 , len); System.out.print(rs); &#125; &#125;&#125; 字节缓冲输出字节缓冲输出流：BufferedOutputStream 作用：可以把低级的字节输出流包装成一个高级的缓冲字节输出流，从而提高写数据的性能 构造器：public BufferedOutputStream(OutputStream os) 原理：缓冲字节输出流自带了 8KB 缓冲池,数据就直接写入到缓冲池中去，性能提高了 1234567891011121314public class BufferedOutputStreamDemo02 &#123; public static void main(String[] args) throws Exception &#123; // 1.写一个原始的字节输出流 OutputStream os = new FileOutputStream(&quot;Demo/src/test.txt&quot;); // 2.把低级的字节输出流包装成一个高级的缓冲字节输出流 BufferedOutputStream bos = new BufferedOutputStream(os); // 3.写数据出去 bos.write(&#x27;a&#x27;); bos.write(100); bos.write(&quot;我爱中国&quot;.getBytes()); bos.close(); &#125;&#125; 字节流性能利用字节流的复制统计各种写法形式下缓冲流的性能执行情况 复制流： 使用低级的字节流按照一个一个字节的形式复制文件 使用低级的字节流按照一个一个字节数组的形式复制文件 使用高级的缓冲字节流按照一个一个字节的形式复制文件 使用高级的缓冲字节流按照一个一个字节数组的形式复制文件 高级的缓冲字节流按照一个一个字节数组的形式复制文件，性能最高，建议使用 字符缓冲输入字符缓冲输入流：BufferedReader 作用：字符缓冲输入流把字符输入流包装成高级的缓冲字符输入流，可以提高字符输入流读数据的性能。 构造器：public BufferedReader(Reader reader) 原理：缓冲字符输入流默认会有一个 8K 的字符缓冲池,可以提高读字符的性能 按照行读取数据的功能：public String readLine() 读取一行数据返回，读取完毕返回 null 1234567891011121314151617public static void main(String[] args) throws Exception &#123; // 1.定义一个原始的字符输入流读取源文件 Reader fr = new FileReader(&quot;Demo/src/test.txt&quot;); // 2.把低级的字符输入流管道包装成一个高级的缓冲字符输入流管道 BufferedReader br = new BufferedReader(fr); // 定义一个字符串变量存储每行数据 String line; while((line = br.readLine()) != null)&#123; System.out.println(line); &#125; br.close(); //淘汰数组循环读取 //char[] buffer = new char[1024]; //int len; //while((len = br.read(buffer)) != -1)&#123; //System.out.println(new String(buffer , 0 , len));&#125; 字符缓冲输出符缓冲输出流：BufferedWriter 作用：把低级的字符输出流包装成一个高级的缓冲字符输出流，提高写字符数据的性能。 构造器：public BufferedWriter(Writer writer) 原理：高级的字符缓冲输出流多了一个 8K 的字符缓冲池，写数据性能极大提高了 字符缓冲输出流多了一个换行的特有功能：public void newLine() 新建一行 12345678public static void main(String[] args) throws Exception &#123; Writer fw = new FileWriter(&quot;Demo/src/test.txt&quot;,true);//追加 BufferedWriter bw = new BufferedWriter(fw); bw.write(&quot;我爱学习Java&quot;); bw.newLine();//换行 bw.close();&#125; 高效原因字符型缓冲流高效的原因：（空间换时间） BufferedReader：每次调用 read 方法，只有第一次从磁盘中读取了 8192（8k）个字符，存储到该类型对象的缓冲区数组中，将其中一个返回给调用者，再次调用 read 方法时，就不需要访问磁盘，直接从缓冲区中拿出一个数据即可，提升了效率 BufferedWriter：每次调用 write 方法，不会直接将字符刷新到文件中，而是存储到字符数组中，等字符数组写满了，才一次性刷新到文件中，减少了和磁盘交互的次数，提升了效率 字节型缓冲流高效的原因： BufferedInputStream：在该类型中准备了一个数组，存储字节信息，当外界调用 read() 方法想获取一个字节的时候，该对象从文件中一次性读取了 8192 个字节到数组中，只返回了第一个字节给调用者。将来调用者再次调用 read 方法时，当前对象就不需要再次访问磁盘，只需要从数组中取出一个字节返回给调用者即可，由于读取的是数组，所以速度非常快。当 8192 个字节全都读取完成之后，再需要读取一个字节，就得让该对象到文件中读取下一个 8192 个字节 BufferedOutputStream：在该类型中准备了一个数组，存储字节信息，当外界调用 write 方法想写出一个字节的时候，该对象直接将这个字节存储到了自己的数组中，而不刷新到文件中。一直到该数组所有 8192 个位置全都占满，该对象才把这个数组中的所有数据一次性写出到目标文件中。如果最后一次循环没有将数组写满，最终在关闭流对象的时候，也会将该数组中的数据刷新到文件中。 注意：字节流和字符流，都是装满时自动写出，或者没满时手动 flush 写出，或 close 时刷新写出 转换流乱码问题字符流读取： 1234代码编码 文件编码 中文情况。UTF-8 UTF-8 不乱码!GBK GBK 不乱码!UTF-8 GBK 乱码! 如果代码编码和读取的文件编码一致，字符流读取的时候不会乱码 如果代码编码和读取的文件编码不一致，字符流读取的时候会乱码 字符输入字符输入转换流：InputStreamReader 作用：解决字符流读取不同编码的乱码问题，把原始的字节流按照默认的编码或指定的编码转换成字符输入流 构造器： public InputStreamReader(InputStream is)：使用当前代码默认编码 UTF-8 转换成字符流 public InputStreamReader(InputStream is, String charset)：指定编码把字节流转换成字符流 123456789101112131415public class InputStreamReaderDemo&#123; public static void main(String[] args) throws Exception &#123; // 1.提取GBK文件的原始字节流 InputStream is = new FileInputStream(&quot;D:\\\\seazean\\\\Netty.txt&quot;); // 2.把原始字节输入流通过转换流，转换成 字符输入转换流InputStreamReader InputStreamReader isr = new InputStreamReader(is, &quot;GBK&quot;); // 3.包装成缓冲流 BufferedReader br = new BufferedReader(isr); //循环读取 String line; while((line = br.readLine()) != null)&#123; System.out.println(line); &#125; &#125;&#125; 字符输出字符输出转换流：OutputStreamWriter 作用：可以指定编码把字节输出流转换成字符输出流，可以指定写出去的字符的编码 构造器： public OutputStreamWriter(OutputStream os)：用默认编码 UTF-8 把字节输出流转换成字符输出流 public OutputStreamWriter(OutputStream os, String charset)：指定编码把字节输出流转换成 1234OutputStream os = new FileOutputStream(&quot;Demo/src/test.txt&quot;);OutputStreamWriter osw = new OutputStreamWriter(os,&quot;GBK&quot;);osw.write(&quot;我在学习Java&quot;); osw.close(); 序列化基本介绍对象序列化：把 Java 对象转换成字节序列的过程，将对象写入到 IO 流中，对象 &#x3D;&gt; 文件中 对象反序列化：把字节序列恢复为 Java 对象的过程，从 IO 流中恢复对象，文件中 &#x3D;&gt; 对象 transient 关键字修饰的成员变量，将不参与序列化 序列化对象序列化流（对象字节输出流）：ObjectOutputStream 作用：把内存中的 Java 对象数据保存到文件中去 构造器：public ObjectOutputStream(OutputStream out) 序列化方法：public final void writeObject(Object obj) 注意：对象如果想参与序列化，对象必须实现序列化接口 implements Serializable ，否则序列化失败 12345678910111213141516171819202122232425public class SerializeDemo01 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建User用户对象 User user = new User(&quot;seazean&quot;,&quot;980823&quot;,&quot;七十一&quot;); // 2.创建低级的字节输出流通向目标文件 OutputStream os = new FileOutputStream(&quot;Demo/src/obj.dat&quot;); // 3.把低级的字节输出流包装成高级的对象字节输出流 ObjectOutputStream ObjectOutputStream oos = new ObjectOutputStream(os); // 4.通过对象字节输出流序列化对象： oos.writeObject(user); // 5.释放资源 oos.close(); System.out.println(&quot;序列化对象成功~~~~&quot;); &#125;&#125;class User implements Serializable &#123; // 加入序列版本号 private static final long serialVersionUID = 1L; private String loginName; private transient String passWord; private String userName; // get+set&#125; 123456// 序列化为二进制数据ByteArrayOutputStream bos = new ByteArrayOutputStream();ObjectOutputStream oos = new ObjectOutputStream(bos);oos.writeObject(obj);\t// 将该对象序列化为二进制数据oos.flush();byte[] bytes = bos.toByteArray(); 反序列对象反序列化（对象字节输入流）：ObjectInputStream 作用：读取序列化的对象文件恢复到 Java 对象中 构造器：public ObjectInputStream(InputStream is) 方法：public final Object readObject() 序列化版本号：private static final long serialVersionUID = 2L 注意：序列化使用的版本号和反序列化使用的版本号一致才可以正常反序列化，否则报错 1234567891011121314public class SerializeDemo02 &#123; public static void main(String[] args) throws Exception &#123; InputStream is = new FileInputStream(&quot;Demo/src/obj.dat&quot;); ObjectInputStream ois = new ObjectInputStream(is); User user = (User)ois.readObject();//反序列化 System.out.println(user); System.out.println(&quot;反序列化完成！&quot;); &#125;&#125;class User implements Serializable &#123; // 加入序列版本号 private static final long serialVersionUID = 1L; //........&#125; 打印流打印流 PrintStream &#x2F; PrintWriter 打印流的作用： 可以方便，快速的写数据出去，可以实现打印什么类型，就是什么类型 PrintStream&#x2F;PrintWriter 不光可以打印数据，还可以写字节数据和字符数据出去 System.out.print() 底层基于打印流实现的 构造器： public PrintStream(OutputStream os) public PrintStream(String filepath) System 类： public static void setOut(PrintStream out)：让系统的输出流向打印流 12345678910111213141516171819public class PrintStreamDemo01 &#123; public static void main(String[] args) throws Exception &#123; PrintStream ps = new PrintStream(&quot;Demo/src/test.txt&quot;); ps.println(任何类型的数据); ps.print(不换行); ps.write(&quot;我爱你&quot;.getBytes()); ps.close(); &#125;&#125;public class PrintStreamDemo02 &#123; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;==seazean0==&quot;); PrintStream ps = new PrintStream(&quot;Demo/src/log.txt&quot;); System.setOut(ps); // 让系统的输出流向打印流 //不输出在控制台，输出到文件里 System.out.println(&quot;==seazean1==&quot;); System.out.println(&quot;==seazean2==&quot;); &#125;&#125; Closetry-with-resources： 1234567try( // 这里只能放置资源对象，用完会自动调用close()关闭)&#123;&#125;catch(Exception e)&#123; e.printStackTrace();&#125; 资源类一定是实现了 Closeable 接口，实现这个接口的类就是资源 有 close() 方法，try-with-resources 会自动调用它的 close() 关闭资源 12345678910111213141516try(\t/** （1）创建一个字节输入流管道与源文件接通。 */\tInputStream is = new FileInputStream(&quot;D:\\\\seazean\\\\图片资源\\\\meinv.jpg&quot;);\t/** （2）创建一个字节输出流与目标文件接通。*/\tOutputStream os = new FileOutputStream(&quot;D:\\\\seazean\\\\meimei.jpg&quot;);\t/** （5）关闭资源！是自动进行的 */)&#123;\tbyte[] buffer = new byte[1024];\tint len = 0;\twhile((len = is.read(buffer)) != -1)&#123; os.write(buffer, 0 , len);\t&#125;\tSystem.out.println(&quot;复制完成！&quot;);&#125;catch (Exception e)&#123;\te.printStackTrace();&#125; PropertiesProperties：属性集对象。就是一个 Map 集合，一个键值对集合 核心作用：Properties 代表的是一个属性文件，可以把键值对数据存入到一个属性文件 属性文件：后缀是 .properties 结尾的文件，里面的内容都是 key&#x3D;value Properties 方法： 方法名 说明 Object setProperty(String key, String value) 设置集合的键和值，底层调用 Hashtable 方法 put String getProperty(String key) 使用此属性列表中指定的键搜索属性 Set stringPropertyNames() 所有键的名称的集合 synchronized void load(Reader r) 从输入字符流读取属性列表（键和元素对） synchronized void load(InputStream in) 加载属性文件的数据到属性集对象中去 void store(Writer w, String comments) 将此属性列表(键和元素对)写入 Properties 表 void store(OutputStream os, String comments) 保存数据到属性文件中去 123456789101112public class PropertiesDemo01 &#123; public static void main(String[] args) throws Exception &#123; // a.创建一个属性集对象：Properties的对象。 Properties properties = new Properties();//&#123;&#125; properties.setProperty(&quot;admin&quot; , &quot;123456&quot;); // b.把属性集对象的数据存入到属性文件中去（重点） OutputStream os = new FileOutputStream(&quot;Demo/src/users.properties&quot;); properties.store(os,&quot;i am very happy!!我保存了用户数据!&quot;); //参数一：被保存数据的输出管道 //参数二：保存心得。就是对象保存的数据进行解释说明！ &#125;&#125; 1234567891011121314public class PropertiesDemo02 &#123; public static void main(String[] args) throws Exception &#123; Properties properties = new Properties();//底层基于map集合 properties.load(new FileInputStream(&quot;Demo/src/users.properties&quot;)); System.out.println(properties); System.out.println(properties.getProperty(&quot;admin&quot;)); Set&lt;String&gt; set = properties.stringPropertyNames(); for (String s : set) &#123; String value = properties.getProperty(s); System.out.println(s + value); &#125; &#125;&#125; RandomIORandomAccessFile 类：该类的实例支持读取和写入随机访问文件 构造器： RandomAccessFile(File file, String mode)：创建随机访问文件流，从 File 参数指定的文件读取，可选择写入 RandomAccessFile(String name, String mode)：创建随机访问文件流，从指定名称文件读取，可选择写入文件 常用方法： public void seek(long pos)：设置文件指针偏移，从该文件开头测量，发生下一次读取或写入(插入+覆盖) public void write(byte[] b)：从指定的字节数组写入 b.length 个字节到该文件 public int read(byte[] b)：从该文件读取最多 b.length 个字节的数据到字节数组 1234567public static void main(String[] args) throws Exception &#123; RandomAccessFile rf = new RandomAccessFile(new File(),&quot;rw&quot;); rf.write(&quot;hello world&quot;.getBytes()); rf.seek(5);//helloxxxxld rf.write(&quot;xxxx&quot;.getBytes()); rf.close();&#125; Commonscommons-io 是 apache 提供的一组有关 IO 操作的类库，可以提高 IO 功能开发的效率 commons-io 工具包提供了很多有关 IO 操作的类： 包 功能描述 org.apache.commons.io 有关 Streams、Readers、Writers、Files 的工具类 org.apache.commons.io.input 输入流相关的实现类，包含 Reader 和 InputStream org.apache.commons.io.output 输出流相关的实现类，包含 Writer 和 OutputStream org.apache.commons.io.serialization 序列化相关的类 IOUtils 和 FileUtils 可以方便的复制文件和文件夹 1234567891011121314151617public class CommonsIODemo01 &#123; public static void main(String[] args) throws Exception &#123; // 1.完成文件复制！ IOUtils.copy(new FileInputStream(&quot;Demo/src/books.xml&quot;), new FileOutputStream(&quot;Demo/new.xml&quot;)); // 2.完成文件复制到某个文件夹下！ FileUtils.copyFileToDirectory(new File(&quot;Demo/src/books.xml&quot;), new File(&quot;D:/it&quot;)); // 3.完成文件夹复制到某个文件夹下！ FileUtils.copyDirectoryToDirectory(new File(&quot;D:\\\\it\\\\图片服务器&quot;) , new File(&quot;D:\\\\&quot;)); // Java从1.7开始提供了一些nio, 自己也有一行代码完成复制的技术。 Files.copy(Paths.get(&quot;Demo/src/books.xml&quot;) , new FileOutputStream(&quot;Demo/new11.txt&quot;)); &#125;&#125; 反射测试框架单元测试的经典框架：Junit，是 Java 语言编写的第三方单元测试框架 单元测试： 单元：在 Java 中，一个类就是一个单元 单元测试：Junit 编写的一小段代码，用来对某个类中的某个方法进行功能测试或业务逻辑测试 Junit 单元测试框架的作用： 用来对类中的方法功能进行有目的的测试，以保证程序的正确性和稳定性 能够独立的测试某个方法或者所有方法的预期正确性 测试方法注意事项：必须是 public 修饰的，没有返回值，没有参数，使用注解@Test修饰 Junit常用注解（Junit 4.xxxx 版本），@Test 测试方法： @Before：用来修饰实例方法，该方法会在每一个测试方法执行之前执行一次 @After：用来修饰实例方法，该方法会在每一个测试方法执行之后执行一次 @BeforeClass：用来静态修饰方法，该方法会在所有测试方法之前只执行一次 @AfterClass：用来静态修饰方法，该方法会在所有测试方法之后只执行一次 Junit 常用注解（Junit5.xxxx 版本），@Test 测试方法： @BeforeEach：用来修饰实例方法，该方法会在每一个测试方法执行之前执行一次 @AfterEach：用来修饰实例方法，该方法会在每一个测试方法执行之后执行一次 @BeforeAll：用来静态修饰方法，该方法会在所有测试方法之前只执行一次 @AfterAll：用来静态修饰方法，该方法会在所有测试方法之后只执行一次 作用： 开始执行的方法：初始化资源 执行完之后的方法：释放资源 1234567891011public class UserService &#123; public String login(String loginName , String passWord)&#123; if(&quot;admin&quot;.equals(loginName)&amp;&amp;&quot;123456&quot;.equals(passWord))&#123; return &quot;success&quot;; &#125; return &quot;用户名或者密码错误！&quot;; &#125; public void chu(int a , int b)&#123; System.out.println(a / b); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839//测试方法的要求：1.必须public修饰 2.没有返回值没有参数 3. 必须使注解@Test修饰public class UserServiceTest &#123; // @Before：用来修饰实例方法，该方法会在每一个测试方法执行之前执行一次。 @Before public void before()&#123; System.out.println(&quot;===before===&quot;); &#125; // @After：用来修饰实例方法，该方法会在每一个测试方法执行之后执行一次。 @After public void after()&#123; System.out.println(&quot;===after===&quot;); &#125; // @BeforeClass：用来静态修饰方法，该方法会在所有测试方法之前只执行一次。 @BeforeClass public static void beforeClass()&#123; System.out.println(&quot;===beforeClass===&quot;); &#125; // @AfterClass：用来静态修饰方法，该方法会在所有测试方法之后只执行一次。 @AfterClass public static void afterClass()&#123; System.out.println(&quot;===afterClass===&quot;); &#125; @Test public void testLogin()&#123; UserService userService = new UserService(); String rs = userService.login(&quot;admin&quot;,&quot;123456&quot;); /**断言预期结果的正确性。 * 参数一：测试失败的提示信息。 * 参数二：期望值。 * 参数三：实际值 */ Assert.assertEquals(&quot;登录业务功能方法有错误，请检查！&quot;,&quot;success&quot;,rs); &#125; @Test public void testChu()&#123; UserService userService = new UserService(); userService.chu(10 , 0); &#125;&#125; 介绍反射反射是指对于任何一个类，在”运行的时候”都可以直接得到这个类全部成分 构造器对象：Constructor 成员变量对象：Field 成员方法对象：Method 核心思想：在运行时获取类编译后的字节码文件对象，然后解析类中的全部成分 反射提供了一个 Class 类型：HelloWorld.java → javac → HelloWorld.class Class c = HelloWorld.class 注意：反射是工作在运行时的技术，只有运行之后才会有 class 类对象 作用：可以在运行时得到一个类的全部成分然后操作，破坏封装性，也可以破坏泛型的约束性。 反射的优点： 可扩展性：应用程序可以利用全限定名创建可扩展对象的实例，来使用来自外部的用户自定义类 类浏览器和可视化开发环境：一个类浏览器需要可以枚举类的成员，可视化开发环境（如 IDE）可以从利用反射中可用的类型信息中受益，以帮助程序员编写正确的代码 调试器和测试工具： 调试器需要能够检查一个类里的私有成员，测试工具可以利用反射来自动地调用类里定义的可被发现的 API 定义，以确保一组测试中有较高的代码覆盖率 反射的缺点： 性能开销：反射涉及了动态类型的解析，所以 JVM 无法对这些代码进行优化，反射操作的效率要比那些非射操作低得多，应该避免在经常被执行的代码或对性能要求很高的程序中使用反射 安全限制：使用反射技术要求程序必须在一个没有安全限制的环境中运行，如果一个程序必须在有安全限制的环境中运行 内部暴露：由于反射允许代码执行一些在正常情况下不被允许的操作（比如访问私有的属性和方法），所以使用反射可能会导致意料之外的副作用，这可能导致代码功能失调并破坏可移植性。反射代码破坏了抽象性，因此当平台发生改变的时候，代码的行为就有可能也随着变化 获取元素获取类反射技术的第一步是先得到 Class 类对象，有三种方式获取： 类名.class 类的对象.getClass() Class.forName(“类的全限名”)：public static Class&lt;?&gt; forName(String className) Class 类下的方法： 方法 作用 String getSimpleName() 获得类名字符串：类名 String getName() 获得类全名：包名+类名 T newInstance() 创建 Class 对象关联类的对象，底层是调用无参数构造器，已经被淘汰 12345678910111213141516171819202122public class ReflectDemo&#123; public static void main(String[] args) throws Exception &#123; // 反射的第一步永远是先得到类的Class文件对象: 字节码文件。 // 1.类名.class Class c1 = Student.class; System.out.println(c1);//class _03反射_获取Class类对象.Student // 2.对象.getClass() Student swk = new Student(); Class c2 = swk.getClass(); System.out.println(c2); // 3.Class.forName(&quot;类的全限名&quot;) // 直接去加载该类的class文件。 Class c3 = Class.forName(&quot;_03反射_获取Class类对象.Student&quot;); System.out.println(c3); System.out.println(c1.getSimpleName()); // 获取类名本身（简名）Student System.out.println(c1.getName()); //获取类的全限名_03反射_获取Class类对象.Student &#125;&#125;class Student&#123;&#125; 获取构造获取构造器的 API： Constructor getConstructor(Class… parameterTypes)：根据参数匹配获取某个构造器，只能拿 public 修饰的构造器 Constructor getDeclaredConstructor(Class… parameterTypes)：根据参数匹配获取某个构造器，只要申明就可以定位，不关心权限修饰符 Constructor[] getConstructors()：获取所有的构造器，只能拿 public 修饰的构造器 Constructor[] getDeclaredConstructors()：获取所有构造器，只要申明就可以定位，不关心权限修饰符 Constructor 的常用 API： 方法 作用 T newInstance(Object… initargs) 创建对象，注入构造器需要的数据 void setAccessible(true) 修改访问权限，true 攻破权限（暴力反射） String getName() 以字符串形式返回此构造函数的名称 int getParameterCount() 返回参数数量 Class&lt;?&gt;[] getParameterTypes 返回参数类型数组 1234567891011121314151617181920212223public class TestStudent01 &#123; @Test public void getDeclaredConstructors()&#123; // a.反射第一步先得到Class类对象 Class c = Student.class ; // b.定位全部构造器，只要申明了就可以拿到 Constructor[] cons = c.getDeclaredConstructors(); // c.遍历这些构造器 for (Constructor con : cons) &#123; System.out.println(con.getName()+&quot;-&gt;&quot;+con.getParameterCount()); &#125; &#125; @Test public void getDeclaredConstructor() throws Exception &#123; // a.反射第一步先得到Class类对象 Class c = Student.class ; // b.定位某个构造器，根据参数匹配，只要申明了就可以获取 //Constructor con = c.getDeclaredConstructor(); // 可以拿到！定位无参数构造器！ Constructor con = c.getDeclaredConstructor(String.class, int.class); //有参数的！! // c.构造器名称和参数 System.out.println(con.getName()+&quot;-&gt;&quot;+con.getParameterCount()); &#125;&#125; 123456789101112public class Student &#123; private String name ; private int age ; private Student()&#123; System.out.println(&quot;无参数构造器被执行~~~~&quot;); &#125; public Student(String name, int age) &#123; System.out.println(&quot;有参数构造器被执行~~~~&quot;); this.name = name; this.age = age; &#125;&#125; 123456789101112131415161718192021222324252627282930//测试方法public class TestStudent02 &#123; // 1.调用无参数构造器得到一个类的对象返回。 @Test public void createObj01() throws Exception &#123; // a.反射第一步是先得到Class类对象 Class c = Student.class ; // b.定位无参数构造器对象 Constructor constructor = c.getDeclaredConstructor(); // c.暴力打开私有构造器的访问权限 constructor.setAccessible(true); // d.通过无参数构造器初始化对象返回 Student swk = (Student) constructor.newInstance(); // 最终还是调用无参数构造器的！ System.out.println(swk);//Student&#123;name=&#x27;null&#x27;, age=0&#125; &#125; // 2.调用有参数构造器得到一个类的对象返回。 @Test public void createObj02() throws Exception &#123; // a.反射第一步是先得到Class类对象 Class c = Student.class ; // b.定位有参数构造器对象 Constructor constructor = c.getDeclaredConstructor(String.class , int.class); // c.通过无参数构造器初始化对象返回 Student swk = (Student) constructor.newInstance(&quot;孙悟空&quot;,500); // 最终还是调用有参数构造器的！ System.out.println(swk);//Student&#123;name=&#x27;孙悟空&#x27;, age=500&#125; &#125;&#125; 获取变量获取 Field 成员变量 API： Field getField(String name)：根据成员变量名获得对应 Field 对象，只能获得 public 修饰 Field getDeclaredField(String name)：根据成员变量名获得对应 Field 对象，所有申明的变量 Field[] getFields()：获得所有的成员变量对应的 Field 对象，只能获得 public 的 Field[] getDeclaredFields()：获得所有的成员变量对应的 Field 对象，只要申明了就可以得到 Field 的方法：给成员变量赋值和取值 方法 作用 void set(Object obj, Object value) 给对象注入某个成员变量数据，obj 是对象，value 是值 Object get(Object obj) 获取指定对象的成员变量的值，obj 是对象，没有对象为 null void setAccessible(true) 暴力反射，设置为可以直接访问私有类型的属性 Class getType() 获取属性的类型，返回 Class 对象 String getName() 获取属性的名称 12345678910111213141516171819202122public class FieldDemo &#123; //获取全部成员变量 @Test public void getDeclaredFields()&#123; // a.先获取class类对象 Class c = Dog.class; // b.获取全部申明的成员变量对象 Field[] fields = c.getDeclaredFields(); for (Field field : fields) &#123; System.out.println(field.getName()+&quot;-&gt;&quot;+field.getType()); &#125; &#125; //获取某个成员变量 @Test public void getDeclaredField() throws Exception &#123; // a.先获取class类对象 Class c = Dog.class; // b.定位某个成员变量对象 :根据名称定位！！ Field ageF = c.getDeclaredField(&quot;age&quot;); System.out.println(ageF.getName()+&quot;-&gt;&quot;+ageF.getType()); &#125;&#125; 12345678910111213141516public class Dog &#123; private String name; private int age ; private String color ; public static String school; public static final String SCHOOL_1 = &quot;宠物学校&quot;; public Dog() &#123; &#125; public Dog(String name, int age, String color) &#123; this.name = name; this.age = age; this.color = color; &#125;&#125; 123456789101112131415161718//测试方法public class FieldDemo02 &#123; @Test public void setField() throws Exception &#123; // a.反射的第一步获取Class类对象 Class c = Dog.class ; // b.定位name成员变量 Field name = c.getDeclaredField(&quot;name&quot;); // c.为这个成员变量赋值！ Dog d = new Dog(); name.setAccessible(true); name.set(d,&quot;泰迪&quot;); System.out.println(d);//Dog&#123;name=&#x27;泰迪&#x27;, age=0, color=&#x27;null&#x27;&#125; // d.获取成员变量的值 String value = name.get(d)+&quot;&quot;; System.out.println(value);//泰迪 &#125;&#125; 获取方法获取 Method 方法 API： Method getMethod(String name,Class…args)：根据方法名和参数类型获得方法对象，public 修饰 Method getDeclaredMethod(String name,Class…args)：根据方法名和参数类型获得方法对象，包括 private Method[] getMethods()：获得类中的所有成员方法对象返回数组，只能获得 public 修饰且包含父类的 Method[] getDeclaredMethods()：获得类中的所有成员方法对象，返回数组，只获得本类申明的方法 Method 常用 API： public Object invoke(Object obj, Object… args)：使用指定的参数调用由此方法对象，obj 对象名 123456789101112131415161718192021222324252627282930313233343536373839404142public class MethodDemo&#123; //获得类中的所有成员方法对象 @Test public void getDeclaredMethods()&#123; // a.先获取class类对象 Class c = Dog.class ; // b.获取全部申明的方法! Method[] methods = c.getDeclaredMethods(); // c.遍历这些方法 for (Method method : methods) &#123; System.out.println(method.getName()+&quot;-&gt;&quot; + method.getParameterCount()+&quot;-&gt;&quot; + method.getReturnType()); &#125; &#125; @Test public void getDeclardMethod() throws Exception &#123; Class c = Dog.class; Method run = c.getDeclaredMethod(&quot;run&quot;); // c.触发方法执行! Dog d = new Dog(); Object o = run.invoke(d); System.out.println(o);// 如果方法没有返回值，结果是null //参数一：方法名称 参数二：方法的参数个数和类型(可变参数！) Method eat = c.getDeclaredMethod(&quot;eat&quot;,String.class); eat.setAccessible(true); // 暴力反射！ //参数一：被触发方法所在的对象 参数二：方法需要的入参值 Object o1 = eat.invoke(d,&quot;肉&quot;); System.out.println(o1);// 如果方法没有返回值，结果是null &#125;&#125;public class Dog &#123; private String name ; public Dog()&#123; &#125; public void run()&#123;System.out.println(&quot;狗跑的贼快~~&quot;);&#125;\tprivate void eat()&#123;System.out.println(&quot;狗吃骨头&quot;);&#125;\tprivate void eat(String name)&#123;System.out.println(&quot;狗吃&quot;+name);&#125;\tpublic static void inAddr()&#123;System.out.println(&quot;在吉山区有一只单身狗！&quot;);&#125;&#125; 暴力攻击泛型只能工作在编译阶段，运行阶段泛型就消失了，反射工作在运行时阶段 反射可以破坏面向对象的封装性（暴力反射） 同时可以破坏泛型的约束性 12345678910111213141516public class ReflectDemo &#123; public static void main(String[] args) throws Exception &#123; List&lt;Double&gt; scores = new ArrayList&lt;&gt;(); scores.add(99.3); scores.add(199.3); scores.add(89.5); // 拓展：通过反射暴力的注入一个其他类型的数据进去。 // a.先得到集合对象的Class文件对象 Class c = scores.getClass(); // b.从ArrayList的Class对象中定位add方法 Method add = c.getDeclaredMethod(&quot;add&quot;, Object.class); // c.触发scores集合对象中的add执行（运行阶段，泛型不能约束了） add.invoke(scores, &quot;字符串&quot;); System.out.println(scores); &#125;&#125; 注解概念注解：类的组成部分，可以给类携带一些额外的信息，提供一种安全的类似注释标记的机制，用来将任何信息或元数据（metadata）与程序元素（类、方法、成员变量等）进行关联 注解是给编译器或 JVM 看的，编译器或 JVM 可以根据注解来完成对应的功能 注解类似修饰符，应用于包、类型、构造方法、方法、成员变量、参数及本地变量的声明语句中 父类中的注解是不能被子类继承的 注解作用： 标记 框架技术多半都是在使用注解和反射，都是属于框架的底层基础技术 在编译时进行格式检查，比如方法重写约束 @Override、函数式接口约束 @FunctionalInterface. 注解格式定义格式：自定义注解用 @interface 关键字，注解默认可以标记很多地方 123修饰符 @interface 注解名&#123; // 注解属性&#125; 使用注解的格式：@注解名 12345678910@Book@MyTestpublic class MyBook &#123; //方法变量都可以注解&#125;@interface Book&#123;&#125;@interface MyTest&#123;&#125; 注解属性普通属性注解可以有属性，**属性名必须带 ()**，在用注解的时候，属性必须赋值，除非属性有默认值 属性的格式： 格式 1：数据类型 属性名() 格式 2：数据类型 属性名() default 默认值 属性适用的数据类型: 八种数据数据类型（int，short，long，double，byte，char，boolean，float）和 String、Class 以上类型的数组形式都支持 123456789101112131415@MyBook(name=&quot;《精通Java基础》&quot;,authors = &#123;&quot;播仔&quot;,&quot;Dlei&quot;,&quot;播妞&quot;&#125; , price = 99.9 )public class AnnotationDemo01 &#123; @MyBook(name=&quot;《精通MySQL数据库入门到删库跑路》&quot;,authors = &#123;&quot;小白&quot;,&quot;小黑&quot;&#125; , price = 19.9 , address = &quot;北京&quot;) public static void main(String[] args) &#123; &#125;&#125;// 自定义一个注解@interface MyBook&#123; String name(); String[] authors(); // 数组 double price(); String address() default &quot;武汉&quot;;&#125; 特殊属性注解的特殊属性名称：value 如果只有一个 value 属性的情况下，使用 value 属性的时候可以省略 value 名称不写 如果有多个属性，且多个属性没有默认值，那么 value 是不能省略的 123456789//@Book(&quot;/deleteBook.action&quot;)@Book(value = &quot;/deleteBook.action&quot; , age = 12)public class AnnotationDemo01&#123;&#125;@interface Book&#123; String value(); int age() default 10;&#125; 元注解元注解是 sun 公司提供的，用来注解自定义注解 元注解有四个： @Target：约束自定义注解可以标记的范围，默认值为任何元素，表示该注解用于什么地方，可用值定义在 ElementType 类中： ElementType.CONSTRUCTOR：用于描述构造器 ElementType.FIELD：成员变量、对象、属性（包括 enum 实例） ElementType.LOCAL_VARIABLE：用于描述局部变量 ElementType.METHOD：用于描述方法 ElementType.PACKAGE：用于描述包 ElementType.PARAMETER：用于描述参数 ElementType.TYPE：用于描述类、接口（包括注解类型）或 enum 声明 @Retention：定义该注解的生命周期，申明注解的作用范围：编译时，运行时，可使用的值定义在 RetentionPolicy 枚举类中： RetentionPolicy.SOURCE：在编译阶段丢弃，这些注解在编译结束之后就不再有任何意义，只作用在源码阶段，生成的字节码文件中不存在，@Override、@SuppressWarnings 都属于这类注解 RetentionPolicy.CLASS：在类加载时丢弃，在字节码文件的处理中有用，运行阶段不存在，默认值 RetentionPolicy.RUNTIME : 始终不会丢弃，运行期也保留该注解，因此可以使用反射机制读取该注解的信息，自定义的注解通常使用这种方式 @Inherited：表示修饰的自定义注解可以被子类继承 @Documented：表示是否将自定义的注解信息添加在 Java 文档中 123456789101112public class AnnotationDemo01&#123; // @MyTest // 只能注解方法 private String name; @MyTest public static void main( String[] args) &#123; &#125;&#125;@Target(ElementType.METHOD) // 申明只能注解方法@Retention(RetentionPolicy.RUNTIME) // 申明注解从写代码一直到运行还在，永远存活！！@interface MyTest&#123;&#125; 注解解析开发中经常要知道一个类的成分上面到底有哪些注解，注解有哪些属性数据，这都需要进行注解的解析 注解解析相关的接口： Annotation：注解类型，该类是所有注解的父类，注解都是一个 Annotation 的对象 AnnotatedElement：该接口定义了与注解解析相关的方法 Class、Method、Field、Constructor 类成分：实现 AnnotatedElement 接口，拥有解析注解的能力 Class 类 API ： Annotation[] getDeclaredAnnotations()：获得当前对象上使用的所有注解，返回注解数组 T getDeclaredAnnotation(Class&lt;T&gt; annotationClass)：根据注解类型获得对应注解对象 T getAnnotation(Class&lt;T&gt; annotationClass)：根据注解类型获得对应注解对象 boolean isAnnotationPresent(Class&lt;Annotation&gt; class)：判断对象是否使用了指定的注解 boolean isAnnotation()：此 Class 对象是否表示注释类型 注解原理：注解本质是特殊接口，继承了 Annotation ，其具体实现类是 Java 运行时生成的动态代理类，通过反射获取注解时，返回的是运行时生成的动态代理对象 $Proxy1，通过代理对象调用自定义注解（接口）的方法，回调 AnnotationInvocationHandler 的 invoke 方法，该方法会从 memberValues 这个 Map 中找出对应的值，而 memberValues 的来源是 Java 常量池 解析注解数据的原理：注解在哪个成分上，就先拿哪个成分对象，比如注解作用在类上，则要该类的 Class 对象，再来拿上面的注解 1234567891011121314151617181920212223242526272829303132333435363738public class AnnotationDemo&#123; @Test public void parseClass() &#123; // 1.定位Class类对象 Class c = BookStore.class; // 2.判断这个类上是否使用了某个注解 if(c.isAnnotationPresent(Book.class))&#123; // 3.获取这个注解对象 Book b = (Book)c.getDeclarAnnotation(Book.class); System.out.println(book.value()); System.out.println(book.price()); System.out.println(Arrays.toString(book.authors())); &#125; &#125; @Test public void parseMethod() throws Exception &#123; Class c = BookStore.class; Method run = c.getDeclaredMethod(&quot;run&quot;); if(run.isAnnotationPresent(Book.class))&#123; Book b = (Book)run.getDeclaredAnnotation(Book.class); sout(上面的三个); &#125; &#125;&#125;@Book(value = &quot;《Java基础到精通》&quot;, price = 99.5, authors = &#123;&quot;张三&quot;,&quot;李四&quot;&#125;)class BookStore&#123; @Book(value = &quot;《Mybatis持久层框架》&quot;, price = 199.5, authors = &#123;&quot;王五&quot;,&quot;小六&quot;&#125;) public void run()&#123; &#125;&#125;@Target(&#123;ElementType.TYPE,ElementType.METHOD&#125;) // 类和成员方法上使用@Retention(RetentionPolicy.RUNTIME) // 注解永久存活@interface Book&#123; String value(); double price() default 100; String[] authors();&#125; XML概述XML介绍： XML 指可扩展标记语言（EXtensible Markup Language） XML 是一种标记语言，很类似 HTML，HTML文件也是XML文档 XML 的设计宗旨是传输数据，而非显示数据 XML 标签没有被预定义，需要自行定义标签 XML 被设计为具有自我描述性，易于阅读 XML 是 W3C 的推荐标准 XML 与 HTML 的区别： XML 不是 HTML 的替代，XML 和 HTML 为不同的目的而设计 XML 被设计为传输和存储数据，其焦点是数据的内容；XMl标签可自定义，便于阅读 HTML 被设计用来显示数据，其焦点是数据的外观；HTML标签被预设好，便于浏览器识别 HTML 旨在显示信息，而 XML 旨在传输信息 创建person.xml 123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;person id=&quot;110&quot;&gt;\t&lt;age&gt;18&lt;/age&gt; &lt;!--年龄--&gt;\t&lt;name&gt;张三&lt;/name&gt; &lt;!--姓名--&gt;\t&lt;sex/&gt; &lt;!--性别--&gt;&lt;/person&gt; 组成XML 文件中常见的组成元素有:文档声明、元素、属性、注释、转义字符、字符区。文件后缀名为 xml 文档声明&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; standalone=&quot;yes&quot; ?&gt;，文档声明必须在第一行，以 &lt;?xml 开头，以 ?&gt; 结束， version：指定 XML 文档版本。必须属性，这里一般选择 1.0 enconding：指定当前文档的编码，可选属性，默认值是 utf-8 standalone：该属性不是必须的，描述 XML 文件是否依赖其他的 xml 文件，取值为 yes&#x2F;no 元素 格式 1：&lt;person&gt;&lt;/person&gt; 格式 2：&lt;person/&gt; 普通元素的结构由开始标签、元素体、结束标签组成 标签由一对尖括号和合法标识符组成，标签必须成对出现。特殊的标签可以不成对，必须有结束标记 &lt;&#x2F;&gt; 元素体：可以是元素，也可以是文本，例如：&lt;person&gt;&lt;name&gt;张三&lt;/name&gt;&lt;/person&gt; 空元素：空元素只有标签，而没有结束标签，但元素必须自己闭合，例如：&lt;sex/&gt; 元素命名：区分大小写、不能使用空格冒号、不建议用 XML、xml、Xml 等开头 必须存在一个根标签，有且只能有一个 属性：&lt;name id=&quot;1&quot; desc=&quot;高富帅&quot;&gt; 属性是元素的一部分，它必须出现在元素的开始标签中 属性的定义格式：属性名=“属性值”，其中属性值必须使用单引或双引号括起来 一个元素可以有 0~N 个属性，但一个元素中不能出现同名属性 属性名不能使用空格 , 不要使用冒号等特殊字符，且必须以字母开头 注释：XML的注释与HTML相同，既以 &lt;!-- 开始，--&gt; 结束。 转义字符XML 中的转义字符与 HTML 一样。因为很多符号已经被文档结构所使用，所以在元素体或属性值中想使用这些符号就必须使用转义字符（也叫实体字符），例如：”&gt;”、”&lt;”、”‘“、”””、”&amp;”XML 中仅有字符 &lt; 和 &amp; 是非法的。省略号、引号和大于号是合法的，把它们替换为实体引用 字符 预定义的转义字符 说明 &lt; &amp;lt; 小于 &gt; &amp;gt; 大于 “ &amp;quot; 双引号 ‘ &amp;apos; 单引号 &amp; &amp;amp; 和号 字符区 123&lt;![CDATA[\t文本数据]]&gt; CDATA 指的是不应由 XML 解析器进行解析的文本数据（Unparsed Character Data） CDATA 部分由 ““ 结束； 大量的转义字符在xml文档中时，会使XML文档的可读性大幅度降低。这时使用CDATA段就会好一些 规则： CDATA 部分不能包含字符串 ]]&gt;，也不允许嵌套的 CDATA 部分 标记 CDATA 部分结尾的 ]]&gt; 不能包含空格或折行 123456789101112131415161718192021222324252627&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;?xml-stylesheet type=&quot;text/css&quot; href=&quot;../css/xml.css&quot; ?&gt;&lt;!-- 7.处理指令：导入外部的css样式控制xml的界面效果，没有啥用，xml不是为了展示好看的！--&gt;&lt;!-- 1.申明 抬头 必须在第一行--&gt;&lt;!-- 2.注释，本处就是注释，必须用前后尖括号围起来 --&gt;&lt;!-- 3.标签（元素），注意一个XML文件只能有一个根标签--&gt;&lt;student&gt; &lt;!-- 4.属性信息：id , desc--&gt; &lt;name id=&quot;1&quot; desc=&quot;高富帅&quot;&gt;西门庆&lt;/name&gt; &lt;age&gt;32&lt;/age&gt; &lt;!-- 5.实体字符：在xml文件中，我们不能直接写小于号，等一些特殊字符 会与xml文件本身的内容冲突报错，此时必须用转义的实体字符。 --&gt; &lt;sql&gt; &lt;!-- select * from student where age &lt; 18 &amp;&amp; age &gt; 10; --&gt; select * from student where age &amp;lt; 18 &amp;amp;&amp;amp; age &amp;gt; 10; &lt;/sql&gt; &lt;!-- 6.字符数据区：在xml文件中，我们不能直接写小于号，等一些特殊字符 会与xml文件本身的内容冲突报错，此时必须用转义的实体字符 或者也可以选择使用字符数据区，里面的内容可以随便了！ --&gt; &lt;sql2&gt; &lt;![CDATA[ select * from student where age &lt; 18 &amp;&amp; age &gt; 10; ]]&gt; &lt;/sql2&gt;&lt;/student&gt; 约束DTDDTD 是文档类型定义（Document Type Definition）。DTD 可以定义在 XML 文档中出现的元素、这些元素出现的次序、它们如何相互嵌套以及 XML 文档结构的其它详细信息。 DTD 规则： 约束元素的嵌套层级 1&lt;!ELEMENT 父标签 （子标签1，子标签2，…）&gt; 约束元素体里面的数据 语法 1&lt;!ELEMENT 标签名字 标签类型&gt; 判断元素 简单元素：没有子元素。 复杂元素：有子元素的元素； 标签类型 标签类型 代码写法 说明 PCDATA (#PCDATA) 被解释的字符串数据 EMPTY EMPTY 即空元素，例如&lt;hr&#x2F;&gt; ANY ANY 即任意类型 代码 1234&lt;!ELEMENT persons (person+)&gt; &lt;!--约束人们至少一个人--&gt; &lt;!ELEMENT person (name,age)&gt;\t&lt;!--约束元素人的子元素必须为姓名、年龄，并且按顺序--&gt; &lt;!ELEMENT name (#PCDATA)&gt; &lt;!--&quot;姓名&quot;元素体为字符串数据--&gt; &lt;!ELEMENT age ANY&gt; &lt;!--&quot;年龄&quot;元素体为任意类型--&gt; 数量词 数量词符号 含义 空 表示元素出现一次 * 表示元素可以出现0到多个 + 表示元素可以出现至少1个 ? 表示元素可以是0或1个 , 表示元素需要按照顺序显示 | 表示元素需要选择其中的某一个 属性声明 语法 12345&lt;!ATTLIST 标签名称 属性名称1 属性类型1 属性说明1 属性名称2 属性类型2 属性说明2 …&gt; 属性类型 属性类型 含义 CDATA 代表属性是文本字符串， eg: ID 代码该属性值唯一，不能以数字开头， eg: ENUMERATED 代表属性值在指定范围内进行枚举 Eg:&lt;!ATTLIST属性名 (社科类|工程类|教育类) “社科类”&gt; “社科类”是默认值，属性如果不设置默认值就是”社科类” 属性说明 属性说明 含义 #REQUIRED 代表属性是必须有的 #IMPLIED 代表属性可有可无 #FIXED 代表属性为固定值，实现方式：book_info CDATA #FIXED “固定值” 代码 1234567&lt;!ATTLIST 书 &lt;!--设置&quot;书&quot;元素的的属性列表--&gt;\tid ID #REQUIRED &lt;!--&quot;id&quot;属性值为必须有--&gt;\t编号 CDATA #IMPLIED &lt;!--&quot;编号&quot;属性可有可无--&gt;\t出版社 (清华|北大) &quot;清华&quot; &lt;!--&quot;出版社&quot;属性值是枚举值，默认为“123”--&gt;\ttype CDATA #FIXED &quot;IT&quot; &lt;!--&quot;type&quot;属性为文本字符串并且固定值为&quot;IT&quot;--&gt;&gt;&lt;!ATTLIST person id CDATA #REQUIRED&gt; &lt;!--id是文本字符串必须有--&gt; SchemaXSD 定义： Schema 语言也可作为 XSD（XML Schema Definition） Schema 约束文件本身也是一个 XML 文件，符合 XML 的语法，这个文件的后缀名 .xsd 一个 XML 中可以引用多个 Schema 约束文件，多个 Schema 使用名称空间区分（名称空间类似于 Java 包名） dtd 里面元素类型的取值比较单一常见的是 PCDATA 类型，但是在 Schema 里面可以支持很多个数据类型 Schema 文件约束 XML 文件的同时也被别的文件约束着 XSD 规则： 创建一个文件，这个文件的后缀名为 .xsd 定义文档声明 schema 文件的根标签为： 在 中定义属性： xmlns&#x3D;http://www.w3.org/2001/XMLSchema 代表当前文件时约束别人的，同时这个文件也对该 Schema 进行约束 在中定义属性 ： targetNamespace &#x3D; 唯一的 url 地址，指定当前这个 schema 文件的名称空间。 名称空间：当其他 xml 使用该 schema 文件，需要引入此空间 在中定义属性 ： elementFormDefault&#x3D;”qualified“，表示当前 schema 文件是一个质量良好的文件。 通过 element 定义元素 判断当前元素是简单元素还是复杂元素 person.xsd 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;schema xmlns=&quot;http://www.w3.org/2001/XMLSchema&quot; &lt;!--本文件是约束别人的，也被约束--&gt; targetNamespace=&quot;http://www.seazean.cn/javase&quot;&lt;!--自己的名称空间--&gt; elementFormDefault=&quot;qualified&quot; &lt;!--本文件是质量好的--&gt;&gt; &lt;element name=&quot;persons&quot;&gt; &lt;!--定义persons复杂元素--&gt; &lt;complexType&gt; &lt;!--复杂的元素--&gt; &lt;sequence&gt; &lt;!--里面的元素必须按照顺序定义--&gt; &lt;element name = &quot;person&quot;&gt; &lt;!--定义person复杂元素--&gt; &lt;complexType&gt; &lt;sequence&gt; &lt;!--定义name和age简单元素--&gt; &lt;element name = &quot;name&quot; type = &quot;string&quot;&gt;&lt;/element&gt; &lt;element name = &quot;age&quot; type = &quot;string&quot;&gt;&lt;/element&gt; &lt;/sequence&gt; &lt;/complexType&gt; &lt;/element&gt; &lt;/sequence&gt; &lt;/complexType&gt; &lt;/element&gt; &lt;/schema&gt; Dom4J解析XML 解析就是从 XML 中获取到数据，DOM 是解析思想 DOM（Document Object Model）：文档对象模型，把文档的各个组成部分看做成对应的对象，把 XML 文件全部加载到内存，在内存中形成一个树形结构，再获取对应的值 Dom4J 实现： Dom4J 解析器构造方法：SAXReader saxReader = new SAXReader() SAXReader 常用 API： public Document read(File file)：Reads a Document from the given File public Document read(InputStream in)：Reads a Document from the given stream using SAX Java Class 类 API： public InputStream getResourceAsStream(String path)：加载文件成为一个字节输入流返回 根元素Document 方法：Element getRootElement() 获取根元素 12345678910111213141516171819// 需求：解析books.xml文件成为一个Document文档树对象，得到根元素对象。public class Dom4JDemo &#123; public static void main(String[] args) throws Exception &#123; // 1.创建一个dom4j的解析器对象：代表整个dom4j框架。 SAXReader saxReader = new SAXReader(); // 2.第一种方式（简单）：通过解析器对象去加载xml文件数据，成为一个Document文档树对象。 //Document document = saxReader.read(new File(&quot;Day13Demo/src/books.xml&quot;)); // 3.第二种方式（代码多点）先把xml文件读成一个字节输入流 // 这里的“/”是直接去src类路径下寻找文件。 InputStream is = Dom4JDemo01.class.getResourceAsStream(&quot;/books.xml&quot;); Document document = saxReader.read(is); System.out.println(document); //org.dom4j.tree.DefaultDocument@27a5f880 [Document: name null] // 4.从document文档树对象中提取根元素对象 Element root = document.getRootElement(); System.out.println(root.getName());//books &#125;&#125; 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;books&gt; &lt;book id=&quot;0001&quot; desc=&quot;第一本书&quot;&gt; &lt;name&gt; JavaWeb开发教程&lt;/name&gt; &lt;author&gt; 张三&lt;/author&gt; &lt;sale&gt;100.00元 &lt;/sale&gt; &lt;/book&gt; &lt;book id=&quot;0002&quot;&gt; &lt;name&gt;三国演义&lt;/name&gt; &lt;author&gt;罗贯中&lt;/author&gt; &lt;sale&gt;100.00元&lt;/sale&gt; &lt;/book&gt; &lt;user&gt; &lt;/user&gt;&lt;/books&gt; 子元素Element 元素的 API: String getName()：取元素的名称。 List elements()：获取当前元素下的全部子元素（一级） List elements(String name)：获取当前元素下的指定名称的全部子元素（一级） Element element(String name)：获取当前元素下的指定名称的某个子元素，默认取第一个（一级） 12345678910111213141516171819202122232425262728public class Dom4JDemo &#123; public static void main(String[] args) throws Exception &#123; SAXReader saxReader = new SAXReader(); Document document = saxReader.read(new File(&quot;Day13Demo/src/books.xml&quot;)); // 3.获取根元素对象 Element root = document.getRootElement(); System.out.println(root.getName()); // 4.获取根元素下的全部子元素 List&lt;Element&gt; sonElements = root.elements(); for (Element sonElement : sonElements) &#123; System.out.println(sonElement.getName()); &#125; // 5.获取根源下的全部book子元素 List&lt;Element&gt; sonElements1 = root.elements(&quot;book&quot;); for (Element sonElement : sonElements1) &#123; System.out.println(sonElement.getName()); &#125; // 6.获取根源下的指定的某个元素 Element son = root.element(&quot;user&quot;); System.out.println(son.getName()); // 默认会提取第一个名称一样的子元素对象返回！ Element son1 = root.element(&quot;book&quot;); System.out.println(son1.attributeValue(&quot;id&quot;)); &#125;&#125; 属性Element 元素的 API： List attributes()：获取元素的全部属性对象 Attribute attribute(String name)：根据名称获取某个元素的属性对象 String attributeValue(String var)：直接获取某个元素的某个属性名称的值 Attribute 对象的 API： String getName()：获取属性名称 String getValue()：获取属性值 1234567891011121314151617181920212223public class Dom4JDemo &#123; public static void main(String[] args) throws Exception &#123; SAXReader saxReader = new SAXReader(); Document document = saxReader.read(new File(&quot;Day13Demo/src/books.xml&quot;)); Element root = document.getRootElement(); // 4.获取book子元素 Element bookEle = root.element(&quot;book&quot;); // 5.获取book元素的全部属性对象 List&lt;Attribute&gt; attributes = bookEle.attributes(); for (Attribute attribute : attributes) &#123; System.out.println(attribute.getName()+&quot;-&gt;&quot;+attribute.getValue()); &#125; // 6.获取Book元素的某个属性对象 Attribute descAttr = bookEle.attribute(&quot;desc&quot;); System.out.println(descAttr.getName()+&quot;-&gt;&quot;+descAttr.getValue()); // 7.可以直接获取元素的属性值 System.out.println(bookEle.attributeValue(&quot;id&quot;)); System.out.println(bookEle.attributeValue(&quot;desc&quot;)); &#125;&#125; 文本Element： String elementText(String name)：可以直接获取当前元素的子元素的文本内容 String elementTextTrim(String name)：去前后空格,直接获取当前元素的子元素的文本内容 String getText()：直接获取当前元素的文本内容 String getTextTrim()：去前后空格,直接获取当前元素的文本内容 1234567891011121314151617181920public class Dom4JDemo &#123; public static void main(String[] args) throws Exception &#123; SAXReader saxReader = new SAXReader(); Document document = saxReader.read(new File(&quot;Day13Demo/src/books.xml&quot;)); Element root = document.getRootElement(); // 4.得到第一个子元素book Element bookEle = root.element(&quot;book&quot;); // 5.直接拿到当前book元素下的子元素文本值 System.out.println(bookEle.elementText(&quot;name&quot;)); System.out.println(bookEle.elementTextTrim(&quot;name&quot;)); // 去前后空格 System.out.println(bookEle.elementText(&quot;author&quot;)); System.out.println(bookEle.elementTextTrim(&quot;author&quot;)); // 去前后空格 // 6.先获取到子元素对象，再获取该文本值 Element bookNameEle = bookEle.element(&quot;name&quot;); System.out.println(bookNameEle.getText()); System.out.println(bookNameEle.getTextTrim());// 去前后空格 &#125;&#125; XPathDom4J 可以用于解析整个 XML 的数据，但是如果要检索 XML 中的某些信息，建议使用 XPath XPath 常用API： List selectNodes(String var1) : 检索出一批节点集合 Node selectSingleNode(String var1) : 检索出一个节点返回 XPath 提供的四种检索数据的写法： 绝对路径：&#x2F;根元素&#x2F;子元素&#x2F;子元素 相对路径：.&#x2F;子元素&#x2F;子元素 (.代表了当前元素) 全文搜索： &#x2F;&#x2F;元素：在全文找这个元素 &#x2F;&#x2F;元素1&#x2F;元素2：在全文找元素1下面的一级元素 2 &#x2F;&#x2F;元素1&#x2F;&#x2F;元素2：在全文找元素1下面的全部元素 2 属性查找： &#x2F;&#x2F;@属性名称：在全文检索属性对象 &#x2F;&#x2F;元素[@属性名称]：在全文检索包含该属性的元素对象 &#x2F;&#x2F;元素[@属性名称&#x3D;值]：在全文检索包含该属性的元素且属性值为该值的元素对象 12345678910111213141516171819202122232425262728293031public class XPathDemo &#123; public static void main(String[] args) throws Exception &#123; SAXReader saxReader = new SAXReader(); InputStream is = XPathDemo.class.getResourceAsStream(&quot;/Contact.xml&quot;); Document document = saxReader.read(is); //1.使用绝对路径定位全部的name名称 List&lt;Node&gt; nameNodes1 = document.selectNodes(&quot;/contactList/contact/name&quot;); for (Node nameNode : nameNodes) &#123; System.out.println(nameNode.getText()); &#125; //2.相对路径。从根元素开始检索，.代表很根元素 List&lt;Node&gt; nameNodes2 = root.selectNodes(&quot;./contact/name&quot;); //3.1 在全文中检索name节点 List&lt;Node&gt; nameNodes3 = root.selectNodes(&quot;//name&quot;);//全部的 //3.2 在全文中检索所有contact下的所有name节点 //包括sql，不外面的 List&lt;Node&gt; nameNodes3 = root.selectNodes(&quot;//contact//name&quot;); //3.3 在全文中检索所有contact下的直接name节点 List&lt;Node&gt; nameNodes3 = root.selectNodes(&quot;//contact/name&quot;);//不包括sql和外面 //4.1 检索全部属性对象 List&lt;Node&gt; attributes1 = root.selectNodes(&quot;//@id&quot;);//包括sql4 //4.2 在全文检索包含该属性的元素对象 List&lt;Node&gt; attributes1 = root.selectNodes(&quot;//contact[@id]&quot;); //4.3 在全文检索包含该属性的元素且属性值为该值的元素对象 Node nodeEle = document.selectSingleNode(&quot;//contact[@id=2]&quot;); Element ele = (Element)nodeEle; System.out.println(ele.elementTextTrim(&quot;name&quot;));//xi &#125;&#125; 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;contactList&gt;&lt;contact id=&quot;1&quot;&gt; &lt;name&gt;小白&lt;/name&gt; &lt;gender&gt;女&lt;/gender&gt; &lt;email&gt;bai@seazean.cn&lt;/email&gt;&lt;/contact&gt;&lt;contact id=&quot;2&quot;&gt; &lt;name&gt;小黑&lt;/name&gt; &lt;gender&gt;男&lt;/gender&gt; &lt;email&gt;hei@seazean.cn&lt;/email&gt; &lt;sql id=&quot;sql4&quot;&gt; &lt;name&gt;sql语句&lt;/name&gt; &lt;/sql&gt;&lt;/contact&gt;&lt;contact id=&quot;3&quot;&gt; &lt;name&gt;小虎&lt;/name&gt; &lt;gender&gt;男&lt;/gender&gt; &lt;email&gt;hu@seazean.cn&lt;/email&gt;&lt;/contact&gt;&lt;contact&gt;&lt;/contact&gt;&lt;name&gt;外面的名称&lt;/name&gt;&lt;/contactList&gt; SDP单例模式基本介绍单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一，提供了一种创建对象的最佳方式 单例设计模式分类两种： 饿汉式：类加载就会导致该单实例对象被创建 懒汉式：类加载不会导致该单实例对象被创建，而是首次使用该对象时才会创建 饿汉式饿汉式在类加载的过程导致该单实例对象被创建，虚拟机会保证类加载的线程安全，但是如果只是为了加载该类不需要实例，则会造成内存的浪费 静态变量的方式： 123456789101112131415public final class Singleton &#123; // 私有构造方法 private Singleton() &#123;&#125; // 在成员位置创建该类的对象 private static final Singleton instance = new Singleton(); // 对外提供静态方法获取该对象 public static Singleton getInstance() &#123; return instance; &#125; // 解决序列化问题 protected Object readResolve() &#123; return INSTANCE; &#125;&#125; 加 final 修饰，所以不会被子类继承，防止子类中不适当的行为覆盖父类的方法，破坏了单例 防止反序列化破坏单例的方式： 对单例声明 transient，然后实现 readObject(ObjectInputStream in) 方法，复用原来的单例 条件：访问权限为 private&#x2F;protected、返回值必须是 Object、异常可以不抛 实现 readResolve() 方法，当 JVM 从内存中反序列化地组装一个新对象，就会自动调用 readResolve 方法返回原来单例 构造方法设置为私有，防止其他类无限创建对象，但是不能防止反射破坏 静态变量初始化在类加载时完成，由 JVM 保证线程安全，能保证单例对象创建时的安全 提供静态方法而不是直接将 INSTANCE 设置为 public，体现了更好的封装性、提供泛型支持、可以改进成懒汉单例设计 静态代码块的方式： 123456789101112131415public class Singleton &#123; // 私有构造方法 private Singleton() &#123;&#125; // 在成员位置创建该类的对象 private static Singleton instance; static &#123; instance = new Singleton(); &#125; // 对外提供静态方法获取该对象 public static Singleton getInstance() &#123; return instance; &#125;&#125; 枚举方式：枚举类型是所用单例实现中唯一一种不会被破坏的单例实现模式 123456789public enum Singleton &#123; INSTANCE; public void doSomething() &#123; System.out.println(&quot;doSomething&quot;); &#125;&#125;public static void main(String[] args) &#123; Singleton.INSTANCE.doSomething();&#125; 问题1：枚举单例是如何限制实例个数的？每个枚举项都是一个实例，是一个静态成员变量 问题2：枚举单例在创建时是否有并发问题？否 问题3：枚举单例能否被反射破坏单例？否，反射创建对象时判断是枚举类型就直接抛出异常 问题4：枚举单例能否被反序列化破坏单例？否 问题5：枚举单例属于懒汉式还是饿汉式？饿汉式 问题6：枚举单例如果希望加入一些单例创建时的初始化逻辑该如何做？添加构造方法 反编译结果： 123public final class Singleton extends java.lang.Enum&lt;Singleton&gt; &#123; // Enum实现序列化接口\tpublic static final Singleton INSTANCE = new Singleton();&#125; 懒汉式 线程不安全 12345678910111213141516public class Singleton &#123; // 私有构造方法 private Singleton() &#123;&#125; // 在成员位置创建该类的对象 private static Singleton instance; // 对外提供静态方法获取该对象 public static Singleton getInstance() &#123; if(instance == null) &#123; // 多线程环境，会出现线程安全问题，可能多个线程同时进入这里 instance = new Singleton(); &#125; return instance; &#125;&#125; 双端检锁机制 在多线程的情况下，可能会出现空指针问题，出现问题的原因是 JVM 在实例化对象的时候会进行优化和指令重排序操作，所以需要使用 volatile 关键字 12345678910111213141516171819public class Singleton &#123; // 私有构造方法 private Singleton() &#123;&#125; private static volatile Singleton instance; // 对外提供静态方法获取该对象 public static Singleton getInstance() &#123; // 第一次判断，如果instance不为null，不进入抢锁阶段，直接返回实例 if(instance == null) &#123; synchronized (Singleton.class) &#123; // 抢到锁之后再次判断是否为null if(instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 静态内部类方式 12345678910111213public class Singleton &#123; // 私有构造方法 private Singleton() &#123;&#125; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; // 对外提供静态方法获取该对象 public static Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 内部类属于懒汉式，类加载本身就是懒惰的，首次调用时加载，然后对单例进行初始化 类加载的时候方法不会被调用，所以不会触发 getInstance 方法调用 invokestatic 指令对内部类进行加载；加载的时候字节码常量池会被加入类的运行时常量池，解析工作是将常量池中的符号引用解析成直接引用，但是解析过程不一定非得在类加载时完成，可以延迟到运行时进行，所以静态内部类实现单例会延迟加载 没有线程安全问题，静态变量初始化在类加载时完成，由 JVM 保证线程安全 破坏单例反序列化将单例对象序列化再反序列化，对象从内存反序列化到程序中会重新创建一个对象，通过反序列化得到的对象是不同的对象，而且得到的对象不是通过构造器得到的，反序列化得到的对象不执行构造器 Singleton 123456789101112public class Singleton implements Serializable &#123;\t//实现序列化接口 // 私有构造方法 private Singleton() &#123;&#125; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; // 对外提供静态方法获取该对象 public static Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 序列化 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) throws Exception &#123; //往文件中写对象 //writeObject2File(); //从文件中读取对象 Singleton s1 = readObjectFromFile(); Singleton s2 = readObjectFromFile(); //判断两个反序列化后的对象是否是同一个对象 System.out.println(s1 == s2); &#125; private static Singleton readObjectFromFile() throws Exception &#123; //创建对象输入流对象 ObjectInputStream ois = new ObjectInputStream(new FileInputStream(&quot;C://a.txt&quot;)); //第一个读取Singleton对象 Singleton instance = (Singleton) ois.readObject(); return instance; &#125; public static void writeObject2File() throws Exception &#123; //获取Singleton类的对象 Singleton instance = Singleton.getInstance(); //创建对象输出流 ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(&quot;C://a.txt&quot;)); //将instance对象写出到文件中 oos.writeObject(instance); &#125;&#125; 解决方法： 在 Singleton 类中添加 readResolve() 方法，在反序列化时被反射调用，如果定义了这个方法，就返回这个方法的值，如果没有定义，则返回新创建的对象 123private Object readResolve() &#123; return SingletonHolder.INSTANCE;&#125; ObjectInputStream 类源码分析： 123456789101112131415161718192021222324public final Object readObject() throws IOException, ClassNotFoundException&#123; //...\tObject obj = readObject0(false);//重点查看readObject0方法&#125;private Object readObject0(boolean unshared) throws IOException &#123; try &#123; switch (tc) &#123; case TC_OBJECT: return checkResolve(readOrdinaryObject(unshared)); &#125; &#125; &#125;private Object readOrdinaryObject(boolean unshared) throws IOException &#123;\t// isInstantiable 返回true，执行 desc.newInstance()，通过反射创建新的单例类 obj = desc.isInstantiable() ? desc.newInstance() : null; // 添加 readResolve 方法后 desc.hasReadResolveMethod() 方法执行结果为true if (obj != null &amp;&amp; handles.lookupException(passHandle) == null &amp;&amp; desc.hasReadResolveMethod()) &#123; // 通过反射调用 Singleton 类中的 readResolve 方法，将返回值赋值给rep变量 // 多次调用ObjectInputStream类中的readObject方法，本质调用定义的readResolve方法，返回的是同一个对象。 Object rep = desc.invokeReadResolve(obj); &#125; return obj;&#125; 反射破解 反射 123456789101112131415161718public class Test &#123; public static void main(String[] args) throws Exception &#123; //获取Singleton类的字节码对象 Class clazz = Singleton.class; //获取Singleton类的私有无参构造方法对象 Constructor constructor = clazz.getDeclaredConstructor(); //取消访问检查 constructor.setAccessible(true); //创建Singleton类的对象s1 Singleton s1 = (Singleton) constructor.newInstance(); //创建Singleton类的对象s2 Singleton s2 = (Singleton) constructor.newInstance(); //判断通过反射创建的两个Singleton对象是否是同一个对象 System.out.println(s1 == s2);\t//false &#125;&#125; 反射方式破解单例的解决方法： 12345678910111213141516171819202122232425public class Singleton &#123; private static volatile Singleton instance; // 私有构造方法 private Singleton() &#123; // 反射破解单例模式需要添加的代码 if(instance != null) &#123; throw new RuntimeException(); &#125; &#125; // 对外提供静态方法获取该对象 public static Singleton getInstance() &#123; if(instance != null) &#123; return instance; &#125; synchronized (Singleton.class) &#123; if(instance != null) &#123; return instance; &#125; instance = new Singleton(); return instance; &#125; &#125;&#125; RuntimeRuntime 类就是使用的单例设计模式中的饿汉式 12345678public class Runtime &#123; private static Runtime currentRuntime = new Runtime(); public static Runtime getRuntime() &#123; return currentRuntime; &#125; private Runtime() &#123;&#125; ...&#125; 使用 Runtime 12345678910111213141516171819public class RuntimeDemo &#123; public static void main(String[] args) throws IOException &#123; //获取Runtime类对象 Runtime runtime = Runtime.getRuntime(); //返回 Java 虚拟机中的内存总量。 System.out.println(runtime.totalMemory()); //返回 Java 虚拟机试图使用的最大内存量。 System.out.println(runtime.maxMemory()); //创建一个新的进程执行指定的字符串命令，返回进程对象 Process process = runtime.exec(&quot;ipconfig&quot;); //获取命令执行后的结果，通过输入流获取 InputStream inputStream = process.getInputStream(); byte[] arr = new byte[1024 * 1024* 100]; int b = inputStream.read(arr); System.out.println(new String(arr,0,b,&quot;gbk&quot;)); &#125;&#125; 代理模式静态代理代理模式：由于某些原因需要给某对象提供一个代理以控制对该对象的访问，访问对象不适合或者不能直接引用为目标对象，代理对象作为访问对象和目标对象之间的中介 Java 中的代理按照代理类生成时机不同又分为静态代理和动态代理，静态代理代理类在编译期就生成，而动态代理代理类则是在 Java 运行时动态生成，动态代理又有 JDK 代理和 CGLib 代理两种 代理（Proxy）模式分为三种角色： 抽象主题（Subject）类：通过接口或抽象类声明真实主题和代理对象实现的业务方法 真实主题（Real Subject）类： 实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象 代理（Proxy）类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，可以访问、控制或扩展真实主题的功能 买票案例，火车站是目标对象，代售点是代理对象 卖票接口： 123public interface SellTickets &#123; void sell();&#125; 火车站，具有卖票功能，需要实现SellTickets接口 12345public class TrainStation implements SellTickets &#123; public void sell() &#123; System.out.println(&quot;火车站卖票&quot;); &#125;&#125; 代售点： 12345678public class ProxyPoint implements SellTickets &#123; private TrainStation station = new TrainStation(); public void sell() &#123; System.out.println(&quot;代理点收取一些服务费用&quot;); station.sell(); &#125;&#125; 测试类： 123456public class Client &#123; public static void main(String[] args) &#123; ProxyPoint pp = new ProxyPoint(); pp.sell(); &#125;&#125; 测试类直接访问的是 ProxyPoint 类对象，也就是 ProxyPoint 作为访问对象和目标对象的中介 JDK使用方式Java 中提供了一个动态代理类 Proxy，Proxy 并不是代理对象的类，而是提供了一个创建代理对象的静态方法 newProxyInstance() 来获取代理对象 static Object newProxyInstance(ClassLoader loader,Class[] interfaces,InvocationHandler h) 参数一：类加载器，负责加载代理类。传入类加载器，代理和被代理对象要用一个类加载器才是父子关系，不同类加载器加载相同的类在 JVM 中都不是同一个类对象 参数二：被代理业务对象的全部实现的接口，代理对象与真实对象实现相同接口，知道为哪些方法做代理 参数三：代理真正的执行方法，也就是代理的处理逻辑 代码实现： 代理工厂：创建代理对象 12345678910111213141516171819public class ProxyFactory &#123; private TrainStation station = new TrainStation();\t//也可以在参数中提供 getProxyObject(TrainStation station) public SellTickets getProxyObject() &#123; //使用 Proxy 获取代理对象 SellTickets sellTickets = (SellTickets) Proxy.newProxyInstance( station.getClass().getClassLoader(), station.getClass().getInterfaces(), new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] args) &#123; System.out.println(&quot;代理点(JDK动态代理方式)&quot;); //执行真实对象 Object result = method.invoke(station, args); return result; &#125; &#125;); return sellTickets; &#125;&#125; 测试类： 123456789public class Client &#123; public static void main(String[] args) &#123; //获取代理对象 ProxyFactory factory = new ProxyFactory(); //必须时代理ji SellTickets proxyObject = factory.getProxyObject(); proxyObject.sell(); &#125;&#125; 实现原理JDK 动态代理方式的优缺点： 优点：可以为任意的接口实现类对象做代理，也可以为被代理对象的所有接口的所有方法做代理，动态代理可以在不改变方法源码的情况下，实现对方法功能的增强，提高了软件的可扩展性，Java 反射机制可以生成任意类型的动态代理类 缺点：只能针对接口或者接口的实现类对象做代理对象，普通类是不能做代理对象的 原因：生成的代理类继承了 Proxy，Java 是单继承的，所以 JDK 动态代理只能代理接口 ProxyFactory 不是代理模式中的代理类，而代理类是程序在运行过程中动态的在内存中生成的类，可以通过 Arthas 工具查看代理类结构： 代理类（$Proxy0）实现了 SellTickets 接口，真实类和代理类实现同样的接口 代理类（$Proxy0）将提供了的匿名内部类对象传递给了父类 代理类（$Proxy0）的修饰符是 public final 1234567891011121314151617181920212223242526// 程序运行过程中动态生成的代理类public final class $Proxy0 extends Proxy implements SellTickets &#123; private static Method m3; public $Proxy0(InvocationHandler invocationHandler) &#123; super(invocationHandler);//InvocationHandler对象传递给父类 &#125; static &#123; m3 = Class.forName(&quot;proxy.dynamic.jdk.SellTickets&quot;).getMethod(&quot;sell&quot;, new Class[0]); &#125; public final void sell() &#123; // 调用InvocationHandler的invoke方法 this.h.invoke(this, m3, null); &#125;&#125;// Java提供的动态代理相关类public class Proxy implements java.io.Serializable &#123;\tprotected InvocationHandler h; protected Proxy(InvocationHandler h) &#123; this.h = h; &#125;&#125; 执行流程如下： 在测试类中通过代理对象调用 sell() 方法 根据多态的特性，执行的是代理类（$Proxy0）中的 sell() 方法 代理类（$Proxy0）中的 sell() 方法中又调用了 InvocationHandler 接口的子实现类对象的 invoke 方法 invoke 方法通过反射执行了真实对象所属类（TrainStation）中的 sell() 方法 源码解析123456789101112131415161718192021222324252627282930313233343536373839public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h)&#123; // InvocationHandler 为空则抛出异常 Objects.requireNonNull(h); // 复制一份 interfaces final Class&lt;?&gt;[] intfs = interfaces.clone(); final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; // 从缓存中查找 class 类型的代理对象，会调用 ProxyClassFactory#apply 方法 Class&lt;?&gt; cl = getProxyClass0(loader, intfs);\t//proxyClassCache = new WeakCache&lt;&gt;(new KeyFactory(), new ProxyClassFactory()) try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; // 获取代理类的构造方法，根据参数 InvocationHandler 匹配获取某个构造器 final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; // 构造方法不是 pubic 的需要启用权限，暴力p if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; // 设置可访问的权限 cons.setAccessible(true); return null; &#125; &#125;); &#125; // cons 是构造方法，并且内部持有 InvocationHandler，在 InvocationHandler 中持有 target 目标对象 return cons.newInstance(new Object[]&#123;h&#125;); &#125; catch (IllegalAccessException|InstantiationException e) &#123;&#125;&#125; Proxy 的静态内部类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081private static final class ProxyClassFactory &#123; // 代理类型的名称前缀 private static final String proxyClassNamePrefix = &quot;$Proxy&quot;; // 生成唯一数字使用，结合上面的代理类型名称前缀一起生成 private static final AtomicLong nextUniqueNumber = new AtomicLong();\t//参数一：Proxy.newInstance 时传递的 //参数二：Proxy.newInstance 时传递的接口集合 @Override public Class&lt;?&gt; apply(ClassLoader loader, Class&lt;?&gt;[] interfaces) &#123; Map&lt;Class&lt;?&gt;, Boolean&gt; interfaceSet = new IdentityHashMap&lt;&gt;(interfaces.length); // 遍历接口集合 for (Class&lt;?&gt; intf : interfaces) &#123; Class&lt;?&gt; interfaceClass = null; try &#123; // 加载接口类到 JVM interfaceClass = Class.forName(intf.getName(), false, loader); &#125; catch (ClassNotFoundException e) &#123; &#125; if (interfaceClass != intf) &#123; throw new IllegalArgumentException( intf + &quot; is not visible from class loader&quot;); &#125; // 如果 interfaceClass 不是接口 直接报错，保证集合内都是接口 if (!interfaceClass.isInterface()) &#123; throw new IllegalArgumentException( interfaceClass.getName() + &quot; is not an interface&quot;); &#125; // 保证接口 interfaces 集合中没有重复的接口 if (interfaceSet.put(interfaceClass, Boolean.TRUE) != null) &#123; throw new IllegalArgumentException( &quot;repeated interface: &quot; + interfaceClass.getName()); &#125; &#125; // 生成的代理类的包名 String proxyPkg = null; // 【生成的代理类访问修饰符 public final】 int accessFlags = Modifier.PUBLIC | Modifier.FINAL; // 检查接口集合内的接口，看看有没有某个接口的访问修饰符不是 public 的 如果不是 public 的接口， // 生成的代理类 class 就必须和它在一个包下，否则访问出现问题 for (Class&lt;?&gt; intf : interfaces) &#123; // 获取访问修饰符 int flags = intf.getModifiers(); if (!Modifier.isPublic(flags)) &#123; accessFlags = Modifier.FINAL; // 获取当前接口的全限定名 包名.类名 String name = intf.getName(); int n = name.lastIndexOf(&#x27;.&#x27;); // 获取包名 String pkg = ((n == -1) ? &quot;&quot; : name.substring(0, n + 1)); if (proxyPkg == null) &#123; proxyPkg = pkg; &#125; else if (!pkg.equals(proxyPkg)) &#123; throw new IllegalArgumentException( &quot;non-public interfaces from different packages&quot;); &#125; &#125; &#125; if (proxyPkg == null) &#123; // if no non-public proxy interfaces, use com.sun.proxy package proxyPkg = ReflectUtil.PROXY_PACKAGE + &quot;.&quot;; &#125; // 获取唯一的编号 long num = nextUniqueNumber.getAndIncrement(); // 包名+ $proxy + 数字，比如 $proxy1 String proxyName = proxyPkg + proxyClassNamePrefix + num; // 【生成二进制字节码，这个字节码写入到文件内】，就是编译好的 class 文件 byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags); try &#123; // 【使用加载器加载二进制到 jvm】，并且返回 class return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); &#125; catch (ClassFormatError e) &#123; &#125; &#125;&#125; CGLIBCGLIB 是一个功能强大，高性能的代码生成包，为没有实现接口的类提供代理，为 JDK 动态代理提供了补充（$$Proxy） CGLIB 是第三方提供的包，所以需要引入 jar 包的坐标： 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt; 代理工厂类： 12345678910111213141516171819202122public class ProxyFactory implements MethodInterceptor &#123; private TrainStation target = new TrainStation(); public TrainStation getProxyObject() &#123; //创建Enhancer对象，类似于JDK动态代理的Proxy类，下一步就是设置几个参数 Enhancer enhancer = new Enhancer(); //设置父类的字节码对象 enhancer.setSuperclass(target.getClass()); //设置回调函数 enhancer.setCallback(new MethodInterceptor() &#123; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println(&quot;代理点收取一些服务费用(CGLIB动态代理方式)&quot;); Object o = methodProxy.invokeSuper(obj, args); return null;//因为返回值为void &#125; &#125;); //创建代理对象 TrainStation obj = (TrainStation) enhancer.create(); return obj; &#125;&#125; CGLIB 的优缺点 优点： CGLIB 动态代理不限定是否具有接口，可以对任意操作进行增强 CGLIB 动态代理无需要原始被代理对象，动态创建出新的代理对象 JDKProxy 仅对接口方法做增强，CGLIB 对所有方法做增强，包括 Object 类中的方法，toString、hashCode 等 缺点：CGLIB 不能对声明为 final 的类或者方法进行代理，因为 CGLIB 原理是动态生成被代理类的子类，继承被代理类 方式对比三种方式对比： 动态代理和静态代理： 动态代理将接口中声明的所有方法都被转移到一个集中的方法中处理（InvocationHandler.invoke），在接口方法数量比较多的时候，可以进行灵活处理，不需要像静态代理那样每一个方法进行中转 静态代理是在编译时就已经将接口、代理类、被代理类的字节码文件确定下来 动态代理是程序在运行后通过反射创建字节码文件交由 JVM 加载 JDK 代理和 CGLIB 代理： JDK 动态代理采用 ProxyGenerator.generateProxyClass() 方法在运行时生成字节码；CGLIB 底层采用 ASM 字节码生成框架，使用字节码技术生成代理类。在 JDK1.6之前比使用 Java 反射效率要高，到 JDK1.8 的时候，JDK 代理效率高于 CGLIB 代理。所以如果有接口或者当前类就是接口使用 JDK 动态代理，如果没有接口使用 CGLIB 代理 代理模式的优缺点： 优点： 代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用 代理对象可以增强目标对象的功能，被用来间接访问底层对象，与原始对象具有相同的 hashCode 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度 缺点：增加了系统的复杂度 代理模式的使用场景： 远程（Remote）代理：本地服务通过网络请求远程服务，需要实现网络通信，处理其中可能的异常。为了良好的代码设计和可维护性，将网络通信部分隐藏起来，只暴露给本地服务一个接口，通过该接口即可访问远程服务提供的功能 防火墙（Firewall）代理：当你将浏览器配置成使用代理功能时，防火墙就将你的浏览器的请求转给互联网，当互联网返回响应时，代理服务器再把它转给你的浏览器 保护（Protect or Access）代理：控制对一个对象的访问，如果需要，可以给不同的用户提供不同级别的使用权限 JVMJVM概述基本介绍JVM：全称 Java Virtual Machine，即 Java 虚拟机，一种规范，本身是一个虚拟计算机，直接和操作系统进行交互，与硬件不直接交互，而操作系统可以帮我们完成和硬件进行交互的工作 特点： Java 虚拟机基于二进制字节码执行，由一套字节码指令集、一组寄存器、一个栈、一个垃圾回收堆、一个方法区等组成 JVM 屏蔽了与操作系统平台相关的信息，从而能够让 Java 程序只需要生成能够在 JVM 上运行的字节码文件，通过该机制实现的跨平台性 Java 代码执行流程：Java 程序 --（编译）--&gt; 字节码文件 --（解释执行）--&gt; 操作系统（Win，Linux） JVM 结构： JVM、JRE、JDK 对比： JDK(Java SE Development Kit)：Java 标准开发包，提供了编译、运行 Java 程序所需的各种工具和资源 JRE( Java Runtime Environment)：Java 运行环境，用于解释执行 Java 的字节码文件 参考书籍：https://book.douban.com/subject/34907497/ 参考视频：https://www.bilibili.com/video/BV1PJ411n7xZ 参考视频：https://www.bilibili.com/video/BV1yE411Z7AP 架构模型Java 编译器输入的指令流是一种基于栈的指令集架构。因为跨平台的设计，Java 的指令都是根据栈来设计的，不同平台 CPU 架构不同，所以不能设计为基于寄存器架构 基于栈式架构的特点： 设计和实现简单，适用于资源受限的系统 使用零地址指令方式分配，执行过程依赖操作栈，指令集更小，编译器容易实现 零地址指令：机器指令的一种，是指令系统中的一种不设地址字段的指令，只有操作码而没有地址码。这种指令有两种情况：一是无需操作数，另一种是操作数为默认的（隐含的），默认为操作数在寄存器（ACC）中，指令可直接访问寄存器 一地址指令：一个操作码对应一个地址码，通过地址码寻找操作数 不需要硬件的支持，可移植性更好，更好实现跨平台 基于寄存器架构的特点： 需要硬件的支持，可移植性差 性能更好，执行更高效，寄存器比内存快 以一地址指令、二地址指令、三地址指令为主 生命周期JVM 的生命周期分为三个阶段，分别为：启动、运行、死亡 启动：当启动一个 Java 程序时，通过引导类加载器（bootstrap class loader）创建一个初始类（initial class），对于拥有 main 函数的类就是 JVM 实例运行的起点 运行： main() 方法是一个程序的初始起点，任何线程均可由在此处启动 在 JVM 内部有两种线程类型，分别为：用户线程和守护线程，JVM 使用的是守护线程，main() 和其他线程使用的是用户线程，守护线程会随着用户线程的结束而结束 执行一个 Java 程序时，真真正正在执行的是一个 Java 虚拟机的进程 JVM 有两种运行模式 Server 与 Client，两种模式的区别在于：Client 模式启动速度较快，Server 模式启动较慢；但是启动进入稳定期长期运行之后 Server 模式的程序运行速度比 Client 要快很多 Server 模式启动的 JVM 采用的是重量级的虚拟机，对程序采用了更多的优化；Client 模式启动的 JVM 采用的是轻量级的虚拟机 死亡： 当程序中的用户线程都中止，JVM 才会退出 程序正常执行结束、程序异常或错误而异常终止、操作系统错误导致终止 线程调用 Runtime 类 halt 方法或 System 类 exit 方法，并且 Java 安全管理器允许这次 exit 或 halt 操作 内存结构内存概述内存结构是 JVM 中非常重要的一部分，是非常重要的系统资源，是硬盘和 CPU 的桥梁，承载着操作系统和应用程序的实时运行，又叫运行时数据区 JVM 内存结构规定了 Java 在运行过程中内存申请、分配、管理的策略，保证了 JVM 的高效稳定运行 Java1.8 以前的内存结构图： Java1.8 之后的内存结果图： 线程运行诊断： 定位：jps 定位进程 ID jstack 进程 ID：用于打印出给定的 Java 进程 ID 或 core file 或远程调试服务的 Java 堆栈信息 常见 OOM 错误： java.lang.StackOverflowError java.lang.OutOfMemoryError：java heap space java.lang.OutOfMemoryError：GC overhead limit exceeded java.lang.OutOfMemoryError：Direct buffer memory java.lang.OutOfMemoryError：unable to create new native thread java.lang.OutOfMemoryError：Metaspace JVM内存虚拟机栈Java 栈Java 虚拟机栈：Java Virtual Machine Stacks，每个线程运行时所需要的内存 每个方法被执行时，都会在虚拟机栈中创建一个栈帧 stack frame（一个方法一个栈帧） Java 虚拟机规范允许 Java 栈的大小是动态的或者是固定不变的 虚拟机栈是每个线程私有的，每个线程只能有一个活动栈帧，对应方法调用到执行完成的整个过程 每个栈由多个栈帧（Frame）组成，对应着每次方法调用时所占用的内存，每个栈帧中存储着： 局部变量表：存储方法里的 Java 基本数据类型以及对象的引用 动态链接：也叫指向运行时常量池的方法引用 方法返回地址：方法正常退出或者异常退出的定义 操作数栈或表达式栈和其他一些附加信息 设置栈内存大小：-Xss size -Xss 1024k 在 JDK 1.4 中默认为 256K，而在 JDK 1.5+ 默认为 1M 虚拟机栈特点： 栈内存不需要进行GC，方法开始执行的时候会进栈，方法调用后自动弹栈，相当于清空了数据 栈内存分配越大越大，可用的线程数越少（内存越大，每个线程拥有的内存越大） 方法内的局部变量是否线程安全： 如果方法内局部变量没有逃离方法的作用访问，它是线程安全的（逃逸分析） 如果是局部变量引用了对象，并逃离方法的作用范围，需要考虑线程安全 异常： 栈帧过多导致栈内存溢出 （超过了栈的容量），会抛出 OutOfMemoryError 异常 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常 局部变量局部变量表也被称之为局部变量数组或本地变量表，本质上定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量 表是建立在线程的栈上，是线程私有的数据，因此不存在数据安全问题 表的容量大小是在编译期确定的，保存在方法的 Code 属性的 maximum local variables 数据项中 表中的变量只在当前方法调用中有效，方法结束栈帧销毁，局部变量表也会随之销毁 表中的变量也是重要的垃圾回收根节点，只要被表中数据直接或间接引用的对象都不会被回收 局部变量表最基本的存储单元是 slot（变量槽）： 参数值的存放总是在局部变量数组的 index0 开始，到数组长度 -1 的索引结束，JVM 为每一个 slot 都分配一个访问索引，通过索引即可访问到槽中的数据 存放编译期可知的各种基本数据类型（8种），引用类型（reference），returnAddress 类型的变量 32 位以内的类型只占一个 slot（包括 returnAddress 类型），64 位的类型（long 和 double）占两个 slot 局部变量表中的槽位是可以重复利用的，如果一个局部变量过了其作用域，那么之后申明的新的局部变量就可能会复用过期局部变量的槽位，从而达到节省资源的目的 操作数栈栈：可以使用数组或者链表来实现 操作数栈：在方法执行过程中，根据字节码指令，往栈中写入数据或提取数据，即入栈（push）或出栈（pop） 保存计算过程的中间结果，同时作为计算过程中变量临时的存储空间，是执行引擎的一个工作区 Java 虚拟机的解释引擎是基于栈的执行引擎，其中的栈指的就是操作数栈 如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中 栈顶缓存技术 ToS（Top-of-Stack Cashing）：将栈顶元素全部缓存在 CPU 的寄存器中，以此降低对内存的读&#x2F;写次数，提升执行的效率 基于栈式架构的虚拟机使用的零地址指令更加紧凑，完成一项操作需要使用很多入栈和出栈指令，所以需要更多的指令分派（instruction dispatch）次数和内存读&#x2F;写次数，由于操作数是存储在内存中的，因此频繁地执行内存读&#x2F;写操作必然会影响执行速度，所以需要栈顶缓存技术 动态链接动态链接是指向运行时常量池的方法引用，涉及到栈操作已经是类加载完成，这个阶段的解析是动态绑定 为了支持当前方法的代码能够实现动态链接，每一个栈帧内部都包含一个指向运行时常量池或该栈帧所属方法的引用 在 Java 源文件被编译成的字节码文件中，所有的变量和方法引用都作为符号引用保存在 class 的常量池中 常量池的作用：提供一些符号和常量，便于指令的识别 返回地址Return Address：存放调用该方法的 PC 寄存器的值 方法的结束有两种方式：正常执行完成、出现未处理的异常，在方法退出后都返回到该方法被调用的位置 正常：调用者的 PC 计数器的值作为返回地址，即调用该方法的指令的下一条指令的地址 异常：返回地址是要通过异常表来确定 正常完成出口：执行引擎遇到任意一个方法返回的字节码指令（return），会有返回值传递给上层的方法调用者 异常完成出口：方法执行的过程中遇到了异常（Exception），并且这个异常没有在方法内进行处理，本方法的异常表中没有搜素到匹配的异常处理器，导致方法退出 两者区别：通过异常完成出口退出的不会给上层调用者产生任何的返回值 附加信息栈帧中还允许携带与 Java 虚拟机实现相关的一些附加信息，例如对程序调试提供支持的信息 本地方法栈本地方法栈是为虚拟机执行本地方法时提供服务的 JNI：Java Native Interface，通过使用 Java 本地接口程序，可以确保代码在不同的平台上方便移植 不需要进行 GC，与虚拟机栈类似，也是线程私有的，有 StackOverFlowError 和 OutOfMemoryError 异常 虚拟机栈执行的是 Java 方法，在 HotSpot JVM 中，直接将本地方法栈和虚拟机栈合二为一 本地方法一般是由其他语言编写，并且被编译为基于本机硬件和操作系统的程序 当某个线程调用一个本地方法时，就进入了不再受虚拟机限制的世界，和虚拟机拥有同样的权限 本地方法可以通过本地方法接口来访问虚拟机内部的运行时数据区 直接从本地内存的堆中分配任意数量的内存 可以直接使用本地处理器中的寄存器 原理：将本地的 C 函数（如 foo）编译到一个共享库（foo.so）中，当正在运行的 Java 程序调用 foo 时，Java 解释器利用 dlopen 接口动态链接和加载 foo.so 后再调用该函数 dlopen 函数：Linux 系统加载和链接共享库 dlclose 函数：卸载共享库 图片来源：https://github.com/CyC2018/CS-Notes/blob/master/notes/Java%20%E8%99%9A%E6%8B%9F%E6%9C%BA.md 程序计数器Program Counter Register 程序计数器（寄存器） 作用：内部保存字节码的行号，用于记录正在执行的字节码指令地址（如果正在执行的是本地方法则为空） 原理： JVM 对于多线程是通过线程轮流切换并且分配线程执行时间，一个处理器只会处理执行一个线程 切换线程需要从程序计数器中来回去到当前的线程上一次执行的行号 特点： 是线程私有的 不会存在内存溢出，是 JVM 规范中唯一一个不出现 OOM 的区域，所以这个空间不会进行 GC Java 反编译指令：javap -v Test.class #20：代表去 Constant pool 查看该地址的指令 123456780: getstatic #20 // PrintStream out = System.out;3: astore_1 // --4: aload_1 // out.println(1);5: iconst_1 // --6: invokevirtual #26 // --9: aload_1 // out.println(2);10: iconst_2 // --11: invokevirtual #26 // -- 堆Heap 堆：是 JVM 内存中最大的一块，由所有线程共享，由垃圾回收器管理的主要区域，堆中对象大部分都需要考虑线程安全的问题 存放哪些资源： 对象实例：类初始化生成的对象，基本数据类型的数组也是对象实例，new 创建对象都使用堆内存 字符串常量池： 字符串常量池原本存放于方法区，JDK7 开始放置于堆中 字符串常量池存储的是 String 对象的直接引用或者对象，是一张 string table 静态变量：静态变量是有 static 修饰的变量，JDK8 时从方法区迁移至堆中 线程分配缓冲区 Thread Local Allocation Buffer：线程私有但不影响堆的共性，可以提升对象分配的效率 设置堆内存指令：-Xmx Size 内存溢出：new 出对象，循环添加字符数据，当堆中没有内存空间可分配给实例，也无法再扩展时，就会抛出 OutOfMemoryError 异常 堆内存诊断工具：（控制台命令） jps：查看当前系统中有哪些 Java 进程 jmap：查看堆内存占用情况 jhsdb jmap --heap --pid 进程id jconsole：图形界面的，多功能的监测工具，可以连续监测 在 Java7 中堆内会存在年轻代、老年代和方法区（永久代）： Young 区被划分为三部分，Eden 区和两个大小严格相同的 Survivor 区。Survivor 区某一时刻只有其中一个是被使用的，另外一个留做垃圾回收时复制对象。在 Eden 区变满的时候，GC 就会将存活的对象移到空闲的 Survivor 区间中，根据 JVM 的策略，在经过几次垃圾回收后，仍然存活于 Survivor 的对象将被移动到 Tenured 区间 Tenured 区主要保存生命周期长的对象，一般是一些老的对象，当一些对象在 Young 复制转移一定的次数以后，对象就会被转移到 Tenured 区 Perm 代主要保存 Class、ClassLoader、静态变量、常量、编译后的代码，在 Java7 中堆内方法区会受到 GC 的管理 分代原因：不同对象的生命周期不同，70%-99% 的对象都是临时对象，优化 GC 性能 123456789public static void main(String[] args) &#123; // 返回Java虚拟机中的堆内存总量 long initialMemory = Runtime.getRuntime().totalMemory() / 1024 / 1024; // 返回Java虚拟机使用的最大堆内存量 long maxMemory = Runtime.getRuntime().maxMemory() / 1024 / 1024; System.out.println(&quot;-Xms : &quot; + initialMemory + &quot;M&quot;);//-Xms : 245M System.out.println(&quot;-Xmx : &quot; + maxMemory + &quot;M&quot;);//-Xmx : 3641M&#125; 方法区方法区：是各个线程共享的内存区域，用于存储已被虚拟机加载的类信息、常量、即时编译器编译后的代码等数据，虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是也叫 Non-Heap（非堆） 方法区是一个 JVM 规范，永久代与元空间都是其一种实现方式 方法区的大小不必是固定的，可以动态扩展，加载的类太多，可能导致永久代内存溢出 (OutOfMemoryError) 方法区的 GC：针对常量池的回收及对类型的卸载，比较难实现 为了避免方法区出现 OOM，在 JDK8 中将堆内的方法区（永久代）移动到了本地内存上，重新开辟了一块空间，叫做元空间，元空间存储类的元信息，静态变量和字符串常量池等放入堆中 类元信息：在类编译期间放入方法区，存放了类的基本信息，包括类的方法、参数、接口以及常量池表 常量池表（Constant Pool Table）是 Class 文件的一部分，存储了类在编译期间生成的字面量、符号引用，JVM 为每个已加载的类维护一个常量池 字面量：基本数据类型、字符串类型常量、声明为 final 的常量值等 符号引用：类、字段、方法、接口等的符号引用 运行时常量池是方法区的一部分 常量池（编译器生成的字面量和符号引用）中的数据会在类加载的加载阶段放入运行时常量池 类在解析阶段将这些符号引用替换成直接引用 除了在编译期生成的常量，还允许动态生成，例如 String 类的 intern() 本地内存基本介绍虚拟机内存：Java 虚拟机在执行的时候会把管理的内存分配成不同的区域，受虚拟机内存大小的参数控制，当大小超过参数设置的大小时就会报 OOM 本地内存：又叫做堆外内存，线程共享的区域，本地内存这块区域是不会受到 JVM 的控制的，不会发生 GC；因此对于整个 Java 的执行效率是提升非常大，但是如果内存的占用超出物理内存的大小，同样也会报 OOM 本地内存概述图： 元空间PermGen 被元空间代替，永久代的类信息、方法、常量池等都移动到元空间区 元空间与永久代区别：元空间不在虚拟机中，使用的本地内存，默认情况下，元空间的大小仅受本地内存限制 方法区内存溢出： JDK1.8 以前会导致永久代内存溢出：java.lang.OutOfMemoryError: PerGen space 1-XX:MaxPermSize=8m #参数设置 JDK1.8 以后会导致元空间内存溢出：java.lang.OutOfMemoryError: Metaspace 1-XX:MaxMetaspaceSize=8m\t#参数设置 元空间内存溢出演示： 1234567891011121314151617181920public class Demo1_8 extends ClassLoader &#123; // 可以用来加载类的二进制字节码 public static void main(String[] args) &#123; int j = 0; try &#123; Demo1_8 test = new Demo1_8(); for (int i = 0; i &lt; 10000; i++, j++) &#123; // ClassWriter 作用是生成类的二进制字节码 ClassWriter cw = new ClassWriter(0); // 版本号， public， 类名, 包名, 父类， 接口 cw.visit(Opcodes.V1_8, Opcodes.ACC_PUBLIC, &quot;Class&quot; + i, null, &quot;java/lang/Object&quot;, null); // 返回 byte[] byte[] code = cw.toByteArray(); // 执行了类的加载 test.defineClass(&quot;Class&quot; + i, code, 0, code.length); // Class 对象 &#125; &#125; finally &#123; System.out.println(j); &#125; &#125;&#125; 直接内存直接内存是 Java 堆外、直接向系统申请的内存区间，不是虚拟机运行时数据区的一部分，也不是《Java 虚拟机规范》中定义的内存区域 直接内存详解参考：NET → NIO → 直接内存 变量位置变量的位置不取决于它是基本数据类型还是引用数据类型，取决于它的声明位置 静态内部类和其他内部类： 一个 class 文件只能对应一个 public 类型的类，这个类可以有内部类，但不会生成新的 class 文件 静态内部类属于类本身，加载到方法区，其他内部类属于内部类的属性，加载到堆（待考证） 类变量： 类变量是用 static 修饰符修饰，定义在方法外的变量，随着 Java 进程产生和销毁 在 Java8 之前把静态变量存放于方法区，在 Java8 时存放在堆中的静态变量区 实例变量： 实例（成员）变量是定义在类中，没有 static 修饰的变量，随着类的实例产生和销毁，是类实例的一部分 在类初始化的时候，从运行时常量池取出直接引用或者值，与初始化的对象一起放入堆中 局部变量： 局部变量是定义在类的方法中的变量 在所在方法被调用时放入虚拟机栈的栈帧中，方法执行结束后从虚拟机栈中弹出， 类常量池、运行时常量池、字符串常量池有什么关系？有什么区别？ 类常量池与运行时常量池都存储在方法区，而字符串常量池在 Jdk7 时就已经从方法区迁移到了 Java 堆中 在类编译过程中，会把类元信息放到方法区，类元信息的其中一部分便是类常量池，主要存放字面量和符号引用，而字面量的一部分便是文本字符 在类加载时将字面量和符号引用解析为直接引用存储在运行时常量池 对于文本字符，会在解析时查找字符串常量池，查出这个文本字符对应的字符串对象的直接引用，将直接引用存储在运行时常量池 什么是字面量？什么是符号引用？ 字面量：java 代码在编译过程中是无法构建引用的，字面量就是在编译时对于数据的一种表示 12int a = 1; //这个1便是字面量String b = &quot;iloveu&quot;;\t//iloveu便是字面量 符号引用：在编译过程中并不知道每个类的地址，因为可能这个类还没有加载，如果在一个类中引用了另一个类，无法知道它的内存地址，只能用它的类名作为符号引用，在类加载完后用这个符号引用去获取内存地址 内存管理内存分配两种方式不分配内存的对象无法进行其他操作，JVM 为对象分配内存的过程：首先计算对象占用空间大小，接着在堆中划分一块内存给新对象 如果内存规整，使用指针碰撞（Bump The Pointer）。所有用过的内存在一边，空闲的内存在另外一边，中间有一个指针作为分界点的指示器，分配内存就仅仅是把指针向空闲那边挪动一段与对象大小相等的距离 如果内存不规整，虚拟机需要维护一个空闲列表（Free List）分配。已使用的内存和未使用的内存相互交错，虚拟机维护了一个列表，记录上哪些内存块是可用的，再分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的内容 TLABTLAB：Thread Local Allocation Buffer，为每个线程在堆内单独分配了一个缓冲区，多线程分配内存时，使用 TLAB 可以避免线程安全问题，同时还能够提升内存分配的吞吐量，这种内存分配方式叫做快速分配策略 栈上分配使用的是栈来进行对象内存的分配 TLAB 分配使用的是 Eden 区域进行内存分配，属于堆内存 堆区是线程共享区域，任何线程都可以访问到堆区中的共享数据，由于对象实例的创建在 JVM 中非常频繁，因此在并发环境下为避免多个线程操作同一地址，需要使用加锁等机制，进而影响分配速度 问题：堆空间都是共享的么？ 不一定，因为还有 TLAB，在堆中划分出一块区域，为每个线程所独占 JVM 是将 TLAB 作为内存分配的首选，但不是所有的对象实例都能够在 TLAB 中成功分配内存，一旦对象在 TLAB 空间分配内存失败时，JVM 就会通过使用加锁机制确保数据操作的原子性，从而直接在堆中分配内存 栈上分配优先于 TLAB 分配进行，逃逸分析中若可进行栈上分配优化，会优先进行对象栈上直接分配内存 参数设置： -XX:UseTLAB：设置是否开启 TLAB 空间 -XX:TLABWasteTargetPercent：设置 TLAB 空间所占用 Eden 空间的百分比大小，默认情况下 TLAB 空间的内存非常小，仅占有整个 Eden 空间的1% -XX:TLABRefillWasteFraction：指当 TLAB 空间不足，请求分配的对象内存大小超过此阈值时不会进行 TLAB 分配，直接进行堆内存分配，否则还是会优先进行 TLAB 分配 逃逸分析即时编译（Just-in-time Compilation，JIT）是一种通过在运行时将字节码翻译为机器码，从而改善性能的技术，在 HotSpot 实现中有多种选择：C1、C2 和 C1+C2，分别对应 Client、Server 和分层编译 C1 编译速度快，优化方式比较保守；C2 编译速度慢，优化方式比较激进 C1+C2 在开始阶段采用 C1 编译，当代码运行到一定热度之后采用 C2 重新编译 逃逸分析并不是直接的优化手段，而是一个代码分析方式，通过动态分析对象的作用域，为优化手段如栈上分配、标量替换和同步消除等提供依据，发生逃逸行为的情况有两种：方法逃逸和线程逃逸 方法逃逸：当一个对象在方法中定义之后，被外部方法引用 全局逃逸：一个对象的作用范围逃出了当前方法或者当前线程，比如对象是一个静态变量、全局变量赋值、已经发生逃逸的对象、作为当前方法的返回值 参数逃逸：一个对象被作为方法参数传递或者被参数引用 线程逃逸：如类变量或实例变量，可能被其它线程访问到 如果不存在逃逸行为，则可以对该对象进行如下优化：同步消除、标量替换和栈上分配 同步消除 线程同步本身比较耗时，如果确定一个对象不会逃逸出线程，不被其它线程访问到，那对象的读写就不会存在竞争，则可以消除对该对象的同步锁，通过 -XX:+EliminateLocks 可以开启同步消除 ( - 号关闭) 标量替换 标量替换：如果把一个对象拆散，将其成员变量恢复到基本类型来访问 标量 (scalar) ：不可分割的量，如基本数据类型和 reference 类型 聚合量 (Aggregate)：一个数据可以继续分解，对象一般是聚合量 如果逃逸分析发现一个对象不会被外部访问，并且该对象可以被拆散，那么经过优化之后，并不直接生成该对象，而是将该对象成员变量分解若干个被这个方法使用的成员变量所代替 参数设置： -XX:+EliminateAllocations：开启标量替换 -XX:+PrintEliminateAllocations：查看标量替换情况 栈上分配 JIT 编译器在编译期间根据逃逸分析的结果，如果一个对象没有逃逸出方法的话，就可能被优化成栈上分配。分配完成后，继续在调用栈内执行，最后线程结束，栈空间被回收，局部变量对象也被回收，这样就无需 GC User 对象的作用域局限在方法 fn 中，可以使用标量替换的优化手段在栈上分配对象的成员变量，这样就不会生成 User 对象，大大减轻 GC 的压力 1234567891011121314151617181920212223242526272829public class JVM &#123; public static void main(String[] args) throws Exception &#123; int sum = 0; int count = 1000000; //warm up for (int i = 0; i &lt; count ; i++) &#123; sum += fn(i); &#125; System.out.println(sum); System.in.read(); &#125; private static int fn(int age) &#123; User user = new User(age); int i = user.getAge(); return i; &#125;&#125;class User &#123; private final int age; public User(int age) &#123; this.age = age; &#125; public int getAge() &#123; return age; &#125;&#125; 分代思想分代介绍Java8 时，堆被分为了两份：新生代和老年代（1:2），在 Java7 时，还存在一个永久代 新生代使用：复制算法 老年代使用：标记 - 清除 或者 标记 - 整理 算法 Minor GC 和 Full GC： Minor GC：回收新生代，新生代对象存活时间很短，所以 Minor GC 会频繁执行，执行的速度比较快 Full GC：回收老年代和新生代，老年代对象其存活时间长，所以 Full GC 很少执行，执行速度会比 Minor GC 慢很多 Eden 和 Survivor 大小比例默认为 8:1:1 分代分配工作机制： 对象优先在 Eden 分配：当创建一个对象的时候，对象会被分配在新生代的 Eden 区，当 Eden 区要满了时候，触发 YoungGC 当进行 YoungGC 后，此时在 Eden 区存活的对象被移动到 to 区，并且当前对象的年龄会加 1，清空 Eden 区 当再一次触发 YoungGC 的时候，会把 Eden 区中存活下来的对象和 to 中的对象，移动到 from 区中，这些对象的年龄会加 1，清空 Eden 区和 to 区 To 区永远是空 Survivor 区，From 区是有数据的，每次 MinorGC 后两个区域互换 From 区和 To 区 也可以叫做 S0 区和 S1 区 晋升到老年代： 长期存活的对象进入老年代：为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄则移动到老年代中 -XX:MaxTenuringThreshold：定义年龄的阈值，对象头中用 4 个 bit 存储，所以最大值是 15，默认也是 15 大对象直接进入老年代：需要连续内存空间的对象，最典型的大对象是很长的字符串以及数组；避免在 Eden 和 Survivor 之间的大量复制；经常出现大对象会提前触发 GC 以获取足够的连续空间分配给大对象 -XX:PretenureSizeThreshold：大于此值的对象直接在老年代分配 动态对象年龄判定：如果在 Survivor 区中相同年龄的对象的所有大小之和超过 Survivor 空间的一半，年龄大于等于该年龄的对象就可以直接进入老年代 空间分配担保： 在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的 如果不成立，虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于将尝试着进行一次 Minor GC；如果小于或者 HandlePromotionFailure 的值不允许冒险，那么就要进行一次 Full GC 回收策略触发条件内存垃圾回收机制主要集中的区域就是线程共享区域：堆和方法区 Minor GC 触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC FullGC 同时回收新生代、老年代和方法区，只会存在一个 FullGC 的线程进行执行，其他的线程全部会被挂起，有以下触发条件： 调用 System.gc()： 在默认情况下，通过 System.gc() 或 Runtime.getRuntime().gc() 的调用，会显式触发 FullGC，同时对老年代和新生代进行回收，但是虚拟机不一定真正去执行，无法保证对垃圾收集器的调用 不建议使用这种方式，应该让虚拟机管理内存。一般情况下，垃圾回收应该是自动进行的，无须手动触发；在一些特殊情况下，如正在编写一个性能基准，可以在运行之间调用 System.gc() 老年代空间不足： 为了避免引起的 Full GC，应当尽量不要创建过大的对象以及数组 通过 -Xmn 参数调整新生代的大小，让对象尽量在新生代被回收掉不进入老年代，可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间 空间分配担保失败 JDK 1.7 及以前的永久代（方法区）空间不足 Concurrent Mode Failure：执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC 手动 GC 测试，VM参数：-XX:+PrintGcDetails 123456789101112131415161718192021222324public void localvarGC1() &#123; byte[] buffer = new byte[10 * 1024 * 1024];//10MB System.gc();\t//输出: 不会被回收, FullGC时被放入老年代&#125;public void localvarGC2() &#123; byte[] buffer = new byte[10 * 1024 * 1024]; buffer = null; System.gc();\t//输出: 正常被回收&#125; public void localvarGC3() &#123; &#123; byte[] buffer = new byte[10 * 1024 * 1024]; &#125; System.gc();\t//输出: 不会被回收, FullGC时被放入老年代 &#125;public void localvarGC4() &#123; &#123; byte[] buffer = new byte[10 * 1024 * 1024]; &#125; int value = 10; System.gc();\t//输出: 正常被回收，slot复用，局部变量过了其作用域 buffer置空&#125; 安全区域安全点 (Safepoint)：程序执行时并非在所有地方都能停顿下来开始 GC，只有在安全点才能停下 Safe Point 的选择很重要，如果太少可能导致 GC 等待的时间太长，如果太多可能导致运行时的性能问题 大部分指令的执行时间都非常短，通常会根据是否具有让程序长时间执行的特征为标准，选择些执行时间较长的指令作为 Safe Point， 如方法调用、循环跳转和异常跳转等 在 GC 发生时，让所有线程都在最近的安全点停顿下来的方法： 抢先式中断：没有虚拟机采用，首先中断所有线程，如果有线程不在安全点，就恢复线程让线程运行到安全点 主动式中断：设置一个中断标志，各个线程运行到各个 Safe Point 时就轮询这个标志，如果中断标志为真，则将自己进行中断挂起 问题：Safepoint 保证程序执行时，在不太长的时间内就会遇到可进入 GC 的 Safepoint，但是当线程处于 Waiting 状态或 Blocked 状态，线程无法响应 JVM 的中断请求，运行到安全点去中断挂起，JVM 也不可能等待线程被唤醒，对于这种情况，需要安全区域来解决 安全区域 (Safe Region)：指在一段代码片段中，对象的引用关系不会发生变化，在这个区域中的任何位置开始 GC 都是安全的 运行流程： 当线程运行到 Safe Region 的代码时，首先标识已经进入了 Safe Region，如果这段时间内发生 GC，JVM 会忽略标识为 Safe Region 状态的线程 当线程即将离开 Safe Region 时，会检查 JVM 是否已经完成 GC，如果完成了则继续运行，否则线程必须等待 GC 完成，收到可以安全离开 SafeRegion 的信号 垃圾判断垃圾介绍垃圾：如果一个或多个对象没有任何的引用指向它了，那么这个对象现在就是垃圾 作用：释放没用的对象，清除内存里的记录碎片，碎片整理将所占用的堆内存移到堆的一端，以便 JVM 将整理出的内存分配给新的对象 垃圾收集主要是针对堆和方法区进行，程序计数器、虚拟机栈和本地方法栈这三个区域属于线程私有的，只存在于线程的生命周期内，线程结束之后就会消失，因此不需要对这三个区域进行垃圾回收 在堆里存放着几乎所有的 Java 对象实例，在 GC 执行垃圾回收之前，首先需要区分出内存中哪些是存活对象，哪些是已经死亡的对象。只有被标记为己经死亡的对象，GC 才会在执行垃圾回收时，释放掉其所占用的内存空间，因此这个过程可以称为垃圾标记阶段，判断对象存活一般有两种方式：引用计数算法和可达性分析算法 引用计数法引用计数算法（Reference Counting）：对每个对象保存一个整型的引用计数器属性，用于记录对象被引用的情况。对于一个对象 A，只要有任何一个对象引用了 A，则 A 的引用计数器就加 1；当引用失效时，引用计数器就减 1；当对象 A 的引用计数器的值为 0，即表示对象A不可能再被使用，可进行回收（Java 没有采用） 优点： 回收没有延迟性，无需等到内存不够的时候才开始回收，运行时根据对象计数器是否为 0，可以直接回收 在垃圾回收过程中，应用无需挂起；如果申请内存时，内存不足，则立刻报 OOM 错误 区域性，更新对象的计数器时，只是影响到该对象，不会扫描全部对象 缺点： 每次对象被引用时，都需要去更新计数器，有一点时间开销 浪费 CPU 资源，即使内存够用，仍然在运行时进行计数器的统计。 无法解决循环引用问题，会引发内存泄露（最大的缺点） 1234567891011public class Test &#123; public Object instance = null; public static void main(String[] args) &#123; Test a = new Test();// a = 1 Test b = new Test();// b = 1 a.instance = b; // b = 2 b.instance = a; // a = 2 a = null; // a = 1 b = null; // b = 1 &#125;&#125; 可达性分析GC Roots可达性分析算法：也可以称为根搜索算法、追踪性垃圾收集 GC Roots 对象： 虚拟机栈中局部变量表中引用的对象：各个线程被调用的方法中使用到的参数、局部变量等 本地方法栈中引用的对象 堆中类静态属性引用的对象 方法区中的常量引用的对象 字符串常量池（string Table）里的引用 同步锁 synchronized 持有的对象 GC Roots 是一组活跃的引用，不是对象，放在 GC Roots Set 集合 工作原理可达性分析算法以根对象集合（GCRoots）为起始点，从上至下的方式搜索被根对象集合所连接的目标对象 分析工作必须在一个保障一致性的快照中进行，否则结果的准确性无法保证，这也是导致 GC 进行时必须 Stop The World 的一个原因 基本原理： 可达性分析算法后，内存中的存活对象都会被根对象集合直接或间接连接着，搜索走过的路径称为引用链 如果目标对象没有任何引用链相连，则是不可达的，就意味着该对象己经死亡，可以标记为垃圾对象 在可达性分析算法中，只有能够被根对象集合直接或者间接连接的对象才是存活对象 三色标记标记算法三色标记法把遍历对象图过程中遇到的对象，标记成以下三种颜色： 白色：尚未访问过 灰色：本对象已访问过，但是本对象引用到的其他对象尚未全部访问 黑色：本对象已访问过，而且本对象引用到的其他对象也全部访问完成 当 Stop The World (STW) 时，对象间的引用是不会发生变化的，可以轻松完成标记，遍历访问过程为： 初始时，所有对象都在白色集合 将 GC Roots 直接引用到的对象挪到灰色集合 从灰色集合中获取对象： 将本对象引用到的其他对象全部挪到灰色集合中 将本对象挪到黑色集合里面 重复步骤 3，直至灰色集合为空时结束 结束后，仍在白色集合的对象即为 GC Roots 不可达，可以进行回收 参考文章：https://www.jianshu.com/p/12544c0ad5c1 并发标记并发标记时，对象间的引用可能发生变化，多标和漏标的情况就有可能发生 多标情况：当 E 变为灰色或黑色时，其他线程断开的 D 对 E 的引用，导致这部分对象仍会被标记为存活，本轮 GC 不会回收这部分内存，这部分本应该回收但是没有回收到的内存，被称之为浮动垃圾 针对并发标记开始后的新对象，通常的做法是直接全部当成黑色，也算浮动垃圾 浮动垃圾并不会影响应用程序的正确性，只是需要等到下一轮垃圾回收中才被清除 漏标情况： 条件一：灰色对象断开了对一个白色对象的引用（直接或间接），即灰色对象原成员变量的引用发生了变化 条件二：其他线程中修改了黑色对象，插入了一条或多条对该白色对象的新引用 结果：导致该白色对象当作垃圾被 GC，影响到了程序的正确性 代码角度解释漏标： 123Object G = objE.fieldG; // 读objE.fieldG = null; // 写objD.fieldG = G; // 写 为了解决问题，可以操作上面三步，将对象 G 记录起来，然后作为灰色对象再进行遍历，比如放到一个特定的集合，等初始的 GC Roots 遍历完（并发标记），再遍历该集合（重新标记） 所以重新标记需要 STW，应用程序一直在运行，该集合可能会一直增加新的对象，导致永远都运行不完 解决方法：添加读写屏障，读屏障拦截第一步，写屏障拦截第二三步，在读写前后进行一些后置处理： 写屏障 + 增量更新：黑色对象新增引用，会将黑色对象变成灰色对象，最后对该节点重新扫描 增量更新 (Incremental Update) 破坏了条件二，从而保证了不会漏标 缺点：对黑色变灰的对象重新扫描所有引用，比较耗费时间 写屏障 (Store Barrier) + SATB：当原来成员变量的引用发生变化之前，记录下原来的引用对象 保留 GC 开始时的对象图，即原始快照 SATB，当 GC Roots 确定后，对象图就已经确定，那后续的标记也应该是按照这个时刻的对象图走，如果期间对白色对象有了新的引用会记录下来，并且将白色对象变灰（说明可达了，并且原始快照中本来就应该是灰色对象），最后重新扫描该对象的引用关系 SATB (Snapshot At The Beginning) 破坏了条件一，从而保证了不会漏标 **读屏障 (Load Barrier)**：破坏条件二，黑色对象引用白色对象的前提是获取到该对象，此时读屏障发挥作用 以 Java HotSpot VM 为例，其并发标记时对漏标的处理方案如下： CMS：写屏障 + 增量更新 G1：写屏障 + SATB ZGC：读屏障 finalizationJava 语言提供了对象终止（finalization）机制来允许开发人员提供对象被销毁之前的自定义处理逻辑 垃圾回收此对象之前，会先调用这个对象的 finalize() 方法，finalize() 方法允许在子类中被重写，用于在对象被回收时进行后置处理，通常在这个方法中进行一些资源释放和清理，比如关闭文件、套接字和数据库连接等 生存 OR 死亡：如果从所有的根节点都无法访问到某个对象，说明对象己经不再使用，此对象需要被回收。但事实上这时候它们暂时处于缓刑阶段。一个无法触及的对象有可能在某个条件下复活自己，所以虚拟机中的对象可能的三种状态： 可触及的：从根节点开始，可以到达这个对象 可复活的：对象的所有引用都被释放，但是对象有可能在 finalize() 中复活 不可触及的：对象的 finalize() 被调用并且没有复活，那么就会进入不可触及状态，不可触及的对象不可能被复活，因为 finalize() 只会被调用一次，等到这个对象再被标记为可回收时就必须回收 永远不要主动调用某个对象的 finalize() 方法，应该交给垃圾回收机制调用，原因： finalize() 时可能会导致对象复活 finalize() 方法的执行时间是没有保障的，完全由 GC 线程决定，极端情况下，若不发生 GC，则 finalize() 方法将没有执行机会，因为优先级比较低，即使主动调用该方法，也不会因此就直接进行回收 一个糟糕的 finalize() 会严重影响 GC 的性能 引用分析无论是通过引用计数算法判断对象的引用数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引用有关，Java 提供了四种强度不同的引用类型 强引用：被强引用关联的对象不会被回收，只有所有 GCRoots 都不通过强引用引用该对象，才能被垃圾回收 强引用可以直接访问目标对象 虚拟机宁愿抛出 OOM 异常，也不会回收强引用所指向对象 强引用可能导致内存泄漏 1Object obj = new Object();//使用 new 一个新对象的方式来创建强引用 软引用（SoftReference）：被软引用关联的对象只有在内存不够的情况下才会被回收 仅（可能有强引用，一个对象可以被多个引用）有软引用引用该对象时，在垃圾回收后，内存仍不足时会再次出发垃圾回收，回收软引用对象 配合引用队列来释放软引用自身，在构造软引用时，可以指定一个引用队列，当软引用对象被回收时，就会加入指定的引用队列，通过这个队列可以跟踪对象的回收情况 软引用通常用来实现内存敏感的缓存，比如高速缓存就有用到软引用；如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时不会耗尽内存 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 弱引用（WeakReference）：被弱引用关联的对象一定会被回收，只能存活到下一次垃圾回收发生之前 仅有弱引用引用该对象时，在垃圾回收时，无论内存是否充足，都会回收弱引用对象 配合引用队列来释放弱引用自身 WeakHashMap 用来存储图片信息，可以在内存不足的时候及时回收，避免了 OOM 123Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null; 虚引用（PhantomReference）：也称为幽灵引用或者幻影引用，是所有引用类型中最弱的一个 一个对象是否有虚引用的存在，不会对其生存时间造成影响，也无法通过虚引用得到一个对象 为对象设置虚引用的唯一目的是在于跟踪垃圾回收过程，能在这个对象被回收时收到一个系统通知 必须配合引用队列使用，主要配合 ByteBuffer 使用，被引用对象回收时会将虚引用入队，由 Reference Handler 线程调用虚引用相关方法释放直接内存 123Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj, null);obj = null; 终结器引用（finalization） 无用属性无用类方法区主要回收的是无用的类 判定一个类是否是无用的类，需要同时满足下面 3 个条件： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例 加载该类的 ClassLoader 已经被回收 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是可以，而并不是和对象一样不使用了就会必然被回收 废弃常量在常量池中存在字符串 “abc”，如果当前没有任何 String 对象引用该常量，说明常量 “abc” 是废弃常量，如果这时发生内存回收的话而且有必要的话（内存不够用），”abc” 就会被系统清理出常量池 静态变量类加载时（第一次访问），这个类中所有静态成员就会被加载到静态变量区，该区域的成员一旦创建，直到程序退出才会被回收 如果是静态引用类型的变量，静态变量区只存储一份对象的引用地址，真正的对象在堆内，如果要回收该对象可以设置引用为 null 参考文章：https://blog.csdn.net/zhengzhb/article/details/7331354 回收算法复制算法复制算法的核心就是，将原有的内存空间一分为二，每次只用其中的一块，在垃圾回收时，将正在使用的对象复制到另一个内存空间中，然后将该内存空间清理，交换两个内存的角色，完成垃圾的回收 应用场景：如果内存中的垃圾对象较多，需要复制的对象就较少，这种情况下适合使用该方式并且效率比较高，反之则不适合 算法优点： 没有标记和清除过程，实现简单，运行速度快 复制过去以后保证空间的连续性，不会出现碎片问题 算法缺点： 主要不足是只使用了内存的一半 对于 G1 这种分拆成为大量 region 的 GC，复制而不是移动，意味着 GC 需要维护 region 之间对象引用关系，不管是内存占用或者时间开销都不小 现在的商业虚拟机都采用这种收集算法回收新生代，因为新生代 GC 频繁并且对象的存活率不高，但是并不是划分为大小相等的两块，而是一块较大的 Eden 空间和两块较小的 Survivor 空间 标记清除标记清除算法，是将垃圾回收分为两个阶段，分别是标记和清除 标记：Collector 从引用根节点开始遍历，标记所有被引用的对象，一般是在对象的 Header 中记录为可达对象，标记的是引用的对象，不是垃圾 清除：Collector 对堆内存从头到尾进行线性的遍历，如果发现某个对象在其 Header 中没有标记为可达对象，则将其回收，把分块连接到空闲列表的单向链表，判断回收后的分块与前一个空闲分块是否连续，若连续会合并这两个分块，之后进行分配时只需要遍历这个空闲列表，就可以找到分块 分配阶段：程序会搜索空闲链表寻找空间大于等于新对象大小 size 的块 block，如果找到的块等于 size，会直接返回这个分块；如果找到的块大于 size，会将块分割成大小为 size 与 block - size 的两部分，返回大小为 size 的分块，并把大小为 block - size 的块返回给空闲列表 算法缺点： 标记和清除过程效率都不高 会产生大量不连续的内存碎片，导致无法给大对象分配内存，需要维护一个空闲链表 标记整理标记整理（压缩）算法是在标记清除算法的基础之上，做了优化改进的算法 标记阶段和标记清除算法一样，也是从根节点开始，对对象的引用进行标记，在清理阶段，并不是简单的直接清理可回收对象，而是将存活对象都向内存另一端移动，然后清理边界以外的垃圾，从而解决了碎片化的问题 优点：不会产生内存碎片 缺点：需要移动大量对象，处理效率比较低 Mark-Sweep Mark-Compact Copying 速度 中等 最慢 最快 空间开销 少（但会堆积碎片） 少（不堆积碎片） 通常需要活对象的 2 倍大小（不堆积碎片） 移动对象 否 是 是 垃圾回收器概述垃圾收集器分类： 按线程数分（垃圾回收线程数），可以分为串行垃圾回收器和并行垃圾回收器 除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行 按照工作模式分，可以分为并发式垃圾回收器和独占式垃圾回收器 并发式垃圾回收器与应用程序线程交替工作，以尽可能减少应用程序的停顿时间 独占式垃圾回收器（Stop the world）一旦运行，就停止应用程序中的所有用户线程，直到垃圾回收过程完全结束 按碎片处理方式分，可分为压缩式垃圾回收器和非压缩式垃圾回收器 压缩式垃圾回收器在回收完成后进行压缩整理，消除回收后的碎片，再分配对象空间使用指针碰撞 非压缩式的垃圾回收器不进行这步操作，再分配对象空间使用空闲列表 按工作的内存区间分，又可分为年轻代垃圾回收器和老年代垃圾回收器 GC 性能指标： 吞吐量：程序的运行时间占总运行时间的比例（总运行时间 &#x3D; 程序的运行时间 + 内存回收的时间） 垃圾收集开销：吞吐量的补数，垃圾收集所用时间与总运行时间的比例 暂停时间：执行垃圾收集时，程序的工作线程被暂停的时间 收集频率：相对于应用程序的执行，收集操作发生的频率 内存占用：Java 堆区所占的内存大小 快速：一个对象从诞生到被回收所经历的时间 垃圾收集器的组合关系： 新生代收集器：Serial、ParNew、Parallel Scavenge 老年代收集器：Serial old、Parallel old、CMS 整堆收集器：G1 红色虚线在 JDK9 移除、绿色虚线在 JDK14 弃用该组合、青色虚线在 JDK14 删除 CMS 垃圾回收器 查看默认的垃圾收回收器： -XX:+PrintcommandLineFlags：查看命令行相关参数（包含使用的垃圾收集器） 使用命令行指令：jinfo -flag 相关垃圾回收器参数 进程 ID SerialSerial：串行垃圾收集器，作用于新生代，是指使用单线程进行垃圾回收，采用复制算法，新生代基本都是复制算法 STW（Stop-The-World）：垃圾回收时，只有一个线程在工作，并且 Java 应用中的所有线程都要暂停，等待垃圾回收的完成 Serial old：执行老年代垃圾回收的串行收集器，内存回收算法使用的是标记-整理算法，同样也采用了串行回收和 STW 机制 Serial old 是 Client 模式下默认的老年代的垃圾回收器 Serial old 在 Server 模式下主要有两个用途： 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用 作为老年代 CMS 收集器的后备垃圾回收方案，在并发收集发生 Concurrent Mode Failure 时使用 开启参数：-XX:+UseSerialGC 等价于新生代用 Serial GC 且老年代用 Serial old GC 优点：简单而高效（与其他收集器的单线程比），对于限定单个 CPU 的环境来说，Serial 收集器由于没有线程交互的开销，可以获得最高的单线程收集效率 缺点：对于交互性较强的应用而言，这种垃圾收集器是不能够接受的，比如 JavaWeb 应用 ParNewPar 是 Parallel 并行的缩写，New 是只能处理的是新生代 并行垃圾收集器在串行垃圾收集器的基础之上做了改进，采用复制算法，将单线程改为了多线程进行垃圾回收，可以缩短垃圾回收的时间 对于其他的行为（收集算法、stop the world、对象分配规则、回收策略等）同 Serial 收集器一样，应用在年轻代，除 Serial 外，只有ParNew GC 能与 CMS 收集器配合工作 相关参数： -XX：+UseParNewGC：表示年轻代使用并行收集器，不影响老年代 -XX:ParallelGCThreads：默认开启和 CPU 数量相同的线程数 ParNew 是很多 JVM 运行在 Server 模式下新生代的默认垃圾收集器 对于新生代，回收次数频繁，使用并行方式高效 对于老年代，回收次数少，使用串行方式节省资源（CPU 并行需要切换线程，串行可以省去切换线程的资源） ParallelParallel Scavenge 收集器是应用于新生代的并行垃圾回收器，采用复制算法、并行回收和 Stop the World 机制 Parallel Old 收集器：是一个应用于老年代的并行垃圾回收器，采用标记-整理算法 对比其他回收器： 其它收集器目标是尽可能缩短垃圾收集时用户线程的停顿时间 Parallel 目标是达到一个可控制的吞吐量，被称为吞吐量优先收集器 Parallel Scavenge 对比 ParNew 拥有自适应调节策略，可以通过一个开关参数打开 GC Ergonomics 应用场景： 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验 高吞吐量可以高效率地利用 CPU 时间，尽快完成程序的运算任务，适合在后台运算而不需要太多交互 停顿时间和吞吐量的关系：新生代空间变小 → 缩短停顿时间 → 垃圾回收变得频繁 → 导致吞吐量下降 在注重吞吐量及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge + Parallel Old 收集器，在 Server 模式下的内存回收性能很好，Java8 默认是此垃圾收集器组合 参数配置： -XX：+UseParallelGC：手动指定年轻代使用 Paralle 并行收集器执行内存回收任务 -XX：+UseParalleloldcc：手动指定老年代使用并行回收收集器执行内存回收任务 上面两个参数，默认开启一个，另一个也会被开启（互相激活），默认 JDK8 是开启的 -XX:+UseAdaptivesizepplicy：设置 Parallel Scavenge 收集器具有自适应调节策略，在这种模式下，年轻代的大小、Eden 和 Survivor 的比例、晋升老年代的对象年龄等参数会被自动调整，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量 -XX:ParallelGcrhreads：设置年轻代并行收集器的线程数，一般与 CPU 数量相等，以避免过多的线程数影响垃圾收集性能 在默认情况下，当 CPU 数量小于 8 个，ParallelGcThreads 的值等于 CPU 数量 当 CPU 数量大于 8 个，ParallelGCThreads 的值等于 3+[5*CPU Count]&#x2F;8] -XX:MaxGCPauseMillis：设置垃圾收集器最大停顿时间（即 STW 的时间），单位是毫秒 对于用户来讲，停顿时间越短体验越好；在服务器端，注重高并发，整体的吞吐量 为了把停顿时间控制在 MaxGCPauseMillis 以内，收集器在工作时会调整 Java 堆大小或其他一些参数 -XX:GCTimeRatio：垃圾收集时间占总时间的比例 &#x3D;1&#x2F;(N+1)，用于衡量吞吐量的大小 取值范围（0，100）。默认值 99，也就是垃圾回收时间不超过 1 与 -xx:MaxGCPauseMillis 参数有一定矛盾性，暂停时间越长，Radio 参数就容易超过设定的比例 CMSCMS 全称 Concurrent Mark Sweep，是一款并发的、使用标记-清除算法、针对老年代的垃圾回收器，其最大特点是让垃圾收集线程与用户线程同时工作 CMS 收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间，停顿时间越短（低延迟）越适合与用户交互的程序，良好的响应速度能提升用户体验 分为以下四个流程： 初始标记：使用 STW 出现短暂停顿，仅标记一下 GC Roots 能直接关联到的对象，速度很快 并发标记：进行 GC Roots 开始遍历整个对象图，在整个回收过程中耗时最长，不需要 STW，可以与用户线程并发运行 重新标记：修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象，比初始标记时间长但远比并发标记时间短，需要 STW（不停顿就会一直变化，采用写屏障 + 增量更新来避免漏标情况） 并发清除：清除标记为可以回收对象，不需要移动存活对象，所以这个阶段可以与用户线程同时并发的 Mark Sweep 会造成内存碎片，不把算法换成 Mark Compact 的原因：Mark Compact 算法会整理内存，导致用户线程使用的对象的地址改变，影响用户线程继续执行 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿 优点：并发收集、低延迟 缺点： 吞吐量降低：在并发阶段虽然不会导致用户停顿，但是会因为占用了一部分线程而导致应用程序变慢，CPU 利用率不够高 CMS 收集器无法处理浮动垃圾，可能出现 Concurrent Mode Failure 导致另一次 Full GC 的产生 浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾（产生了新对象），这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，CMS 收集需要预留出一部分内存，不能等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS，导致很长的停顿时间 标记 - 清除算法导致的空间碎片，往往出现老年代空间无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC；为新对象分配内存空间时，将无法使用指针碰撞（Bump the Pointer）技术，而只能够选择空闲列表（Free List）执行内存分配 参数设置： -XX：+UseConcMarkSweepGC：手动指定使用 CMS 收集器执行内存回收任务 开启该参数后会自动将 -XX:+UseParNewGC 打开，即：ParNew + CMS + Serial old的组合 -XX:CMSInitiatingoccupanyFraction：设置堆内存使用率的阈值，一旦达到该阈值，便开始进行回收 JDK5 及以前版本的默认值为 68，即当老年代的空间使用率达到 68% 时，会执行一次CMS回收 JDK6 及以上版本默认值为 92% -XX:+UseCMSCompactAtFullCollection：用于指定在执行完 Full GC 后对内存空间进行压缩整理，以此避免内存碎片的产生，由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长 -XX:CMSFullGCsBeforecompaction：设置在执行多少次 Full GC 后对内存空间进行压缩整理 -XX:ParallelCMSThreads：设置 CMS 的线程数量 CMS 默认启动的线程数是 (ParallelGCThreads+3)&#x2F;4，ParallelGCThreads 是年轻代并行收集器的线程数 收集线程占用的 CPU 资源多于25%，对用户程序影响可能较大；当 CPU 资源比较紧张时，受到 CMS 收集器线程的影响，应用程序的性能在垃圾回收阶段可能会非常糟糕 G1G1 特点G1（Garbage-First）是一款面向服务端应用的垃圾收集器，应用于新生代和老年代、采用标记-整理算法、软实时、低延迟、可设定目标（最大 STW 停顿时间）的垃圾回收器，用于代替 CMS，适用于较大的堆（&gt;4 ~ 6G），在 JDK9 之后默认使用 G1 G1 对比其他处理器的优点： 并发与并行： 并行性：G1 在回收期间，可以有多个 GC 线程同时工作，有效利用多核计算能力，此时用户线程 STW 并发性：G1 拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此不会在整个回收阶段发生完全阻塞应用程序的情况 其他的垃圾收集器使用内置的 JVM 线程执行 GC 的多线程操作，而 G1 GC 可以采用应用线程承担后台运行的 GC 工作，JVM 的 GC 线程处理速度慢时，系统会调用应用程序线程加速垃圾回收过程 分区算法： 从分代上看，G1 属于分代型垃圾回收器，区分年轻代和老年代，年轻代依然有 Eden 区和 Survivor 区。从堆结构上看，新生代和老年代不再物理隔离，不用担心每个代内存是否足够，这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 GC 将整个堆划分成约 2048 个大小相同的独立 Region 块，每个 Region 块大小根据堆空间的实际大小而定，整体被控制在 1MB 到 32 MB之间且为 2 的 N 次幂，所有 Region 大小相同，在 JVM 生命周期内不会被改变。G1 把堆划分成多个大小相等的独立区域，使得每个小空间可以单独进行垃圾回收 新的区域 Humongous：本身属于老年代区，当出现了一个巨型对象超出了分区容量的一半，该对象就会进入到该区域。如果一个 H 区装不下一个巨型对象，那么 G1 会寻找连续的 H 分区来存储，为了能找到连续的 H 区，有时候不得不启动 Full GC G1 不会对巨型对象进行拷贝，回收时被优先考虑，G1 会跟踪老年代所有 incoming 引用，这样老年代 incoming 引用为 0 的巨型对象就可以在新生代垃圾回收时处理掉 Region 结构图： 空间整合： CMS：标记-清除算法、内存碎片、若干次 GC 后进行一次碎片整理 G1：整体来看是基于标记 - 整理算法实现的收集器，从局部（Region 之间）上来看是基于复制算法实现的，两种算法都可以避免内存碎片 可预测的停顿时间模型（软实时 soft real-time）：可以指定在 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒 由于分块的原因，G1 可以只选取部分区域进行内存回收，这样缩小了回收的范围，对于全局停顿情况也能得到较好的控制 G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间，通过过去回收的经验获得），在后台维护一个优先列表，每次根据允许的收集时间优先回收价值最大的 Region，保证了 G1 收集器在有限的时间内可以获取尽可能高的收集效率 相比于 CMS GC，G1 未必能做到 CMS 在最好情况下的延时停顿，但是最差情况要好很多 G1 垃圾收集器的缺点： 相较于 CMS，G1 还不具备全方位、压倒性优势。比如在用户程序运行过程中，G1 无论是为了垃圾收集产生的内存占用还是程序运行时的额外执行负载都要比 CMS 要高 从经验上来说，在小内存应用上 CMS 的表现大概率会优于 G1，而 G1 在大内存应用上则发挥其优势，平衡点在 6-8GB 之间 应用场景： 面向服务端应用，针对具有大内存、多处理器的机器 需要低 GC 延迟，并具有大堆的应用程序提供解决方案 记忆集记忆集 Remembered Set 在新生代中，每个 Region 都有一个 Remembered Set，用来被哪些其他 Region 里的对象引用（谁引用了我就记录谁） 程序对 Reference 类型数据写操作时，产生一个 Write Barrier 暂时中断操作，检查该对象和 Reference 类型数据是否在不同的 Region（跨代引用），不同就将相关引用信息记录到 Reference 类型所属的 Region 的 Remembered Set 之中 进行内存回收时，在 GC 根节点的枚举范围中加入 Remembered Set 即可保证不对全堆扫描也不会有遗漏 垃圾收集器在新生代中建立了记忆集这样的数据结构，可以理解为它是一个抽象类，具体实现记忆集的三种方式： 字长精度 对象精度 卡精度(卡表) 卡表（Card Table）在老年代中，是一种对记忆集的具体实现，主要定义了记忆集的记录精度、与堆内存的映射关系等，卡表中的每一个元素都对应着一块特定大小的内存块，这个内存块称之为卡页（card page），当存在跨代引用时，会将卡页标记为 dirty，JVM 对于卡页的维护也是通过写屏障的方式 收集集合 CSet 代表每次 GC 暂停时回收的一系列目标分区，在任意一次收集暂停中，CSet 所有分区都会被释放，内部存活的对象都会被转移到分配的空闲分区中。年轻代收集 CSet 只容纳年轻代分区，而混合收集会通过启发式算法，在老年代候选回收分区中，筛选出回收收益最高的分区添加到 CSet 中 CSet of Young Collection CSet of Mix Collection 工作原理G1 中提供了三种垃圾回收模式：YoungGC、Mixed GC 和 Full GC，在不同的条件下被触发 当堆内存使用达到一定值（默认 45%）时，开始老年代并发标记过程 标记完成马上开始混合回收过程 顺时针：Young GC → Young GC + Concurrent Mark → Mixed GC 顺序，进行垃圾回收 Young GC：发生在年轻代的 GC 算法，一般对象（除了巨型对象）都是在 eden region 中分配内存，当所有 eden region 被耗尽无法申请内存时，就会触发一次 Young GC，G1 停止应用程序的执行 STW，把活跃对象放入老年代，垃圾对象回收 回收过程： 扫描根：根引用连同 RSet 记录的外部引用作为扫描存活对象的入口 更新 RSet：处理 dirty card queue 更新 RS，此后 RSet 准确的反映对象的引用关系 dirty card queue：类似缓存，产生了引用先记录在这里，然后更新到 RSet 作用：产生引用直接更新 RSet 需要线程同步开销很大，使用队列性能好 处理 RSet：识别被老年代对象指向的 Eden 中的对象，这些被指向的对象被认为是存活的对象，把需要回收的分区放入 Young CSet 中进行回收 复制对象：Eden 区内存段中存活的对象会被复制到 survivor 区，survivor 区内存段中存活的对象如果年龄未达阈值，年龄会加1，达到阀值会被会被复制到 old 区中空的内存分段，如果 survivor 空间不够，Eden 空间的部分数据会直接晋升到老年代空间 处理引用：处理 Soft，Weak，Phantom，JNI Weak 等引用，最终 Eden 空间的数据为空，GC 停止工作 **Concurrent Mark **： 初始标记：标记从根节点直接可达的对象，这个阶段是 STW 的，并且会触发一次年轻代 GC 并发标记 (Concurrent Marking)：在整个堆中进行并发标记（应用程序并发执行），可能被 YoungGC 中断。会计算每个区域的对象活性，即区域中存活对象的比例，若区域中的所有对象都是垃圾，则这个区域会被立即回收（实时回收），给浮动垃圾准备出更多的空间，把需要收集的 Region 放入 CSet 当中 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中，这阶段需要停顿线程，但是可并行执行（防止漏标） 筛选回收：并发清理阶段，首先对 CSet 中各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划，也需要 STW Mixed GC：当很多对象晋升到老年代时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即 Mixed GC，除了回收整个 young region，还会回收一部分的 old region，过程同 YGC 注意：是一部分老年代，而不是全部老年代，可以选择哪些老年代 region 收集，对垃圾回收的时间进行控制 在 G1 中，Mixed GC 可以通过 -XX:InitiatingHeapOccupancyPercent 设置阈值 Full GC：对象内存分配速度过快，Mixed GC 来不及回收，导致老年代被填满，就会触发一次 Full GC，G1 的 Full GC 算法就是单线程执行的垃圾回收，会导致异常长时间的暂停时间，需要进行不断的调优，尽可能的避免 Full GC 产生 Full GC 的原因： 晋升时没有足够的空间存放晋升的对象 并发处理过程完成之前空间耗尽，浮动垃圾 相关参数 -XX:+UseG1GC：手动指定使用 G1 垃圾收集器执行内存回收任务 -XX:G1HeapRegionSize：设置每个 Region 的大小。值是 2 的幂，范围是 1MB 到 32MB 之间，目标是根据最小的 Java 堆大小划分出约 2048 个区域，默认是堆内存的 1&#x2F;2000 -XX:MaxGCPauseMillis：设置期望达到的最大 GC 停顿时间指标，JVM会尽力实现，但不保证达到，默认值是 200ms -XX:+ParallelGcThread：设置 STW 时 GC 线程数的值，最多设置为 8 -XX:ConcGCThreads：设置并发标记线程数，设置为并行垃圾回收线程数 ParallelGcThreads 的1&#x2F;4左右 -XX:InitiatingHeapoccupancyPercent：设置触发并发 Mixed GC 周期的 Java 堆占用率阈值，超过此值，就触发 GC，默认值是 45 -XX:+ClassUnloadingWithConcurrentMark：并发标记类卸载，默认启用，所有对象都经过并发标记后，就可以知道哪些类不再被使用，当一个类加载器的所有类都不再使用，则卸载它所加载的所有类 -XX:G1NewSizePercent：新生代占用整个堆内存的最小百分比（默认5％） -XX:G1MaxNewSizePercent：新生代占用整个堆内存的最大百分比（默认60％） -XX:G1ReservePercent=10：保留内存区域，防止 to space（Survivor中的 to 区）溢出 调优G1 的设计原则就是简化 JVM 性能调优，只需要简单的三步即可完成调优： 开启 G1 垃圾收集器 设置堆的最大内存 设置最大的停顿时间（STW） 不断调优暂停时间指标： XX:MaxGCPauseMillis=x 可以设置启动应用程序暂停的时间，G1会根据这个参数选择 CSet 来满足响应时间的设置 设置到 100ms 或者 200ms 都可以（不同情况下会不一样），但设置成50ms就不太合理 暂停时间设置的太短，就会导致出现 G1 跟不上垃圾产生的速度，最终退化成 Full GC 对这个参数的调优是一个持续的过程，逐步调整到最佳状态 不要设置新生代和老年代的大小： 避免使用 -Xmn 或 -XX:NewRatio 等相关选项显式设置年轻代大小，G1 收集器在运行的时候会调整新生代和老年代的大小，从而达到我们为收集器设置的暂停时间目标 设置了新生代大小相当于放弃了 G1 的自动调优，我们只需要设置整个堆内存的大小，剩下的交给 G1 自己去分配各个代的大小 ZGCZGC 收集器是一个可伸缩的、低延迟的垃圾收集器，基于 Region 内存布局的，不设分代，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的标记压缩算法 在 CMS 和 G1 中都用到了写屏障，而 ZGC 用到了读屏障 染色指针：直接将少量额外的信息存储在指针上的技术，从 64 位的指针中拿高 4 位来标识对象此时的状态 染色指针可以使某个 Region 的存活对象被移走之后，这个 Region 立即就能够被释放和重用 可以直接从指针中看到引用对象的三色标记状态（Marked0、Marked1）、是否进入了重分配集、是否被移动过（Remapped）、是否只能通过 finalize() 方法才能被访问到（Finalizable） 可以大幅减少在垃圾收集过程中内存屏障的使用数量，写屏障的目的通常是为了记录对象引用的变动情况，如果将这些信息直接维护在指针中，显然就可以省去一些专门的记录操作 可以作为一种可扩展的存储结构用来记录更多与对象标记、重定位过程相关的数据 内存多重映射：多个虚拟地址指向同一个物理地址 可并发的标记压缩算法：染色指针标识对象是否被标记或移动，读屏障保证在每次应用程序或 GC 程序访问对象时先根据染色指针的标识判断是否被移动，如果被移动就根据转发表访问新的移动对象，并更新引用，不会像 G1 一样必须等待垃圾回收完成才能访问 ZGC 目标： 停顿时间不会超过 10ms 停顿时间不会随着堆的增大而增大（不管多大的堆都能保持在 10ms 以下） 可支持几百 M，甚至几 T 的堆大小（最大支持4T） ZGC 的工作过程可以分为 4 个阶段： 并发标记（Concurrent Mark）： 遍历对象图做可达性分析的阶段，也要经过初始标记和最终标记，需要短暂停顿 并发预备重分配（Concurrent Prepare for Relocate）：根据特定的查询条件统计得出本次收集过程要清理哪些 Region，将这些 Region 组成重分配集（Relocation Set） 并发重分配（Concurrent Relocate）： 重分配是 ZGC 执行过程中的核心阶段，这个过程要把重分配集中的存活对象复制到新的 Region 上，并为重分配集中的每个 Region 维护一个转发表（Forward Table），记录从旧地址到新地址的转向关系 并发重映射（Concurrent Remap）：修正整个堆中指向重分配集中旧对象的所有引用，ZGC 的并发映射并不是一个必须要立即完成的任务，ZGC 很巧妙地把并发重映射阶段要做的工作，合并到下一次垃圾收集循环中的并发标记阶段里去完成，因为都是要遍历所有对象，这样合并节省了一次遍历的开销 ZGC 几乎在所有地方并发执行的，除了初始标记的是 STW 的，但这部分的实际时间是非常少的，所以响应速度快，在尽可能对吞吐量影响不大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟 优点：高吞吐量、低延迟 缺点：浮动垃圾，当 ZGC 准备要对一个很大的堆做一次完整的并发收集，其全过程要持续十分钟以上，由于应用的对象分配速率很高，将创造大量的新对象产生浮动垃圾 参考文章：https://www.cnblogs.com/jimoer/p/13170249.html 总结Serial GC、Parallel GC、Concurrent Mark Sweep GC 这三个 GC 不同： 最小化地使用内存和并行开销，选 Serial GC 最大化应用程序的吞吐量，选 Parallel GC 最小化 GC 的中断或停顿时间，选 CMS GC 内存泄漏泄露溢出内存泄漏（Memory Leak）：是指程序中已动态分配的堆内存由于某种原因程序未释放或无法释放，造成系统内存的浪费，导致程序运行速度减慢甚至系统崩溃等严重后果 可达性分析算法来判断对象是否是不再使用的对象，本质都是判断一个对象是否还被引用。由于代码的实现不同就会出现很多种内存泄漏问题，让 JVM 误以为此对象还在引用中，无法回收，造成内存泄漏 内存溢出（out of memory）指的是申请内存时，没有足够的内存可以使用 内存泄漏和内存溢出的关系：内存泄漏的越来越多，最终会导致内存溢出 几种情况静态集合静态集合类的生命周期与 JVM 程序一致，则容器中的对象在程序结束之前将不能被释放，从而造成内存泄漏。原因是长生命周期的对象持有短生命周期对象的引用，尽管短生命周期的对象不再使用，但是因为长生命周期对象持有它的引用而导致不能被回收 1234567public class MemoryLeak &#123; static List list = new ArrayList(); public void oomTest()&#123; Object obj = new Object();//局部变量 list.add(obj); &#125;&#125; 单例模式单例模式和静态集合导致内存泄露的原因类似，因为单例的静态特性，它的生命周期和 JVM 的生命周期一样长，所以如果单例对象持有外部对象的引用，那么这个外部对象也不会被回收，那么就会造成内存泄漏 内部类内部类持有外部类的情况，如果一个外部类的实例对象调用方法返回了一个内部类的实例对象，即使那个外部类实例对象不再被使用，但由于内部类持有外部类的实例对象，这个外部类对象也不会被回收，造成内存泄漏 连接相关数据库连接、网络连接和 IO 连接等，当不再使用时，需要显式调用 close 方法来释放与连接，垃圾回收器才会回收对应的对象，否则将会造成大量的对象无法被回收，从而引起内存泄漏 不合理域变量不合理的作用域，一个变量的定义的作用范围大于其使用范围，很有可能会造成内存泄漏；如果没有及时地把对象设置为 null，也有可能导致内存泄漏的发生 1234567public class UsingRandom &#123; private String msg; public void receiveMsg()&#123; msg = readFromNet();// 从网络中接受数据保存到 msg 中 saveDB(msg); // 把 msg 保存到数据库中 &#125;&#125; 通过 readFromNet 方法把接收消息保存在 msg 中，然后调用 saveDB 方法把内容保存到数据库中，此时 msg 已经可以被回收，但是 msg 的生命周期与对象的生命周期相同，造成 msg 不能回收，产生内存泄漏 解决： msg 变量可以放在 receiveMsg 方法内部，当方法使用完，msg 的生命周期也就结束，就可以被回收了 在使用完 msg 后，把 msg 设置为 null，这样垃圾回收器也会回收 msg 的内存空间。 改变哈希当一个对象被存储进 HashSet 集合中以后，就不能修改这个对象中的那些参与计算哈希值的字段，否则对象修改后的哈希值与最初存储进 HashSet 集合中时的哈希值不同，这种情况下使用该对象的当前引用作为的参数去 HashSet 集合中检索对象返回 false，导致无法从 HashSet 集合中单独删除当前对象，造成内存泄漏 缓存泄露内存泄漏的一个常见来源是缓存，一旦把对象引用放入到缓存中，就会很容易被遗忘 使用 WeakHashMap 代表缓存，当除了自身有对 key 的引用外没有其他引用，map 会自动丢弃此值 案例分析12345678910111213141516171819202122232425public class Stack &#123; private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() &#123; elements = new Object[DEFAULT_INITIAL_CAPACITY]; &#125; public void push(Object e) &#123; //入栈 ensureCapacity(); elements[size++] = e; &#125; public Object pop() &#123; //出栈 if (size == 0) throw new EmptyStackException(); return elements[--size]; &#125; private void ensureCapacity() &#123; if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); &#125;&#125; 程序并没有明显错误，但 pop 函数存在内存泄漏问题，因为 pop 函数只是把栈顶索引下移一位，并没有把上一个出栈索引处的引用置空，导致栈数组一直强引用着已经出栈的对象 解决方法： 1234567public Object pop() &#123; if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; return result;&#125; 类加载对象访存存储结构一个 Java 对象内存中存储为三部分：对象头（Header）、实例数据（Instance Data）和对齐填充 （Padding） 对象头： 普通对象：分为两部分 Mark Word：用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等等 12hash(25) + age(4) + lock(3) = 32bit #32位系统unused(25+1) + hash(31) + age(4) + lock(3) = 64bit\t#64位系统 Klass Word：类型指针，指向该对象的 Class 类对象的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例；在 64 位系统中，开启指针压缩（-XX:+UseCompressedOops）或者 JVM 堆的最大值小于 32G，这个指针也是 4byte，否则是 8byte（就是 Java 中的一个引用的大小） 12345|-----------------------------------------------------|| Object Header (64 bits) ||---------------------------|-------------------------|| Mark Word (32 bits)\t| Klass Word (32 bits) ||---------------------------|-------------------------| 数组对象：如果对象是一个数组，那在对象头中还有一块数据用于记录数组长度（12 字节） 12345|-------------------------------------------------------------------------------|| Object Header (96 bits) ||-----------------------|-----------------------------|-------------------------|| Mark Word(32bits) | Klass Word(32bits) | array length(32bits) ||-----------------------|-----------------------------|-------------------------| 实例数据：实例数据部分是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的，都需要记录起来 对齐填充：Padding 起占位符的作用。64 位系统，由于 HotSpot VM 的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，就是对象的大小必须是 8 字节的整数倍，而对象头部分正好是 8 字节的倍数（1 倍或者 2 倍），因此当对象实例数据部分没有对齐时，就需要通过对齐填充来补全 32 位系统： 一个 int 在 java 中占据 4byte，所以 Integer 的大小为： 1private final int value; 12# 需要补位4byte4(Mark Word) + 4(Klass Word) + 4(data) + 4(Padding) = 16byte int[] arr = new int[10] 12# 由于需要8位对齐，所以最终大小为56byte4(Mark Word) + 4(Klass Word) + 4(length) + 4*10(10个int大小) + 4(Padding) = 56sbyte 实际大小浅堆（Shallow Heap）：对象本身占用的内存，不包括内部引用对象的大小，32 位系统中一个对象引用占 4 个字节，每个对象头占用 8 个字节，根据堆快照格式不同，对象的大小会同 8 字节进行对齐 JDK7 中的 String：2个 int 值共占 8 字节，value 对象引用占用 4 字节，对象头 8 字节，对齐后占 24 字节，为 String 对象的浅堆大小，与 value 实际取值无关，无论字符串长度如何，浅堆大小始终是 24 字节 123private final char value[];private int hash;private int hash32; 保留集（Retained Set）：对象 A 的保留集指当对象 A 被垃圾回收后，可以被释放的所有的对象集合（包括 A 本身），所以对象 A 的保留集就是只能通过对象 A 被直接或间接访问到的所有对象的集合，就是仅被对象 A 所持有的对象的集合 深堆（Retained Heap）：指对象的保留集中所有的对象的浅堆大小之和，一个对象的深堆指只能通过该对象访问到的（直接或间接）所有对象的浅堆之和，即对象被回收后，可以释放的真实空间 对象的实际大小：一个对象所能触及的所有对象的浅堆大小之和，也就是通常意义上我们说的对象大小 下图显示了一个简单的对象引用关系图，对象 A 引用了 C 和 D，对象 B 引用了 C 和 E。那么对象 A 的浅堆大小只是 A 本身，A 的实际大小为 A、C、D 三者之和，A 的深堆大小为 A 与 D 之和，由于对象 C 还可以通过对象 B 访问到 C，因此 C 不在对象 A 的深堆范围内 内存分析工具 MAT 提供了一种叫支配树的对象图，体现了对象实例间的支配关系 基本性质： 对象 A 的子树（所有被对象 A 支配的对象集合）表示对象 A 的保留集（retained set），即深堆 如果对象 A 支配对象 B，那么对象 A 的直接支配者也支配对象 B 支配树的边与对象引用图的边不直接对应 左图表示对象引用图，右图表示左图所对应的支配树： 比如：对象 F 与对象 D 相互引用，因为到对象 F 的所有路径必然经过对象 D，因此对象 D 是对象 F 的直接支配者 参考文章：https://www.yuque.com/u21195183/jvm/nkq31c 节约内存 尽量使用基本数据类型 满足容量前提下，尽量用小字段 尽量用数组，少用集合，数组中是可以使用基本类型的，但是集合中只能放包装类型，如果需要使用集合，推荐比较节约内存的集合工具：fastutil 一个 ArrayList 集合，如果里面放了 10 个数字，占用多少内存： 12private transient Object[] elementData;private int size; Mark Word 占 4byte，Klass Word 占 4byte，一个 int 字段占 4byte，elementData 数组占 12byte，数组中 10 个 Integer 对象占 10×16，所以整个集合空间大小为 184byte（深堆） 时间用 long&#x2F;int 表示，不用 Date 或者 String 对象访问JVM 是通过栈帧中的对象引用访问到其内部的对象实例： 句柄访问：Java 堆中会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据和类型数据各自的具体地址信息 优点：reference 中存储的是稳定的句柄地址，在对象被移动（垃圾收集）时只会改变句柄中的实例数据指针，而 reference 本身不需要被修改 直接指针（HotSpot 采用）：Java 堆对象的布局必须考虑如何放置访问类型数据的相关信息，reference 中直接存储的对象地址 优点：速度更快，节省了一次指针定位的时间开销 缺点：对象被移动时（如进行 GC 后的内存重新排列），对象的 reference 也需要同步更新 参考文章：https://www.cnblogs.com/afraidToForget/p/12584866.html 对象创建生命周期在 Java 中，对象的生命周期包括以下几个阶段： 创建阶段 (Created)： 应用阶段 (In Use)：对象至少被一个强引用持有着 不可见阶段 (Invisible)：程序的执行已经超出了该对象的作用域，不再持有该对象的任何强引用 不可达阶段 (Unreachable)：该对象不再被任何强引用所持有，包括 GC Root 的强引用 收集阶段 (Collected)：垃圾回收器对该对象的内存空间重新分配做好准备，该对象如果重写了 finalize() 方法，则会去执行该方法 终结阶段 (Finalized)：等待垃圾回收器对该对象空间进行回收，当对象执行完 finalize() 方法后仍然处于不可达状态时进入该阶段 对象空间重分配阶段 (De-allocated)：垃圾回收器对该对象的所占用的内存空间进行回收或者再分配 参考文章：https://blog.csdn.net/sodino/article/details/38387049 创建时机类在第一次实例化加载一次，后续实例化不再加载，引用第一次加载的类 Java 对象创建时机： 使用 new 关键字创建对象：由执行类实例创建表达式而引起的对象创建 使用 Class 类的 newInstance 方法（反射机制） 使用 Constructor 类的 newInstance 方法（反射机制） 12345678910public class Student &#123; private int id; public Student(Integer id) &#123; this.id = id; &#125; public static void main(String[] args) throws Exception &#123; Constructor&lt;Student&gt; c = Student.class.getConstructor(Integer.class); Student stu = c.newInstance(123); &#125;&#125; 使用 newInstance 方法的这两种方式创建对象使用的就是 Java 的反射机制，事实上 Class 的 newInstance 方法内部调用的也是 Constructor 的 newInstance 方法 使用 Clone 方法创建对象：用 clone 方法创建对象的过程中并不会调用任何构造函数，要想使用 clone 方法，我们就必须先实现 Cloneable 接口并实现其定义的 clone 方法 使用（反）序列化机制创建对象：当反序列化一个对象时，JVM 会创建一个单独的对象，在此过程中，JVM 并不会调用任何构造函数，为了反序列化一个对象，需要让类实现 Serializable 接口 从 Java 虚拟机层面看，除了使用 new 关键字创建对象的方式外，其他方式全部都是通过转变为 invokevirtual 指令直接创建对象的 创建过程创建对象的过程： 判断对象对应的类是否加载、链接、初始化 为对象分配内存：指针碰撞、空闲链表。当一个对象被创建时，虚拟机就会为其分配内存来存放对象的实例变量及其从父类继承过来的实例变量，即使从隐藏变量也会被分配空间（继承部分解释了为什么会隐藏） 处理并发安全问题： 采用 CAS 配上自旋保证更新的原子性 每个线程预先分配一块 TLAB 初始化分配的空间：虚拟机将分配到的内存空间都初始化为零值（不包括对象头），保证对象实例字段在不赋值时可以直接使用，程序能访问到这些字段的数据类型所对应的零值 设置对象的对象头：将对象的所属类（类的元数据信息）、对象的 HashCode、对象的 GC 信息、锁信息等数据存储在对象头中 执行 init 方法进行实例化：实例变量初始化、实例代码块初始化 、构造函数初始化 实例变量初始化与实例代码块初始化： 对实例变量直接赋值或者使用实例代码块赋值，编译器会将其中的代码放到类的构造函数中去，并且这些代码会被放在对超类构造函数的调用语句之后（Java 要求构造函数的第一条语句必须是超类构造函数的调用语句），构造函数本身的代码之前 构造函数初始化： Java 要求在实例化类之前，必须先实例化其超类，以保证所创建实例的完整性，在准备实例化一个类的对象前，首先准备实例化该类的父类，如果该类的父类还有父类，那么准备实例化该类的父类的父类，依次递归直到递归到 Object 类。然后从 Object 类依次对以下各类进行实例化，初始化父类中的变量和执行构造函数 承上启下 一个实例变量在对象初始化的过程中会被赋值几次？一个实例变量最多可以被初始化 4 次 JVM 在为一个对象分配完内存之后，会给每一个实例变量赋予默认值，这个实例变量被第一次赋值；在声明实例变量的同时对其进行了赋值操作，那么这个实例变量就被第二次赋值；在实例代码块中又对变量做了初始化操作，那么这个实例变量就被第三次赋值；；在构造函数中也对变量做了初始化操作，那么这个实例变量就被第四次赋值 类的初始化过程与类的实例化过程的异同？ 类的初始化是指类加载过程中的初始化阶段对类变量按照代码进行赋值的过程；类的实例化是指在类完全加载到内存中后创建对象的过程（类的实例化触发了类的初始化，先初始化才能实例化） 假如一个类还未加载到内存中，那么在创建一个该类的实例时，具体过程是怎样的？（经典案例） 123456789101112131415161718192021222324252627282930313233public class StaticTest &#123; public static void main(String[] args) &#123; staticFunction();//调用静态方法，触发初始化 &#125; static StaticTest st = new StaticTest(); static &#123; //静态代码块 System.out.println(&quot;1&quot;); &#125; &#123; // 实例代码块 System.out.println(&quot;2&quot;); &#125; StaticTest() &#123; // 实例构造器 System.out.println(&quot;3&quot;); System.out.println(&quot;a=&quot; + a + &quot;,b=&quot; + b); &#125; public static void staticFunction() &#123; // 静态方法 System.out.println(&quot;4&quot;); &#125; int a = 110; // 实例变量 static int b = 112; // 静态变量&#125;/* Output: 2 3 a=110,b=0 1 4 *///:~ static StaticTest st = new StaticTest();： 实例实例化不一定要在类初始化结束之后才开始 在同一个类加载器下，一个类型只会被初始化一次。所以一旦开始初始化一个类，无论是否完成后续都不会再重新触发该类型的初始化阶段了（只考虑在同一个类加载器下的情形）。因此在实例化上述程序中的 st 变量时，实际上是把实例化嵌入到了静态初始化流程中，并且在上面的程序中，嵌入到了静态初始化的起始位置，这就导致了实例初始化完全发生在静态初始化之前，这也是导致 a 为 110，b 为 0 的原因 代码等价于： 1234567891011public class StaticTest &#123; &lt;clinit&gt;()&#123; a = 110; // 实例变量 System.out.println(&quot;2&quot;);\t// 实例代码块 System.out.println(&quot;3&quot;);\t// 实例构造器中代码的执行 System.out.println(&quot;a=&quot; + a + &quot;,b=&quot; + b); // 实例构造器中代码的执行 类变量st被初始化 System.out.println(&quot;1&quot;);\t//静态代码块 类变量b被初始化为112 &#125;&#125; 加载过程生命周期类是在运行期间第一次使用时动态加载的（不使用不加载），而不是一次性加载所有类，因为一次性加载会占用很多的内存，加载的类信息存放于一块成为方法区的内存空间 包括 7 个阶段： 加载（Loading） 链接：验证（Verification）、准备（Preparation）、解析（Resolution） 初始化（Initialization） 使用（Using） 卸载（Unloading） 加载阶段加载是类加载的其中一个阶段，注意不要混淆 加载过程完成以下三件事： 通过类的完全限定名称获取定义该类的二进制字节流（二进制字节码） 将该字节流表示的静态存储结构转换为方法区的运行时存储结构（Java 类模型） 将字节码文件加载至方法区后，在堆中生成一个代表该类的 Class 对象，作为该类在方法区中的各种数据的访问入口 其中二进制字节流可以从以下方式中获取： 从 ZIP 包读取，成为 JAR、EAR、WAR 格式的基础 从网络中获取，最典型的应用是 Applet 由其他文件生成，例如由 JSP 文件生成对应的 Class 类 运行时计算生成，例如动态代理技术，在 java.lang.reflect.Proxy 使用 ProxyGenerator.generateProxyClass 生成字节码 方法区内部采用 C++ 的 instanceKlass 描述 Java 类的数据结构： _java_mirror 即 Java 的类镜像，例如对 String 来说就是 String.class，作用是把 class 暴露给 Java 使用 _super 即父类、_fields 即成员变量、_methods 即方法、_constants 即常量池、_class_loader 即类加载器、_vtable 虚方法表、_itable 接口方法表 加载过程： 如果这个类还有父类没有加载，先加载父类 加载和链接可能是交替运行的 Class 对象和 _java_mirror 相互持有对方的地址，堆中对象通过 instanceKlass 和元空间进行交互 创建数组类有些特殊，因为数组类本身并不是由类加载器负责创建，而是由 JVM 在运行时根据需要而直接创建的，但数组的元素类型仍然需要依靠类加载器去创建，创建数组类的过程： 如果数组的元素类型是引用类型，那么遵循定义的加载过程递归加载和创建数组的元素类型 JVM 使用指定的元素类型和数组维度来创建新的数组类 基本数据类型由启动类加载器加载 链接阶段验证确保 Class 文件的字节流中包含的信息是否符合 JVM 规范，保证被加载类的正确性，不会危害虚拟机自身的安全 主要包括四种验证： 文件格式验证 语义检查，但凡在语义上不符合规范的，虚拟机不会给予验证通过 是否所有的类都有父类的存在（除了 Object 外，其他类都应该有父类） 是否一些被定义为 final 的方法或者类被重写或继承了 非抽象类是否实现了所有抽象方法或者接口方法 是否存在不兼容的方法 字节码验证，试图通过对字节码流的分析，判断字节码是否可以被正确地执行 在字节码的执行过程中，是否会跳转到一条不存在的指令 函数的调用是否传递了正确类型的参数 变量的赋值是不是给了正确的数据类型 栈映射帧（StackMapTable）在这个阶段用于检测在特定的字节码处，其局部变量表和操作数栈是否有着正确的数据类型 符号引用验证，Class 文件在其常量池会通过字符串记录将要使用的其他类或者方法 准备准备阶段为静态变量（类变量）分配内存并设置初始值，使用的是方法区的内存： 说明：实例变量不会在这阶段分配内存，它会在对象实例化时随着对象一起被分配在堆中，类加载发生在所有实例化操作之前，并且类加载只进行一次，实例化可以进行多次 类变量初始化： static 变量分配空间和赋值是两个步骤：分配空间在准备阶段完成，赋值在初始化阶段完成 如果 static 变量是 final 的基本类型以及字符串常量，那么编译阶段值（方法区）就确定了，准备阶段会显式初始化 如果 static 变量是 final 的，但属于引用类型或者构造器方法的字符串，赋值在初始化阶段完成 实例： 初始值一般为 0 值，例如下面的类变量 value 被初始化为 0 而不是 123： 1public static int value = 123; 常量 value 被初始化为 123 而不是 0： 1public static final int value = 123; Java 并不支持 boolean 类型，对于 boolean 类型，内部实现是 int，由于 int 的默认值是 0，故 boolean 的默认值就是 false 解析将常量池中类、接口、字段、方法的符号引用替换为直接引用（内存地址）的过程： 符号引用：一组符号来描述目标，可以是任何字面量，属于编译原理方面的概念，如：包括类和接口的全限名、字段的名称和描述符、方法的名称和方法描述符（因为类还没有加载完，很多方法是找不到的） 直接引用：直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄，如果有了直接引用，那说明引用的目标必定已经存在于内存之中 例如：在 com.demo.Solution 类中引用了 com.test.Quest，把 com.test.Quest 作为符号引用存进类常量池，在类加载完后，用这个符号引用去方法区找这个类的内存地址 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等 在类加载阶段解析的是非虚方法，静态绑定 也可以在初始化阶段之后再开始解析，这是为了支持 Java 的动态绑定 通过解析操作，符号引用就可以转变为目标方法在类的虚方法表中的位置，从而使得方法被成功调用 123456789101112131415public class Load2 &#123; public static void main(String[] args) throws Exception&#123; ClassLoader classloader = Load2.class.getClassLoader(); // cloadClass 加载类方法不会导致类的解析和初始化，也不会加载D Class&lt;?&gt; c = classloader.loadClass(&quot;cn.jvm.t3.load.C&quot;); // new C();会导致类的解析和初始化，从而解析初始化D System.in.read(); &#125;&#125;class C &#123;\tD d = new D();&#125;class D &#123;&#125; 初始化介绍初始化阶段才真正开始执行类中定义的 Java 程序代码，在准备阶段，类变量已经赋过一次系统要求的初始值；在初始化阶段，通过程序制定的计划去初始化类变量和其它资源，执行 在编译生成 class 文件时，编译器会产生两个方法加于 class 文件中，一个是类的初始化方法 clinit，另一个是实例的初始化方法 init 类构造器 () 与实例构造器 () 不同，它不需要程序员进行显式调用，在一个类的生命周期中，类构造器最多被虚拟机调用一次，而实例构造器则会被虚拟机调用多次，只要程序员创建对象 类在第一次实例化加载一次，把 class 读入内存，后续实例化不再加载，引用第一次加载的类 clinit()：类构造器，由编译器自动收集类中所有类变量的赋值动作和静态语句块中的语句合并产生的 作用：是在类加载过程中的初始化阶段进行静态变量初始化和执行静态代码块 如果类中没有静态变量或静态代码块，那么 clinit 方法将不会被生成 clinit 方法只执行一次，在执行 clinit 方法时，必须先执行父类的clinit方法 static 变量的赋值操作和静态代码块的合并顺序由源文件中出现的顺序决定 static 不加 final 的变量都在初始化环节赋值 线程安全问题： 虚拟机会保证一个类的 () 方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的 () 方法，其它线程都阻塞等待，直到活动线程执行 () 方法完毕 如果在一个类的 () 方法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽 特别注意：静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问 1234567public class Test &#123; static &#123; //i = 0; // 给变量赋值可以正常编译通过 System.out.print(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 () 方法，两者不同的是： 在初始化一个接口时，并不会先初始化它的父接口，所以执行接口的 () 方法不需要先执行父接口的 () 方法 在初始化一个类时，不会先初始化所实现的接口，所以接口的实现类在初始化时不会执行接口的 () 方法 只有当父接口中定义的变量使用时，父接口才会初始化 时机类的初始化是懒惰的，只有在首次使用时才会被装载，JVM 不会无条件地装载 Class 类型，Java 虚拟机规定，一个类或接口在初次使用前，必须要进行初始化 主动引用：虚拟机规范中并没有强制约束何时进行加载，但是规范严格规定了有且只有下列情况必须对类进行初始化（加载、验证、准备都会发生）： 当创建一个类的实例时，使用 new 关键字，或者通过反射、克隆、反序列化（前文讲述的对象的创建时机） 当调用类的静态方法或访问静态字段时，遇到 getstatic、putstatic、invokestatic 这三条字节码指令，如果类没有进行过初始化，则必须先触发其初始化 getstatic：程序访问类的静态变量（不是静态常量，常量会被加载到运行时常量池） putstatic：程序给类的静态变量赋值 invokestatic ：调用一个类的静态方法 使用 java.lang.reflect 包的方法对类进行反射调用时，如果类没有进行初始化，则需要先触发其初始化 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化，但这条规则并不适用于接口 当虚拟机启动时，需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类 MethodHandle 和 VarHandle 可以看作是轻量级的反射调用机制，而要想使用这两个调用， 就必须先使用 findStaticVarHandle 来初始化要调用的类 补充：当一个接口中定义了 JDK8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化 被动引用：所有引用类的方式都不会触发初始化，称为被动引用 通过子类引用父类的静态字段，不会导致子类初始化，只会触发父类的初始化 通过数组定义来引用类，不会触发此类的初始化。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法 常量（final 修饰）在编译阶段会存入调用类的常量池中，本质上没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化 调用 ClassLoader 类的 loadClass() 方法加载一个类，并不是对类的主动使用，不会导致类的初始化 initinit 指的是实例构造器，主要作用是在类实例化过程中执行，执行内容包括成员变量初始化和代码块的执行 实例化即调用 ()V ，虚拟机会保证这个类的构造方法的线程安全，先为实例变量分配内存空间，再执行赋默认值，然后根据源码中的顺序执行赋初值或代码块，没有成员变量初始化和代码块则不会执行 类实例化过程：父类的类构造器() -&gt; 子类的类构造器() -&gt; 父类的成员变量和实例代码块 -&gt; 父类的构造函数 -&gt; 子类的成员变量和实例代码块 -&gt; 子类的构造函数 new 关键字会创建对象并复制 dup 一个对象引用，一个调用 方法，另一个用来赋值给接收者 卸载阶段时机：执行了 System.exit() 方法，程序正常执行结束，程序在执行过程中遇到了异常或错误而异常终止，由于操作系统出现错误而导致Java 虚拟机进程终止 卸载类即该类的 Class 对象被 GC，卸载类需要满足3个要求: 该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象 该类没有在其他任何地方被引用 该类的类加载器的实例已被 GC，一般是可替换类加载器的场景，如 OSGi、JSP 的重加载等，很难达成 在 JVM 生命周期类，由 JVM 自带的类加载器加载的类是不会被卸载的，自定义的类加载器加载的类是可能被卸载。因为 JVM 会始终引用启动、扩展、系统类加载器，这些类加载器始终引用它们所加载的类，这些类始终是可及的 类加载器类加载类加载方式： 隐式加载：不直接在代码中调用 ClassLoader 的方法加载类对象 创建类对象、使用类的静态域、创建子类对象、使用子类的静态域 在 JVM 启动时，通过三大类加载器加载 class 显式加载： ClassLoader.loadClass(className)：只加载和连接，不会进行初始化 Class.forName(String name, boolean initialize, ClassLoader loader)：使用 loader 进行加载和连接，根据参数 initialize 决定是否初始化 类的唯一性： 在 JVM 中表示两个 class 对象判断为同一个类存在的两个必要条件： 类的完整类名必须一致，包括包名 加载这个类的 ClassLoader（指 ClassLoader 实例对象）必须相同 这里的相等，包括类的 Class 对象的 equals() 方法、isAssignableFrom() 方法、isInstance() 方法的返回结果为 true，也包括使用 instanceof 关键字做对象所属关系判定结果为 true 命名空间： 每个类加载器都有自己的命名空间，命名空间由该加载器及所有的父加载器所加载的类组成 在同一命名空间中，不会出现类的完整名字（包括类的包名）相同的两个类 基本特征： 可见性，子类加载器可以访问父加载器加载的类型，但是反过来是不允许的 单一性，由于父加载器的类型对于子加载器是可见的，所以父加载器中加载过的类型，不会在子加载器中重复加载 加载器类加载器是 Java 的核心组件，用于加载字节码到 JVM 内存，得到 Class 类的对象 从 Java 虚拟机规范来讲，只存在以下两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader）：使用 C++ 实现，是虚拟机自身的一部分 自定义类加载器（User-Defined ClassLoader）：Java 虚拟机规范将所有派生于抽象类 ClassLoader 的类加载器都划分为自定义类加载器，使用 Java 语言实现，独立于虚拟机 从 Java 开发人员的角度看： 启动类加载器（Bootstrap ClassLoader）： 处于安全考虑，Bootstrap 启动类加载器只加载包名为 java、javax、sun 等开头的类 类加载器负责加载在 JAVA_HOME/jre/lib 或 sun.boot.class.path 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的类，并且是虚拟机识别的类库加载到虚拟机内存中 仅按照文件名识别，如 rt.jar 名字不符合的类库即使放在 lib 目录中也不会被加载 启动类加载器无法被 Java 程序直接引用，编写自定义类加载器时，如果要把加载请求委派给启动类加载器，直接使用 null 代替 扩展类加载器（Extension ClassLoader）： 由 ExtClassLoader (sun.misc.Launcher$ExtClassLoader) 实现，上级为 Bootstrap，显示为 null 将 JAVA_HOME/jre/lib/ext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中 开发者可以使用扩展类加载器，创建的 JAR 放在此目录下，会由扩展类加载器自动加载 应用程序类加载器（Application ClassLoader）： 由 AppClassLoader(sun.misc.Launcher$AppClassLoader) 实现，上级为 Extension 负责加载环境变量 classpath 或系统属性 java.class.path 指定路径下的类库 这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，因此称为系统类加载器 可以直接使用这个类加载器，如果应用程序中没有自定义类加载器，这个就是程序中默认的类加载器 自定义类加载器：由开发人员自定义的类加载器，上级是 Application 12345678910111213141516171819202122public static void main(String[] args) &#123; //获取系统类加载器 ClassLoader systemClassLoader = ClassLoader.getSystemClassLoader(); System.out.println(systemClassLoader);//sun.misc.Launcher$AppClassLoader@18b4aac2 //获取其上层 扩展类加载器 ClassLoader extClassLoader = systemClassLoader.getParent(); System.out.println(extClassLoader);//sun.misc.Launcher$ExtClassLoader@610455d6 //获取其上层 获取不到引导类加载器 ClassLoader bootStrapClassLoader = extClassLoader.getParent(); System.out.println(bootStrapClassLoader);//null //对于用户自定义类来说：使用系统类加载器进行加载 ClassLoader classLoader = ClassLoaderTest.class.getClassLoader(); System.out.println(classLoader);//sun.misc.Launcher$AppClassLoader@18b4aac2 //String 类使用引导类加载器进行加载的 --&gt; java核心类库都是使用启动类加载器加载的 ClassLoader classLoader1 = String.class.getClassLoader(); System.out.println(classLoader1);//null&#125; 补充两个类加载器： SecureClassLoader 扩展了 ClassLoader，新增了几个与使用相关的代码源和权限定义类验证（对 class 源码的访问权限）的方法，一般不会直接跟这个类打交道，更多是与它的子类 URLClassLoader 有所关联 ClassLoader 是一个抽象类，很多方法是空的没有实现，而 URLClassLoader 这个实现类为这些方法提供了具体的实现，并新增了 URLClassPath 类协助取得 Class 字节流等功能。在编写自定义类加载器时，如果没有太过于复杂的需求，可以直接继承 URLClassLoader 类，这样就可以避免去编写 findClass() 方法及其获取字节码流的方式，使自定义类加载器编写更加简洁 常用APIClassLoader 类，是一个抽象类，其后所有的类加载器都继承自 ClassLoader（不包括启动类加载器） 获取 ClassLoader 的途径： 获取当前类的 ClassLoader：clazz.getClassLoader() 获取当前线程上下文的 ClassLoader：Thread.currentThread.getContextClassLoader() 获取系统的 ClassLoader：ClassLoader.getSystemClassLoader() 获取调用者的 ClassLoader：DriverManager.getCallerClassLoader() ClassLoader 类常用方法： getParent()：返回该类加载器的超类加载器 loadclass(String name)：加载名为 name 的类，返回结果为 Class 类的实例，该方法就是双亲委派模式 findclass(String name)：查找二进制名称为 name 的类，返回结果为 Class 类的实例，该方法会在检查完父类加载器之后被 loadClass() 方法调用 findLoadedClass(String name)：查找名称为 name 的已经被加载过的类，final 修饰无法重写 defineClass(String name, byte[] b, int off, int len)：将字节流解析成 JVM 能够识别的类对象 resolveclass(Class&lt;?&gt; c)：链接指定的 Java 类，可以使类的 Class 对象创建完成的同时也被解析 InputStream getResourceAsStream(String name)：指定资源名称获取输入流 加载模型加载机制在 JVM 中，对于类加载模型提供了三种，分别为全盘加载、双亲委派、缓存机制 全盘加载：当一个类加载器负责加载某个 Class 时，该 Class 所依赖和引用的其他 Class 也将由该类加载器负责载入，除非显示指定使用另外一个类加载器来载入 双亲委派：某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父加载器，依次递归，如果父加载器可以完成类加载任务，就成功返回；只有当父加载器无法完成此加载任务时，才自己去加载 缓存机制：会保证所有加载过的 Class 都会被缓存，当程序中需要使用某个 Class 时，类加载器先从缓存区中搜寻该 Class，只有当缓存区中不存在该 Class 对象时，系统才会读取该类对应的二进制数据，并将其转换成 Class 对象存入缓冲区（方法区）中 这就是修改了 Class 后，必须重新启动 JVM，程序所做的修改才会生效的原因 双亲委派双亲委派模型（Parents Delegation Model）：该模型要求除了顶层的启动类加载器外，其它类加载器都要有父类加载器，这里的父子关系一般通过组合关系（Composition）来实现，而不是继承关系（Inheritance） 工作过程：一个类加载器首先将类加载请求转发到父类加载器，只有当父类加载器无法完成时才尝试自己加载 双亲委派机制的优点： 可以避免某一个类被重复加载，当父类已经加载后则无需重复加载，保证全局唯一性 Java 类随着它的类加载器一起具有一种带有优先级的层次关系，从而使得基础类得到统一 保护程序安全，防止类库的核心 API 被随意篡改 例如：在工程中新建 java.lang 包，接着在该包下新建 String 类，并定义 main 函数 12345public class String &#123; public static void main(String[] args) &#123; System.out.println(&quot;demo info&quot;); &#125;&#125; 此时执行 main 函数会出现异常，在类 java.lang.String 中找不到 main 方法。因为双亲委派的机制，java.lang.String 的在启动类加载器（Bootstrap）得到加载，启动类加载器优先级更高，在核心 jre 库中有其相同名字的类文件，但该类中并没有 main 方法 双亲委派机制的缺点：检查类是否加载的委托过程是单向的，这个方式虽然从结构上看比较清晰，使各个 ClassLoader 的职责非常明确，但顶层的 ClassLoader 无法访问底层的 ClassLoader 所加载的类（可见性） 源码分析1234567891011121314151617181920212223242526272829303132333435363738394041protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // 调用当前类加载器的 findLoadedClass(name)，检查当前类加载器是否已加载过指定 name 的类 Class c = findLoadedClass(name); // 当前类加载器如果没有加载过 if (c == null) &#123; long t0 = System.nanoTime(); try &#123; // 判断当前类加载器是否有父类加载器 if (parent != null) &#123; // 如果当前类加载器有父类加载器，则调用父类加载器的 loadClass(name,false) // 父类加载器的 loadClass 方法，又会检查自己是否已经加载过 c = parent.loadClass(name, false); &#125; else &#123; // 当前类加载器没有父类加载器，说明当前类加载器是 BootStrapClassLoader // 则调用 BootStrap ClassLoader 的方法加载类 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; &#125; if (c == null) &#123; // 如果调用父类的类加载器无法对类进行加载，则用自己的 findClass() 方法进行加载 // 可以自定义 findClass() 方法 long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; // 链接指定的 Java 类，可以使类的 Class 对象创建完成的同时也被解析 resolveClass(c); &#125; return c; &#125;&#125; 破坏委派双亲委派模型并不是一个具有强制性约束的模型，而是 Java 设计者推荐给开发者的类加载器实现方式 破坏双亲委派模型的方式： 自定义 ClassLoader 如果不想破坏双亲委派模型，只需要重写 findClass 方法 如果想要去破坏双亲委派模型，需要去**重写 loadClass **方法 引入线程上下文类加载器 Java 提供了很多服务提供者接口（Service Provider Interface，SPI），允许第三方为这些接口提供实现。常见的有 JDBC、JCE、JNDI 等。这些 SPI 接口由 Java 核心库来提供，而 SPI 的实现代码则是作为 Java 应用所依赖的 jar 包被包含进类路径 classpath 里，SPI 接口中的代码需要加载具体的实现类： SPI 的接口是 Java 核心库的一部分，是由引导类加载器来加载的 SPI 的实现类是由系统类加载器加载，引导类加载器是无法找到 SPI 的实现类，因为双亲委派模型中 BootstrapClassloader 无法委派 AppClassLoader 来加载类 JDK 开发人员引入了线程上下文类加载器（Thread Context ClassLoader），这种类加载器可以通过 Thread 类的 setContextClassLoader 方法进行设置线程上下文类加载器，在执行线程中抛弃双亲委派加载模式，使程序可以逆向使用类加载器，使 Bootstrap 加载器拿到了 Application 加载器加载的类，破坏了双亲委派模型 实现程序的动态性，如代码热替换（Hot Swap）、模块热部署（Hot Deployment） IBM 公司主导的 JSR一291（OSGiR4.2）实现模块化热部署的关键是它自定义的类加载器机制的实现，每一个程序模块（OSGi 中称为 Bundle）都有一个自己的类加载器，当更换一个 Bundle 时，就把 Bundle 连同类加载器一起换掉以实现代码的热替换，在 OSGi 环境下，类加载器不再双亲委派模型推荐的树状结构，而是进一步发展为更加复杂的网状结构 当收到类加载请求时，OSGi 将按照下面的顺序进行类搜索: 将以 java.* 开头的类，委派给父类加载器加载 否则，将委派列表名单内的类，委派给父类加载器加载 否则，将 Import 列表中的类，委派给 Export 这个类的 Bundle 的类加载器加载 否则，查找当前 Bundle 的 ClassPath，使用自己的类加载器加载 否则，查找类是否在自己的 Fragment Bundle 中，如果在就委派给 Fragment Bundle 类加载器加载 否则，查找 Dynamic Import 列表的 Bundle，委派给对应 Bundle 的类加载器加载 否则，类查找失败 热替换是指在程序的运行过程中，不停止服务，只通过替换程序文件来修改程序的行为，热替换的关键需求在于服务不能中断，修改必须立即表现正在运行的系统之中 沙箱机制沙箱机制（Sandbox）：将 Java 代码限定在虚拟机特定的运行范围中，并且严格限制代码对本地系统资源访问，来保证对代码的有效隔离，防止对本地系统造成破坏 沙箱限制系统资源访问，包括 CPU、内存、文件系统、网络，不同级别的沙箱对资源访问的限制也不一样 JDK1.0：Java 中将执行程序分成本地代码和远程代码两种，本地代码默认视为可信任的，而远程代码被看作是不受信的。对于授信的本地代码，可以访问一切本地资源，而对于非授信的远程代码不可以访问本地资源，其实依赖于沙箱机制。如此严格的安全机制也给程序的功能扩展带来障碍，比如当用户希望远程代码访问本地系统的文件时候，就无法实现 JDK1.1：针对安全机制做了改进，增加了安全策略。允许用户指定代码对本地资源的访问权限 JDK1.2：改进了安全机制，增加了代码签名，不论本地代码或是远程代码都会按照用户的安全策略设定，由类加载器加载到虚拟机中权限不同的运行空间，来实现差异化的代码执行权限控制 JDK1.6：当前最新的安全机制，引入了域（Domain）的概念。虚拟机会把所有代码加载到不同的系统域和应用域，不同的保护域对应不一样的权限。系统域部分专门负责与关键资源进行交互，而各个应用域部分则通过系统域的部分代理来对各种需要的资源进行访问 自定义对于自定义类加载器的实现，只需要继承 ClassLoader 类，覆写 findClass 方法即可 作用：隔离加载类、修改类加载的方式、拓展加载源、防止源码泄漏 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//自定义类加载器，读取指定的类路径classPath下的class文件public class MyClassLoader extends ClassLoader&#123; private String classPath; public MyClassLoader(String classPath) &#123; this.classPath = classPath; &#125; public MyClassLoader(ClassLoader parent, String byteCodePath) &#123; super(parent); this.classPath = classPath; &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; BufferedInputStream bis = null; ByteArrayOutputStream baos = null; try &#123; // 获取字节码文件的完整路径 String fileName = classPath + className + &quot;.class&quot;; // 获取一个输入流 bis = new BufferedInputStream(new FileInputStream(fileName)); // 获取一个输出流 baos = new ByteArrayOutputStream(); // 具体读入数据并写出的过程 int len; byte[] data = new byte[1024]; while ((len = bis.read(data)) != -1) &#123; baos.write(data, 0, len); &#125; // 获取内存中的完整的字节数组的数据 byte[] byteCodes = baos.toByteArray(); // 调用 defineClass()，将字节数组的数据转换为 Class 的实例。 Class clazz = defineClass(null, byteCodes, 0, byteCodes.length); return clazz; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (baos != null) baos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; if (bis != null) bis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null; &#125;&#125; 123456789101112public static void main(String[] args) &#123; MyClassLoader loader = new MyClassLoader(&quot;D:\\Workspace\\Project\\JVM_study\\src\\java1\\&quot;); try &#123; Class clazz = loader.loadClass(&quot;Demo1&quot;); System.out.println(&quot;加载此类的类的加载器为：&quot; + clazz.getClassLoader().getClass().getName());//MyClassLoader System.out.println(&quot;加载当前类的类的加载器的父类加载器为：&quot; + clazz.getClassLoader().getParent().getClass().getName());//sun.misc.Launcher$AppClassLoader &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;&#125; JDK9为了保证兼容性，JDK9 没有改变三层类加载器架构和双亲委派模型，但为了模块化系统的顺利运行做了一些变动： 扩展机制被移除，扩展类加载器由于向后兼容性的原因被保留，不过被重命名为平台类加载器（platform classloader），可以通过 ClassLoader 的新方法 getPlatformClassLoader() 来获取 JDK9 基于模块化进行构建（原来的 rt.jar 和 tools.jar 被拆分成数个 JMOD 文件），其中 Java 类库就满足了可扩展的需求，那就无须再保留 &lt;JAVA_HOME&gt;\\lib\\ext 目录，此前使用这个目录或者 java.ext.dirs 系统变量来扩展 JDK 功能的机制就不需要再存在 启动类加载器、平台类加载器、应用程序类加载器全都继承于 jdk.internal.loader.BuiltinClassLoader 运行机制执行过程Java 文件编译执行的过程： 类加载器：用于装载字节码文件（.class文件） 运行时数据区：用于分配存储空间 执行引擎：执行字节码文件或本地方法 垃圾回收器：用于对 JVM 中的垃圾内容进行回收 字节码跨平台性Java 语言：跨平台的语言（write once ，run anywhere） 当 Java 源代码成功编译成字节码后，在不同的平台上面运行无须再次编译 让一个 Java 程序正确地运行在 JVM 中，Java 源码就必须要被编译为符合 JVM 规范的字节码 编译过程中的编译器： 前端编译器： Sun 的全量式编译器 javac、 Eclipse 的增量式编译器 ECJ，把源代码编译为字节码文件 .class IntelliJ IDEA 使用 javac 编译器 Eclipse 中，当开发人员编写完代码后保存时，ECJ 编译器就会把未编译部分的源码逐行进行编译，而非每次都全量编译，因此 ECJ 的编译效率会比 javac 更加迅速和高效 前端编译器并不会直接涉及编译优化等方面的技术，具体优化细节移交给 HotSpot 的 JIT 编译器负责 后端运行期编译器：HotSpot VM 的 C1、C2 编译器，也就是 JIT 编译器，Graal 编译器 JIT 编译器：执行引擎部分详解 Graal 编译器：JDK10 HotSpot 加入的一个全新的即时编译器，编译效果短短几年时间就追平了 C2 静态提前编译器：AOT (Ahead Of Time Compiler）编译器，直接把源代码编译成本地机器代码 JDK 9 引入，是与即时编译相对立的一个概念，即时编译指的是在程序的运行过程中将字节码转换为机器码，AOT 是程序运行之前便将字节码转换为机器码 优点：JVM 加载已经预编译成二进制库，可以直接执行，不必等待即时编译器的预热，减少 Java 应用第一次运行慢的现象 缺点： 破坏了 Java 一次编译，到处运行，必须为每个不同硬件编译对应的发行包 降低了 Java 链接过程的动态性，加载的代码在编译期就必须全部已知 语言发展机器码：各种用二进制编码方式表示的指令，与 CPU 紧密相关，所以不同种类的 CPU 对应的机器指令不同 指令：指令就是把机器码中特定的 0 和 1 序列，简化成对应的指令，例如 mov，inc 等，可读性稍好，但是不同的硬件平台的同一种指令（比如 mov），对应的机器码也可能不同 指令集：不同的硬件平台支持的指令是有区别的，每个平台所支持的指令，称之为对应平台的指令集 x86 指令集，对应的是 x86 架构的平台 ARM 指令集，对应的是 ARM 架构的平台 汇编语言：用助记符代替机器指令的操作码，用地址符号或标号代替指令或操作数的地址 在不同的硬件平台，汇编语言对应着不同的机器语言指令集，通过汇编过程转换成机器指令 计算机只认识指令码，汇编语言编写的程序也必须翻译成机器指令码，计算机才能识别和执行 高级语言：为了使计算机用户编程序更容易些，后来就出现了各种高级计算机语言 字节码：是一种中间状态（中间码）的二进制代码，比机器码更抽象，需要直译器转译后才能成为机器码 字节码为了实现特定软件运行和软件环境，与硬件环境无关 通过编译器和虚拟机器实现，编译器将源码编译成字节码，虚拟机器将字节码转译为可以直接执行的指令 类结构文件结构字节码是一种二进制的类文件，是编译之后供虚拟机解释执行的二进制字节码文件，一个 class 文件对应一个 public 类型的类或接口 字节码内容是 JVM 的字节码指令，不是机器码，C、C++ 经由编译器直接生成机器码，所以执行效率比 Java 高 JVM 官方文档：https://docs.oracle.com/javase/specs/jvms/se8/html/index.html 根据 JVM 规范，类文件结构如下： 123456789101112131415161718ClassFile &#123;\tu4 magic; u2 minor_version; u2 major_version; u2 constant_pool_count; cp_info constant_pool[constant_pool_count-1]; u2 access_flags; u2 this_class; u2 super_class; u2 interfaces_count; u2 interfaces[interfaces_count]; u2 fields_count; field_info fields[fields_count]; u2 methods_count; method_info methods[methods_count]; u2 attributes_count; attribute_info attributes[attributes_count];&#125; 类型 名称 说明 长度 数量 u4 magic 魔数，识别类文件格式 4个字节 1 u2 minor_version 副版本号(小版本) 2个字节 1 u2 major_version 主版本号(大版本) 2个字节 1 u2 constant_pool_count 常量池计数器 2个字节 1 cp_info constant_pool 常量池表 n个字节 constant_pool_count-1 u2 access_flags 访问标识 2个字节 1 u2 this_class 类索引 2个字节 1 u2 super_class 父类索引 2个字节 1 u2 interfaces_count 接口计数 2个字节 1 u2 interfaces 接口索引集合 2个字节 interfaces_count u2 fields_count 字段计数器 2个字节 1 field_info fields 字段表 n个字节 fields_count u2 methods_count 方法计数器 2个字节 1 method_info methods 方法表 n个字节 methods_count u2 attributes_count 属性计数器 2个字节 1 attribute_info attributes 属性表 n个字节 attributes_count Class 文件格式采用一种类似于 C 语言结构体的方式进行数据存储，这种结构中只有两种数据类型：无符号数和表 无符号数属于基本的数据类型，以 u1、u2、u4、u8 来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，无符号数可以用来描述数字、索引引用、数量值或者按照 UTF-8 编码构成字符串 表是由多个无符号数或者其他表作为数据项构成的复合数据类型，表都以 _info 结尾，用于描述有层次关系的数据，整个 Class 文件本质上就是一张表，由于表没有固定长度，所以通常会在其前面加上个数说明 获取方式： HelloWorld.java 执行 javac -parameters -d . HellowWorld.java指令 写入文件指令 javap -v xxx.class &gt;xxx.txt IDEA 插件 jclasslib 魔数版本魔数：每个 Class 文件开头的 4 个字节的无符号整数称为魔数（Magic Number），是 Class 文件的标识符，代表这是一个能被虚拟机接受的有效合法的 Class 文件， 魔数值固定为 0xCAFEBABE，不符合则会抛出错误 使用魔数而不是扩展名来进行识别主要是基于安全方面的考虑，因为文件扩展名可以随意地改动 版本：4 个 字节，5 6两个字节代表的是编译的副版本号 minor_version，而 7 8 两个字节是编译的主版本号 major_version 不同版本的 Java 编译器编译的 Class 文件对应的版本是不一样的，高版本的 Java 虚拟机可以执行由低版本编译器生成的 Class 文件，反之 JVM 会抛出异常 java.lang.UnsupportedClassVersionError 主版本（十进制） 副版本（十进制） 编译器版本 45 3 1.1 46 0 1.2 47 0 1.3 48 0 1.4 49 0 1.5 50 0 1.6 51 0 1.7 52 0 1.8 53 0 1.9 54 0 1.10 55 0 1.11 图片来源：https://www.bilibili.com/video/BV1PJ411n7xZ 常量池常量池中常量的数量是不固定的，所以在常量池的入口需要放置一项 u2 类型的无符号数，代表常量池计数器（constant_pool_count），这个容量计数是从 1 而不是 0 开始，是为了满足后面某些指向常量池的索引值的数据在特定情况下需要表达不引用任何一个常量池项目，这种情况可用索引值 0 来表示 constant_pool 是一种表结构，以1 ~ constant_pool_count - 1为索引，表明有多少个常量池表项。表项中存放编译时期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池 字面量（Literal） ：基本数据类型、字符串类型常量、声明为 final 的常量值等 符号引用（Symbolic References）：类和接口的全限定名、字段的名称和描述符、方法的名称和描述符 全限定名：com&#x2F;test&#x2F;Demo 这个就是类的全限定名，仅仅是把包名的 . 替换成 /，为了使连续的多个全限定名之间不产生混淆，在使用时最后一般会加入一个 ; 表示全限定名结束 简单名称：指没有类型和参数修饰的方法或者字段名称，比如字段 x 的简单名称就是 x 描述符：用来描述字段的数据类型、方法的参数列表（包括数量、类型以及顺序）和返回值 标志符 含义 B 基本数据类型 byte C 基本数据类型 char D 基本数据类型 double F 基本数据类型 float I 基本数据类型 int J 基本数据类型 long S 基本数据类型 short Z 基本数据类型 boolean V 代表 void 类型 L 对象类型，比如：Ljava/lang/Object;，不同方法间用;隔开 [ 数组类型，代表一维数组。比如：double[][][] is [[[D 常量类型和结构： 类型 标志(或标识) 描述 CONSTANT_utf8_info 1 UTF-8编码的字符串 CONSTANT_Integer_info 3 整型字面量 CONSTANT_Float_info 4 浮点型字面量 CONSTANT_Long_info 5 长整型字面量 CONSTANT_Double_info 6 双精度浮点型字面量 CONSTANT_Class_info 7 类或接口的符号引用 CONSTANT_String_info 8 字符串类型字面量 CONSTANT_Fieldref_info 9 字段的符号引用 CONSTANT_Methodref_info 10 类中方法的符号引用 CONSTANT_InterfaceMethodref_info 11 接口中方法的符号引用 CONSTANT_NameAndType_info 12 字段或方法的符号引用 CONSTANT_MethodHandle_info 15 表示方法句柄 CONSTANT_MethodType_info 16 标志方法类型 CONSTANT_InvokeDynamic_info 18 表示一个动态方法调用点 18 种常量没有出现 byte、short、char，boolean 的原因：编译之后都可以理解为 Integer 访问标识访问标识（access_flag），又叫访问标志、访问标记，该标识用两个字节表示，用于识别一些类或者接口层次的访问信息，包括这个 Class 是类还是接口，是否定义为 public类型，是否定义为 abstract类型等 类的访问权限通常为 ACC_ 开头的常量 每一种类型的表示都是通过设置访问标记的 32 位中的特定位来实现的，比如若是 public final 的类，则该标记为 ACC_PUBLIC | ACC_FINAL 使用 ACC_SUPER 可以让类更准确地定位到父类的方法，确定类或接口里面的 invokespecial 指令使用的是哪一种执行语义，现代编译器都会设置并且使用这个标记 标志名称 标志值 含义 ACC_PUBLIC 0x0001 标志为 public 类型 ACC_FINAL 0x0010 标志被声明为 final，只有类可以设置 ACC_SUPER 0x0020 标志允许使用 invokespecial 字节码指令的新语义，JDK1.0.2之后编译出来的类的这个标志默认为真，使用增强的方法调用父类方法 ACC_INTERFACE 0x0200 标志这是一个接口 ACC_ABSTRACT 0x0400 是否为 abstract 类型，对于接口或者抽象类来说，次标志值为真，其他类型为假 ACC_SYNTHETIC 0x1000 标志此类并非由用户代码产生（由编译器产生的类，没有源码对应） ACC_ANNOTATION 0x2000 标志这是一个注解 ACC_ENUM 0x4000 标志这是一个枚举 索引集合类索引、父类索引、接口索引集合 类索引用于确定这个类的全限定名 父类索引用于确定这个类的父类的全限定名，Java 语言不允许多重继承，所以父类索引只有一个，除了Object 之外，所有的 Java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为0 接口索引集合就用来描述这个类实现了哪些接口 interfaces_count 项的值表示当前类或接口的直接超接口数量 interfaces[] 接口索引集合，被实现的接口将按 implements 语句后的接口顺序从左到右排列在接口索引集合中 长度 含义 u2 this_class u2 super_class u2 interfaces_count u2 interfaces[interfaces_count] 字段表字段 fields 用于描述接口或类中声明的变量，包括类变量以及实例变量，但不包括方法内部、代码块内部声明的局部变量以及从父类或父接口继承。字段叫什么名字、被定义为什么数据类型，都是无法固定的，只能引用常量池中的常量来描述 fields_count（字段计数器），表示当前 class 文件 fields 表的成员个数，用两个字节来表示 fields[]（字段表）： 表中的每个成员都是一个 fields_info 结构的数据项，用于表示当前类或接口中某个字段的完整描述 字段访问标识： 标志名称 标志值 含义 ACC_PUBLIC 0x0001 字段是否为public ACC_PRIVATE 0x0002 字段是否为private ACC_PROTECTED 0x0004 字段是否为protected ACC_STATIC 0x0008 字段是否为static ACC_FINAL 0x0010 字段是否为final ACC_VOLATILE 0x0040 字段是否为volatile ACC_TRANSTENT 0x0080 字段是否为transient ACC_SYNCHETIC 0x1000 字段是否为由编译器自动产生 ACC_ENUM 0x4000 字段是否为enum 字段名索引：根据该值查询常量池中的指定索引项即可 描述符索引：用来描述字段的数据类型、方法的参数列表和返回值 字符 类型 含义 B byte 有符号字节型树 C char Unicode字符，UTF-16编码 D double 双精度浮点数 F float 单精度浮点数 I int 整型数 J long 长整数 S short 有符号短整数 Z boolean 布尔值true&#x2F;false V void 代表void类型 L Classname reference 一个名为Classname的实例 [ reference 一个一维数组 属性表集合：属性个数存放在 attribute_count 中，属性具体内容存放在 attribute 数组中，一个字段还可能拥有一些属性，用于存储更多的额外信息，比如初始化值、一些注释信息等 12345ConstantValue_attribute&#123; u2 attribute_name_index; u4 attribute_length; u2 constantvalue_index;&#125; 对于常量属性而言，attribute_length 值恒为2 方法表方法表是 methods 指向常量池索引集合，其中每一个 method_info 项都对应着一个类或者接口中的方法信息，完整描述了每个方法的签名 如果这个方法不是抽象的或者不是 native 的，字节码中就会体现出来 methods 表只描述当前类或接口中声明的方法，不包括从父类或父接口继承的方法 methods 表可能会出现由编译器自动添加的方法，比如初始化方法 和实例化方法 重载（Overload）一个方法，除了要与原方法具有相同的简单名称之外，还要求必须拥有一个与原方法不同的特征签名，特征签名就是一个方法中各个参数在常量池中的字段符号引用的集合，因为返回值不会包含在特征签名之中，因此 Java 语言里无法仅仅依靠返回值的不同来对一个已有方法进行重载。但在 Class 文件格式中，特征签名的范围更大一些，只要描述符不是完全一致的两个方法就可以共存 methods_count（方法计数器）：表示 class 文件 methods 表的成员个数，使用两个字节来表示 methods[]（方法表）：每个表项都是一个 method_info 结构，表示当前类或接口中某个方法的完整描述 方法表结构如下： 类型 名称 含义 数量 u2 access_flags 访问标志 1 u2 name_index 字段名索引 1 u2 descriptor_index 描述符索引 1 u2 attrubutes_count 属性计数器 1 attribute_info attributes 属性集合 attributes_count 方法表访问标志： 标志名称 标志值 含义 ACC_PUBLIC 0x0001 字段是否为 public ACC_PRIVATE 0x0002 字段是否为 private ACC_PROTECTED 0x0004 字段是否为 protected ACC_STATIC 0x0008 字段是否为 static ACC_FINAL 0x0010 字段是否为 final ACC_VOLATILE 0x0040 字段是否为 volatile ACC_TRANSTENT 0x0080 字段是否为 transient ACC_SYNCHETIC 0x1000 字段是否为由编译器自动产生 ACC_ENUM 0x4000 字段是否为 enum 属性表属性表集合，指的是 Class 文件所携带的辅助信息，比如该 Class 文件的源文件的名称，以及任何带有 RetentionPolicy.CLASS 或者 RetentionPolicy.RUNTIME 的注解，这类信息通常被用于 Java 虚拟机的验证和运行，以及 Java 程序的调试。字段表、方法表都可以有自己的属性表，用于描述某些场景专有的信息 attributes_ count（属性计数器）：表示当前文件属性表的成员个数 attributes[]（属性表）：属性表的每个项的值必须是 attribute_info 结构 属性的通用格式： 12345ConstantValue_attribute&#123; u2 attribute_name_index;\t//属性名索引 u4 attribute_length; //属性长度 u2 attribute_info; //属性表&#125; 属性类型： 属性名称 使用位置 含义 Code 方法表 Java 代码编译成的字节码指令 ConstantValue 字段表 final 关键字定义的常量池 Deprecated 类、方法、字段表 被声明为 deprecated 的方法和字段 Exceptions 方法表 方法抛出的异常 EnclosingMethod 类文件 仅当一个类为局部类或者匿名类是才能拥有这个属性，这个属性用于标识这个类所在的外围方法 InnerClass 类文件 内部类列表 LineNumberTable Code 属性 Java 源码的行号与字节码指令的对应关系 LocalVariableTable Code 属性 方法的局部变量描述 StackMapTable Code 属性 JDK1.6 中新增的属性，供新的类型检查检验器检查和处理目标方法的局部变量和操作数有所需要的类是否匹配 Signature 类，方法表，字段表 用于支持泛型情况下的方法签名 SourceFile 类文件 记录源文件名称 SourceDebugExtension 类文件 用于存储额外的调试信息 Syothetic 类，方法表，字段表 标志方法或字段为编泽器自动生成的 LocalVariableTypeTable 类 使用特征签名代替描述符，是为了引入泛型语法之后能描述泛型参数化类型而添加 RuntimeVisibleAnnotations 类，方法表，字段表 为动态注解提供支持 RuntimelnvisibleAnnotations 类，方法表，字段表 用于指明哪些注解是运行时不可见的 RuntimeVisibleParameterAnnotation 方法表 作用与 RuntimeVisibleAnnotations 属性类似，只不过作用对象为方法 RuntirmelnvisibleParameterAnniotation 方法表 作用与 RuntimelnvisibleAnnotations 属性类似，作用对象哪个为方法参数 AnnotationDefauit 方法表 用于记录注解类元素的默认值 BootstrapMethods 类文件 用于保存 invokeddynanic 指令引用的引导方式限定符 编译指令javacjavac：编译命令，将 java 源文件编译成 class 字节码文件 javac xx.java 不会在生成对应的局部变量表等信息，使用 javac -g xx.java 可以生成所有相关信息 javapjavap 反编译生成的字节码文件，根据 class 字节码文件，反解析出当前类对应的 code 区 （字节码指令）、局部变量表、异常表和代码行偏移量映射表、常量池等信息 用法：javap 1234567891011121314151617-help --help -? 输出此用法消息-version 版本信息-public 仅显示公共类和成员-protected 显示受保护的/公共类和成员-package 显示程序包/受保护的/公共类和成员 (默认)-p -private 显示所有类和成员 #常用的以下三个-v -verbose 输出附加信息-l 输出行号和本地变量表-c 对代码进行反汇编\t#反编译-s 输出内部类型签名-sysinfo 显示正在处理的类的系统信息 (路径, 大小, 日期, MD5 散列)-constants 显示最终常量-classpath &lt;path&gt; 指定查找用户类文件的位置-cp &lt;path&gt; 指定查找用户类文件的位置-bootclasspath &lt;path&gt; 覆盖引导类文件的位置 指令集执行指令Java 字节码属于 JVM 基本执行指令。由一个字节长度的代表某种操作的操作码（opcode）以及零至多个代表此操作所需参数的操作数（operand）所构成，虚拟机中许多指令并不包含操作数，只有一个操作码（零地址指令） 由于限制了 Java 虚拟机操作码的长度为一个字节（0~255），所以指令集的操作码总数不可能超过 256 条 在 JVM 的指令集中，大多数的指令都包含了其操作所对应的数据类型信息。例如 iload 指令用于从局部变量表中加载 int 型的数据到操作数栈中，而 fload 指令加载的则是 float 类型的数据 i 代表对 int 类型的数据操作 l 代表 long s 代表 short b 代表 byte c 代表 char f 代表 float d 代表 double 大部分的指令都没有支持 byte、char、short、boolean 类型，编译器会在编译期或运行期将 byte 和 short 类型的数据带符号扩展（Sign-Extend-）为相应的 int 类型数据，将 boolean 和 char 类型数据零位扩展（Zero-Extend）为相应的 int 类型数据 在做值相关操作时: 一个指令，可以从局部变量表、常量池、堆中对象、方法调用、系统调用中等取得数据，这些数据（可能是值，也可能是对象的引用）被压入操作数栈 一个指令，也可以从操作数栈中取出一到多个值（pop 多次），完成赋值、加减乘除、方法传参、系统调用等等操作 加载存储加载和存储指令用于将数据从栈帧的局部变量表和操作数栈之间来回传递 局部变量压栈指令：将给定的局部变量表中的数据压入操作数栈 xload、xload_n，x 表示取值数据类型，为 i、l、f、d、a， n 为 0 到 3 指令 xload_n 表示将第 n 个局部变量压入操作数栈，aload_n 表示将一个对象引用压栈 指令 xload n 通过指定参数的形式，把局部变量压入操作数栈，局部变量数量超过 4 个时使用这个命令 常量入栈指令：将常数压入操作数栈，根据数据类型和入栈内容的不同，又分为 const、push、ldc 指令 push：包括 bipush 和 sipush，区别在于接收数据类型的不同，bipush 接收 8 位整数作为参数，sipush 接收 16 位整数 ldc：如果以上指令不能满足需求，可以使用 ldc 指令，接收一个 8 位的参数，该参数指向常量池中的 int、 float 或者 String 的索引，将指定的内容压入堆栈。ldc_w 接收两个 8 位参数，能支持的索引范围更大，如果要压入的元素是 long 或 double 类型的，则使用 ldc2_w 指令 aconst_null 将 null 对象引用压入栈，iconst_m1 将 int 类型常量 -1 压入栈，iconst_0 将 int 类型常量 0 压入栈 出栈装入局部变量表指令：将操作数栈中栈顶元素弹出后，装入局部变量表的指定位置，用于给局部变量赋值 xstore、xstore_n，x 表示取值类型为 i、l、f、d、a， n 为 0 到 3 xastore 表示存入数组，x 取值为 i、l、f、d、a、b、c、s 扩充局部变量表的访问索引的指令：wide 算术指令算术指令用于对两个操作数栈上的值进行某种特定运算，并把计算结果重新压入操作数栈 没有直接支持 byte、 short、 char 和 boolean 类型的算术指令，对于这些数据的运算，都使用 int 类型的指令来处理，数组类型也是转换成 int 数组 加法指令：iadd、ladd、fadd、dadd 减法指令：isub、lsub、fsub、dsub 乘法指令：imu、lmu、fmul、dmul 除法指令：idiv、ldiv、fdiv、ddiv 求余指令：irem、lrem、frem、drem（remainder 余数） 取反指令：ineg、lneg、fneg、dneg （negation 取反） 自增指令：iinc（直接在局部变量 slot 上进行运算，不用放入操作数栈） 位运算指令，又可分为： 位移指令：ishl、ishr、 iushr、lshl、lshr、 lushr 按位或指令：ior、lor 按位与指令：iand、land 按位异或指令：ixor、lxor 比较指令：dcmpg、dcmpl、 fcmpg、fcmpl、lcmp 运算模式： 向最接近数舍入模式，JVM 在进行浮点数计算时，所有的运算结果都必须舍入到适当的精度，非精确结果必须舍入为可被表示的最接近的精确值，如果有两种可表示形式与该值一样接近，将优先选择最低有效位为零的 向零舍入模式：将浮点数转换为整数时，该模式将在目标数值类型中选择一个最接近但是不大于原值的数字作为最精确的舍入结果 NaN 值：当一个操作产生溢出时，将会使用有符号的无穷大表示，如果某个操作结果没有明确的数学定义，将使用 NaN 值来表示 12double j = i / 0.0;System.out.println(j);//无穷大，NaN: not a number **分析 i++**：从字节码角度分析：a++ 和 ++a 的区别是先执行 iload 还是先执行 iinc 123456 4 iload_1 //存入操作数栈 5 iinc 1 by 1\t//自增i++ 8 istore_3 //把操作数栈没有自增的数据的存入局部变量表 9 iinc 2 by 1\t//++i12 iload_2 //加载到操作数栈13 istore 4 //存入局部变量表，这个存入没有 _ 符号，_只能到3 12345678public class Demo &#123; public static void main(String[] args) &#123; int a = 10; int b = a++ + ++a + a--; System.out.println(a);\t//11 System.out.println(b);\t//34 &#125;&#125; 判断结果： 1234567891011public class Demo &#123; public static void main(String[] args) &#123; int i = 0; int x = 0; while (i &lt; 10) &#123; x = x++; i++; &#125; System.out.println(x); // 结果是 0 &#125;&#125; 类型转换类型转换指令可以将两种不同的数值类型进行相互转换，除了 boolean 之外的七种类型 宽化类型转换： JVM 支持以下数值的宽化类型转换（widening numeric conversion），小范围类型到大范围类型的安全转换 从 int 类型到 long、float 或者 double 类型，对应的指令为 i2l、i2f、i2d 从 long 类型到 float、 double 类型，对应的指令为 l2f、l2d 从 float 类型到 double 类型，对应的指令为 f2d 精度损失问题 宽化类型转换是不会因为超过目标类型最大值而丢失信息 从 int 转换到 float 或者 long 类型转换到 double 时，将可能发生精度丢失 从 byte、char 和 short 类型到 int 类型的宽化类型转换实际上是不存在的，JVM 把它们当作 int 处理 窄化类型转换： Java 虚拟机直接支持以下窄化类型转换： 从 int 类型至 byte、 short 或者 char 类型，对应的指令有 i2b、i2c、i2s 从 long 类型到 int 类型，对应的指令有 l2i 从 float 类型到 int 或者 long 类型，对应的指令有:f2i、f2l 从 double 类型到 int、long 或 float 者类型，对应的指令有 d2i、d2、d2f 精度损失问题： 窄化类型转换可能会导致转换结果具备不同的正负号、不同的数量级，转换过程可能会导致数值丢失精度 将一个浮点值窄化转换为整数类型 T（T 限于 int 或 long 类型之一）时，将遵循以下转换规则： 如果浮点值是 NaN，那转换结果就是 int 或 long 类型的 0 如果浮点值不是无穷大的话，浮点值使用 IEEE 754 的向零舍入模式取整，获得整数值 v，如果 v 在目标类型 T 的表示范围之内，那转换结果就是 v，否则将根据 v 的符号，转换为 T 所能表示的最大或者最小正数 创建访问创建指令： 创建类实例指令：new，接收一个操作数指向常量池的索引，表示要创建的类型，执行完成后将对象的引用压入栈 1230: new #2 // class com/jvm/bytecode/Demo3: dup4: invokespecial #3 // Method &quot;&lt;init&gt;&quot;:()V dup 是复制操作数栈栈顶的内容，需要两份引用原因： 一个要配合 invokespecial 调用该对象的构造方法 :()V （会消耗掉栈顶一个引用） 一个要配合 astore_1 赋值给局部变量 创建数组的指令：newarray、anewarray、multianewarray newarray：创建基本类型数组 anewarray：创建引用类型数组 multianewarray：创建多维数组 字段访问指令：对象创建后可以通过对象访问指令获取对象实例或数组实例中的字段或者数组元素 访问类字段（static字段，或者称为类变量）的指令：getstatic、putstatic 访问类实例字段（非static字段，或者称为实例变量）的指令：getfield、 putfield 类型检查指令：检查类实例或数组类型的指令 checkcast：用于检查类型强制转换是否可以进行，如果可以进行 checkcast 指令不会改变操作数栈，否则它会抛出 ClassCastException 异常 instanceof：判断给定对象是否是某一个类的实例，会将判断结果压入操作数栈 方法指令方法调用指令：invokevirtual、 invokeinterface、invokespecial、invokestatic、invokedynamic 方法调用章节详解 操作数栈JVM 提供的操作数栈管理指令，可以用于直接操作操作数栈的指令 pop、pop2：将一个或两个元素从栈顶弹出，并且直接废弃 dup、dup2，dup_x1、dup2_x1，dup_x2、dup2_x2：复制栈顶一个或两个数值并重新压入栈顶 swap：将栈最顶端的两个 slot 数值位置交换，JVM 没有提供交换两个 64 位数据类型数值的指令 nop：一个非常特殊的指令，字节码为 0x00，和汇编语言中的 nop 一样，表示什么都不做，一般可用于调试、占位等 控制转移比较指令：比较栈顶两个元素的大小，并将比较结果入栈 lcmp：比较两个 long 类型值 fcmpl：比较两个 float 类型值（当遇到NaN时，返回-1） fcmpg：比较两个 float 类型值（当遇到NaN时，返回1） dcmpl：比较两个 double 类型值（当遇到NaN时，返回-1） dcmpg：比较两个 double 类型值（当遇到NaN时，返回1） 条件跳转指令： 指令 说明 ifeq equals，当栈顶int类型数值等于0时跳转 ifne not equals，当栈顶in类型数值不等于0时跳转 iflt lower than，当栈顶in类型数值小于0时跳转 ifle lower or equals，当栈顶in类型数值小于等于0时跳转 ifgt greater than，当栈顶int类型数组大于0时跳转 ifge greater or equals，当栈顶in类型数值大于等于0时跳转 ifnull 为 null 时跳转 ifnonnull 不为 null 时跳转 比较条件跳转指令： 指令 说明 if_icmpeq 比较栈顶两 int 类型数值大小（下同），当前者等于后者时跳转 if_icmpne 当前者不等于后者时跳转 if_icmplt 当前者小于后者时跳转 if_icmple 当前者小于等于后者时跳转 if_icmpgt 当前者大于后者时跳转 if_icmpge 当前者大于等于后者时跳转 if_acmpeq 当结果相等时跳转 if_acmpne 当结果不相等时跳转 多条件分支跳转指令： tableswitch：用于 switch 条件跳转，case 值连续 lookupswitch：用于 switch 条件跳转，case 值不连续 无条件跳转指令： goto：用来进行跳转到指定行号的字节码 goto_w：无条件跳转（宽索引） 异常处理处理机制抛出异常指令：athrow 指令 JVM 处理异常（catch 语句）不是由字节码指令来实现的，而是采用异常表来完成的 代码： 12345678910public static void main(String[] args) &#123; int i = 0; try &#123; i = 10; &#125; catch (Exception e) &#123; i = 20; &#125; finally &#123; i = 30; &#125;&#125; 字节码： 多出一个 Exception table 的结构，**[from, to) 是前闭后开的检测范围**，一旦这个范围内的字节码执行出现异常，则通过 type 匹配异常类型，如果一致，进入 target 所指示行号 11 行的字节码指令 astore_2 是将异常对象引用存入局部变量表的 slot 2 位置，因为异常出现时，只能进入 Exception table 中一个分支，所以局部变量表 slot 2 位置被共用 12345678910111213141516171819202122232425262728293031 0: iconst_0 1: istore_1 // 0 -&gt; i\t-&gt;赋值 2: bipush 10 // try 10 放入操作数栈顶 4: istore_1 // 10 -&gt; i 将操作数栈顶数据弹出，存入局部变量表的 slot1 5: bipush 30 // 【finally】 7: istore_1 // 30 -&gt; i 8: goto 27 // return ----------------------------------- 11: astore_2 // catch Exceptin -&gt; e ---------------------- 12: bipush 20 // 14: istore_1 // 20 -&gt; i 15: bipush 30 // 【finally】 17: istore_1 // 30 -&gt; i 18: goto 27 // return ----------------------------------- 21: astore_3 // catch any -&gt; slot 3 ---------------------- 22: bipush 30 // 【finally】 24: istore_1 // 30 -&gt; i 25: aload_3 // 将局部变量表的slot 3数据弹出，放入操作数栈栈顶 26: athrow // throw 抛出异常 27: returnException table:\t// 任何阶段出现任务异常都会执行 finally\tfrom to target type 2\t5 11 Class java/lang/Exception 2 5 21 any // 剩余的异常类型，比如 Error 11 15 21 any // 剩余的异常类型，比如 ErrorLineNumberTable: ...LocalVariableTable:\tStart Length Slot Name Signature\t12 3 2 e Ljava/lang/Exception;\t0 28 0 args [Ljava/lang/String;\t2 26 1 i I finallyfinally 中的代码被复制了 3 份，分别放入 try 流程，catch 流程以及 catch 剩余的异常类型流程（上节案例） 代码： 1234567public static int test() &#123; try &#123; return 10; &#125; finally &#123; return 20; &#125;&#125; 字节码： 12345678910 0: bipush 10 // 10 放入栈顶 2: istore_0 // 10 -&gt; slot 0 【从栈顶移除了】 3: bipush 20 // 20 放入栈顶 5: ireturn // 返回栈顶 int(20) 6: astore_1 // catch any 存入局部变量表的 slot1 7: bipush 20 // 20 放入栈顶 9: ireturn // 返回栈顶 int(20)Exception table:\tfrom to target type 0\t3 6 any return 吞异常 1234567public static int test() &#123; try &#123; return 10; &#125; finally &#123; return 20; &#125;&#125; 12345678910 0: bipush 10 // 10 放入栈顶 2: istore_0 // 10 -&gt; slot 0 【从栈顶移除了】 3: bipush 20 // 20 放入栈顶 5: ireturn // 返回栈顶 int(20) 6: astore_1 // catch any 存入局部变量表的 slot1 7: bipush 20 // 20 放入栈顶 9: ireturn // 返回栈顶 int(20)Exception table:\tfrom to target type 0\t3 6 any 由于 finally 中的 ireturn 被插入了所有可能的流程，因此返回结果以 finally 的为准 字节码中没有 athrow ，表明如果在 finally 中出现了 return，会吞掉异常 不吞异常 1234567891011121314public class Demo &#123; public static void main(String[] args) &#123; int result = test(); System.out.println(result);//10\t&#125;\tpublic static int test() &#123; int i = 10; try &#123; return i;//返回10 &#125; finally &#123; i = 20; &#125; &#125;&#125; 12345678910111213141516 0: bipush 10 // 10 放入栈顶 2: istore_0 // 10 赋值给i，放入slot 0 3: iload_0 // i(10)加载至操作数栈 4: istore_1 // 10 -&gt; slot 1，【暂存至 slot 1，目的是为了固定返回值】 5: bipush 20 // 20 放入栈顶 7: istore_0 // 20 slot 0 8: iload_1 // slot 1(10) 载入 slot 1 暂存的值 9: ireturn // 返回栈顶的 int(10) 10: astore_2\t// catch any -&gt; slot 2 存入局部变量表的 slot2 11: bipush 20 13: istore_0 14: aload_2 15: athrow // 不会吞掉异常Exception table:\tfrom to target type 3 5 10 any 同步控制方法级的同步：是隐式的，无须通过字节码指令来控制，它实现在方法调用和返回操作之中，虚拟机可以从方法常量池的方法表结构中的 ACC_SYNCHRONIZED 访问标志得知一个方法是否声明为同步方法 方法内指定指令序列的同步：有 monitorenter 和 monitorexit 两条指令来支持 synchronized 关键字的语义 montiorenter：进入并获取对象监视器，即为栈顶对象加锁 monitorexit：释放并退出对象监视器，即为栈顶对象解锁 执行流程原始 Java 代码： 12345678public class Demo &#123; public static void main(String[] args) &#123; int a = 10; int b = Short.MAX_VALUE + 1; int c = a + b; System.out.println(c); &#125;&#125; javap -v Demo.class：省略 常量池载入运行时常量池 方法区字节码载入方法区 main 线程开始运行，分配栈帧内存：（操作数栈stack&#x3D;2，局部变量表locals&#x3D;4） 执行引擎开始执行字节码 bipush 10：将一个 byte 压入操作数栈（其长度会补齐 4 个字节），类似的指令 sipush 将一个 short 压入操作数栈（其长度会补齐 4 个字节） ldc 将一个 int 压入操作数栈 ldc2_w 将一个 long 压入操作数栈（分两次压入，因为 long 是 8 个字节） 这里小的数字都是和字节码指令存在一起，超过 short 范围的数字存入了常量池 istore_1：将操作数栈顶数据弹出，存入局部变量表的 slot 1 ldc #3：从常量池加载 #3 数据到操作数栈Short.MAX_VALUE 是 32767，所以 32768 &#x3D; Short.MAX_VALUE + 1 实际是在编译期间计算完成 istore_2：将操作数栈顶数据弹出，存入局部变量表的 slot 2 iload_1：将局部变量表的 slot 1 数据弹出，放入操作数栈栈顶 iload_2：将局部变量表的 slot 2 数据弹出，放入操作数栈栈顶 iadd：执行相加操作 istore_3：将操作数栈顶数据弹出，存入局部变量表的 slot 3 getstatic #4：获取静态字段 iload_3： invokevirtual #5： 找到常量池 #5 项 定位到方法区 java&#x2F;io&#x2F;PrintStream.println:(I)V 方法 生成新的栈帧（分配 locals、stack等） 传递参数，执行新栈帧中的字节码 执行完毕，弹出栈帧 清除 main 操作数栈内容 return：完成 main 方法调用，弹出 main 栈帧，程序结束 执行引擎基本介绍执行引擎：Java 虚拟机的核心组成部分之一，类加载主要任务是负责装载字节码到其内部，但字节码并不能够直接运行在操作系统之上，需要执行引擎将字节码指令解释&#x2F;编译为对应平台上的本地机器指令，进行执行 虚拟机是一个相对于物理机的概念，这两种机器都有代码执行能力： 物理机的执行引擎是直接建立在处理器、缓存、指令集和操作系统层面上 虚拟机的执行引擎是由软件自行实现的，可以不受物理条件制约地定制指令集与执行引擎的结构体系 Java 是半编译半解释型语言，将解释执行与编译执行二者结合起来进行： 解释器：根据预定义的规范对字节码采用逐行解释的方式执行，将每条字节码文件中的内容翻译为对应平台的本地机器指令执行 即时编译器（JIT : Just In Time Compiler）：虚拟机运行时将源代码直接编译成和本地机器平台相关的机器码后再执行，并存入 Code Cache，下次遇到相同的代码直接执行，效率高 执行方式HotSpot VM 采用解释器与即时编译器并存的架构，解释器和即时编译器能够相互协作，去选择最合适的方式来权衡编译本地代码和直接解释执行代码的时间 HostSpot JVM 的默认执行方式： 当程序启动后，解释器可以马上发挥作用立即执行，省去编译器编译的时间（解释器存在的必要性） 随着程序运行时间的推移，即时编译器逐渐发挥作用，根据热点探测功能，将有价值的字节码编译为本地机器指令，以换取更高的程序执行效率 HotSpot VM 可以通过 VM 参数设置程序执行方式： -Xint：完全采用解释器模式执行程序 -Xcomp：完全采用即时编译器模式执行程序。如果即时编译出现问题，解释器会介入执行 -Xmixed：采用解释器 + 即时编译器的混合模式共同执行程序 热点探测热点代码：被 JIT 编译器编译的字节码，根据代码被调用执行的频率而定，一个被多次调用的方法或者一个循环次数较多的循环体都可以被称之为热点代码 热点探测：JIT 编译器在运行时会针热点代码做出深度优化，将其直接编译为对应平台的本地机器指令进行缓存，以提升程序执行性能 JIT 编译在默认情况是异步进行的，当触发某方法或某代码块的优化时，先将其放入编译队列，然后由编译线程进行编译，编译之后的代码放在 CodeCache 中，通过 -XX:-BackgroundCompilation 参数可以关闭异步编译 CodeCache 用于缓存编译后的机器码、动态生成的代码和本地方法代码 JNI 如果 CodeCache 区域被占满，编译器被停用，字节码将不会编译为机器码，应用程序继续运行，但运行性能会降低很多 HotSpot VM 采用的热点探测方式是基于计数器的热点探测，为每一个方法都建立 2 个不同类型的计数器：方法调用计数器（Invocation Counter）和回边计数器（BackEdge Counter） 方法调用计数器：用于统计方法被调用的次数，默认阈值在 Client 模式 下是 1500 次，在 Server 模式下是 10000 次（需要进行激进的优化），超过这个阈值，就会触发 JIT 编译，阈值可以通过虚拟机参数 -XX:CompileThreshold 设置 工作流程：当一个方法被调用时， 会先检查该方法是否存在被 JIT 编译过的版本，存在则使用编译后的本地代码来执行；如果不存在则将此方法的调用计数器值加 1，然后判断方法调用计数器与回边计数器值之和是否超过方法调用计数器的阈值，如果超过阈值会向即时编译器提交一个该方法的代码编译请求 回边计数器：统计一个方法中循环体代码执行的次数，在字节码中控制流向后跳转的指令称为回边 如果一个方法中的循环体需要执行多次，可以优化为为栈上替换，简称 OSR (On StackReplacement) 编译，OSR 替换循环代码体的入口，C1、C2 替换的是方法调用的入口，OSR 编译后会出现方法的整段代码被编译了，但是只有循环体部分才执行编译后的机器码，其他部分仍是解释执行 分层编译HotSpot VM 内嵌两个 JIT 编译器，分别为 Client Compiler 和 Server Compiler，简称 C1 编译器和 C2 编译器 C1 编译器会对字节码进行简单可靠的优化，耗时短，以达到更快的编译速度，C1 编译器的优化方法： 方法内联：将调用的函数代码编译到调用点处，这样可以减少栈帧的生成，减少参数传递以及跳转过程 方法内联能够消除方法调用的固定开销，任何方法除非被内联，否则调用都会有固定开销，来源于保存程序在该方法中的执行位置，以及新建、压入和弹出新方法所使用的栈帧。 1234private static int square(final int i) &#123;\treturn i * i;&#125;System.out.println(square(9)); square 是热点方法，会进行内联，把方法内代码拷贝粘贴到调用者的位置： 1System.out.println(9 * 9); 还能够进行常量折叠（constant folding）的优化： 1System.out.println(81); 冗余消除：根据运行时状况进行代码折叠或削除 内联缓存：是一种加快动态绑定的优化技术（方法调用部分详解） C2 编译器进行耗时较长的优化以及激进优化，优化的代码执行效率更高，当激进优化的假设不成立时，再退回使用 C1 编译，这也是使用分层编译的原因 C2 的优化主要是在全局层面，逃逸分析是优化的基础：标量替换、栈上分配、同步消除 VM 参数设置： -client：指定 Java 虚拟机运行在 Client 模式下，并使用 C1 编译器 -server：指定 Java 虚拟机运行在 Server 模式下，并使用 C2 编译器 -server -XX:+TieredCompilation：在 1.8 之前，分层编译默认是关闭的，可以添加该参数开启 分层编译策略 (Tiered Compilation)：程序解释执行可以触发 C1 编译，将字节码编译成机器码，加上性能监控，C2 编译会根据性能监控信息进行激进优化，JVM 将执行状态分成了 5 个层次： 0 层，解释执行（Interpreter） 1 层，使用 C1 即时编译器编译执行（不带 profiling） 2 层，使用 C1 即时编译器编译执行（带基本的 profiling） 3 层，使用 C1 即时编译器编译执行（带完全的 profiling） 4 层，使用 C2 即时编译器编译执行（C1 和 C2 协作运行） 说明：profiling 是指在运行过程中收集一些程序执行状态的数据，例如方法的调用次数，循环的回边次数等 参考文章：https://www.jianshu.com/p/20bd2e9b1f03 方法调用方法识别Java 虚拟机识别方法的关键在于类名、方法名以及方法描述符（method descriptor） 方法描述符是由方法的参数类型以及返回类型所构成，Java 层面叫方法特征签名 在同一个类中，如果同时出现多个名字相同且描述符也相同的方法，那么 Java 虚拟机会在类的验证阶段报错 JVM 根据名字和描述符来判断的，只要返回值不一样（方法描述符不一样），其它完全一样，在 JVM 中是允许的，但 Java 语言不允许 1234567// 返回值类型不同，编译阶段直接报错public static Integer invoke(Object... args) &#123; return 1;&#125;public static int invoke(Object... args) &#123; return 2;&#125; 调用机制方法调用并不等于方法执行，方法调用阶段唯一的任务就是确定被调用方法的版本，不是方法的具体运行过程 在 JVM 中，将符号引用转换为直接引用有两种机制： 静态链接：当一个字节码文件被装载进 JVM 内部时，如果被调用的目标方法在编译期可知，且运行期保持不变，将调用方法的符号引用转换为直接引用的过程称之为静态链接（类加载的解析阶段） 动态链接：被调用的方法在编译期无法被确定下来，只能在程序运行期将调用方法的符号引用转换为直接引用，由于这种引用转换过程具备动态性，因此被称为动态链接（初始化后的解析阶段） 对应方法的绑定（分配）机制：静态绑定和动态绑定，编译器已经区分了重载的方法（静态绑定和动态绑定），因此可以认为虚拟机中不存在重载 非虚方法： 非虚方法在编译期就确定了具体的调用版本，这个版本在运行时是不可变的 静态方法、私有方法、final 方法、实例构造器、父类方法都是非虚方法 所有普通成员方法、实例方法、被重写的方法都是虚方法 动态类型语言和静态类型语言： 在于对类型的检查是在编译期还是在运行期，满足前者就是静态类型语言，反之则是动态类型语言 静态语言是判断变量自身的类型信息；动态类型语言是判断变量值的类型信息，变量没有类型信息 Java 是静态类型语言（尽管 Lambda 表达式为其增加了动态特性），JS，Python 是动态类型语言 12String s = &quot;abc&quot;; //Javainfo = &quot;abc&quot;; //Python 调用指令五种指令普通调用指令： invokestatic：调用静态方法 invokespecial：调用私有方法、构造器，和父类的实例方法或构造器，以及所实现接口的默认方法 invokevirtual：调用所有虚方法（虚方法分派） invokeinterface：调用接口方法 动态调用指令： invokedynamic：动态解析出需要调用的方法 Java7 为了实现动态类型语言支持而引入了该指令，但是并没有提供直接生成 invokedynamic 指令的方法，需要借助 ASM 这种底层字节码工具来产生 invokedynamic 指令 Java8 的 lambda 表达式的出现，invokedynamic 指令在 Java 中才有了直接生成方式 指令对比： 普通调用指令固化在虚拟机内部，方法的调用执行不可干预，根据方法的符号引用链接到具体的目标方法 动态调用指令支持用户确定方法 invokestatic 和 invokespecial 指令调用的方法称为非虚方法，虚拟机能够直接识别具体的目标方法 invokevirtual 和 invokeinterface 指令调用的方法称为虚方法，虚拟机需要在执行过程中根据调用者的动态类型来确定目标方法 指令说明： 如果虚拟机能够确定目标方法有且仅有一个，比如说目标方法被标记为 final，那么可以不通过动态绑定，直接确定目标方法 普通成员方法是由 invokevirtual 调用，属于动态绑定，即支持多态 符号引用在编译过程中，虚拟机并不知道目标方法的具体内存地址，Java 编译器会暂时用符号引用来表示该目标方法，这一符号引用包括目标方法所在的类或接口的名字，以及目标方法的方法名和方法描述符 符号引用存储在方法区常量池中，根据目标方法是否为接口方法，分为接口符号引用和非接口符号引用： 123456Constant pool:... #16 = InterfaceMethodref #27.#29\t// 接口... #22 = Methodref #1.#33\t// 非接口... 对于非接口符号引用，假定该符号引用所指向的类为 C，则 Java 虚拟机会按照如下步骤进行查找： 在 C 中查找符合名字及描述符的方法 如果没有找到，在 C 的父类中继续搜索，直至 Object 类 如果没有找到，在 C 所直接实现或间接实现的接口中搜索，这一步搜索得到的目标方法必须是非私有、非静态的。如果有多个符合条件的目标方法，则任意返回其中一个 对于接口符号引用，假定该符号引用所指向的接口为 I，则 Java 虚拟机会按照如下步骤进行查找： 在 I 中查找符合名字及描述符的方法 如果没有找到，在 Object 类中的公有实例方法中搜索 如果没有找到，则在 I 的超接口中搜索，这一步的搜索结果的要求与非接口符号引用步骤 3 的要求一致 执行流程1234567891011121314151617public class Demo &#123; public Demo() &#123; &#125; private void test1() &#123; &#125; private final void test2() &#123; &#125; public void test3() &#123; &#125; public static void test4() &#123; &#125; public static void main(String[] args) &#123; Demo3_9 d = new Demo3_9(); d.test1(); d.test2(); d.test3(); d.test4(); Demo.test4(); &#125;&#125; 几种不同的方法调用对应的字节码指令： 1234567891011121314150: new #2 // class cn/jvm/t3/bytecode/Demo3: dup4: invokespecial #3 // Method &quot;&lt;init&gt;&quot;:()V7: astore_18: aload_19: invokespecial #4 // Method test1:()V12: aload_113: invokespecial #5 // Method test2:()V16: aload_117: invokevirtual #6 // Method test3:()V20: aload_121: pop22: invokestatic #7 // Method test4:()V25: invokestatic #7 // Method test4:()V28: return invokespecial 调用该对象的构造方法 :()V invokevirtual 调用对象的成员方法 d.test4() 是通过对象引用调用一个静态方法，在调用 invokestatic 之前执行了 pop 指令，把对象引用从操作数栈弹掉 不建议使用 对象.静态方法() 的方式调用静态方法，多了 aload 和 pop 指令 成员方法与静态方法调用的区别是：执行方法前是否需要对象引用 多态原理执行原理Java 虚拟机中关于方法重写的判定基于方法描述符，如果子类定义了与父类中非私有、非静态方法同名的方法，只有当这两个方法的参数类型以及返回类型一致，Java 虚拟机才会判定为重写 理解多态： 多态有编译时多态和运行时多态，即静态绑定和动态绑定 前者是通过方法重载实现，后者是通过重写实现（子类覆盖父类方法，虚方法表） 虚方法：运行时动态绑定的方法，对比静态绑定的非虚方法调用来说，虚方法调用更加耗时 方法重写的本质： 找到操作数栈的第一个元素所执行的对象的实际类型，记作 C 如果在类型 C 中找到与描述符和名称都相符的方法，则进行访问权限校验（私有的），如果通过则返回这个方法的直接引用，查找过程结束；如果不通过，则返回 java.lang.IllegalAccessError 异常 找不到，就会按照继承关系从下往上依次对 C 的各个父类进行第二步的搜索和验证过程 如果始终没有找到合适的方法，则抛出 java.lang.AbstractMethodError 异常 虚方法表在虚拟机工作过程中会频繁使用到动态绑定，每次动态绑定的过程中都要重新在类的元数据中搜索合适目标，影响到执行效率。为了提高性能，JVM 采取了一种用空间换取时间的策略来实现动态绑定，在每个类的方法区建立一个虚方法表（virtual method table），实现使用索引表来代替查找，可以快速定位目标方法 invokevirtual 所使用的虚方法表（virtual method table，vtable），执行流程 先通过栈帧中的对象引用找到对象，分析对象头，找到对象的实际 Class Class 结构中有 vtable，查表得到方法的具体地址，执行方法的字节码 invokeinterface 所使用的接口方法表（interface method table，itable） 虚方法表会在类加载的链接阶段被创建并开始初始化，类的变量初始值准备完成之后，JVM 会把该类的方法表也初始化完毕 虚方法表的执行过程： 对于静态绑定的方法调用而言，实际引用是一个指向方法的指针 对于动态绑定的方法调用而言，实际引用是方法表的索引值，也就是方法的间接地址。Java 虚拟机获取调用者的实际类型，并在该实际类型的虚方法表中，根据索引值获得目标方法内存偏移量（指针） 为了优化对象调用方法的速度，方法区的类型信息会增加一个指针，该指针指向一个记录该类方法的方法表。每个类中都有一个虚方法表，本质上是一个数组，每个数组元素指向一个当前类及其祖先类中非私有的实例方法 方法表满足以下的特质： 其一，子类方法表中包含父类方法表中的所有方法，并且在方法表中的索引值与父类方法表种的索引值相同 其二，非重写的方法指向父类的方法表项，与父类共享一个方法表项，重写的方法指向本身自己的实现，这就是为什么多态情况下可以访问父类的方法。 Passenger 类的方法表包括两个方法，分别对应 0 号和 1 号。方法表调换了 toString 方法和 passThroughImmigration 方法的位置，是因为 toString 方法的索引值需要与 Object 类中同名方法的索引值一致，为了保持简洁，这里不考虑 Object 类中的其他方法。 虚方法表对性能的影响： 使用了方法表的动态绑定与静态绑定相比，仅仅多出几个内存解引用操作：访问栈上的调用者、读取调用者的动态类型、读取该类型的方法表、读取方法表中某个索引值所对应的目标方法，但是相对于创建并初始化 Java 栈帧这操作的开销可以忽略不计 上述优化的效果看上去不错，但实际上仅存在于解释执行中，或者即时编译代码的最坏情况。因为即时编译还拥有另外两种性能更好的优化手段：内联缓存（inlining cache）和方法内联（method inlining） 1234567891011121314151617181920212223class Person &#123; public String toString() &#123; return &quot;I&#x27;m a person.&quot;; &#125; public void eat() &#123;&#125; public void speak() &#123;&#125;&#125;class Boy extends Person &#123; public String toString() &#123; return &quot;I&#x27;m a boy&quot;; &#125; public void speak() &#123;&#125; public void fight() &#123;&#125;&#125;class Girl extends Person &#123; public String toString() &#123; return &quot;I&#x27;m a girl&quot;; &#125; public void speak() &#123;&#125; public void sing() &#123;&#125;&#125; 参考文档：https://www.cnblogs.com/kaleidoscope/p/9790766.html 内联缓存内联缓存：是一种加快动态绑定的优化技术，能够缓存虚方法调用中调用者的动态类型以及该类型所对应的目标方法。在之后的执行过程中，如果碰到已缓存的类型，便会直接调用该类型所对应的目标方法；反之内联缓存则会退化至使用基于方法表的动态绑定 多态的三个术语： 单态 (monomorphic)：指的是仅有一种状态的情况 多态 (polymorphic)：指的是有限数量种状态的情况，二态（bimorphic）是多态的其中一种 超多态 (megamorphic)：指的是更多种状态的情况，通常用一个具体数值来区分多态和超多态，在这个数值之下，称之为多态，否则称之为超多态 对于内联缓存来说，有对应的单态内联缓存、多态内联缓存： 单态内联缓存：只缓存了一种动态类型以及所对应的目标方法，实现简单，比较所缓存的动态类型，如果命中，则直接调用对应的目标方法。 多态内联缓存：缓存了多个动态类型及其目标方法，需要逐个将所缓存的动态类型与当前动态类型进行比较，如果命中，则调用对应的目标方法 为了节省内存空间，Java 虚拟机只采用单态内联缓存，没有命中的处理方法： 替换单态内联缓存中的纪录，类似于 CPU 中的数据缓存，对数据的局部性有要求，即在替换内联缓存之后的一段时间内，方法调用的调用者的动态类型应当保持一致，从而能够有效地利用内联缓存 劣化为超多态状态，这也是 Java 虚拟机的具体实现方式，这种状态实际上放弃了优化的机会，将直接访问方法表来动态绑定目标方法，但是与替换内联缓存纪录相比节省了写缓存的额外开销 虽然内联缓存附带内联二字，但是并没有内联目标方法 参考文章：https://time.geekbang.org/column/intro/100010301 代码优化语法糖语法糖：指 Java 编译器把 *.java 源码编译为 *.class 字节码的过程中，自动生成和转换的一些代码，主要是为了减轻程序员的负担 构造器12public class Candy1 &#123;&#125; 1234567public class Candy1 &#123; // 这个无参构造是编译器帮助我们加上的 public Candy1() &#123; super(); // 即调用父类 Object 的无参构造方法，即调用 java/lang/Object.&quot; &lt;init&gt;&quot;:()V &#125;&#125; 拆装箱12Integer x = 1;int y = x; 这段代码在 JDK 5 之前是无法编译通过的，必须改写为代码片段2： 12Integer x = Integer.valueOf(1);int y = x.intValue(); JDK5 以后编译阶段自动转换成上述片段 泛型擦除泛型也是在 JDK 5 开始加入的特性，但 Java 在编译泛型代码后会执行泛型擦除的动作，即泛型信息在编译为字节码之后就丢失了，实际的类型都当做了 Object 类型来处理： 123List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.add(10); // 实际调用的是 List.add(Object e)Integer x = list.get(0); // 实际调用的是 Object obj = List.get(int index); 编译器真正生成的字节码中，还要额外做一个类型转换的操作： 12// 需要将 Object 转为 IntegerInteger x = (Integer)list.get(0); 如果前面的 x 变量类型修改为 int 基本类型那么最终生成的字节码是： 12// 需要将 Object 转为 Integer, 并执行拆箱操作int x = ((Integer)list.get(0)).intValue(); 可变参数123456789public class Candy4 &#123; public static void foo(String... args) &#123; String[] array = args; // 直接赋值 System.out.println(array); &#125; public static void main(String[] args) &#123; foo(&quot;hello&quot;, &quot;world&quot;); &#125;&#125; 可变参数 String... args 其实是 String[] args ， Java 编译器会在编译期间将上述代码变换为： 123public static void main(String[] args) &#123;\tfoo(new String[]&#123;&quot;hello&quot;, &quot;world&quot;&#125;);&#125; 注意：如果调用了 foo() 则等价代码为 foo(new String[]&#123;&#125;) ，创建了一个空的数组，而不会传递 null 进去 foreach数组的循环： 1234int[] array = &#123;1, 2, 3, 4, 5&#125;; // 数组赋初值的简化写法也是语法糖for (int e : array) &#123;\tSystem.out.println(e);&#125; 编译后为循环取数： 1234for(int i = 0; i &lt; array.length; ++i) &#123;\tint e = array[i];\tSystem.out.println(e);&#125; 集合的循环： 1234List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5);for (Integer i : list) &#123;\tSystem.out.println(i);&#125; 编译后转换为对迭代器的调用： 123456List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5);Iterator iter = list.iterator();while(iter.hasNext()) &#123; Integer e = (Integer)iter.next(); System.out.println(e);&#125; 注意：foreach 循环写法，能够配合数组以及所有实现了 Iterable 接口的集合类一起使用，其中 Iterable 用来获取集合的迭代器 switch字符串switch 可以作用于字符串和枚举类： 12345678910switch (str) &#123; case &quot;hello&quot;: &#123; System.out.println(&quot;h&quot;); break; &#125; case &quot;world&quot;: &#123; System.out.println(&quot;w&quot;); break; &#125;&#125; 注意：switch 配合 String 和枚举使用时，变量不能为 null 会被编译器转换为： 1234567891011121314151617181920byte x = -1;switch(str.hashCode()) &#123; case 99162322: // hello 的 hashCode if (str.equals(&quot;hello&quot;)) &#123; x = 0; &#125; break; case 113318802: // world 的 hashCode if (str.equals(&quot;world&quot;)) &#123; x = 1; &#125;&#125;switch(x) &#123; case 0: System.out.println(&quot;h&quot;); break; case 1: System.out.println(&quot;w&quot;); break;&#125; 总结： 执行了两遍 switch，第一遍是根据字符串的 hashCode 和 equals 将字符串的转换为相应 byte 类型，第二遍才是利用 byte 执行进行比较 hashCode 是为了提高效率，减少可能的比较；而 equals 是为了防止 hashCode 冲突 枚举switch 枚举的例子，原始代码： 123456789101112131415enum Sex &#123;\tMALE, FEMALE&#125;public class Candy7 &#123; public static void foo(Sex sex) &#123; switch (sex) &#123; case MALE: System.out.println(&quot;男&quot;); break; case FEMALE: System.out.println(&quot;女&quot;); break; &#125;\t&#125;&#125; 编译转换后的代码： 12345678910111213141516171819202122232425/*** 定义一个合成类（仅 jvm 使用，对我们不可见）* 用来映射枚举的 ordinal 与数组元素的关系* 枚举的 ordinal 表示枚举对象的序号，从 0 开始* 即 MALE 的 ordinal()=0，FEMALE 的 ordinal()=1*/static class $MAP &#123; // 数组大小即为枚举元素个数，里面存储 case 用来对比的数字 static int[] map = new int[2]; static &#123; map[Sex.MALE.ordinal()] = 1; map[Sex.FEMALE.ordinal()] = 2;\t&#125;&#125;public static void foo(Sex sex) &#123; int x = $MAP.map[sex.ordinal()]; switch (x) &#123; case 1: System.out.println(&quot;男&quot;); break; case 2: System.out.println(&quot;女&quot;); break; &#125;&#125; 枚举类JDK 7 新增了枚举类： 123enum Sex &#123;\tMALE, FEMALE&#125; 编译转换后： 12345678910111213141516171819public final class Sex extends Enum&lt;Sex&gt; &#123; public static final Sex MALE; public static final Sex FEMALE; private static final Sex[] $VALUES; static &#123; MALE = new Sex(&quot;MALE&quot;, 0); FEMALE = new Sex(&quot;FEMALE&quot;, 1); $VALUES = new Sex[]&#123;MALE, FEMALE&#125;; &#125; private Sex(String name, int ordinal) &#123; super(name, ordinal); &#125; public static Sex[] values() &#123; return $VALUES.clone(); &#125; public static Sex valueOf(String name) &#123; return Enum.valueOf(Sex.class, name); &#125;&#125; try-w-rJDK 7 开始新增了对需要关闭的资源处理的特殊语法 try-with-resources，格式： 123try(资源变量 = 创建资源对象)&#123;&#125; catch( ) &#123;&#125; 其中资源对象需要实现 AutoCloseable 接口，例如 InputStream、OutputStream、Connection、Statement、ResultSet 等接口都实现了 AutoCloseable ，使用 try-withresources可以不用写 finally 语句块，编译器会帮助生成关闭资源代码： 12345try(InputStream is = new FileInputStream(&quot;d:\\\\1.txt&quot;)) &#123;\tSystem.out.println(is);&#125; catch (IOException e) &#123;\te.printStackTrace();&#125; 转换成： addSuppressed(Throwable e)：添加被压制异常，是为了防止异常信息的丢失（fianlly 中如果抛出了异常） 1234567891011121314151617181920212223242526272829try &#123; InputStream is = new FileInputStream(&quot;d:\\\\1.txt&quot;); Throwable t = null; try &#123; System.out.println(is); &#125; catch (Throwable e1) &#123; // t 是我们代码出现的异常 t = e1; throw e1; &#125; finally &#123; // 判断了资源不为空 if (is != null) &#123; // 如果我们代码有异常 if (t != null) &#123; try &#123; is.close(); &#125; catch (Throwable e2) &#123; // 如果 close 出现异常，作为被压制异常添加 t.addSuppressed(e2); &#125; &#125; else &#123; // 如果我们代码没有异常，close 出现的异常就是最后 catch 块中的 e is.close(); &#125; &#125;\t&#125;&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 方法重写方法重写时对返回值分两种情况： 父子类的返回值完全一致 子类返回值可以是父类返回值的子类 123456789101112class A &#123; public Number m() &#123; return 1; &#125;&#125;class B extends A &#123; @Override // 子类m方法的返回值是Integer是父类m方法返回值Number的子类 public Integer m() &#123; return 2; &#125;&#125; 对于子类，Java 编译器会做如下处理： 12345678910class B extends A &#123; public Integer m() &#123; return 2; &#125;\t// 此方法才是真正重写了父类 public Number m() 方法\tpublic synthetic bridge Number m() &#123; // 调用 public Integer m() return m(); &#125;&#125; 其中桥接方法比较特殊，仅对 Java 虚拟机可见，并且与原来的 public Integer m() 没有命名冲突 匿名内部类无参优化源代码： 12345678910public class Candy11 &#123; public static void main(String[] args) &#123; Runnable runnable = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;ok&quot;); &#125; &#125;; &#125;&#125; 转化后代码： 12345678910111213// 额外生成的类final class Candy11$1 implements Runnable &#123; Candy11$1() &#123; &#125; public void run() &#123; System.out.println(&quot;ok&quot;); &#125;&#125;public class Candy11 &#123; public static void main(String[] args) &#123; Runnable runnable = new Candy11$1(); &#125;&#125; 带参优化引用局部变量的匿名内部类，源代码： 12345678910public class Candy11 &#123; public static void test(final int x) &#123; Runnable runnable = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;ok:&quot; + x); &#125; &#125;; &#125;&#125; 转换后代码： 1234567891011121314final class Candy11$1 implements Runnable &#123; int val$x; Candy11$1(int x) &#123; this.val$x = x; &#125; public void run() &#123; System.out.println(&quot;ok:&quot; + this.val$x); &#125;&#125;public class Candy11 &#123; public static void test(final int x) &#123; Runnable runnable = new Candy11$1(x); &#125;&#125; 局部变量在底层创建为内部类的成员变量，必须是 final 的原因： 在 Java 中方法调用是值传递的，在匿名内部类中对变量的操作都是基于原变量的副本，不会影响到原变量的值，所以原变量的值的改变也无法同步到副本中 外部变量为 final 是在编译期以强制手段确保用户不会在内部类中做修改原变量值的操作，也是防止外部操作修改了变量而内部类无法随之变化出现的影响 在创建 Candy11$1 对象时，将 x 的值赋值给了 Candy11$1 对象的 val 属性，x 不应该再发生变化了，因为发生变化，this.val$x 属性没有机会再跟着变化 反射优化12345678910111213public class Reflect1 &#123; public static void foo() &#123; System.out.println(&quot;foo...&quot;); &#125; public static void main(String[] args) throws Exception &#123; Method foo = Reflect1.class.getMethod(&quot;foo&quot;); for (int i = 0; i &lt;= 16; i++) &#123; System.out.printf(&quot;%d\\t&quot;, i); foo.invoke(null); &#125; System.in.read(); &#125;&#125; foo.invoke 0 ~ 15 次调用的是 MethodAccessor 的实现类 NativeMethodAccessorImpl.invoke0()，本地方法执行速度慢；当调用到第 16 次时，会采用运行时生成的类 sun.reflect.GeneratedMethodAccessor1 代替 123456789101112131415161718public Object invoke(Object obj, Object[] args)throws Exception &#123; // inflationThreshold 膨胀阈值，默认 15 if (++numInvocations &gt; ReflectionFactory.inflationThreshold() &amp;&amp; !ReflectUtil.isVMAnonymousClass(method.getDeclaringClass())) &#123; MethodAccessorImpl acc = (MethodAccessorImpl) new MethodAccessorGenerator(). generateMethod(method.getDeclaringClass(), method.getName(), method.getParameterTypes(), method.getReturnType(), method.getExceptionTypes(), method.getModifiers()); parent.setDelegate(acc); &#125; // 【调用本地方法实现】 return invoke0(method, obj, args);&#125;private static native Object invoke0(Method m, Object obj, Object[] args); 1234567891011121314public class GeneratedMethodAccessor1 extends MethodAccessorImpl &#123; // 如果有参数，那么抛非法参数异常 block4 : &#123; if (arrobject == null || arrobject.length == 0) break block4; throw new IllegalArgumentException(); &#125; try &#123; // 【可以看到，已经是直接调用方法】 Reflect1.foo(); // 因为没有返回值 return null; &#125; //....&#125; 通过查看 ReflectionFactory 源码可知： sun.reflect.noInflation 可以用来禁用膨胀，直接生成 GeneratedMethodAccessor1，但首次生成比较耗时，如果仅反射调用一次，不划算 sun.reflect.inflationThreshold 可以修改膨胀阈值 系统优化性能调优性能指标性能指标主要是吞吐量、响应时间、QPS、TPS 等、并发用户数等，而这些性能指标又依赖于系统服务器的资源，如 CPU、内存、磁盘 IO、网络 IO 等，对于这些指标数据的收集，通常可以根据Java本身的工具或指令进行查询 几个重要的指标： 停顿时间（响应时间）：提交请求和返回该请求的响应之间使用的时间，比如垃圾回收中 STW 的时间 吞吐量：对单位时间内完成的工作量（请求）的量度（可以对比 GC 的性能指标） 并发数：同一时刻，对服务器有实际交互的请求数 QPS：Queries Per Second，每秒处理的查询量 TPS：Transactions Per Second，每秒产生的事务数 内存占用：Java 堆区所占的内存大小 优化步骤对于一个系统要部署上线时，则一定会对 JVM 进行调整，不经过任何调整直接上线，容易出现线上系统频繁 FullGC 造成系统卡顿、CPU 使用频率过高、系统无反应等问题 性能监控：通过运行日志、堆栈信息、线程快照等信息监控是否出现 GC 频繁、OOM、内存泄漏、死锁、响应时间过长等情况 性能分析： 打印 GC 日志，通过 GCviewer 或者 http://gceasy.io 来分析异常信息 运用命令行工具、jstack、jmap、jinfo 等 dump 出堆文件，使用内存分析工具分析文件 使用阿里 Arthas、jconsole、JVisualVM 来实时查看 JVM 状态 jstack 查看堆栈信息 性能调优： 适当增加内存，根据业务背景选择垃圾回收器 优化代码，控制内存使用 增加机器，分散节点压力 合理设置线程池线程数量 使用中间件提高程序效率，比如缓存、消息队列等 参数调优对于 JVM 调优，主要就是调整年轻代、老年代、元空间的内存空间大小及使用的垃圾回收器类型 设置堆的初始大小和最大大小，为了防止垃圾收集器在初始大小、最大大小之间收缩堆而产生额外的时间，通常把最大、初始大小设置为相同的值 12-Xms：设置堆的初始化大小-Xmx：设置堆的最大大小 设置年轻代中 Eden 区和两个 Survivor 区的大小比例。该值如果不设置，则默认比例为 8:1:1。Java 官方通过增大 Eden 区的大小，来减少 YGC 发生的次数，虽然次数减少了，但 Eden 区满的时候，由于占用的空间较大，导致释放缓慢，此时 STW 的时间较长，因此需要按照程序情况去调优 1-XX:SurvivorRatio 年轻代和老年代默认比例为 1:2，可以通过调整二者空间大小比率来设置两者的大小。 12-XX:newSize 设置年轻代的初始大小-XX:MaxNewSize 设置年轻代的最大大小， 初始大小和最大大小两个值通常相同 线程堆栈的设置：每个线程默认会开启 1M 的堆栈，用于存放栈帧、调用参数、局部变量等，但一般 256K 就够用，通常减少每个线程的堆栈，可以产生更多的线程，但这实际上还受限于操作系统 1-Xss 对每个线程stack大小的调整,-Xss128k 一般一天超过一次 FullGC 就是有问题，首先通过工具查看是否出现内存泄露，如果出现内存泄露则调整代码，没有的话则调整 JVM 参数 系统 CPU 持续飙高的话，首先先排查代码问题，如果代码没问题，则咨询运维或者云服务器供应商，通常服务器重启或者服务器迁移即可解决 如果数据查询性能很低下的话，如果系统并发量并没有多少，则应更加关注数据库的相关问题 如果服务器配置还不错，JDK8 开始尽量使用 G1 或者新生代和老年代组合使用并行垃圾回收器 命令行篇jpsjps（Java Process Statu）：显示指定系统内所有的 HotSpot 虚拟机进程（查看虚拟机进程信息），可用于查询正在运行的虚拟机进程，进程的本地虚拟机 ID 与操作系统的进程 ID 是一致的，是唯一的 使用语法：jps [options] [hostid] options 参数： -q：仅仅显示 LVMID（local virtual machine id），即本地虚拟机唯一 id，不显示主类的名称等 -l：输出应用程序主类的全类名或如果进程执行的是 jar 包，则输出 jar 完整路径 -m：输出虚拟机进程启动时传递给主类 main()的参数 -v：列出虚拟机进程启动时的JVM参数，比如 -Xms20m -Xmx50m是启动程序指定的 jvm 参数 ostid 参数：RMI注册表中注册的主机名，如果想要远程监控主机上的 java 程序，需要安装 jstatd jstatjstat（JVM Statistics Monitoring Tool）：用于监视 JVM 各种运行状态信息的命令行工具，可以显示本地或者远程虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据，在没有 GUI 的图形界面，只提供了纯文本控制台环境的服务器上，它是运行期定位虚拟机性能问题的首选工具，常用于检测垃圾回收问题以及内存泄漏问题 使用语法：jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]] 查看命令相关参数：jstat-h 或 jstat-help vmid 是进程 id 号 option 参数： 类装载相关： -class：显示 ClassLoader 的相关信息，类的装载、卸载数量、总空间、类装载所消耗的时间等 垃圾回收相关： -gc：显示与GC相关的堆信息，年轻代、老年代、永久代等的容量、已用空间、GC时间合计等信息 -gccapacity：显示内容与 -gc 基本相同，但输出主要关注 Java 堆各个区域使用到的最大、最小空间 -gcutil：显示内容与 -gc 基本相同，但输出主要关注已使用空间占总空间的百分比 -gccause：与 -gcutil 功能一样，但是会额外输出导致最后一次或当前正在发生的 GC 产生的原因 -gcnew：显示新生代 GC 状况 -gcnewcapacity：显示内容与 -gcnew 基本相同，输出主要关注使用到的最大、最小空间 -geold：显示老年代 GC 状况 -gcoldcapacity：显示内容与 -gcold 基本相同，输出主要关注使用到的最大、最小空间 -gcpermcapacity：显示永久代使用到的最大、最小空间 JIT 相关： -compiler：显示 JIT 编译器编译过的方法、耗时等信息 -printcompilation：输出已经被 JIT 编译的方法 jinfojinfo（Configuration Info for Java）：查看虚拟机配置参数信息，也可用于调整虚拟机的配置参数，开发人员可以很方便地找到 Java 虚拟机参数的当前值 使用语法：jinfo [options] pid options 参数： no option：输出全部的参数和系统属性 -flag name：输出对应名称的参数 -flag [+-]name：开启或者关闭对应名称的参数 只有被标记为manageable的参数才可以被动态修改 -flag name&#x3D;value：设定对应名称的参数 -flags：输出全部的参数 -sysprops：输出系统属性 jmapjmap（JVM Memory Map）：获取 dump 文件，还可以获取目标 Java 进程的内存相关信息，包括 Java 堆各区域的使用情况、堆中对象的统计信息、类加载信息等 使用语法： jmap [options] &lt;pid&gt; jmap [options] &lt;executable &lt;core&gt; jmap [options] [server_id@] &lt;remote server IP or hostname&gt; option 参数： -dump：生成 dump 文件（Java堆转储快照，二进制文件），-dump:live 只保存堆中的存活对象 -heap：输出整个堆空间的详细信息，包括 GC 的使用、堆配置信息，以及内存的使用信息等 -histo：输出堆空间中对象的统计信息，包括类、实例数量和合计容量，-histo:live 只统计堆中的存活对象 -J ：传递参数给 jmap 启动的 jvm -finalizerinfo：显示在 F-Queue 中等待 Finalizer 线程执行 finalize 方法的对象，仅 linux&#x2F;solaris 平台有效 -permstat：以 ClassLoader 为统计口径输出永久代的内存状态信息，仅 linux&#x2F;solaris 平台有效 -F：当虚拟机进程对 -dump 选项没有任何响应时，强制执行生成 dump 文件，仅 linux&#x2F;solaris 平台有效 jhatjhat（JVM Heap Analysis Tool）：Sun JDK 提供的 jhat 命令与 jmap 命令搭配使用，用于分析 jmap 生成的 heap dump 文件（堆转储快照），jhat 内置了一个微型的 HTTP&#x2F;HTML 服务器，生成 dump 文件的分析结果后，用户可以在浏览器中查看分析结果 使用语法：jhat &lt;options&gt; &lt;dumpfile&gt; options 参数： -stack false｜true：关闭｜打开对象分配调用栈跟踪 -refs false｜true：关闭｜打开对象引用跟踪 -port port-number：设置 jhat HTTP Server 的端口号，默认 7000 -exclude exclude-file：执行对象查询时需要排除的数据成员 -baseline exclude-file：指定一个基准堆转储 -debug int：设置 debug 级别 -version：启动后显示版本信息就退出 -J ：传入启动参数，比如 -J-Xmx512m 说明：jhat 命令在 JDK9、JDK10 中已经被删除，官方建议用 VisualVM 代替 jstackjstack（JVM Stack Trace）：用于生成虚拟机指定进程当前时刻的线程快照（虚拟机堆栈跟踪），线程快照就是当前虚拟机内指定进程的每一条线程正在执行的方法堆栈的集合 线程快照的作用：可用于定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等问题，用 jstack 显示各个线程调用的堆栈情况 使用语法：jstack [options] pid options 参数： -F：当正常输出的请求不被响应时，强制输出线程堆栈 -l：除堆栈外，显示关于锁的附加信息 -m：如果调用本地方法的话，可以显示 C&#x2F;C++ 的堆栈 在 thread dump 中的几种状态： 死锁：Deadlock 等待资源：Waiting on condition 等待获取监视器：Waiting on monitor entry 阻塞：Blocked 执行中：Runnable 暂停：Suspended 对象等待中：Object.wait() 或 TIMED＿WAITING 停止：Parked jcmdjcmd 是一个多功能命令行工具，可以用来实现前面除了 jstat 之外所有命令的功能，比如 dump、内存使用、查看 Java 进程、导出线程信息、执行 GC、JVM 运行时间等 jcmd -l：列出所有的JVM进程 jcmd 进程号 help：针对指定的进程，列出支持的所有具体命令 Thread.print：可以替换 jstack 指令 GC.class_histogram：可以替换 jmap 中的 -histo 操作 GC.heap_dump：可以替换 jmap 中的 -dump 操作 GC.run：可以查看GC的执行情况 VM.uptime：可以查看程序的总执行时间，可以替换 jstat 指令中的 -t 操作 VM.system_properties：可以替换 jinfo -sysprops 进程 id VM.flags：可以获取 JVM 的配置参数信息 jstatdjstatd 是一个 RMI 服务端程序，相当于代理服务器，建立本地计算机与远程监控工具的通信，jstatd 服务器将本机的 Java 应用程序信息传递到远程计算机 远程主机信息收集，前面的指令只涉及到监控本机的 Java 应用程序，而在这些工具中，一些监控工具也支持对远程计算机的监控（如 jps、jstat），为了启用远程监控，则需要配合使用 jstatd 工具。 GUI工具工具的使用此处不再多言，推荐一个写的非常好的文章，JVM 调优部分的笔记全部参考此文章编写 视频链接：https://www.bilibili.com/video/BV1PJ411n7xZ?p=304 文章链接：https://www.yuque.com/u21195183/jvm/lv1zot 运行参数参数选项添加 JVM 参数选项：进入 Run&#x2F;Debug Configurations → VM options 设置参数 标准参数选项：java [-options] class [args...] 或 java [-options] -jar jarfile [args...] 命令：-? -help 可以输出此命令的相关选项 12345C:\\Users\\Seazean&gt;java -versionjava version &quot;1.8.0_221&quot;Java(TM) SE Runtime Environment (build 1.8.0_221-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.221-b11, mixed mode)# mixed mode 字样，代表当前系统使用的是混合模式 Hotspot JVM 有两种模式，分别是 Server 和 Client，分别通过 -server 和- client 设置模式： 32 位系统上，默认使用 Client 类型的 JVM，要使用 Server 模式，机器配置至少有 2 个以上的内核和 2G 以上的物理内存，Client 模式适用于对内存要求较小的桌面应用程序，默认使用 Serial 串行垃圾收集器 64 位系统上，只支持 Server 模式的 JVM，适用于需要大内存的应用程序，默认使用并行垃圾收集器 -X 参数选项： 12345678910111213141516171819202122-Xmixed 混合模式执行 (默认)-Xint 仅解释模式执行-Xbootclasspath:&lt;用;分隔的目录和zip/jar文件&gt;设置搜索路径以引导类和资源-Xbootclasspath/a:&lt;用;分隔的目录和zip/jar文件&gt;附加在引导类路径末尾-Xbootclasspath/p:&lt;用;分隔的目录和zip/jar文件&gt;置于引导类路径之前-Xdiag 显示附加诊断消息-Xnoclassgc 禁用类垃圾收集-Xincgc 启用增量垃圾收集-Xloggc:&lt;file&gt; 将 GC 状态记录在文件中 (带时间戳)-Xbatch 禁用后台编译-Xprof 输出 cpu 配置文件数据-Xfuture 启用最严格的检查, 预期将来的默认值-Xrs 减少 Java/VM 对操作系统信号的使用 (请参阅文档)-Xcheck:jni 对 JNI 函数执行其他检查-Xshare:off 不尝试使用共享类数据-Xshare:auto 在可能的情况下使用共享类数据 (默认)-Xshare:on 要求使用共享类数据, 否则将失败。-XshowSettings 显示所有设置并继续-XshowSettings:all 显示所有设置并继续-XshowSettings:vm 显示所有与 vm 相关的设置并继续-XshowSettings:properties\t显示所有属性设置并继续-XshowSettings:locale 显示所有与区域设置相关的设置并继续 -XX 参数选项： 123456#Boolean类型格式-XX:+&lt;option&gt; 启用option属性-XX:-&lt;option&gt; 禁用option属性#非Boolean类型格式-XX:&lt;option&gt;=&lt;number&gt; 设置option数值，可以带单位如k/K/m/M/g/G-XX:&lt;option&gt;=&lt;string&gt; 设置option字符值 程序运行中： 1234# 设置Boolean类型参数jinfo -flag [+|-]&lt;name&gt; &lt;pid&gt;# 设置非Boolean类型参数jinfo -flag &lt;name&gt;=&lt;value&gt; &lt;pid&gt; 打印参数1234-XX:+PrintCommandLineFlags 程序运行时JVM默认设置或用户手动设置的XX选项-XX:+PrintFlagsInitial 打印所有XX选项的默认值-XX:+PrintFlagsFinal 打印所有XX选项的实际值-XX:+PrintVMOptions 打印JVM的参数 内存参数1234567891011121314151617181920212223# 栈-Xss128k &lt;==&gt; -XX:ThreadStackSize=128k 设置线程栈的大小为128K# 堆-Xms2048m &lt;==&gt; -XX:InitialHeapSize=2048m 设置JVM初始堆内存为2048M（默认为物理内存的1/64）-Xmx2048m &lt;==&gt; -XX:MaxHeapSize=2048m 设置JVM最大堆内存为2048M（默认为物理内存的1/4）-Xmn2g &lt;==&gt; -XX:NewSize=2g 设置年轻代大小为2G-XX:SurvivorRatio=8 设置Eden区与Survivor区的比值，默认为8-XX:NewRatio=2 设置老年代与年轻代的比例，默认为2-XX:+UseAdaptiveSizePolicy 设置大小比例自适应，默认开启-XX:PretenureSizeThreadshold=1024 设置让大于此阈值的对象直接分配在老年代，只对Serial、ParNew收集器有效-XX:MaxTenuringThreshold=15 设置新生代晋升老年代的年龄限制，默认为15-XX:TargetSurvivorRatio 设置MinorGC结束后Survivor区占用空间的期望比例# 方法区-XX:MetaspaceSize / -XX:PermSize=256m 设置元空间/永久代初始值为256M-XX:MaxMetaspaceSize / -XX:MaxPermSize=256m 设置元空间/永久代最大值为256M-XX:+UseCompressedOops 使用压缩对象-XX:+UseCompressedClassPointers 使用压缩类指针-XX:CompressedClassSpaceSize 设置Klass Metaspace的大小，默认1G# 直接内存-XX:MaxDirectMemorySize 指定DirectMemory容量，默认等于Java堆最大值 说明：参数前面是+号说明是开启，如果是- 号说明是关闭 OOM参数1234-XX:+HeapDumpOnOutMemoryError 内存出现OOM时生成Heap转储文件，两者互斥-XX:+HeapDumpBeforeFullGC 出现FullGC时生成Heap转储文件，两者互斥-XX:HeapDumpPath=&lt;path&gt; 指定heap转储文件的存储路径，默认当前目录-XX:OnOutOfMemoryError=&lt;path&gt; 指定可行性程序或脚本的路径，当发生OOM时执行脚本 日志参数1234567891011121314-XX:+PrintGC &lt;==&gt; -verbose:gc 打印简要日志信息-XX:+PrintGCDetails 打印详细日志信息-XX:+PrintGCTimeStamps 打印程序启动到GC发生的时间，搭配-XX:+PrintGCDetails使用-XX:+PrintGCDateStamps 打印GC发生时的时间戳，搭配-XX:+PrintGCDetails使用-XX:+PrintHeapAtGC 打印GC前后的堆信息，如下图-Xloggc:&lt;file&gt; 输出GC导指定路径下的文件中-XX:+TraceClassLoading 监控类的加载-XX:+PrintTenuringDistribution\t打印JVM在每次MinorGC后当前使用的Survivor中对象的年龄分布-XX:+PrintGCApplicationStoppedTime 打印GC时线程的停顿时间-XX:+PrintGCApplicationConcurrentTime 打印垃圾收集之前应用未中断的执行时间-XX:+PrintReferenceGC 打印回收了多少种不同引用类型的引用-XX:+UseGCLogFileRotation 启用GC日志文件的自动转储-XX:NumberOfGCLogFiles=1 设置GC日志文件的循环数目-XX:GCLogFileSize=1M 设置GC日志文件的大小 其他参数12345678-XX:+DisableExplicitGC 禁用hotspot执行System.gc()，默认禁用-XX:+DoEscapeAnalysis 开启逃逸分析-XX:+UseBiasedLocking 开启偏向锁-XX:+UseLargePages 开启使用大页面-XX:+PrintTLAB 打印TLAB的使用情况-XX:TLABSize 设置TLAB大小-XX:ReservedCodeCacheSize=&lt;n&gt;[g|m|k]、-XX:InitialCodeCacheSize=&lt;n&gt;[g|m|k] 指定代码缓存大小-XX:+UseCodeCacheFlushing 放弃一些被编译的代码，避免代码缓存被占满时JVM切换到interpreted-only的情况 代码获取Java 提供了 java.lang.management 包用于监视和管理 Java 虚拟机和 Java 运行时中的其他组件，允许本地或远程监控和管理运行的 Java 虚拟机。ManagementFactory 类较为常用，Runtime 类可获取内存、CPU 核数等相关的数据，通过使用这些方法，可以监控应用服务器的堆内存使用情况，设置一些阈值进行报警等处理 123456789101112131415161718public class MemoryMonitor &#123; public static void main(String[] args) &#123; MemoryMXBean memorymbean = ManagementFactory.getMemoryMXBean(); MemoryUsage usage = memorymbean.getHeapMemoryUsage(); System.out.println(&quot;INIT HEAP: &quot; + usage.getInit() / 1024 / 1024 + &quot;m&quot;); System.out.println(&quot;MAX HEAP: &quot; + usage.getMax() / 1024 / 1024 + &quot;m&quot;); System.out.println(&quot;USE HEAP: &quot; + usage.getUsed() / 1024 / 1024 + &quot;m&quot;); System.out.println(&quot; Full Information:&quot;); System.out.println(&quot;Heap Memory Usage: &quot; + memorymbean.getHeapMemoryUsage()); System.out.println(&quot;Non-Heap Memory Usage: &quot; + memorymbean.getNonHeapMemoryUsage()); System.out.println(&quot;====通过java来获取相关系统状态====&quot;); System.out.println(&quot;当前堆内存大小totalMemory &quot; + (int) Runtime.getRuntime().totalMemory() / 1024 / 1024 + &quot;m&quot;);// 当前堆内存大小 System.out.println(&quot;空闲堆内存大小freeMemory &quot; + (int) Runtime.getRuntime().freeMemory() / 1024 / 1024 + &quot;m&quot;);// 空闲堆内存大小 System.out.println(&quot;最大可用总堆内存maxMemory &quot; + Runtime.getRuntime().maxMemory() / 1024 / 1024 + &quot;m&quot;);// 最大可用总堆内存大小 &#125;&#125; 日志分析日志分类HotSpot VM 的 GC 按照回收区域分为两类：一种是部分收集（Partial GC），一种是整堆收集（Full GC） 部分收集（Partial GC）：不是完整收集整个 Java 堆的垃圾收集。其中又分为： 新生代收集（Minor GC&#x2F;Young GC）：只是新生代（Eden&#x2F;S0、S1）的垃圾收集 老年代收集（Major GC&#x2F;Old GC）：只是老年代的垃圾收集，只有 CMS GC 会有单独收集老年代的行为 混合收集（Mixed GC）：收集整个新生代以及部分老年代的垃圾收集，只有 G1 GC 会有这种行为 整堆收集（Full GC）：收集整个 Java 堆和方法区的垃圾收集。 Minor GC&#x2F;Young GC 日志： 1[GC (Allocation Failure) [PSYoungGen: 31744K-&gt;2192K (36864K) ] 31744K-&gt;2200K (121856K), 0.0139308 secs] [Times: user=0.05 sys=0.01, real=0.01 secs] Full GC 日志： 1[Full GC (Metadata GC Threshold) [PSYoungGen: 5104K-&gt;0K (132096K) ] [Par01dGen: 416K-&gt;5453K (50176K) ]5520K-&gt;5453K (182272K), [Metaspace: 20637K-&gt;20637K (1067008K) ], 0.0245883 secs] [Times: user=0.06 sys=0.00, real=0.02 secs] 日志解析通过日志看垃圾收集器： Serial 收集器：新生代显示 [DefNew，即 Default New Generation ParNew 收集器：新生代显示 [ParNew，即 Parallel New Generation Parallel Scavenge 收集器：新生代显示 [PSYoungGen，JDK1.7 使用的 PSYoungGen Parallel Old 收集器：老年代显示 [ParoldGen G1 收集器：显示 garbage-first heap 通过日志看 GC 原因： Allocation Failure：表明本次引起 GC 的原因是因为新生代中没有足够的区域存放需要分配的数据 Metadata GCThreshold：Metaspace 区不足 FErgonomics：JVM 自适应调整导致的 GC System：调用了 System.gc() 方法 通过日志看 GC 前后情况：GC 前内存占用 → GC 后内存占用（该区域内存总大小） 1[PSYoungGen: 5986K-&gt;696K (8704K)] 5986K-&gt;704K (9216K) 中括号内：GC 回收前年轻代堆大小 → 回收后大小（年轻代堆总大小） 括号外：GC 回收前年轻代和老年代大小 → 回收后大小（年轻代和老年代总大小） Minor GC 堆内存总容量 &#x3D; 9&#x2F;10 年轻代 + 老年代，Survivor 区只计算 from 部分，而 JVM 默认年轻代中 Eden 区和 Survivor 区的比例关系：Eden:S0:S1&#x3D;8:1:1 通过日志看 GC 时间：GC 日志中有三个时间 user、sys、real user：进程执行用户态代码（核心之外）所使用的时间，这是执行此进程所使用的实际 CPU 时间，其他进程和此进程阻塞的时间并不包括在内，在垃圾收集的情况下，表示 GC 线程执行所使用的 CPU 总时间。 sys：进程在内核态消耗的 CPU 时间，即在内核执行系统调用或等待系统事件所使用的 CPU 时间 real：程序从开始到结束所用的时钟时间，这个时间包括其他进程使用的时间片和进程阻塞的时间（比如等待 I&#x2F;O 完成），对于并行 GC，这个数字应该接近（用户时间＋系统时间）除以垃圾收集器使用的线程数 由于多核的原因，一般的 GC 事件中，real time 小于 sys time＋user time，因为是多个线程并发的去做 GC。如果 real＞sys＋user 的话，则说明 IO 负载非常重或 CPU 不够用 分析工具GCEasy 是一款在线的 GC 日志分析器，可以通过 GC 日志分析进行内存泄露检测、GC 暂停原因分析、JVM 配置建议优化等功能，大多数功能是免费的 官网地址：https://gceasy.io/ GCViewer 是一款离线的 GC 日志分析器，用于可视化 Java VM 选项 -verbose:gc 和 .NET 生成的数据 -Xloggc:，还可以计算与垃圾回收相关的性能指标（吞吐量、累积的暂停、最长的暂停等），当通过更改世代大小或设置初始堆大小来调整特定应用程序的垃圾回收时，此功能非常有用 源码下载：https://github.com/chewiebug/GCViewer 运行版本下载：https://github.com/chewiebug/GCViewer/wiki/Changelog 参考文章：https://www.yuque.com/u21195183/jvm/ukmb3k ALG递归概述算法：解题方案的准确而完整的描述，是一系列解决问题的清晰指令，代表着用系统的方法解决问题的策略机制 递归：程序调用自身的编程技巧 递归： 直接递归：自己的方法调用自己 间接递归：自己的方法调用别的方法，别的方法又调用自己 递归如果控制的不恰当，会形成递归的死循环，从而导致栈内存溢出错误 参考书籍：https://book.douban.com/subject/35263893/ 算法核心思想递归的三要素（理论）： 递归的终结点 递归的公式 递归的方向：必须走向终结点 1234567891011// f(x)=f(x-1)+1; f(1)=1; f(10)=?// 1.递归的终结点： f(1) = 1// 2.递归的公式：f(x) = f(x - 1) + 1// 3.递归的方向：必须走向终结点public static int f(int x)&#123; if(x == 1)&#123; return 1; &#125;else&#123; return f(x-1) + 1; &#125;&#125; 公式转换12345678910111213// 已知： f(x) = f(x + 1) + 2, f(1) = 1。求：f(10) = ?// 公式转换// f(x-1)=f(x-1+1)+2 =&gt; f(x)=f(x-1)+2//（1）递归的公式： f(n) = f(n-1)- 2 ;//（2）递归的终结点： f(1) = 1//（3）递归的方向：必须走向终结点。public static int f(int n)&#123; if(n == 1)&#123; return 1; &#125;else&#123; return f(n-1) - 2; &#125;&#125; 注意事项以上理论只能针对于规律化递归，如果是非规律化是不能套用以上公式的！非规律化递归的问题：文件搜索，啤酒问题。 应用猴子吃桃猴子第一天摘了若干个桃子，当即吃了一半，觉得好不过瘾，然后又多吃了一个。第二天又吃了前一天剩下的一半，觉得好不过瘾，然后又多吃了一个。以后每天都是如此。等到第十天再吃的时候发现只有1个桃子，问猴子第一天总共摘了多少个桃子？ 12345678910111213/*（1）公式： f(x+1)=f(x)-f(x)/2-1; ==&gt; 2f(x+1) = f(x) - 2 ==&gt; f(x)=2f(x+1)+2（2）终结点：f(10) = 1（3）递归的方向：走向了终结点*/public static int f(int x)&#123; if(x == 10)&#123; return 1; &#125; else &#123; return 2*f(x+1)+2 &#125;&#125; 递归求和1234567//（1）递归的终点接：f(1) = 1//（2）递归的公式： f(n) = f(n-1) + n//（3）递归的方向必须走向终结点：public static int f(int n)&#123; if(n == 1 ) return 1; return f(n-1) + n;&#125; 汉诺塔12345678910111213141516public class Hanoi &#123; public static void main(String[] args) &#123; hanoi(&#x27;X&#x27;, &#x27;Y&#x27;, &#x27;Z&#x27;, 3); &#125; // 将n个块分治的从x移动到z，y为辅助柱 private static void hanoi(char x, char y, char z, int n) &#123; if (n == 1) &#123; System.out.println(x + &quot;→&quot; + z); // 直接将x的块移动到z &#125; else &#123; hanoi(x, z, y, n - 1); // 分治处理n-1个块，先将n-1个块借助z移到y System.out.println(x + &quot;→&quot; + z); // 然后将x最下面的块（最大的）移动到z hanoi(y, x, z, n - 1); // 最后将n-1个块从y移动到z，x为辅助柱 &#125; &#125;&#125; 时间复杂度 O(2^n) 啤酒问题非规律化递归问题，啤酒 2 元 1 瓶，4 个盖子可以换 1 瓶，2 个空瓶可以换 1 瓶 12345678910111213141516171819202122232425262728293031public class BeerDemo&#123; // 定义一个静态变量存储可以喝酒的总数 public static int totalNum; public static int lastBottleNum; public static int lastCoverNum; public static void main(String[] args) &#123; buyBeer(10); System.out.println(&quot;总数：&quot;+totalNum); System.out.println(&quot;剩余盖子：&quot;+ lastCoverNum); System.out.println(&quot;剩余瓶子：&quot;+ lastBottleNum); &#125; public static void buyBeer(int money)&#123; int number = money / 2; totalNum += number; // 算出当前剩余的全部盖子和瓶子数，换算成金额继续购买。 int currentBottleNum = lastBottleNum + number ; int currentCoverNum = lastCoverNum + number ; // 把他们换算成金额 int totalMoney = 0 ; totalMoney += (currentBottleNum/2)*2; // 除2代表可以换几个瓶子，乘2代表换算成钱，秒！ lastBottleNum = currentBottleNum % 2 ;// 取余//算出剩余的瓶子 totalMoney += (currentCoverNum / 4) * 2; lastCoverNum = currentCoverNum % 4 ; // 继续拿钱买酒 if(totalMoney &gt;= 2)&#123; buyBeer(totalMoney); &#125; &#125;&#125; 排序冒泡排序冒泡排序（Bubble Sort）：两个数比较大小，较大的数下沉，较小的数冒起来 算法描述：每次从数组的第一个位置开始两两比较，把较大的元素与较小的元素进行层层交换，最终把当前最大的一个元素存入到数组当前的末尾 实现思路： 确定总共需要冒几轮：数组的长度-1 每轮两两比较几次 12345678910111213141516171819202122232425// 0 1位置比较，大的放后面，然后1 2位置比较，大的继续放后面，一轮循环最后一位是最大值public class BubbleSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;55, 22, 2, 5, 1, 3, 8, 5, 7, 4, 3, 99, 88&#125;; int flag;//标记本趟排序是否发生了交换 //比较i和i+1，不需要再比最后一个位置 for (int i = 0; i &lt; arr.length - 1; i++) &#123; flag = 0; //最后i位不需要比，已经排序好 for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; flag = 1;//发生了交换 &#125; &#125; //没有发生交换，证明已经有序，不需要继续排序，节省时间 if(flag == 0) &#123; break; &#125; &#125; System.out.println(Arrays.toString(arr)); &#125;&#125; 冒泡排序时间复杂度：最坏情况 元素比较的次数为：(N-1)+(N-2)+(N-3)+...+2+1=((N-1)+1)*(N-1)/2=N^2/2-N/2 元素交换的次数为：(N-1)+(N-2)+(N-3)+...+2+1=((N-1)+1)*(N-1)/2=N^2/2-N/2 总执行次数为：(N^2/2-N/2)+(N^2/2-N/2)=N^2-N 按照大 O 推导法则，保留函数中的最高阶项那么最终冒泡排序的时间复杂度为 O(N^2) 选择排序简单选择选择排序（Selection-sort）：一种简单直观的排序算法 算法描述：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕 实现思路： 控制选择几轮：数组的长度 - 1 控制每轮从当前位置开始比较几次 12345678910111213141516171819public class SelectSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;55, 22, 2, 5, 1, 3, 8, 5, 7, 4, 3, 99, 88&#125;; for (int i = 0; i &lt; arr.length - 1; i++) &#123; //获取最小索引位置 int minIndex = i; for (int j = i + 1; j &lt; arr.length; j++) &#123; if (arr[minIndex] &gt; arr[j]) &#123; minIndex = j; &#125; &#125; //交换元素 int temp = arr[i]; arr[i] = arr[minIndex]; arr[minIndex] = temp; &#125; System.out.println(Arrays.toString(arr)); &#125;&#125; 选择排序时间复杂度： 数据比较次数：(N-1)+(N-2)+(N-3)+...+2+1=((N-1)+1)*(N-1)/2=N^2/2-N/2 数据交换次数：N-1 时间复杂度：N^2/2-N/2+（N-1）=N^2/2+N/2-1 根据大 O 推导法则，保留最高阶项，去除常数因子，时间复杂度为 O(N^2) 堆排序堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法，堆结构是一个近似完全二叉树的结构，并同时满足子结点的键值或索引总是小于（或者大于）父节点 优先队列：堆排序每次上浮过程都会将最大或者最小值放在堆顶，应用于优先队列可以将优先级最高的元素浮到堆顶 实现思路： 将初始待排序关键字序列（R1,R2….Rn）构建成大顶堆，并通过上浮对堆进行调整，此堆为初始的无序区，堆顶为最大数 将堆顶元素 R[1] 与最后一个元素 R[n] 交换，此时得到新的无序区（R1,R2,……Rn-1）和新的有序区 Rn，且满足 R[1,2…n-1]&lt;&#x3D;R[n] 交换后新的堆顶 R[1] 可能违反堆的性质，因此需要对当前无序区（R1,R2,……Rn-1）调整为新堆，然后再次将 R[1] 与无序区最后一个元素交换，得到新的无序区（R1,R2….Rn-2）和新的有序区（Rn-1,Rn），不断重复此过程直到有序区的元素个数为 n-1，则整个排序过程完成 floor：向下取整 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class HeapSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;55, 22, 2, 5, 1, 3, 8, 5, 7, 4, 3, 99, 88&#125;; heapSort(arr, arr.length - 1); System.out.println(Arrays.toString(arr)); &#125; //high为数组最大索引 private static void heapSort(int[] arr, int high) &#123; //建堆，逆排序，因为堆排序定义的交换顺序是从当前结点往下交换，逆序排可以避免多余的交换 //i初始值是最后一个节点的父节点，如果参数是数组长度len，则 i = len / 2 -1 for (int i = (high - 1) / 2; i &gt;= 0; i--) &#123; //调整函数 sift(arr, i, high); &#125; //从尾索引开始排序 for (int i = high; i &gt; 0; i--) &#123; //将最大的节点放入末尾 int temp = arr[0]; arr[0] = arr[i]; arr[i] = temp; //继续寻找最大的节点 sift(arr, 0, i - 1); &#125; &#125; //调整函数，调整arr[low]的元素，从索引low到high的范围调整 private static void sift(int[] arr, int low, int high) &#123; //暂存调整元素 int temp = arr[low]; int i = low, j = low * 2 + 1;//j是左节点 while (j &lt;= high) &#123; //判断是否有右孩子，并且比较左右孩子中较大的节点 if (j &lt; high &amp;&amp; arr[j] &lt; arr[j + 1]) &#123; j++; //指向右孩子 &#125; if (temp &lt; arr[j]) &#123; arr[i] = arr[j]; i = j; //继续向下调整 j = 2 * i + 1; &#125; else &#123; //temp &gt; arr[j]，说明也大于j的孩子，探测结束 break; &#125; &#125; //将被调整的节点放入最终的位置 arr[i] = temp; &#125;&#125; 堆排序的时间复杂度是 O(nlogn) 插入排序直接插入插入排序（Insertion Sort）：在要排序的一组数中，假定前 n-1 个数已经排好序，现在将第 n 个数插到这个有序数列中，使得这 n 个数也是排好顺序的，如此反复循环，直到全部排好顺序 123456789101112131415161718public class InsertSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;55, 22, 2, 5, 1, 3, 8, 5, 7, 4, 3, 99, 88&#125;; for (int i = 1; i &lt; arr.length; i++) &#123; for (int j = i; j &gt; 0; j--) &#123; // 比较索引j处的值和索引j-1处的值， // 如果索引j-1处的值比索引j处的值大，则交换数据， // 如果不大，那么就找到合适的位置了，退出循环即可； if (arr[j - 1] &gt; arr[j]) &#123; int temp = arr[j]; arr[j] = arr[j - 1]; arr[j - 1] = temp; &#125; &#125; &#125; System.out.println(Arrays.toString(arr)); &#125;&#125; 插入排序时间复杂度： 比较的次数为：(N-1)+(N-2)+(N-3)+...+2+1=((N-1)+1)*(N-1)/2=N^2/2-N/2 交换的次数为：(N-1)+(N-2)+(N-3)+...+2+1=((N-1)+1)(N-1)/2=N^2/2-N/2 总执行次数为：(N^2/2-N/2)+(N^2/2-N/2)=N^2-N 按照大 O 推导法则，保留函数中的最高阶项那么最终插入排序的时间复杂度为 O(N^2) 希尔排序希尔排序（Shell Sort）：也是一种插入排序，也称为缩小增量排序 实现思路： 选定一个增长量 h，按照增长量 h 作为数据分组的依据，对数据进行分组 对分好组的每一组数据完成插入排序 减小增长量，最小减为 1，重复第二步操作 希尔排序的核心在于间隔序列的设定，既可以提前设定好间隔序列，也可以动态的定义间隔序列，希尔排序就是插入排序增加了间隔 12345678910111213141516171819202122232425262728public class ShellSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;55, 22, 2, 5, 1, 3, 8, 5, 7, 4, 3, 99, 88&#125;; // 确定增长量h的初始值 int h = 1; while (h &lt; arr.length / 2) &#123; h = 2 * h + 1; &#125; // 希尔排序 while (h &gt;= 1) &#123; // 找到待插入的元素 for (int i = h; i &lt; arr.length; i++) &#123; // 把待插入的元素插到有序数列中 for (int j = i; j &gt;= h; j -= h) &#123; // 待插入的元素是arr[j]，比较arr[j]和arr[j-h] if (arr[j] &lt; arr[j - h]) &#123; int temp = arr[j]; arr[j] = arr[j - h]; arr[j - h] = temp; &#125; &#125; &#125; // 减小h的值，减小规则为： h = h / 2; &#125; System.out.println(Arrays.toString(arr)); &#125;&#125; 在希尔排序中，增长量 h 并没有固定的规则，有很多论文研究了各种不同的递增序列，但都无法证明某个序列是最好的，所以对于希尔排序的时间复杂度分析就认为 O(nlogn) 归并排序实现方式归并排序（Merge Sort）：建立在归并操作上的一种有效的排序算法，该算法是采用分治法的典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并。 实现思路： 一组数据拆分成两个元素相等的子组，并对每一个子组继续拆分，直到拆分后的每个子组的元素个数是1为止 将相邻的两个子组进行合并成一个有序的大组 不断的重复步骤2，直到最终只有一个组为止 归并步骤：每次比较两端最小的值，把最小的值放在辅助数组的左边 实现代码1234567891011121314151617181920212223242526272829303132333435363738public class MergeSort &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;55, 22, 2, 5, 1, 3, 8, 5, 7, 4, 3, 99, 88&#125;; mergeSort(arr, 0, arr.length - 1); System.out.println(Arrays.toString(arr)); &#125;\t// low为arr最小索引，high为最大索引 public static void mergeSort(int[] arr, int low, int high) &#123; // low == high 时说明只有一个元素了，直接返回 if (low &lt; high) &#123; int mid = (low + high) / 2; mergeSort(arr, low, mid); // 归并排序前半段 mergeSort(arr, mid + 1, high);\t// 归并排序后半段 merge(arr, low, mid, high); // 将两段有序段合成一段有序段 &#125; &#125; private static void merge(int[] arr, int low, int mid, int high) &#123; int index = 0; // 定义左右指针 int left = low, right = mid + 1; int[] assist = new int[high - low + 1]; while (left &lt;= mid &amp;&amp; right &lt;= high) &#123; assist[index++] = arr[left] &lt; arr[right] ? arr[left++] : arr[right++]; &#125; while (left &lt;= mid) &#123; assist[index++] = arr[left++]; &#125; while (right &lt;= high) &#123; assist[index++] = arr[right++]; &#125; for (int k = 0; k &lt; assist.length; k++) &#123; arr[low++] = assist[k]; &#125; &#125;&#125; 用树状图来描述归并，假设元素的个数为 n，那么使用归并排序拆分的次数为 log2(n)，即层数，每次归并需要做 n 次对比，最终得出的归并排序的时间复杂度为 log2(n)*n，根据大O推导法则，忽略底数，最终归并排序的时间复杂度为 O(nlogn) 归并排序的缺点：需要申请额外的数组空间，导致空间复杂度提升，是典型的以空间换时间的操作 快速排序快速排序（Quick Sort）：通过分治思想对冒泡排序的改进，基本过程是通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，以此达到整个数据变成有序序列 实现思路： 从数列中挑出一个元素，称为基准（pivot） 重新排序数列，所有比基准值小的摆放在基准前面，所有比基准值大的摆在基准的后面（相同的数可以到任一边），在这个分区退出之后，该基准就处于数列的中间位置，这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序 123456789101112131415161718192021222324252627282930313233343536373839404142public class QuickSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;55, 22, 2, 5, 1, 3, 8, 5, 7, 4, 3, 99, 88&#125;; quickSort(arr, 0, arr.length - 1); System.out.println(Arrays.toString(arr)); &#125; public static void quickSort(int[] arr, int low, int high) &#123; // 递归结束的条件 if (low &gt;= high) &#123; return; &#125; int left = low; int right = high; // 基准数 int temp = arr[left]; while (left &lt; right) &#123; // 用 &gt;= 可以防止多余的交换 while (arr[right] &gt;= temp &amp;&amp; right &gt; left) &#123; right--; &#125; // 做判断防止相等 if (right &gt; left) &#123; // 到这里说明 arr[right] &lt; temp arr[left] = arr[right];// 此时把arr[right]元素视为空 left++; &#125; while (arr[left] &lt;= temp &amp;&amp; left &lt; right) &#123; left++; &#125; if (right &gt; left) &#123; arr[right] = arr[left]; right--; &#125; &#125; // left == right arr[left] = temp; quickSort(arr, low, left-1); quickSort(arr, right + 1, high); &#125;&#125; 快速排序和归并排序的区别： 快速排序是另外一种分治的排序算法，将一个数组分成两个子数组，将两部分独立的排序 归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题 快速排序和归并排序是互补的：归并排序将数组分成两个子数组分别排序，并将有序的子数组归并从而将整个数组排序，而快速排序的方式则是当两个数组都有序时，整个数组自然就有序了 在归并排序中，一个数组被等分为两半，归并调用发生在处理整个数组之前，在快速排序中，切分数组的位置取决于数组的内容，递归调用发生在处理整个数组之后 时间复杂度： 最优情况：每一次切分选择的基准数字刚好将当前序列等分。把数组的切分看做是一个树，共切分了 logn 次，所以，最优情况下快速排序的时间复杂度为 O(nlogn) 最坏情况：每一次切分选择的基准数字是当前序列中最大数或者最小数，这使得每次切分都会有一个子组，那么总共就得切分n次，所以最坏情况下，快速排序的时间复杂度为 O(n^2) 平均情况：每一次切分选择的基准数字不是最大值和最小值，也不是中值，这种情况用数学归纳法证明，快速排序的时间复杂度为 O(nlogn) 推荐视频：https://www.bilibili.com/video/BV1b7411N798?t=1001&amp;p=81 参考文章：https://blog.csdn.net/nrsc272420199/article/details/82587933 基数排序基数排序（Radix Sort）：又叫桶排序和箱排序，借助多关键字排序的思想对单逻辑关键字进行排序的方法 计数排序其实是桶排序的一种特殊情况，当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶，每个桶内的数据值都是相同的，省掉了桶内排序的时间 按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前 解释：先排低位再排高位，可以说明在高位相等的情况下低位是递增的，如果高位也是递增，则数据有序 实现思路： 获得最大数的位数，可以通过将最大数变为 String 类型，再求长度 将所有待比较数值（正整数）统一为同样的数位长度，位数较短的数前面补零 从最低位开始，依次进行一次排序 从最低位排序一直到最高位（个位 → 十位 → 百位 → … →最高位）排序完成以后，数列就变成一个有序序列 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class BucketSort &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;576, 22, 26, 548, 1, 3, 843, 536, 735, 43, 3, 912, 88&#125;; bucketSort(arr); System.out.println(Arrays.toString(arr)); &#125; private static void bucketSort(int[] arr) &#123; // 桶的个数固定为10个（个位是0~9），数组长度为了防止所有的数在同一行 int[][] bucket = new int[10][arr.length]; //记录每个桶中的有多少个元素 int[] elementCounts = new int[10]; //获取数组的最大元素 int max = arr[0]; for (int i = 1; i &lt; arr.length; i++) &#123; max = max &gt; arr[i] ? max : arr[i]; &#125; String maxEle = Integer.toString(max); //将数组中的元素放入桶中，最大数的位数相当于需要几次放入桶中 for (int i = 0, step = 1; i &lt; maxEle.length(); i++, step *= 10) &#123; for (int j = 0; j &lt; arr.length; j++) &#123; //获取最后一位的数据，也就是索引 int index = (arr[j] / step) % 10; //放入具体位置 bucket[index][elementCounts[index]] = arr[j]; //存储每个桶的数量 elementCounts[index]++; &#125; //收集回数组 for (int j = 0, index = 0; j &lt; 10; j++) &#123; //先进先出 int position = 0; //桶中有元素就取出 while (elementCounts[j] &gt; 0) &#123; arr[index] = bucket[j][position]; elementCounts[j]--; position++; index++; &#125; &#125; &#125; &#125;&#125; 空间换时间 推荐视频：https://www.bilibili.com/video/BV1b7411N798?p=86 参考文章：https://www.toutiao.com/a6593273307280179715/?iid=6593273307280179715 算法总结稳定性稳定性：在待排序的记录序列中，存在多个具有相同的关键字的记录，若经过排序，这些记录的相对次序保持不变，即在原序列中 r[i]=r[j]，且 r[i] 在 r[j] 之前，而在排序后的序列中，r[i] 仍在 r[j] 之前，则称这种排序算法是稳定的，否则称为不稳定的 如果一组数据只需要一次排序，则稳定性一般是没有意义的，如果一组数据需要多次排序，稳定性是有意义的。 冒泡排序：只有当 arr[i]&gt;arr[i+1] 的时候，才会交换元素的位置，而相等的时候并不交换位置，所以冒泡排序是一种稳定排序算法 选择排序：是给每个位置选择当前元素最小的，例如有数据{5(1)，8 ，5(2)， 3， 9 }，第一遍选择到的最小元素为3，所以5(1)会和3进行交换位置，此时5(1)到了5(2)后面，破坏了稳定性，所以是不稳定的排序算法 插入排序：比较是从有序序列的末尾开始，也就是想要插入的元素和已经有序的最大者开始比起，如果比它大则直接插入在其后面，否则一直往前找直到找到它该插入的位置。如果碰见一个和插入元素相等的，那么把要插入的元素放在相等元素的后面。相等元素的前后顺序没有改变，从原无序序列出去的顺序就是排好序后的顺序，所以插入排序是稳定的 希尔排序：按照不同步长对元素进行插入排序，虽然一次插入排序是稳定的，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱，所以希尔排序是不稳定的 归并排序在归并的过程中，只有 arr[i]&lt;arr[i+1] 的时候才会交换位置，如果两个元素相等则不会交换位置，所以它并不会破坏稳定性，归并排序是稳定的 快速排序：快排需要一个基准值，在基准值的右侧找一个比基准值小的元素，在基准值的左侧找一个比基准值大的元素，然后交换这两个元素，此时会破坏稳定性，所以快速排序是一种不稳定的算法 记忆口诀： 情绪不稳定，快些选一堆好友来聊天 快：快速排序、些：希尔排序、选：选择排序、堆：堆排序 算法对比 补充问题海量数据问题： 海量数据排序： 外部排序：归并 + 败者树 基数排序：https://time.geekbang.org/column/article/42038 海量数据查询： 布隆过滤器判断是否存在 构建索引：B+ 树、跳表 查找正常查找：从第一个元素开始遍历，一个一个的往后找，综合查找比较耗时 二分查找也称折半查找（Binary Search）是一种效率较高的查找方法，数组必须是有序数组 过程：每次先与中间的元素进行比较，如果大于往右边找，如果小于往左边找，如果等于就返回该元素索引位置，如果没有该元素，返回 -1 时间复杂度：O(logn) 12345678910111213141516171819202122232425262728293031/*定义一个方法，记录开始的索引位置和结束的索引位置。取出中间索引位置的值，拿元素与中间位置的值进行比较，如果小于中间值，结束位置=中间索引-1.取出中间索引位置的值，拿元素与中间位置的值进行比较，如果大于中间值，开始位置=中间索引+1.循环正常执行的条件：开始位置索引&lt;=结束位置索引。否则说明寻找完毕但是没有该元素值返回-1.*/public class binarySearch &#123; public static void main(String[] args) &#123; int[] arr = &#123;10, 14, 21, 38, 45, 47, 53, 81, 87, 99&#125;; System.out.println(&quot;81的索引是：&quot; + binarySearch(arr,81)); &#125; public static int binarySearch(int[] arr, int des) &#123; int start = 0; int end = arr.length - 1; // 确保不会出现重复查找，越界 while (start &lt;= end) &#123; // 计算出中间索引值 int mid = (start + end) / 2; if (des == arr[mid]) &#123; return mid; &#125; else if (des &gt; arr[mid]) &#123; start = mid + 1; &#125; else if (des &lt; arr[mid]) &#123; end = mid - 1; &#125; &#125; // 如果上述循环执行完毕还没有返回索引，说明根本不存在该元素值，直接返回-1 return -1; &#125;&#125; 查找第一个匹配的元素： 1234567891011121314151617181920212223public static int binarySearch(int[] arr, int des) &#123; int start = 0; int end = arr.length - 1; while (start &lt;= end) &#123; int mid = (start + end) / 2; if (des == arr[mid]) &#123; //如果 mid 等于 0，那这个元素已经是数组的第一个元素，那肯定是我要找的 if (mid == 0 || a[mid - 1] != des) &#123; return mid; &#125; else &#123; //a[mid]前面的一个元素 a[mid-1]也等于 value， //要找的元素肯定出现在[low, mid-1]之间 high = mid - 1 &#125; &#125; else if (des &gt; arr[mid]) &#123; start = mid + 1; &#125; else if (des &lt; arr[mid]) &#123; end = mid - 1; &#125; &#125; return -1; &#125; 匹配BFBrute Force 暴力匹配算法： 1234567891011121314151617181920212223242526public static void main(String[] args) &#123; String s = &quot;seazean&quot;; String t = &quot;az&quot;; System.out.println(match(s,t));//2&#125;public static int match(String s,String t) &#123; int k = 0; int i = k, j = 0; //防止越界 while (i &lt; s.length() &amp;&amp; j &lt; t.length()) &#123; if (s.charAt(i) == t.charAt(j)) &#123; ++i; ++j; &#125; else &#123; k++; i = k; j = 0; &#125; &#125; //说明是匹配成功 if (j &gt;= t.length()) &#123; return k; &#125; return 0;&#125; 平均时间复杂度：O(m+n)，最坏时间复杂度：O(m*n) RK把主串得长度记为 n，模式串得长度记为 m，通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小，如果某个子串的哈希值与模式串相等，再去对比值是否相等（防止哈希冲突），那就说明对应的子串和模式串匹配了 因为哈希值是一个数字，数字之间比较是否相等是非常快速的 第一部分计算哈希值的时间复杂度为 O(n)，第二部分对比的时间复杂度为 O(1)，整体平均时间复杂度为 O(n)，最坏为 O(n*m) KMPKMP 匹配： next 数组的核心就是自己匹配自己，主串代表后缀，模式串代表前缀 nextVal 数组的核心就是回退失配 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class Kmp &#123; public static void main(String[] args) &#123; String s = &quot;acababaabc&quot;; String t = &quot;abaabc&quot;; //[-1, 0, 0, 1, 1, 2] System.out.println(Arrays.toString(getNext(t))); //[-1, 0, -1, 1, 0, 2] System.out.println(Arrays.toString(getNextVal(t))); //5 System.out.println(kmp(s, t)); &#125; private static int kmp(String s, String t) &#123; int[] next = getNext(t); int i = 0, j = 0; while (i &lt; s.length() &amp;&amp; j &lt; t.length()) &#123; //j==-1时说明第一个位置匹配失败，所以将s的下一个和t的首字符比较 if (j == -1 || s.charAt(i) == t.charAt(j)) &#123; i++; j++; &#125; else &#123; //模式串右移，比较s的当前位置与t的next[j]位置 j = next[j]; &#125; &#125; if (j &gt;= t.length()) &#123; return i - j + 1; &#125; return -1; &#125;\t//next数组 private static int[] getNext(String t) &#123; int[] next = new int[t.length()]; next[0] = -1; int j = -1; int i = 0; while (i &lt; t.length() - 1) &#123; // 根据已知的前j位推测第j+1位 // j=-1说明首位就没有匹配，即t[0]!=t[i]，说明next[i+1]没有最大前缀，为0 if (j == -1 || t.charAt(i) == t.charAt(j)) &#123; // 因为模式串已经匹配到了索引j处，说明之前的位都是相等的 // 因为是自己匹配自己，所以模式串就是前缀，主串就是后缀，j就是最长公共前缀 // 当i+1位置不匹配时（i位之前匹配），可以跳转到j+1位置对比，next[i+1]=j+1 i++; j++; next[i] = j; &#125; else &#123; //i位置的数据和j位置的不相等，所以回退对比i和next[j]位置的数据 j = next[j]; &#125; &#125; return next; &#125;\t//nextVal private static int[] getNextVal(String t) &#123; int[] nextVal = new int[t.length()]; nextVal[0] = -1; int j = -1; int i = 0; while (i &lt; t.length() - 1) &#123; if (j == -1 || t.charAt(i) == t.charAt(j)) &#123; i++; j++; // 如果t[i+1] == t[next(i+1)]=next[j+1]，回退后仍然失配，所以要继续回退 if (t.charAt(i) == t.charAt(j)) &#123; nextVal[i] = nextVal[j]; &#125; else &#123; nextVal[i] = j; &#125; &#125; else &#123; j = nextVal[j]; &#125; &#125; return nextVal; &#125;&#125; 平均和最坏时间复杂度都是 O(m+n) 参考文章：https://www.cnblogs.com/tangzhengyue/p/4315393.html 树二叉树二叉树中，任意一个节点的度要小于等于 2 节点：在树结构中,每一个元素称之为节点 度：每一个节点的子节点数量称之为度 排序树存储结构二叉排序树（BST），又称二叉查找树或者二叉搜索树 每一个节点上最多有两个子节点 左子树上所有节点的值都小于根节点的值 右子树上所有节点的值都大于根节点的值 不存在重复的节点 代码实现 节点类： 123456789private static class TreeNode &#123; int key; TreeNode left; //左节点 TreeNode right; //右节点 private TreeNode(int key) &#123; this.key = key; &#125;&#125; 查找节点： 12345678910111213141516171819202122232425262728 // 递归查找private static TreeNode search(TreeNode root, int key) &#123; //递归结束的条件 if (root == null) &#123; return null; &#125; if (key == root.key) &#123; return root; &#125; else if (key &gt; root.key) &#123; return search(root.right, key); &#125; else &#123; return search(root.left, key); &#125;&#125;// 非递归private static TreeNode search1(TreeNode root, int key) &#123; while (root != null) &#123; if (key == root.key) &#123; return root; &#125; else if (key &gt; root.key) &#123; root = root.right; &#125; else &#123; root = root.left; &#125; &#125; return null;&#125; 插入节点： 12345678910111213141516private static int insert(TreeNode root, int key) &#123; if (root == null) &#123; root = new TreeNode(key); root.left = null; root.right = null; return 1; &#125; else &#123; if (key == root.key) &#123; return 0; &#125; else if (key &gt; root.key) &#123; return insert(root.right, key); &#125; else &#123; return insert(root.left, key); &#125; &#125;&#125; 构造函数： 1234567891011// 构造函数，返回根节点private static TreeNode createBST(int[] arr) &#123; if (arr.length &gt; 0) &#123; TreeNode root = new TreeNode(arr[0]); for (int i = 1; i &lt; arr.length; i++) &#123; insert(root, arr[i]); &#125; return root; &#125; return null;&#125; 删除节点：要删除节点12，先找到节点19，然后移动并替换节点12 代码链接：https://leetcode-cn.com/submissions/detail/190232548/ 参考视频：https://www.bilibili.com/video/BV1iJ411E7xW?t=756&amp;p=86 图片来源：https://leetcode-cn.com/problems/delete-node-in-a-bst/solution/tu-jie-yi-dong-jie-dian-er-bu-shi-xiu-ga-edtn/ 平衡树平衡二叉树（AVL）的特点： 二叉树左右两个子树的高度差不超过 1 任意节点的左右两个子树都是一颗平衡二叉树 平衡二叉树旋转： 旋转触发时机：当添加一个节点之后，该树不再是一颗平衡二叉树 平衡二叉树和二叉查找树对比结构图 左旋：将根节点的右侧往左拉，原先的右子节点变成新的父节点，并把多余的左子节点出让，给已经降级的根节点当右子节点 右旋：将根节点的左侧往右拉，左子节点变成了新的父节点，并把多余的右子节点出让，给已经降级根节点当左子节点 推荐文章：https://pdai.tech/md/algorithm/alg-basic-tree-balance.html 红黑树红黑树的特点： 每一个节点可以是红或者黑 红黑树不是高度平衡的，它的平衡是通过自己的红黑规则进行实现的 红黑树的红黑规则有哪些： 每一个节点或是红色的，或者是黑色的 根节点必须是黑色 如果一个节点没有子节点或者父节点，则该节点相应的指针属性值为 Nil，这些 Nil 视为叶节点，每个叶节点 (Nil) 是黑色的 如果某一个节点是红色，那么它的子节点必须是黑色（不能出现两个红色节点相连的情况） 对每一个节点，从该节点到其所有后代叶节点的简单路径上，均包含相同数目的黑色节点 红黑树与 AVL 树的比较： AVL 树是更加严格的平衡，可以提供更快的查找速度，适用于读取查找密集型任务 红黑树只是做到近似平衡，并不是严格的平衡，红黑树的插入删除比 AVL 树更便于控制，红黑树更适合于插入修改密集型任务 红黑树整体性能略优于 AVL 树，AVL 树的旋转比红黑树的旋转多，更加难以平衡和调试，插入和删除的效率比红黑树慢 红黑树添加节点的默认颜色为红色，效率高 红黑树添加节点后如何保持红黑规则： 根节点位置 直接变为黑色 非根节点位置 父节点为黑色 不需要任何操作,默认红色即可 父节点为红色 叔叔节点为红色 将”父节点”设为黑色,将”叔叔节点”设为黑色 将”祖父节点”设为红色 如果”祖父节点”为根节点,则将根节点再次变成黑色 叔叔节点为黑色 将”父节点”设为黑色 将”祖父节点”设为红色 以”祖父节点”为支点进行旋转 并查集基本实现并查集是一种树型的数据结构，有以下特点： 每个元素都唯一的对应一个结点 每一组数据中的多个元素都在同一颗树中 一个组中的数据对应的树和另外一个组中的数据对应的树之间没有任何联系 元素在树中并没有子父级关系的硬性要求 可以高效地进行如下操作： 查询元素 p 和元素 q 是否属于同一组 合并元素 p 和元素 q 所在的组 存储结构： 合并方式： 代码实现： 类实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class UF &#123; //记录节点元素和该元素所在分组的标识 private int[] eleAndGroup; //记录分组的个数 private int count; //初始化并查集 public UF(int N) &#123; //初始化分组数量 this.count = N; //初始化eleAndGroup数量 this.eleAndGroup = new int[N]; //初始化eleAndGroup中的元素及其所在分组的标识符，eleAndGroup索引作为每个节点的元素 //每个索引处的值就是该组的索引，就是该元素所在的组的标识符 for (int i = 0; i &lt; eleAndGroup.length; i++) &#123; eleAndGroup[i] = i; &#125; &#125; //查询p所在的分组的标识符 public int find(int p) &#123; return eleAndGroup[p]; &#125; //判断并查集中元素p和元素q是否在同一分组中 public boolean connect(int p, int q) &#123; return find(p) == find(q); &#125; //把p元素所在分组和q元素所在分组合并 public void union(int p, int q) &#123; //判断元素q和p是否已经在同一个分组中，如果已经在同一个分组中，则结束方法就可以了 if (connect(p, q)) &#123; return; &#125; int pGroup = find(p);//找到p所在分组的标识符 int qGroup = find(q);//找到q所在分组的标识符 //合并组，让p所在组的 所有元素 的组标识符变为q所在分组的标识符 for (int i = 0; i &lt; eleAndGroup.length; i++) &#123; if (eleAndGroup[i] == pGroup) &#123; eleAndGroup[i] = qGroup; &#125; &#125; //分组个数-1 this.count--; &#125;&#125; 测试代码： 1234567891011121314151617181920212223public static void main(String[] args) &#123; //创建并查集对象 UF uf = new UF(5); System.out.println(uf); //从控制台录入两个合并的元素，调用union方法合并，观察合并后并查集的分组 Scanner sc = new Scanner(System.in); while (true) &#123; System.out.println(&quot;输入第一个要合并的元素&quot;); int p = sc.nextInt(); System.out.println(&quot;输入第二个要合并的元素&quot;); int q = sc.nextInt(); if (uf.connect(p, q)) &#123; System.out.println(p + &quot;元素已经和&quot; + q + &quot;元素已经在同一个组&quot;); continue; &#125; uf.union(p, q); System.out.println(&quot;当前并查集中还有：&quot; + uf.count() + &quot;个分组&quot;); System.out.println(uf); System.out.println(&quot;********************&quot;); &#125;&#125; 最坏情况下 union 算法的时间复杂度也是 O(N^2) 优化实现让每个索引处的节点都指向它的父节点，当 eleGroup[i] &#x3D; i 时，说明 i 是根节点 1234567891011121314151617181920212223242526//查询p所在的分组的标识符，递归寻找父标识符，直到找到根节点public int findRoot(int p) &#123; while (p != eleAndGroup[p]) &#123; p = eleAndGroup[p]; &#125; //p == eleGroup[p]，说明p是根节点 return p;&#125;//判断并查集中元素p和元素q是否在同一分组中public boolean connect(int p, int q) &#123; return findRoot(p) == findRoot(q);&#125;//把p元素所在分组和q元素所在分组合并public void union(int p, int q) &#123; //找到p q对应的根节点 int pRoot = findRoot(p); int qRoot = findRoot(q); if (pRoot == qRoot) &#123; return; &#125; //让p所在树的节点根节点为q的所在的根节点，只需要把根节点改一下，时间复杂度 O(1) eleAndGroup[pRoot] = qRoot; this.count-&#125; 平均时间复杂度为 O(N)，最坏时间复杂度是 O(N^2) 继续优化：路径压缩，保证每次把小树合并到大树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class UF_Tree_Weighted &#123; private int[] eleAndGroup; private int count; private int[] size;//存储每一个根结点对应的树中的保存的节点的个数 //初始化并查集 public UF_Tree_Weighted(int N) &#123; this.count = N; this.eleAndGroup = new int[N]; for (int i = 0; i &lt; eleAndGroup.length; i++) &#123; eleAndGroup[i] = i; &#125; this.size = new int[N]; //默认情况下，size中每个索引处的值都是1 for (int i = 0; i &lt; size.length; i++) &#123; size[i] = 1; &#125; &#125;\t//查询p所在的分组的标识符，父标识符 public int findRoot(int p) &#123; while (p != eleAndGroup[p]) &#123; p = eleAndGroup[p]; &#125; return p; &#125; //判断并查集中元素p和元素q是否在同一分组中 public boolean connect(int p, int q) &#123; return findRoot(p) == findRoot(q); &#125; //把p元素所在分组和q元素所在分组合并 public void union(int p, int q) &#123; //找到p q对应的根节点 int pRoot = findRoot(p); int qRoot = findRoot(q); if (pRoot == qRoot) &#123; return; &#125; //判断pRoot对应的树大还是qRoot对应的树大，最终需要把较小的树合并到较大的树中 if (size[pRoot] &lt; size[qRoot]) &#123; eleAndGroup[pRoot] = qRoot; size[qRoot] += size[pRoot]; &#125; else &#123; eleAndGroup[qRoot] = pRoot; size[pRoot] += size[qRoot]; &#125; //组的数量-1、 this.count--; &#125;&#125; 应用场景并查集存储的每一个整数表示的是一个大型计算机网络中的计算机： 可以通过 connected(int p, int q) 来检测该网络中的某两台计算机之间是否连通 可以调用 union(int p,int q) 使得 p 和 q 之间连通，这样两台计算机之间就可以通信 畅通工程：某省调查城镇交通状况，得到现有城镇道路统计表，表中列出了每条道路直接连通的城镇。省政府畅通工程的目标是使全省任何两个城镇间都可以实现交通，但不一定有直接的道路相连，只要互相间接通过道路可达即可，问最少还需要建设多少条道路？ 解题思路： 创建一个并查集 UF_Tree_Weighted(20) 分别调用 union(0,1)、union(6,9)、union(3,8)、union(5,11)、union(2,12)、union(6,10)、union(4,8)，表示已经修建好的道路把对应的城市连接起来 如果城市全部连接起来，那么并查集中剩余的分组数目为 1，所有的城市都在一个树中，只需要获取当前并查集中剩余的数目减去 1，就是还需要修建的道路数目 123456789101112131415161718public static void main(String[] args)throws Exception &#123; Scanner sc = new Scanner(System.in); //读取城市数目，初始化并查集 int number = sc.nextInt(); //读取已经修建好的道路数目 int roadNumber = sc.nextInt(); UF_Tree_Weighted uf = new UF_Tree_Weighted(number); //循环读取已经修建好的道路，并调用union方法 for (int i = 0; i &lt; roadNumber; i++) &#123; int p = sc.nextInt(); int q = sc.nextInt(); uf.union(p,q); &#125; //获取剩余的分组数量 int groupNumber = uf.count(); //计算出还需要修建的道路 System.out.println(&quot;还需要修建&quot;+(groupNumber-1)+&quot;道路，城市才能相通&quot;);&#125; 参考视频：https://www.bilibili.com/video/BV1iJ411E7xW?p=142 字典树基本介绍Trie 树，也叫字典树，是一种专门处理字符串匹配的树形结构，用来解决在一组字符串集合中快速查找某个字符串的问题，Trie 树的本质就是利用字符串之间的公共前缀，将重复的前缀合并在一起 根节点不包含任何信息 每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串 红色节点并不都是叶子节点 注意：要查找的是字符串“he”，从根节点开始，沿着某条路径来匹配，可以匹配成功。但是路径的最后一个节点“e”并不是红色的，也就是说，“he”是某个字符串的前缀子串，但并不能完全匹配任何字符串 实现Trie通过一个下标与字符一一映射的数组，来存储子节点的指针 时间复杂度是 O(n)（n 表示要查找字符串的长度） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Trie &#123; private TrieNode root = new TrieNode(&#x27;/&#x27;); //插入一个字符 public void insert(char[] chars) &#123; TrieNode p = root; for (int i = 0; i &lt; chars.length; i++) &#123; //获取字符的索引位置 int index = chars[i] - &#x27;a&#x27;; if (p.children[index] == null) &#123; TrieNode node = new TrieNode(chars[i]); p.children[index] = node; &#125; p = p.children[index]; &#125; p.isEndChar = true; &#125; //查找一个字符串 public boolean find(char[] chars) &#123; TrieNode p = root; for (int i = 0; i &lt; chars.length; i++) &#123; int index = chars[i] - &#x27;a&#x27;; if (p.children[index] == null) &#123; return false; &#125; p = p.children[index]; &#125; if (p.isEndChar) &#123; //完全匹配 return true; &#125; else &#123; // 不能完全匹配，只是前缀 return false; &#125; &#125; private class TrieNode &#123; char data; TrieNode[] children = new TrieNode[26];//26个英文字母 boolean isEndChar = false;//结尾字符为true public TrieNode(char data) &#123; this.data = data; &#125; &#125;&#125; 优化TrieTrie 树是非常耗内存，采取空间换时间的思路。Trie 树的变体有很多，可以在一定程度上解决内存消耗的问题。比如缩点优化，对只有一个子节点的节点，而且此节点不是一个串的结束节点，可以将此节点与子节点合并 参考文章：https://time.geekbang.org/column/article/72414 图图的邻接表形式： 1234567891011121314151617181920212223242526272829303132public class AGraph &#123; private VertexNode[] adjList; //邻接数组 private int vLen, eLen; //顶点数和边数 public AGraph(int vLen, int eLen) &#123; this.vLen = vLen; this.eLen = eLen; adjList = new VertexNode[vLen]; &#125; //弧节点 private class ArcNode &#123; int adjVex; //该边所指向的顶点的位置 ArcNode nextArc; //下一条边（弧） //int info //添加权值 public ArcNode(int adjVex) &#123; this.adjVex = adjVex; nextArc = null; &#125; &#125; //表顶点 private class VertexNode &#123; char data; //顶点信息 ArcNode firstArc; //指向第一条边的指针 public VertexNode(char data) &#123; this.data = data; firstArc = null; &#125; &#125;&#125; 图的邻接矩阵形式： 1234567891011121314151617181920212223public class MGraph &#123; private int[][] edges; //邻接矩阵定义，有权图将int改为float private int vLen; //顶点数 private int eLen; //边数 private VertexNode[] vex; //存放节点信息 public MGraph(int vLen, int eLen) &#123; this.vLen = vLen; this.eLen = eLen; this.edges = new int[vLen][vLen]; this.vex = new VertexNode[vLen]; &#125; private class VertexNode &#123; int num; //顶点编号 String info; //顶点信息 public VertexNode(int num) &#123; this.num = num; this.info = null; &#125; &#125;&#125; 图相关的算法需要很多的流程图，此处不再一一列举，推荐参考书籍《数据结构高分笔记》 位图基本介绍布隆过滤器：一种数据结构，是一个很长的二进制向量（位数组）和一系列随机映射函数（哈希函数），既然是二进制，每个空间存放的不是 0 就是 1，但是初始默认值都是 0，所以布隆过滤器不存数据只存状态 这种数据结构是高效且性能很好的，但缺点是具有一定的错误识别率和删除难度。并且理论情况下，添加到集合中的元素越多，误报的可能性就越大 工作流程向布隆过滤器中添加一个元素 key 时，会通过多个 hash 函数得到多个哈希值，在位数组中把对应下标的值置为 1 布隆过滤器查询一个数据，是否在二进制的集合中，查询过程如下： 通过 K 个哈希函数计算该数据，对应计算出的 K 个 hash 值 通过 hash 值找到对应的二进制的数组下标 判断方法：如果存在一处位置的二进制数据是 0，那么该数据一定不存在。如果都是 1，则认为数据存在集合中（会误判） 布隆过滤器优缺点： 优点： 二进制组成的数组，占用内存极少，并且插入和查询速度都足够快 去重方便：当字符串第一次存储时对应的位数组下标设置为 1，当第二次存储相同字符串时，因为对应位置已设置为 1，所以很容易知道此值已经存在 缺点： 随着数据的增加，误判率会增加：添加数据是通过计算数据的 hash 值，不同的字符串可能哈希出来的位置相同，导致无法确定到底是哪个数据存在，这种情况可以适当增加位数组大小或者调整哈希函数 无法删除数据：可能存在几个数据占据相同的位置，所以删除一位会导致很多数据失效 总结：布隆过滤器判断某个元素存在，小概率会误判。如果判断某个元素不在，那这个元素一定不在 参考文章：https://www.cnblogs.com/ysocean/p/12594982.html Guava引入 Guava 的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;28.0-jre&lt;/version&gt;&lt;/dependency&gt; 指定误判率为（0.01）： 123456789101112131415public static void main(String[] args) &#123; // 创建布隆过滤器对象 BloomFilter&lt;Integer&gt; filter = BloomFilter.create( Funnels.integerFunnel(), 1500, 0.01); // 判断指定元素是否存在 System.out.println(filter.mightContain(1)); System.out.println(filter.mightContain(2)); // 将元素添加进布隆过滤器 filter.put(1); filter.put(2); System.out.println(filter.mightContain(1)); System.out.println(filter.mightContain(2));&#125; 实现布隆1234567891011121314151617181920212223242526272829class MyBloomFilter &#123; //布隆过滤器容量 private static final int DEFAULT_SIZE = 2 &lt;&lt; 28; //bit数组，用来存放key private static BitSet bitSet = new BitSet(DEFAULT_SIZE); //后面hash函数会用到，用来生成不同的hash值，随意设置 private static final int[] ints = &#123;1, 6, 16, 38, 58, 68&#125;; //add方法，计算出key的hash值，并将对应下标置为true public void add(Object key) &#123; Arrays.stream(ints).forEach(i -&gt; bitSet.set(hash(key, i))); &#125; //判断key是否存在，true不一定说明key存在，但是false一定说明不存在 public boolean isContain(Object key) &#123; boolean result = true; for (int i : ints) &#123; //短路与，只要有一个bit位为false，则返回false result = result &amp;&amp; bitSet.get(hash(key, i)); &#125; return result; &#125; //hash函数，借鉴了hashmap的扰动算法 private int hash(Object key, int i) &#123; int h; return key == null ? 0 : (i * (DEFAULT_SIZE - 1) &amp; ((h = key.hashCode()) ^ (h &gt;&gt;&gt; 16))); &#125;&#125;"},{"title":"DB","path":"/wiki/javaNode/DB.html","content":"MySQL简介数据库数据库：DataBase，简称 DB，存储和管理数据的仓库 数据库的优势： 可以持久化存储数据 方便存储和管理数据 使用了统一的方式操作数据库 SQL 数据库、数据表、数据的关系介绍： 数据库 用于存储和管理数据的仓库 一个库中可以包含多个数据表 数据表 数据库最重要的组成部分之一 由纵向的列和横向的行组成（类似 excel 表格） 可以指定列名、数据类型、约束等 一个表中可以存储多条数据 数据：想要永久化存储的数据 参考视频：https://www.bilibili.com/video/BV1zJ411M7TB 参考专栏：https://time.geekbang.org/column/intro/139 参考书籍：https://book.douban.com/subject/35231266/ MySQLMySQL 数据库是一个最流行的关系型数据库管理系统之一，关系型数据库是将数据保存在不同的数据表中，而且表与表之间可以有关联关系，提高了灵活性 缺点：数据存储在磁盘中，导致读写性能差，而且数据关系复杂，扩展性差 MySQL 所使用的 SQL 语句是用于访问数据库最常用的标准化语言 MySQL 配置： MySQL 安装：https://www.jianshu.com/p/ba48f1e386f0 MySQL 配置： 修改 MySQL 默认字符集：安装 MySQL 之后第一件事就是修改字符集编码 123456789vim /etc/mysql/my.cnf添加如下内容：[mysqld]character-set-server=utf8collation-server=utf8_general_ci[client]default-character-set=utf8 启动 MySQL 服务： 1systemctl start/restart mysql 登录 MySQL： 123mysql -u root -p 敲回车，输入密码初始密码查看：cat /var/log/mysqld.log在root@localhost: 后面的就是初始密码 查看默认字符集命令： 1SHOW VARIABLES LIKE &#x27;char%&#x27;; 修改MySQL登录密码： 1234set global validate_password_policy=0;set global validate_password_length=1; set password=password(&#x27;密码&#x27;); 授予远程连接权限（MySQL 内输入）： 1234-- 授权grant all privileges on *.* to &#x27;root&#x27; @&#x27;%&#x27; identified by &#x27;密码&#x27;;-- 刷新flush privileges; 修改 MySQL 绑定 IP： 1234cd /etc/mysql/mysql.conf.dsudo chmod 666 mysqld.cnf vim mysqld.cnf # bind-address = 127.0.0.1注释该行 关闭 Linux 防火墙 12systemctl stop firewalld.service# 放行3306端口 体系架构整体架构体系结构详解： 第一层：网络连接层 一些客户端和链接服务，包含本地 Socket 通信和大多数基于客户端&#x2F;服务端工具实现的 TCP&#x2F;IP 通信，主要完成一些类似于连接处理、授权认证、及相关的安全方案 在该层上引入了连接池 Connection Pool 的概念，管理缓冲用户连接，线程处理等需要缓存的需求 在该层上实现基于 SSL 的安全链接，服务器也会为安全接入的每个客户端验证它所具有的操作权限 第二层：核心服务层 查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，所有的内置函数（日期、数学、加密函数等） Management Serveices &amp; Utilities：系统管理和控制工具，备份、安全、复制、集群等 SQL Interface：接受用户的 SQL 命令，并且返回用户需要查询的结果 Parser：SQL 语句分析器 Optimizer：查询优化器 Caches &amp; Buffers：查询缓存，服务器会查询内部的缓存，如果缓存空间足够大，可以在大量读操作的环境中提升系统性能 所有跨存储引擎的功能在这一层实现，如存储过程、触发器、视图等 在该层服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定表的查询顺序，是否利用索引等， 最后生成相应的执行操作 MySQL 中服务器层不管理事务，事务是由存储引擎实现的 第三层：存储引擎层 Pluggable Storage Engines：存储引擎接口，MySQL 区别于其他数据库的重要特点就是其存储引擎的架构模式是插件式的（存储引擎是基于表的，而不是数据库） 存储引擎真正的负责了 MySQL 中数据的存储和提取，服务器通过 API 和存储引擎进行通信 不同的存储引擎具有不同的功能，共用一个 Server 层，可以根据开发的需要，来选取合适的存储引擎 第四层：系统文件层 数据存储层，主要是将数据存储在文件系统之上，并完成与存储引擎的交互 File System：文件系统，保存配置文件、数据文件、日志文件、错误文件、二进制文件等 建立连接连接器池化技术：对于访问数据库来说，建立连接的代价是比较昂贵的，因为每个连接对应一个用来交互的线程，频繁的创建关闭连接比较耗费资源，有必要建立数据库连接池，以提高访问的性能 连接建立 TCP 以后需要做权限验证，验证成功后可以进行执行 SQL。如果这时管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限，只有再新建的连接才会使用新的权限设置 MySQL 服务器可以同时和多个客户端进行交互，所以要保证每个连接会话的隔离性（事务机制部分详解） 整体的执行流程： 权限信息grant 语句会同时修改数据表和内存，判断权限的时候使用的是内存数据 flush privileges 语句本身会用数据表（磁盘）的数据重建一份内存权限数据，所以在权限数据可能存在不一致的情况下使用，这种不一致往往是由于直接用 DML 语句操作系统权限表导致的，所以尽量不要使用这类语句 连接状态客户端如果长时间没有操作，连接器就会自动断开，时间是由参数 wait_timeout 控制的，默认值是 8 小时。如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒：Lost connection to MySQL server during query 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接；短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个 为了减少连接的创建，推荐使用长连接，但是过多的长连接会造成 OOM，解决方案： 定期断开长连接，使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连 1KILL CONNECTION id MySQL 5.7 版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源，这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态 SHOW PROCESSLIST：查看当前 MySQL 在进行的线程，可以实时地查看 SQL 的执行情况，其中的 Command 列显示为 Sleep 的这一行，就表示现在系统里面有一个空闲连接 参数 含义 ID 用户登录 mysql 时系统分配的 connection_id，可以使用函数 connection_id() 查看 User 显示当前用户，如果不是 root，这个命令就只显示用户权限范围的 sql 语句 Host 显示这个语句是从哪个 ip 的哪个端口上发的，可以用来跟踪出现问题语句的用户 db 显示这个进程目前连接的是哪个数据库 Command 显示当前连接的执行的命令，一般取值为休眠 Sleep、查询 Query、连接 Connect 等 Time 显示这个状态持续的时间，单位是秒 State 显示使用当前连接的 sql 语句的状态，以查询为例，需要经过 copying to tmp table、sorting result、sending data等状态才可以完成 Info 显示执行的 sql 语句，是判断问题语句的一个重要依据 Sending data 状态表示 MySQL 线程开始访问数据行并把结果返回给客户端，而不仅仅只是返回给客户端，是处于执行器过程中的任意阶段。由于在 Sending data 状态下，MySQL 线程需要做大量磁盘读取操作，所以是整个查询中耗时最长的状态 执行流程查询缓存工作流程当执行完全相同的 SQL 语句的时候，服务器就会直接从缓存中读取结果，当数据被修改，之前的缓存会失效，修改比较频繁的表不适合做查询缓存 查询过程： 客户端发送一条查询给服务器 服务器先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果（一般是 K-V 键值对），否则进入下一阶段 分析器进行 SQL 分析，再由优化器生成对应的执行计划 执行器根据优化器生成的执行计划，调用存储引擎的 API 来执行查询 将结果返回给客户端 大多数情况下不建议使用查询缓存，因为查询缓存往往弊大于利 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能费力地把结果存起来，还没使用就被一个更新全清空了，对于更新压力大的数据库来说，查询缓存的命中率会非常低 除非业务就是有一张静态表，很长时间才会更新一次，比如一个系统配置表，那这张表上的查询才适合使用查询缓存 缓存配置 查看当前 MySQL 数据库是否支持查询缓存： 1SHOW VARIABLES LIKE &#x27;have_query_cache&#x27;;\t-- YES 查看当前 MySQL 是否开启了查询缓存： 1SHOW VARIABLES LIKE &#x27;query_cache_type&#x27;;\t-- OFF 参数说明： OFF 或 0：查询缓存功能关闭 ON 或 1：查询缓存功能打开，查询结果符合缓存条件即会缓存，否则不予缓存；可以显式指定 SQL_NO_CACHE 不予缓存 DEMAND 或 2：查询缓存功能按需进行，显式指定 SQL_CACHE 的 SELECT 语句才缓存，其它不予缓存 12SELECT SQL_CACHE id, name FROM customer; -- SQL_CACHE:查询结果可缓存SELECT SQL_NO_CACHE id, name FROM customer;-- SQL_NO_CACHE:不使用查询缓存 查看查询缓存的占用大小： 1SHOW VARIABLES LIKE &#x27;query_cache_size&#x27;;-- 单位是字节 1048576 / 1024 = 1024 = 1KB 查看查询缓存的状态变量： 1SHOW STATUS LIKE &#x27;Qcache%&#x27;; 参数 含义 Qcache_free_blocks 查询缓存中的可用内存块数 Qcache_free_memory 查询缓存的可用内存量 Qcache_hits 查询缓存命中数 Qcache_inserts 添加到查询缓存的查询数 Qcache_lowmen_prunes 由于内存不足而从查询缓存中删除的查询数 Qcache_not_cached 非缓存查询的数量（由于 query_cache_type 设置而无法缓存或未缓存） Qcache_queries_in_cache 查询缓存中注册的查询数 Qcache_total_blocks 查询缓存中的块总数 配置 my.cnf： 1234sudo chmod 666 /etc/mysql/my.cnfvim my.cnf# mysqld中配置缓存query_cache_type=1 重启服务既可生效，执行 SQL 语句进行验证 ，执行一条比较耗时的 SQL 语句，然后再多执行几次，查看后面几次的执行时间；获取通过查看查询缓存的缓存命中数，来判定是否走查询缓存 缓存失效查询缓存失效的情况： SQL 语句不一致，要想命中查询缓存，查询的 SQL 语句必须一致，因为缓存中 key 是查询的语句，value 是查询结构 12select count(*) from tb_item;Select count(*) from tb_item;\t-- 不走缓存，首字母不一致 当查询语句中有一些不确定查询时，则不会缓存，比如：now()、current_date()、curdate()、curtime()、rand()、uuid()、user()、database() 123SELECT * FROM tb_item WHERE updatetime &lt; NOW() LIMIT 1;SELECT USER();SELECT DATABASE(); 不使用任何表查询语句： 1SELECT &#x27;A&#x27;; 查询 mysql、information_schema、performance_schema 等系统表时，不走查询缓存： 1SELECT * FROM information_schema.engines; 在跨存储引擎的存储过程、触发器或存储函数的主体内执行的查询，缓存失效 如果表更改，则使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除，包括使用 MERGE 映射到已更改表的表的查询，比如：INSERT、UPDATE、DELETE、ALTER TABLE、DROP TABLE、DROP DATABASE 分析器没有命中查询缓存，就开始了 SQL 的真正执行，分析器会对 SQL 语句做解析 1SELECT * FROM t WHERE id = 1; 解析器：处理语法和解析查询，生成一课对应的解析树 先做词法分析，输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么代表什么。从输入的 select 这个关键字识别出来这是一个查询语句；把字符串 t 识别成 表名 t，把字符串 id 识别成列 id 然后做语法分析，根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。如果语句不对，就会收到 You have an error in your SQL syntax 的错误提醒 预处理器：进一步检查解析树的合法性，比如数据表和数据列是否存在、别名是否有歧义等 优化器成本分析优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序 根据搜索条件找出所有可能的使用的索引 成本分析，执行成本由 I&#x2F;O 成本和 CPU 成本组成，计算全表扫描和使用不同索引执行 SQL 的代价 找到一个最优的执行方案，用最小的代价去执行语句 在数据库里面，扫描行数是影响执行代价的因素之一，扫描的行数越少意味着访问磁盘的次数越少，消耗的 CPU 资源越少，优化器还会结合是否使用临时表、是否排序等因素进行综合判断 统计数据MySQL 中保存着两种统计数据： innodb_table_stats 存储了表的统计数据，每一条记录对应着一个表的统计数据 innodb_index_stats 存储了索引的统计数据，每一条记录对应着一个索引的一个统计项的数据 MySQL 在真正执行语句之前，并不能精确地知道满足条件的记录有多少条，只能根据统计信息来估算记录，统计信息就是索引的区分度，一个索引上不同的值的个数（比如性别只能是男女，就是 2 ），称之为基数（cardinality），基数越大说明区分度越好 通过采样统计来获取基数，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数 在 MySQL 中，有两种存储统计数据的方式，可以通过设置参数 innodb_stats_persistent 的值来选择： ON：表示统计信息会持久化存储（默认），采样页数 N 默认为 20，可以通过 innodb_stats_persistent_sample_pages 指定，页数越多统计的数据越准确，但消耗的资源更大 OFF：表示统计信息只存储在内存，采样页数 N 默认为 8，也可以通过系统变量设置（不推荐，每次重新计算浪费资源） 数据表是会持续更新的，两种统计信息的更新方式： 设置 innodb_stats_auto_recalc 为 1，当发生变动的记录数量超过表大小的 10% 时，自动触发重新计算，不过是异步进行 调用 ANALYZE TABLE t 手动更新统计信息，只对信息做重新统计（不是重建表），没有修改数据，这个过程中加了 MDL 读锁并且是同步进行，所以会暂时阻塞系统 EXPLAIN 执行计划在优化器阶段生成，如果 explain 的结果预估的 rows 值跟实际情况差距比较大，可以执行 analyze 命令重新修正信息 错选索引采样统计本身是估算数据，或者 SQL 语句中的字段选择有问题时，可能导致 MySQL 没有选择正确的执行索引 解决方法： 采用 force index 强行选择一个索引 1SELECT * FROM user FORCE INDEX(name) WHERE NAME=&#x27;seazean&#x27;; 可以考虑修改 SQL 语句，引导 MySQL 使用期望的索引 新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引 执行器开始执行的时候，要先判断一下当前连接对表有没有执行查询的权限，如果没有就会返回没有权限的错误，在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。如果有权限，就打开表继续执行，执行器就会根据表的引擎定义，去使用这个引擎提供的接口 引擎层Server 层和存储引擎层的交互是以记录为单位的，存储引擎会将单条记录返回给 Server 层做进一步处理，并不是直接返回所有的记录 工作流程： 首先根据二级索引选择扫描范围，获取第一条符合二级索引条件的记录，进行回表查询，将聚簇索引的记录返回 Server 层，由 Server 判断记录是否符合要求 然后在二级索引上继续扫描下一个符合条件的记录 推荐阅读：https://mp.weixin.qq.com/s/YZ-LckObephrP1f15mzHpA 终止流程终止语句终止线程中正在执行的语句： 1KILL QUERY thread_id KILL 不是马上终止的意思，而是告诉执行线程这条语句已经不需要继续执行，可以开始执行停止的逻辑（类似于打断）。因为对表做增删改查操作，会在表上加 MDL 读锁，如果线程被 KILL 时就直接终止，那这个 MDL 读锁就没机会被释放了 命令 KILL QUERYthread_id_A 的执行流程： 把 session A 的运行状态改成 THD::KILL_QUERY（将变量 killed 赋值为 THD::KILL_QUERY） 给 session A 的执行线程发一个信号，让 session A 来处理这个 THD::KILL_QUERY 状态 会话处于等待状态（锁阻塞），必须满足是一个可以被唤醒的等待，必须有机会去判断线程的状态，如果不满足就会造成 KILL 失败 典型场景：innodb_thread_concurrency 为 2，代表并发线程上限数设置为 2 session A 执行事务，session B 执行事务，达到线程上限；此时 session C 执行事务会阻塞等待，session D 执行 kill query C 无效 C 的逻辑是每 10 毫秒判断是否可以进入 InnoDB 执行，如果不行就调用 nanosleep 函数进入 sleep 状态，没有去判断线程状态 补充：执行 Ctrl+C 的时候，是 MySQL 客户端另外启动一个连接，然后发送一个 KILL QUERY 命令 终止连接断开线程的连接： 1KILL CONNECTION id 断开连接后执行 SHOW PROCESSLIST 命令，如果这条语句的 Command 列显示 Killed，代表线程的状态是 KILL_CONNECTION，说明这个线程有语句正在执行，当前状态是停止语句执行中，终止逻辑耗时较长 超大事务执行期间被 KILL，这时回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长 大查询回滚，如果查询过程中生成了比较大的临时文件，删除临时文件可能需要等待 IO 资源，导致耗时较长 DDL 命令执行到最后阶段被 KILL，需要删除中间过程的临时文件，也可能受 IO 资源影响耗时较久 总结：KILL CONNECTION 本质上只是把客户端的 SQL 连接断开，后面的终止流程还是要走 KILL QUERY 一个事务被 KILL 之后，持续处于回滚状态，不应该强行重启整个 MySQL 进程，应该等待事务自己执行完成，因为重启后依然继续做回滚操作的逻辑 常用工具mysqlmysql 不是指 mysql 服务，而是指 mysql 的客户端工具 1mysql [options] [database] -u –user&#x3D;name：指定用户名 -p –password[&#x3D;name]：指定密码 -h –host&#x3D;name：指定服务器IP或域名 -P –port&#x3D;#：指定连接端口 -e –execute&#x3D;name：执行SQL语句并退出，在控制台执行SQL语句，而不用连接到数据库执行 示例： 12mysql -h 127.0.0.1 -P 3306 -u root -pmysql -uroot -p2143 db01 -e &quot;select * from tb_book&quot;; adminmysqladmin 是一个执行管理操作的客户端程序，用来检查服务器的配置和当前状态、创建并删除数据库等 通过 mysqladmin --help 指令查看帮助文档 1mysqladmin -uroot -p2143 create &#x27;test01&#x27;; binlog服务器生成的日志文件以二进制格式保存，如果需要检查这些文本，就要使用 mysqlbinlog 日志管理工具 1mysqlbinlog [options] log-files1 log-files2 ... -d –database&#x3D;name：指定数据库名称，只列出指定的数据库相关操作 -o –offset&#x3D;#：忽略掉日志中的前 n 行命令。 -r –result-file&#x3D;name：将输出的文本格式日志输出到指定文件。 -s –short-form：显示简单格式，省略掉一些信息。 –start-datatime&#x3D;date1 –stop-datetime&#x3D;date2：指定日期间隔内的所有日志 –start-position&#x3D;pos1 –stop-position&#x3D;pos2：指定位置间隔内的所有日志 dump命令介绍mysqldump 客户端工具用来备份数据库或在不同数据库之间进行数据迁移，备份内容包含创建表，及插入表的 SQL 语句 123mysqldump [options] db_name [tables]mysqldump [options] --database/-B db1 [db2 db3...]mysqldump [options] --all-databases/-A 连接选项： -u –user&#x3D;name：指定用户名 -p –password[&#x3D;name]：指定密码 -h –host&#x3D;name：指定服务器 IP 或域名 -P –port&#x3D;#：指定连接端口 输出内容选项： –add-drop-database：在每个数据库创建语句前加上 Drop database 语句 –add-drop-table：在每个表创建语句前加上 Drop table 语句 , 默认开启，不开启 (–skip-add-drop-table) -n –no-create-db：不包含数据库的创建语句 -t –no-create-info：不包含数据表的创建语句 -d –no-data：不包含数据 -T, –tab&#x3D;name：自动生成两个文件：一个 .sql 文件，创建表结构的语句；一个 .txt 文件，数据文件，相当于 select into outfile 示例： 12mysqldump -uroot -p2143 db01 tb_book --add-drop-database --add-drop-table &gt; amysqldump -uroot -p2143 -T /tmp test city 数据备份命令行方式： 备份命令：mysqldump -u root -p 数据库名称 &gt; 文件保存路径 恢复 登录MySQL数据库：mysql -u root p 删除已经备份的数据库 重新创建与备份数据库名称相同的数据库 使用该数据库 导入文件执行：source 备份文件全路径 更多方式参考：https://time.geekbang.org/column/article/81925 图形化界面： 备份 恢复 importmysqlimport 是客户端数据导入工具，用来导入mysqldump 加 -T 参数后导出的文本文件 1mysqlimport [options] db_name textfile1 [textfile2...] 示例： 1mysqlimport -uroot -p2143 test /tmp/city.txt 导入 sql 文件，可以使用 MySQL 中的 source 指令 : 1source 文件全路径 showmysqlshow 客户端对象查找工具，用来很快地查找存在哪些数据库、数据库中的表、表中的列或者索引 1mysqlshow [options] [db_name [table_name [col_name]]] –count：显示数据库及表的统计信息（数据库，表 均可以不指定） -i：显示指定数据库或者指定表的状态信息 示例： 123456#查询每个数据库的表的数量及表中记录的数量mysqlshow -uroot -p1234 --count#查询test库中每个表中的字段书，及行数mysqlshow -uroot -p1234 test --count#查询test库中book表的详细情况mysqlshow -uroot -p1234 test book --count 单表操作SQL SQL Structured Query Language：结构化查询语言 定义了操作所有关系型数据库的规则，每种数据库操作的方式可能会存在不一样的地方，称为“方言” SQL 通用语法 SQL 语句可以单行或多行书写，以分号结尾。 可使用空格和缩进来增强语句的可读性。 MySQL 数据库的 SQL 语句不区分大小写，关键字建议使用大写。 数据库的注释： 单行注释：– 注释内容 #注释内容（MySQL 特有） 多行注释：&#x2F;* 注释内容 *&#x2F; SQL 分类 DDL（Data Definition Language）数据定义语言 用来定义数据库对象：数据库，表，列等。关键字：create、drop,、alter 等 DML（Data Manipulation Language）数据操作语言 用来对数据库中表的数据进行增删改。关键字：insert、delete、update 等 DQL（Data Query Language）数据查询语言 用来查询数据库中表的记录(数据)。关键字：select、where 等 DCL（Data Control Language）数据控制语言 用来定义数据库的访问权限和安全级别，及创建用户。关键字：grant， revoke等 DDL数据库 R(Retrieve)：查询 查询所有数据库： 1SHOW DATABASES; 查询某个数据库的创建语句 123SHOW CREATE DATABASE 数据库名称; -- 标准语法SHOW CREATE DATABASE mysql; -- 查看mysql数据库的创建格式 C(Create)：创建 创建数据库 123CREATE DATABASE 数据库名称;-- 标准语法CREATE DATABASE db1; -- 创建db1数据库 创建数据库（判断，如果不存在则创建） 1CREATE DATABASE IF NOT EXISTS 数据库名称; 创建数据库，并指定字符集 1CREATE DATABASE 数据库名称 CHARACTER SET 字符集名称; 例如：创建db4数据库、如果不存在则创建，指定字符集为gbk 12345-- 创建db4数据库、如果不存在则创建，指定字符集为gbkCREATE DATABASE IF NOT EXISTS db4 CHARACTER SET gbk;-- 查看db4数据库的字符集SHOW CREATE DATABASE db4; U(Update)：修改 修改数据库的字符集 1ALTER DATABASE 数据库名称 CHARACTER SET 字符集名称; 常用字符集： 1234567--查询所有支持的字符集SHOW CHARSET;--查看所有支持的校对规则SHOW COLLATION;-- 字符集: utf8,latinI,GBK,,GBK是utf8的子集-- 校对规则: ci 大小定不敏感，cs或bin大小写敏感 D(Delete)：删除 删除数据库： 1DROP DATABASE 数据库名称; 删除数据库(判断，如果存在则删除)： 1DROP DATABASE IF EXISTS 数据库名称; 使用数据库： 查询当前正在使用的数据库名称 1SELECT DATABASE(); 使用数据库 12USE 数据库名称； -- 标准语法USE db4; -- 使用db4数据库 数据表 R(Retrieve)：查询 查询数据库中所有的数据表 123USE mysql;-- 使用mysql数据库SHOW TABLES;-- 查询库中所有的表 查询表结构 1DESC 表名; 查询表字符集 1SHOW TABLE STATUS FROM 库名 LIKE &#x27;表名&#x27;; C(Create)：创建 创建数据表 1234567CREATE TABLE 表名( 列名1 数据类型1, 列名2 数据类型2, .... 列名n 数据类型n);-- 注意：最后一列，不需要加逗号 复制表 123CREATE TABLE 表名 LIKE 被复制的表名; -- 标准语法CREATE TABLE product2 LIKE product; -- 复制product表到product2表 数据类型 数据类型 说明 INT 整数类型 DOUBLE 小数类型 DATE 日期，只包含年月日：yyyy-MM-dd DATETIME 日期，包含年月日时分秒：yyyy-MM-dd HH:mm:ss TIMESTAMP 时间戳类型，包含年月日时分秒：yyyy-MM-dd HH:mm:ss如果不给这个字段赋值或赋值为 NULL，则默认使用当前的系统时间 CHAR 字符串，定长类型 VARCHAR 字符串，变长类型name varchar(20) 代表姓名最大 20 个字符：zhangsan 8 个字符，张三 2 个字符 INT(n)：n 代表位数 3：int（9）显示结果为 000000010 3：int（3）显示结果为 010 varchar(n)：n 表示的是字符数 例如： 1234567891011-- 使用db3数据库USE db3;-- 创建一个product商品表CREATE TABLE product(\tid INT, -- 商品编号\tNAME VARCHAR(30),\t-- 商品名称\tprice DOUBLE, -- 商品价格\tstock INT, -- 商品库存\tinsert_time DATE -- 上架时间); ​ U(Update)：修改 修改表名 1ALTER TABLE 表名 RENAME TO 新的表名; 修改表的字符集 1ALTER TABLE 表名 CHARACTER SET 字符集名称; 添加一列 1ALTER TABLE 表名 ADD 列名 数据类型; 修改列数据类型 1ALTER TABLE 表名 MODIFY 列名 新数据类型; 修改列名称和数据类型 1ALTER TABLE 表名 CHANGE 列名 新列名 新数据类型; 删除列 1ALTER TABLE 表名 DROP 列名; D(Delete)：删除 删除数据表 1DROP TABLE 表名; 删除数据表(判断，如果存在则删除) 1DROP TABLE IF EXISTS 表名; DMLINSERT 新增表数据 新增格式 1：给指定列添加数据 1INSERT INTO 表名(列名1,列名2...) VALUES (值1,值2...); 新增格式 2：默认给全部列添加数据 1INSERT INTO 表名 VALUES (值1,值2,值3,...); 新增格式 3：批量添加数据 12345-- 给指定列批量添加数据INSERT INTO 表名(列名1,列名2,...) VALUES (值1,值2,...),(值1,值2,...)...;-- 默认给所有列批量添加数据 INSERT INTO 表名 VALUES (值1,值2,值3,...),(值1,值2,值3,...)...; 字符串拼接 1CONCAT(string1,string2,&#x27;&#x27;,...) 注意事项 列名和值的数量以及数据类型要对应 除了数字类型，其他数据类型的数据都需要加引号(单引双引都可以，推荐单引) UPDATE 修改表数据语法 标准语法 1UPDATE 表名 SET 列名1 = 值1,列名2 = 值2,... [where 条件]; 修改电视的价格为1800、库存为36 12UPDATE product SET price=1800,stock=36 WHERE NAME=&#x27;电视&#x27;;SELECT * FROM product;-- 查看所有商品信息 注意事项 修改语句中必须加条件 如果不加条件，则将所有数据都修改 DELETE 删除表数据语法 1DELETE FROM 表名 [WHERE 条件]; 注意事项 删除语句中必须加条件 如果不加条件，则将所有数据删除 ​ DQL查询语法数据库查询遵循条件在前的原则 12345678910111213141516SELECT DISTINCT\t&lt;select list&gt;FROM\t&lt;left_table&gt; &lt;join_type&gt;JOIN\t&lt;right_table&gt; ON &lt;join_condition&gt;\t-- 连接查询在多表查询部分详解WHERE\t&lt;where_condition&gt;GROUP BY\t&lt;group_by_list&gt;HAVING\t&lt;having_condition&gt;ORDER BY\t&lt;order_by_condition&gt;LIMIT\t&lt;limit_params&gt; 执行顺序： 1234567891011121314151617FROM\t&lt;left_table&gt;ON &lt;join_condition&gt;&lt;join_type&gt; JOIN\t&lt;right_table&gt;WHERE &lt;where_condition&gt;GROUP BY &lt;group_by_list&gt;HAVING &lt;having_condition&gt;SELECT DISTINCT &lt;select list&gt;ORDER BY\t&lt;order_by_condition&gt;LIMIT &lt;limit_params&gt; 查询全部 查询全部的表数据 12345-- 标准语法SELECT * FROM 表名;-- 查询product表所有数据(常用)SELECT * FROM product; 查询指定字段的表数据 1SELECT 列名1,列名2,... FROM 表名; 去除重复查询：只有值全部重复的才可以去除，需要创建临时表辅助查询 1SELECT DISTINCT 列名1,列名2,... FROM 表名; 计算列的值（四则运算） 123456SELECT 列名1 运算符(+ - * /) 列名2 FROM 表名;/*如果某一列值为null，可以进行替换\tifnull(表达式1,表达式2)\t表达式1：想替换的列\t表达式2：想替换的值*/ 例如： 12345-- 查询商品名称和库存，库存数量在原有基础上加10SELECT NAME,stock+10 FROM product;-- 查询商品名称和库存，库存数量在原有基础上加10。进行null值判断SELECT NAME,IFNULL(stock,0)+10 FROM product; 起别名 1SELECT 列名1,列名2,... AS 别名 FROM 表名; 例如： 123-- 查询商品名称和库存，库存数量在原有基础上加10。进行null值判断，起别名为getSum,AS可以省略。SELECT NAME,IFNULL(stock,0)+10 AS getsum FROM product;SELECT NAME,IFNULL(stock,0)+10 getsum FROM product; 条件查询 条件查询语法 1SELECT 列名 FROM 表名 WHERE 条件; 条件分类 符号 功能 &gt; 大于 &lt; 小于 &gt;&#x3D; 大于等于 &lt;&#x3D; 小于等于 &#x3D; 等于 &lt;&gt; 或 !&#x3D; 不等于 BETWEEN … AND … 在某个范围之内(都包含) IN(…) 多选一 LIKE 模糊查询：_单个任意字符、%任意个字符、[] 匹配集合内的字符LIKE &#39;[^AB]%&#39; ：不以 A 和 B 开头的任意文本 IS NULL 是NULL IS NOT NULL 不是NULL AND 或 &amp;&amp; 并且 OR 或 || 或者 NOT 或 ! 非，不是 UNION 对两个结果集进行并集操作并进行去重，同时进行默认规则的排序 UNION ALL 对两个结果集进行并集操作不进行去重，不进行排序 例如： 123456789101112131415161718192021222324252627282930-- 查询库存大于20的商品信息SELECT * FROM product WHERE stock &gt; 20;-- 查询品牌为华为的商品信息SELECT * FROM product WHERE brand=&#x27;华为&#x27;;-- 查询金额在4000 ~ 6000之间的商品信息SELECT * FROM product WHERE price &gt;= 4000 AND price &lt;= 6000;SELECT * FROM product WHERE price BETWEEN 4000 AND 6000;-- 查询库存为14、30、23的商品信息SELECT * FROM product WHERE stock=14 OR stock=30 OR stock=23;SELECT * FROM product WHERE stock IN(14,30,23);-- 查询库存为null的商品信息SELECT * FROM product WHERE stock IS NULL;-- 查询库存不为null的商品信息SELECT * FROM product WHERE stock IS NOT NULL;-- 查询名称以&#x27;小米&#x27;为开头的商品信息SELECT * FROM product WHERE NAME LIKE &#x27;小米%&#x27;;-- 查询名称第二个字是&#x27;为&#x27;的商品信息SELECT * FROM product WHERE NAME LIKE &#x27;_为%&#x27;;-- 查询名称为四个字符的商品信息 4个下划线SELECT * FROM product WHERE NAME LIKE &#x27;____&#x27;;-- 查询名称中包含电脑的商品信息SELECT * FROM product WHERE NAME LIKE &#x27;%电脑%&#x27;; 函数查询聚合函数聚合函数：将一列数据作为一个整体，进行纵向的计算 聚合函数语法 1SELECT 函数名(列名) FROM 表名 [WHERE 条件] 聚合函数分类 函数名 功能 COUNT(列名) 统计数量（一般选用不为 null 的列） MAX(列名) 最大值 MIN(列名) 最小值 SUM(列名) 求和 AVG(列名) 平均值（会忽略 null 行） 例如 1234567891011121314151617-- 计算product表中总记录条数 7SELECT COUNT(*) FROM product;-- 获取最高价格SELECT MAX(price) FROM product;-- 获取最高价格的商品名称SELECT NAME,price FROM product WHERE price = (SELECT MAX(price) FROM product);-- 获取最低库存SELECT MIN(stock) FROM product;-- 获取最低库存的商品名称SELECT NAME,stock FROM product WHERE stock = (SELECT MIN(stock) FROM product);-- 获取总库存数量SELECT SUM(stock) FROM product;-- 获取品牌为小米的平均商品价格SELECT AVG(price) FROM product WHERE brand=&#x27;小米&#x27;; 文本函数CONCAT()：用于连接两个字段 12SELECT CONCAT(TRIM(col1), &#x27;(&#x27;, TRIM(col2), &#x27;)&#x27;) AS concat_col FROM mytable-- 许多数据库会使用空格把一个值填充为列宽，连接的结果出现一些不必要的空格，使用TRIM()可以去除首尾空格 函数名称 作 用 LENGTH 计算字符串长度函数，返回字符串的字节长度 CONCAT 合并字符串函数，返回结果为连接参数产生的字符串，参数可以使一个或多个 INSERT 替换字符串函数 LOWER 将字符串中的字母转换为小写 UPPER 将字符串中的字母转换为大写 LEFT 从左侧字截取符串，返回字符串左边的若干个字符 RIGHT 从右侧字截取符串，返回字符串右边的若干个字符 TRIM 删除字符串左右两侧的空格 REPLACE 字符串替换函数，返回替换后的新字符串 SUBSTRING 截取字符串，返回从指定位置开始的指定长度的字符换 REVERSE 字符串反转（逆序）函数，返回与原始字符串顺序相反的字符串 数字函数 函数名称 作 用 ABS 求绝对值 SQRT 求二次方根 MOD 求余数 CEIL 和 CEILING 两个函数功能相同，都是返回不小于参数的最小整数，即向上取整 FLOOR 向下取整，返回值转化为一个BIGINT RAND 生成一个0~1之间的随机数，传入整数参数是，用来产生重复序列 ROUND 对所传参数进行四舍五入 SIGN 返回参数的符号 POW 和 POWER 两个函数的功能相同，都是所传参数的次方的结果值 SIN 求正弦值 ASIN 求反正弦值，与函数 SIN 互为反函数 COS 求余弦值 ACOS 求反余弦值，与函数 COS 互为反函数 TAN 求正切值 ATAN 求反正切值，与函数 TAN 互为反函数 COT 求余切值 日期函数 函数名称 作 用 CURDATE 和 CURRENT_DATE 两个函数作用相同，返回当前系统的日期值 CURTIME 和 CURRENT_TIME 两个函数作用相同，返回当前系统的时间值 NOW 和 SYSDATE 两个函数作用相同，返回当前系统的日期和时间值 MONTH 获取指定日期中的月份 MONTHNAME 获取指定日期中的月份英文名称 DAYNAME 获取指定曰期对应的星期几的英文名称 DAYOFWEEK 获取指定日期对应的一周的索引位置值 WEEK 获取指定日期是一年中的第几周，返回值的范围是否为 0〜52 或 1〜53 DAYOFYEAR 获取指定曰期是一年中的第几天，返回值范围是1~366 DAYOFMONTH 获取指定日期是一个月中是第几天，返回值范围是1~31 YEAR 获取年份，返回值范围是 1970〜2069 TIME_TO_SEC 将时间参数转换为秒数 SEC_TO_TIME 将秒数转换为时间，与TIME_TO_SEC 互为反函数 DATE_ADD 和 ADDDATE 两个函数功能相同，都是向日期添加指定的时间间隔 DATE_SUB 和 SUBDATE 两个函数功能相同，都是向日期减去指定的时间间隔 ADDTIME 时间加法运算，在原始时间上添加指定的时间 SUBTIME 时间减法运算，在原始时间上减去指定的时间 DATEDIFF 获取两个日期之间间隔，返回参数 1 减去参数 2 的值 DATE_FORMAT 格式化指定的日期，根据参数返回指定格式的值 WEEKDAY 获取指定日期在一周内的对应的工作日索引 正则查询正则表达式（Regular Expression）是指一个用来描述或者匹配一系列符合某个句法规则的字符串的单个字符串 123SELECT * FROM emp WHERE name REGEXP &#x27;^T&#x27;;\t-- 匹配以T开头的name值SELECT * FROM emp WHERE name REGEXP &#x27;2$&#x27;;\t-- 匹配以2结尾的name值SELECT * FROM emp WHERE name REGEXP &#x27;[uvw]&#x27;;-- 匹配包含 uvw 的name值 符号 含义 ^ 在字符串开始处进行匹配 $ 在字符串末尾处进行匹配 . 匹配任意单个字符, 包括换行符 […] 匹配出括号内的任意字符 [^…] 匹配不出括号内的任意字符 a* 匹配零个或者多个a(包括空串) a+ 匹配一个或者多个a(不包括空串) a? 匹配零个或者一个a a1|a2 匹配a1或a2 a(m) 匹配m个a a(m,) 至少匹配m个a a(m,n) 匹配m个a 到 n个a a(,n) 匹配0到n个a (…) 将模式元素组成单一元素 排序查询 排序查询语法 1SELECT 列名 FROM 表名 [WHERE 条件] ORDER BY 列名1 排序方式1,列名2 排序方式2; 排序方式 12ASC:升序DESC:降序 注意：多个排序条件，当前边的条件值一样时，才会判断第二条件 例如 12345678-- 按照库存升序排序SELECT * FROM product ORDER BY stock ASC;-- 查询名称中包含手机的商品信息。按照金额降序排序SELECT * FROM product WHERE NAME LIKE &#x27;%手机%&#x27; ORDER BY price DESC;-- 按照金额升序排序，如果金额相同，按照库存降序排列SELECT * FROM product ORDER BY price ASC,stock DESC; 分组查询分组查询会进行去重 分组查询语法 1SELECT 列名 FROM 表名 [WHERE 条件] GROUP BY 分组列名 [HAVING 分组后条件过滤] [ORDER BY 排序列名 排序方式]; WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤 分组规定： GROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前 NULL 的行会单独分为一组 大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型 例如 1234567891011-- 按照品牌分组，获取每组商品的总金额SELECT brand,SUM(price) FROM product GROUP BY brand;-- 对金额大于4000元的商品，按照品牌分组,获取每组商品的总金额SELECT brand,SUM(price) FROM product WHERE price &gt; 4000 GROUP BY brand;-- 对金额大于4000元的商品，按照品牌分组，获取每组商品的总金额，只显示总金额大于7000元的SELECT brand,SUM(price) AS getSum FROM product WHERE price &gt; 4000 GROUP BY brand HAVING getSum &gt; 7000;-- 对金额大于4000元的商品，按照品牌分组，获取每组商品的总金额，只显示总金额大于7000元的、并按照总金额的降序排列SELECT brand,SUM(price) AS getSum FROM product WHERE price &gt; 4000 GROUP BY brand HAVING getSum &gt; 7000 ORDER BY getSum DESC; 分页查询 分页查询语法 1SELECT 列名 FROM 表名 [WHERE 条件] GROUP BY 分组列名 [HAVING 分组后条件过滤] [ORDER BY 排序列名 排序方式] LIMIT 开始索引,查询条数; 公式：开始索引 &#x3D; (当前页码-1) * 每页显示的条数 例如 1234SELECT * FROM product LIMIT 0,2; -- 第一页 开始索引=(1-1) * 2SELECT * FROM product LIMIT 2,2; -- 第二页 开始索引=(2-1) * 2SELECT * FROM product LIMIT 4,2; -- 第三页 开始索引=(3-1) * 2SELECT * FROM product LIMIT 6,2; -- 第四页 开始索引=(4-1) * 2 多表操作约束分类约束介绍约束：对表中的数据进行限定，保证数据的正确性、有效性、完整性 约束的分类： 约束 说明 PRIMARY KEY 主键约束 PRIMARY KEY AUTO_INCREMENT 主键、自动增长 UNIQUE 唯一约束 NOT NULL 非空约束 FOREIGN KEY 外键约束 FOREIGN KEY ON UPDATE CASCADE 外键级联更新 FOREIGN KEY ON DELETE CASCADE 外键级联删除 主键约束 主键约束特点： 主键约束默认包含非空和唯一两个功能 一张表只能有一个主键 主键一般用于表中数据的唯一标识 建表时添加主键约束 12345CREATE TABLE 表名(\t列名 数据类型 PRIMARY KEY, 列名 数据类型, ...); 删除主键约束 1ALTER TABLE 表名 DROP PRIMARY KEY; 建表后单独添加主键约束 1ALTER TABLE 表名 MODIFY 列名 数据类型 PRIMARY KEY; 例如 1234567891011-- 创建student表CREATE TABLE student(\tid INT PRIMARY KEY -- 给id添加主键约束);-- 添加数据INSERT INTO student VALUES (1),(2);-- 主键默认唯一，添加重复数据，会报错INSERT INTO student VALUES (2);-- 主键默认非空，不能添加null的数据INSERT INTO student VALUES (NULL); 主键自增主键自增约束可以为空，并自动增长。删除某条数据不影响自增的下一个数值，依然按照前一个值自增 建表时添加主键自增约束 12345CREATE TABLE 表名(\t列名 数据类型 PRIMARY KEY AUTO_INCREMENT, 列名 数据类型, ...); 删除主键自增约束 1ALTER TABLE 表名 MODIFY 列名 数据类型; 建表后单独添加主键自增约束 1ALTER TABLE 表名 MODIFY 列名 数据类型 AUTO_INCREMENT; 例如 123456789-- 创建student2表CREATE TABLE student2(\tid INT PRIMARY KEY AUTO_INCREMENT -- 给id添加主键自增约束);-- 添加数据INSERT INTO student2 VALUES (1),(2);-- 添加null值，会自动增长INSERT INTO student2 VALUES (NULL),(NULL);-- 3，4 唯一约束唯一约束：约束不能有重复的数据 建表时添加唯一约束 12345CREATE TABLE 表名(\t列名 数据类型 UNIQUE, 列名 数据类型, ...); 删除唯一约束 1ALTER TABLE 表名 DROP INDEX 列名; 建表后单独添加唯一约束 1ALTER TABLE 表名 MODIFY 列名 数据类型 UNIQUE; 非空约束 建表时添加非空约束 12345CREATE TABLE 表名(\t列名 数据类型 NOT NULL, 列名 数据类型, ...); 删除非空约束 1ALTER TABLE 表名 MODIFY 列名 数据类型; 建表后单独添加非空约束 1ALTER TABLE 表名 MODIFY 列名 数据类型 NOT NULL; 外键约束 外键约束：让表和表之间产生关系，从而保证数据的准确性 建表时添加外键约束 12345CREATE TABLE 表名(\t列名 数据类型 约束, ... CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名)); 删除外键约束 1ALTER TABLE 表名 DROP FOREIGN KEY 外键名; 建表后单独添加外键约束 1ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名); 例如 123456789101112131415161718192021222324-- 创建user用户表CREATE TABLE USER(\tid INT PRIMARY KEY AUTO_INCREMENT, -- id\tname VARCHAR(20) NOT NULL -- 姓名);-- 添加用户数据INSERT INTO USER VALUES (NULL,&#x27;张三&#x27;),(NULL,&#x27;李四&#x27;),(NULL,&#x27;王五&#x27;);-- 创建orderlist订单表CREATE TABLE orderlist(\tid INT PRIMARY KEY AUTO_INCREMENT, -- id\tnumber VARCHAR(20) NOT NULL, -- 订单编号\tuid INT, -- 订单所属用户\tCONSTRAINT ou_fk1 FOREIGN KEY (uid) REFERENCES USER(id) -- 添加外键约束);-- 添加订单数据INSERT INTO orderlist VALUES (NULL,&#x27;hm001&#x27;,1),(NULL,&#x27;hm002&#x27;,1),(NULL,&#x27;hm003&#x27;,2),(NULL,&#x27;hm004&#x27;,2),(NULL,&#x27;hm005&#x27;,3),(NULL,&#x27;hm006&#x27;,3);-- 添加一个订单，但是没有所属用户。无法添加INSERT INTO orderlist VALUES (NULL,&#x27;hm007&#x27;,8);-- 删除王五这个用户，但是订单表中王五还有很多个订单呢。无法删除DELETE FROM USER WHERE NAME=&#x27;王五&#x27;; 外键级联级联操作：当把主表中的数据进行删除或更新时，从表中有关联的数据的相应操作，包括 RESTRICT、CASCADE、SET NULL 和 NO ACTION RESTRICT 和 NO ACTION相同， 是指限制在子表有关联记录的情况下， 父表不能更新 CASCADE 表示父表在更新或者删除时，更新或者删除子表对应的记录 SET NULL 则表示父表在更新或者删除的时候，子表的对应字段被SET NULL 级联操作： 添加级联更新 1ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名) ON UPDATE [CASCADE | RESTRICT | SET NULL]; 添加级联删除 1ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名) ON DELETE CASCADE; 同时添加级联更新和级联删除 1ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名) ON UPDATE CASCADE ON DELETE CASCADE; 多表设计一对一多表：有多张数据表，而表与表之间有一定的关联关系，通过外键约束实现，分为一对一、一对多、多对多三类 举例：人和身份证 实现原则：在任意一个表建立外键，去关联另外一个表的主键 1234567891011121314151617-- 创建person表CREATE TABLE person(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id\tNAME VARCHAR(20) -- 姓名);-- 添加数据INSERT INTO person VALUES (NULL,&#x27;张三&#x27;),(NULL,&#x27;李四&#x27;);-- 创建card表CREATE TABLE card(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id\tnumber VARCHAR(20) UNIQUE NOT NULL,\t-- 身份证号\tpid INT UNIQUE, -- 外键列\tCONSTRAINT cp_fk1 FOREIGN KEY (pid) REFERENCES person(id));-- 添加数据INSERT INTO card VALUES (NULL,&#x27;12345&#x27;,1),(NULL,&#x27;56789&#x27;,2); 一对多举例：用户和订单、商品分类和商品 实现原则：在多的一方，建立外键约束，来关联一的一方主键 1234567891011121314151617-- 创建user表CREATE TABLE USER(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id\tNAME VARCHAR(20) -- 姓名);-- 添加数据INSERT INTO USER VALUES (NULL,&#x27;张三&#x27;),(NULL,&#x27;李四&#x27;);-- 创建orderlist表CREATE TABLE orderlist(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id\tnumber VARCHAR(20), -- 订单编号\tuid INT, -- 外键列\tCONSTRAINT ou_fk1 FOREIGN KEY (uid) REFERENCES USER(id));-- 添加数据INSERT INTO orderlist VALUES (NULL,&#x27;hm001&#x27;,1),(NULL,&#x27;hm002&#x27;,1),(NULL,&#x27;hm003&#x27;,2),(NULL,&#x27;hm004&#x27;,2); 多对多举例：学生和课程。一个学生可以选择多个课程，一个课程也可以被多个学生选择 实现原则：借助第三张表中间表，中间表至少包含两个列，这两个列作为中间表的外键，分别关联两张表的主键 1234567891011121314151617181920212223242526-- 创建student表CREATE TABLE student(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id\tNAME VARCHAR(20) -- 学生姓名);-- 添加数据INSERT INTO student VALUES (NULL,&#x27;张三&#x27;),(NULL,&#x27;李四&#x27;);-- 创建course表CREATE TABLE course(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id\tNAME VARCHAR(10) -- 课程名称);-- 添加数据INSERT INTO course VALUES (NULL,&#x27;语文&#x27;),(NULL,&#x27;数学&#x27;);-- 创建中间表CREATE TABLE stu_course(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id\tsid INT, -- 用于和student表中的id进行外键关联\tcid INT, -- 用于和course表中的id进行外键关联\tCONSTRAINT sc_fk1 FOREIGN KEY (sid) REFERENCES student(id), -- 添加外键约束\tCONSTRAINT sc_fk2 FOREIGN KEY (cid) REFERENCES course(id) -- 添加外键约束);-- 添加数据INSERT INTO stu_course VALUES (NULL,1,1),(NULL,1,2),(NULL,2,1),(NULL,2,2); 连接查询内外连接内连接连接查询的是两张表有交集的部分数据，两张表分为驱动表和被驱动表，如果结果集中的每条记录都是两个表相互匹配的组合，则称这样的结果集为笛卡尔积 内连接查询，若驱动表中的记录在被驱动表中找不到匹配的记录时，则该记录不会加到最后的结果集 显式内连接： 1SELECT 列名 FROM 表名1 [INNER] JOIN 表名2 ON 条件; 隐式内连接：内连接中 WHERE 子句和 ON 子句是等价的 1SELECT 列名 FROM 表名1,表名2 WHERE 条件; STRAIGHT_JOIN与 JOIN 类似，只不过左表始终在右表之前读取，只适用于内连接 外连接外连接查询，若驱动表中的记录在被驱动表中找不到匹配的记录时，则该记录也会加到最后的结果集，只是对于被驱动表中不匹配过滤条件的记录，各个字段使用 NULL 填充 应用实例：查学生成绩，也想展示出缺考的人的成绩 左外连接：选择左侧的表为驱动表，查询左表的全部数据，和左右两张表有交集部分的数据 1SELECT 列名 FROM 表名1 LEFT [OUTER] JOIN 表名2 ON 条件; 右外连接：选择右侧的表为驱动表，查询右表的全部数据，和左右两张表有交集部分的数据 1SELECT 列名 FROM 表名1 RIGHT [OUTER] JOIN 表名2 ON 条件; 关联查询自关联查询：同一张表中有数据关联，可以多次查询这同一个表 数据准备 123456789-- 创建员工表CREATE TABLE employee(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 员工编号\tNAME VARCHAR(20), -- 员工姓名\tmgr INT, -- 上级编号\tsalary DOUBLE -- 员工工资);-- 添加数据INSERT INTO employee VALUES (1001,&#x27;孙悟空&#x27;,1005,9000.00),..,(1009,&#x27;宋江&#x27;,NULL,16000.00); 数据查询 12345678910111213141516171819-- 查询所有员工的姓名及其直接上级的姓名，没有上级的员工也需要查询/*分析\t员工信息 employee表\t条件：employee.mgr = employee.id\t查询左表的全部数据，和左右两张表有交集部分数据，左外连接*/SELECT\te1.id,\te1.name,\te1.mgr,\te2.id,\te2.nameFROM\temployee e1LEFT OUTER JOIN\temployee e2ON\te1.mgr = e2.id; 查询结果 12345678910id name\tmgr id name1001\t孙悟空 1005\t1005\t唐僧1002\t猪八戒 1005\t1005\t唐僧1003\t沙和尚 1005\t1005\t唐僧1004\t小白龙 1005\t1005\t唐僧1005\t唐僧 NULL NULL NULL1006\t武松 1009 1009 宋江1007\t李逵 1009 1009 宋江1008\t林冲 1009 1009 宋江1009\t宋江 NULL NULL NULL 连接原理Index Nested-Loop Join 算法：查询驱动表得到数据集，然后根据数据集中的每一条记录的关联字段再分别到被驱动表中查找匹配（走索引），所以驱动表只需要访问一次，被驱动表要访问多次 MySQL 将查询驱动表后得到的记录成为驱动表的扇出，连接查询的成本：单次访问驱动表的成本 + 扇出值 * 单次访问被驱动表的成本，优化器会选择成本最小的表连接顺序（确定谁是驱动表，谁是被驱动表）生成执行计划，进行连接查询，优化方式： 减少驱动表的扇出（让数据量小的表来做驱动表） 降低访问被驱动表的成本 说明：STRAIGHT_JOIN 是查一条驱动表，然后根据关联字段去查被驱动表，要访问多次驱动表，所以需要优化为 INL 算法 Block Nested-Loop Join 算法：一种空间换时间的优化方式，基于块的循环连接，执行连接查询前申请一块固定大小的内存作为连接缓冲区 Join Buffer，先把若干条驱动表中的扇出暂存在缓冲区，每一条被驱动表中的记录一次性的与 Buffer 中多条记录进行匹配（扫描全部数据，一条一条的匹配），因为是在内存中完成，所以速度快，并且降低了 I&#x2F;O 成本 Join Buffer 可以通过参数 join_buffer_size 进行配置，默认大小是 256 KB 在成本分析时，对于很多张表的连接查询，连接顺序有非常多，MySQL 如果挨着进行遍历计算成本，会消耗很多资源 提前结束某种连接顺序的成本评估：维护一个全局变量记录当前成本最小的连接方式，如果一种顺序只计算了一部分就已经超过了最小成本，可以提前结束计算 系统变量 optimizer_search_depth：如果连接表的个数小于该变量，就继续穷举分析每一种连接数量，反之只对数量与 depth 值相同的表进行分析，该值越大成本分析的越精确 系统变量 optimizer_prune_level：控制启发式规则的启用，这些规则就是根据以往经验指定的，不满足规则的连接顺序不分析成本 连接优化BKABatched Key Access 算法是对 NLJ 算法的优化，在读取被驱动表的记录时使用顺序 IO，Extra 信息中会有 Batched Key Access 信息 使用 BKA 的表的 JOIN 过程如下： 连接驱动表将满足条件的记录放入 Join Buffer，并将两表连接的字段放入一个 DYNAMIC_ARRAY ranges 中 在进行表的过接过程中，会将 ranges 相关的信息传入 Buffer 中，进行被驱动表主建的查找及排序操作 调用步骤 2 中产生的有序主建，顺序读取被驱动表的数据 当缓冲区的数据被读完后，会重复进行步骤 2、3，直到记录被读取完 使用 BKA 优化需要设进行设置： 1SET optimizer_switch=&#x27;mrr=on,mrr_cost_based=off,batched_key_access=on&#x27;; 说明：前两个参数的作用是启用 MRR，因为 BKA 算法的优化要依赖于 MRR（系统优化 → 内存优化 → Read 详解） BNL问题BNL 即 Block Nested-Loop Join 算法，由于要访问多次被驱动表，会产生两个问题： Join 语句多次扫描一个冷表，并且语句执行时间小于 1 秒，就会在再次扫描冷表时，把冷表的数据页移到 LRU 链表头部，导致热数据被淘汰，影响业务的正常运行 这种情况冷表的数据量要小于整个 Buffer Pool 的 old 区域，能够完全放入 old 区，才会再次被读时加到 young，否则读取下一段时就已经把上一段淘汰 Join 语句在循环读磁盘和淘汰内存页，进入 old 区域的数据页很可能在 1 秒之内就被淘汰，就会导致 MySQL 实例的 Buffer Pool 在这段时间内 young 区域的数据页没有被合理地淘汰 大表 Join 操作虽然对 IO 有影响，但是在语句执行结束后对 IO 的影响随之结束。但是对 Buffer Pool 的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率 优化将 BNL 算法转成 BKA 算法，优化方向： 在被驱动表上建索引，这样就可以根据索引进行顺序 IO 使用临时表，在临时表上建立索引，将被驱动表和临时表进行连接查询 驱动表 t1，被驱动表 t2，使用临时表的工作流程： 把表 t1 中满足条件的数据放在临时表 tmp_t 中 给临时表 tmp_t 的关联字段加上索引，使用 BKA 算法 让表 t2 和 tmp_t 做 Join 操作（临时表是被驱动表） 补充：MySQL 8.0 支持 hash join，join_buffer 维护的不再是一个无序数组，而是一个哈希表，查询效率更高，执行效率比临时表更高 嵌套查询查询分类查询语句中嵌套了查询语句，将嵌套查询称为子查询，FROM 子句后面的子查询的结果集称为派生表 根据结果分类： 结果是单行单列：可以将查询的结果作为另一条语句的查询条件，使用运算符判断 1SELECT 列名 FROM 表名 WHERE 列名=(SELECT 列名/聚合函数(列名) FROM 表名 [WHERE 条件]); 结果是多行单列：可以作为条件，使用运算符 IN 或 NOT IN 进行判断 1SELECT 列名 FROM 表名 WHERE 列名 [NOT] IN (SELECT 列名 FROM 表名 [WHERE 条件]); 结果是多行多列：查询的结果可以作为一张虚拟表参与查询 12345678910SELECT 列名 FROM 表名 [别名],(SELECT 列名 FROM 表名 [WHERE 条件]) [别名] [WHERE 条件];-- 查询订单表orderlist中id大于4的订单信息和所属用户USER信息SELECT * FROM USER u,\t(SELECT * FROM orderlist WHERE id&gt;4) o WHERE u.id=o.uid; 相关性分类： 不相关子查询：子查询不依赖外层查询的值，可以单独运行出结果 相关子查询：子查询的执行需要依赖外层查询的值 查询优化不相关子查询的结果集会被写入一个临时表，并且在写入时去重，该过程称为物化，存储结果集的临时表称为物化表 系统变量 tmp_table_size 或者 max_heap_table_size 为表的最值 小于系统变量时，内存中可以保存，会为建立基于内存的 MEMORY 存储引擎的临时表，并建立哈希索引 大于任意一个系统变量时，物化表会使用基于磁盘的 InnoDB 存储引擎来保存结果集中的记录，索引类型为 B+ 树 物化后，嵌套查询就相当于外层查询的表和物化表进行内连接查询，然后经过优化器选择成本最小的表连接顺序执行查询 子查询物化会产生建立临时表的成本，但是将子查询转化为连接查询可以充分发挥优化器的作用，所以引入：半连接 t1 和 t2 表进行半连接，对于 t1 表中的某条记录，只需要关心在 t2 表中是否存在，而不需要关心有多少条记录与之匹配，最终结果集只保留 t1 的记录 半连接只是执行子查询的一种方式，MySQL 并没有提供面向用户的半连接语法 参考书籍：https://book.douban.com/subject/35231266/ 联合查询UNION 是取这两个子查询结果的并集，并进行去重，同时进行默认规则的排序（union 是行加起来，join 是列加起来） UNION ALL 是对两个结果集进行并集操作不进行去重，不进行排序 1(select 1000 as f) union (select id from t1 order by id desc limit 2); #t1表中包含id 为 1-1000 的数据 语句的执行流程： 创建一个内存临时表，这个临时表只有一个整型字段 f，并且 f 是主键字段 执行第一个子查询，得到 1000 这个值，并存入临时表中 执行第二个子查询，拿到第一行 id&#x3D;1000，试图插入临时表中，但由于 1000 这个值已经存在于临时表了，违反了唯一性约束，所以插入失败，然后继续执行 取到第二行 id&#x3D;999，插入临时表成功 从临时表中按行取出数据，返回结果并删除临时表，结果中包含两行数据分别是 1000 和 999 查询练习数据准备： 123456789101112131415161718192021222324252627282930313233343536373839404142-- 创建db4数据库CREATE DATABASE db4;-- 使用db4数据库USE db4;-- 创建user表CREATE TABLE USER(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 用户id\tNAME VARCHAR(20), -- 用户姓名\tage INT -- 用户年龄);-- 订单表CREATE TABLE orderlist(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 订单id\tnumber VARCHAR(30), -- 订单编号\tuid INT, -- 外键字段\tCONSTRAINT ou_fk1 FOREIGN KEY (uid) REFERENCES USER(id));-- 商品分类表CREATE TABLE category(\tid INT PRIMARY KEY AUTO_INCREMENT, -- 商品分类id\tNAME VARCHAR(10) -- 商品分类名称);-- 商品表CREATE TABLE product(\tid INT PRIMARY KEY AUTO_INCREMENT, -- 商品id\tNAME VARCHAR(30), -- 商品名称\tcid INT, -- 外键字段\tCONSTRAINT cp_fk1 FOREIGN KEY (cid) REFERENCES category(id));-- 中间表CREATE TABLE us_pro(\tupid INT PRIMARY KEY AUTO_INCREMENT, -- 中间表id\tuid INT, -- 外键字段。需要和用户表的主键产生关联\tpid INT, -- 外键字段。需要和商品表的主键产生关联\tCONSTRAINT up_fk1 FOREIGN KEY (uid) REFERENCES USER(id),\tCONSTRAINT up_fk2 FOREIGN KEY (pid) REFERENCES product(id)); 数据查询： 查询用户的编号、姓名、年龄、订单编号 数据：用户的编号、姓名、年龄在 user 表，订单编号在 orderlist 表 条件：user.id &#x3D; orderlist.uid 12345678SELECT\tu.*,\to.numberFROM\tUSER u,\torderlist oWHERE\tu.id = o.uid; 查询所有的用户，显示用户的编号、姓名、年龄、订单编号。 123456789SELECT\tu.*,\to.numberFROM\tUSER uLEFT OUTER JOIN\torderlist oON\tu.id = o.uid; 查询用户年龄大于 23 岁的信息，显示用户的编号、姓名、年龄、订单编号 12345678910SELECT\tu.*,\to.numberFROM\tUSER u,\torderlist oWHERE\tu.id = o.uid\tAND\tu.age &gt; 23; 12345678SELECT\tu.*,\to.numberFROM\t(SELECT * FROM USER WHERE age &gt; 23) u,-- 嵌套查询\torderlist oWHERE\tu.id = o.uid; 查询张三和李四用户的信息，显示用户的编号、姓名、年龄、订单编号。 12345678910SELECT\tu.*,\to.numberFROM\tUSER u,\torderlist oWHERE\tu.id=o.uid\tAND\tu.name IN (&#x27;张三&#x27;,&#x27;李四&#x27;); 查询所有的用户和该用户能查看的所有的商品，显示用户的编号、姓名、年龄、商品名称 数据：用户的编号、姓名、年龄在 user 表，商品名称在 product 表，中间表 us_pro 条件：us_pro.uid &#x3D; user.id AND us_pro.pid &#x3D; product.id 12345678910111213SELECT\tu.id,\tu.name,\tu.age,\tp.nameFROM\tUSER u,\tproduct p,\tus_pro upWHERE\tup.uid = u.id\tAND\tup.pid=p.id; 查询张三和李四这两个用户可以看到的商品，显示用户的编号、姓名、年龄、商品名称。 123456789101112131415SELECT\tu.id,\tu.name,\tu.age,\tp.nameFROM\tUSER u,\tproduct p,\tus_pro upWHERE\tup.uid=u.id\tAND\tup.pid=p.id\tAND\tu.name IN (&#x27;张三&#x27;,&#x27;李四&#x27;); 高级结构视图基本介绍视图概念：视图是一种虚拟存在的数据表，这个虚拟的表并不在数据库中实际存在 本质：将一条 SELECT 查询语句的结果封装到了一个虚拟表中，所以在创建视图的时候，工作重心要放在这条 SELECT 查询语句上 作用：将一些比较复杂的查询语句的结果，封装到一个虚拟表中，再有相同查询需求时，直接查询该虚拟表 优点： 简单：使用视图的用户不需要关心表的结构、关联条件和筛选条件，因为虚拟表中已经是过滤好的结果集 安全：使用视图的用户只能访问查询的结果集，对表的权限管理并不能限制到某个行某个列 数据独立，一旦视图的结构确定，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响 视图创建 创建视图 1234CREATE [OR REPLACE] VIEW 视图名称 [(列名列表)] AS 查询语句[WITH [CASCADED | LOCAL] CHECK OPTION]; WITH [CASCADED | LOCAL] CHECK OPTION 决定了是否允许更新数据使记录不再满足视图的条件： LOCAL：只要满足本视图的条件就可以更新 CASCADED：必须满足所有针对该视图的所有视图的条件才可以更新， 默认值 例如 123456789101112131415161718192021222324252627-- 数据准备 cityid\tNAME\tcid1\t深圳 12\t上海 13\t纽约 24\t莫斯科 3-- 数据准备 countryid\tNAME1\t中国2\t美国3\t俄罗斯-- 创建city_country视图，保存城市和国家的信息(使用指定列名)CREATE VIEW city_country (city_id,city_name,country_name)AS SELECT c1.id, c1.name, c2.name FROM city c1, country c2 WHERE c1.cid=c2.id; 视图查询 查询所有数据表，视图也会查询出来 12SHOW TABLES;SHOW TABLE STATUS [\\G]; 查询视图 1SELECT * FROM 视图名称; 查询某个视图创建 1SHOW CREATE VIEW 视图名称; 视图修改视图表数据修改，会自动修改源表中的数据，因为更新的是视图中的基表中的数据 修改视图表中的数据 1UPDATE 视图名称 SET 列名 = 值 WHERE 条件; 修改视图的结构 12345678910111213141516171819ALTER [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;]VIEW 视图名称 [(列名列表)] AS 查询语句[WITH [CASCADED | LOCAL] CHECK OPTION]-- 将视图中的country_name修改为nameALTER VIEW city_country (city_id,city_name,name) AS SELECT c1.id, c1.name, c2.name FROM city c1, country c2 WHERE c1.cid=c2.id; 视图删除 删除视图 1DROP VIEW 视图名称; 如果存在则删除 1DROP VIEW IF EXISTS 视图名称; 存储过程基本介绍存储过程和函数：存储过程和函数是事先经过编译并存储在数据库中的一段 SQL 语句的集合 存储过程和函数的好处： 提高代码的复用性 减少数据在数据库和应用服务器之间的传输，提高传输效率 减少代码层面的业务处理 一次编译永久有效 存储过程和函数的区别： 存储函数必须有返回值 存储过程可以没有返回值 基本操作DELIMITER： DELIMITER 关键字用来声明 sql 语句的分隔符，告诉 MySQL 该段命令已经结束 MySQL 语句默认的分隔符是分号，但是有时需要一条功能 sql 语句中包含分号，但是并不作为结束标识，这时使用 DELIMITER 来指定分隔符： 1DELIMITER 分隔符 存储过程的创建调用查看和删除： 创建存储过程 1234567891011-- 修改分隔符为$DELIMITER $-- 标准语法CREATE PROCEDURE 存储过程名称(参数...)BEGIN\tsql语句;END$-- 修改分隔符为分号DELIMITER ; 调用存储过程 1CALL 存储过程名称(实际参数); 查看存储过程 1SELECT * FROM mysql.proc WHERE db=&#x27;数据库名称&#x27;; 删除存储过程 1DROP PROCEDURE [IF EXISTS] 存储过程名称; 练习： 数据准备 12345id\tNAME\tage gender\tscore1\t张三 23 男 952\t李四 24 男 983\t王五 25 女 1004\t赵六 26 女 90 创建 stu_group() 存储过程，封装分组查询总成绩，并按照总成绩升序排序的功能 12345678910111213DELIMITER $CREATE PROCEDURE stu_group()BEGIN\tSELECT gender,SUM(score) getSum FROM student GROUP BY gender ORDER BY getSum ASC; END$DELIMITER ;-- 调用存储过程CALL stu_group();-- 删除存储过程DROP PROCEDURE IF EXISTS stu_group; 存储语法变量使用存储过程是可以进行编程的，意味着可以使用变量、表达式、条件控制语句等，来完成比较复杂的功能 定义变量：DECLARE 定义的是局部变量，只能用在 BEGIN END 范围之内 1DECLARE 变量名 数据类型 [DEFAULT 默认值]; 变量的赋值 12SET 变量名 = 变量值;SELECT 列名 INTO 变量名 FROM 表名 [WHERE 条件]; 数据准备：表 student 12345id\tNAME\tage gender\tscore1\t张三 23 男 952\t李四 24 男 983\t王五 25 女 1004\t赵六 26 女 90 定义两个 int 变量，用于存储男女同学的总分数 123456789101112131415DELIMITER $CREATE PROCEDURE pro_test3()BEGIN\t-- 定义两个变量\tDECLARE men,women INT;\t-- 查询男同学的总分数，为men赋值\tSELECT SUM(score) INTO men FROM student WHERE gender=&#x27;男&#x27;;\t-- 查询女同学的总分数，为women赋值\tSELECT SUM(score) INTO women FROM student WHERE gender=&#x27;女&#x27;;\t-- 使用变量\tSELECT men,women;END$DELIMITER ;-- 调用存储过程CALL pro_test3(); IF语句 if 语句标准语法 12345IF 判断条件1 THEN 执行的sql语句1;[ELSEIF 判断条件2 THEN 执行的sql语句2;]...[ELSE 执行的sql语句n;]END IF; 数据准备：表 student 12345id\tNAME\tage gender\tscore1\t张三 23 男 952\t李四 24 男 983\t王五 25 女 1004\t赵六 26 女 90 根据总成绩判断：全班 380 分及以上学习优秀、320 ~ 380 学习良好、320 以下学习一般 123456789101112131415161718DELIMITER $CREATE PROCEDURE pro_test4()BEGIN\tDECLARE total INT; -- 定义总分数变量\tDECLARE description VARCHAR(10); -- 定义分数描述变量\tSELECT SUM(score) INTO total FROM student; -- 为总分数变量赋值\t-- 判断总分数\tIF total &gt;= 380 THEN SET description = &#x27;学习优秀&#x27;;\tELSEIF total &gt;=320 AND total &lt; 380 THEN SET description = &#x27;学习良好&#x27;;\tELSE SET description = &#x27;学习一般&#x27;;\tEND IF;END$DELIMITER ;-- 调用pro_test4存储过程CALL pro_test4(); 参数传递 参数传递的语法 IN：代表输入参数，需要由调用者传递实际数据，默认的OUT：代表输出参数，该参数可以作为返回值INOUT：代表既可以作为输入参数，也可以作为输出参数 123456789DELIMITER $-- 标准语法CREATE PROCEDURE 存储过程名称([IN|OUT|INOUT] 参数名 数据类型)BEGIN\t执行的sql语句;END$DELIMITER ; 输入总成绩变量，代表学生总成绩，输出分数描述变量，代表学生总成绩的描述 1234567891011121314151617181920DELIMITER $CREATE PROCEDURE pro_test6(IN total INT, OUT description VARCHAR(10))BEGIN\t-- 判断总分数\tIF total &gt;= 380 THEN SET description = &#x27;学习优秀&#x27;;\tELSEIF total &gt;= 320 AND total &lt; 380 THEN SET description = &#x27;学习不错&#x27;;\tELSE SET description = &#x27;学习一般&#x27;;\tEND IF;END$DELIMITER ;-- 调用pro_test6存储过程CALL pro_test6(310,@description);CALL pro_test6((SELECT SUM(score) FROM student), @description);-- 查询总成绩描述SELECT @description; 查看参数方法 @变量名 : 用户会话变量，代表整个会话过程他都是有作用的，类似于全局变量 @@变量名 : 系统变量 CASE 标准语法 1 123456CASE 表达式 WHEN 值1 THEN 执行sql语句1; [WHEN 值2 THEN 执行sql语句2;] ... [ELSE 执行sql语句n;]END CASE; 标准语法 2 123456sCASE WHEN 判断条件1 THEN 执行sql语句1; [WHEN 判断条件2 THEN 执行sql语句2;] ... [ELSE 执行sql语句n;]END CASE; 演示 12345678910111213141516171819202122DELIMITER $CREATE PROCEDURE pro_test7(IN total INT)BEGIN\t-- 定义变量\tDECLARE description VARCHAR(10);\t-- 使用case判断\tCASE\tWHEN total &gt;= 380 THEN SET description = &#x27;学习优秀&#x27;;\tWHEN total &gt;= 320 AND total &lt; 380 THEN SET description = &#x27;学习不错&#x27;;\tELSE SET description = &#x27;学习一般&#x27;;\tEND CASE; -- 查询分数描述信息\tSELECT description;END$DELIMITER ;-- 调用pro_test7存储过程CALL pro_test7(390);CALL pro_test7((SELECT SUM(score) FROM student)); WHILE while 循环语法 1234WHILE 条件判断语句 DO\t循环体语句;\t条件控制语句;END WHILE; 计算 1~100 之间的偶数和 123456789101112131415161718192021DELIMITER $CREATE PROCEDURE pro_test6()BEGIN\t-- 定义求和变量\tDECLARE result INT DEFAULT 0;\t-- 定义初始化变量\tDECLARE num INT DEFAULT 1;\t-- while循环\tWHILE num &lt;= 100 DO IF num % 2 = 0 THEN SET result = result + num; END IF; SET num = num + 1;\tEND WHILE;\t-- 查询求和结果\tSELECT result;END$DELIMITER ;-- 调用pro_test6存储过程CALL pro_test6(); REPEAT repeat 循环标准语法 123456初始化语句;REPEAT\t循环体语句;\t条件控制语句;\tUNTIL 条件判断语句END REPEAT; 计算 1~10 之间的和 1234567891011121314151617181920212223DELIMITER $CREATE PROCEDURE pro_test9()BEGIN\t-- 定义求和变量\tDECLARE result INT DEFAULT 0;\t-- 定义初始化变量\tDECLARE num INT DEFAULT 1;\t-- repeat循环\tREPEAT -- 累加 SET result = result + num; -- 让num+1 SET num = num + 1; -- 停止循环 UNTIL num &gt; 10\tEND REPEAT;\t-- 查询求和结果\tSELECT result;END$DELIMITER ;-- 调用pro_test9存储过程CALL pro_test9(); LOOPLOOP 实现简单的循环，退出循环的条件需要使用其他的语句定义，通常可以使用 LEAVE 语句实现，如果不加退出循环的语句，那么就变成了死循环 loop 循环标准语法 123456[循环名称:] LOOP\t条件判断语句 [LEAVE 循环名称;]\t循环体语句;\t条件控制语句;END LOOP 循环名称; 计算 1~10 之间的和 123456789101112131415161718192021222324DELIMITER $CREATE PROCEDURE pro_test10()BEGIN\t-- 定义求和变量\tDECLARE result INT DEFAULT 0;\t-- 定义初始化变量\tDECLARE num INT DEFAULT 1;\t-- loop循环\tl:LOOP -- 条件成立，停止循环 IF num &gt; 10 THEN LEAVE l; END IF; -- 累加 SET result = result + num; -- 让num+1 SET num = num + 1;\tEND LOOP l;\t-- 查询求和结果\tSELECT result;END$DELIMITER ;-- 调用pro_test10存储过程CALL pro_test10(); 游标游标是用来存储查询结果集的数据类型，在存储过程和函数中可以使用光标对结果集进行循环的处理 游标可以遍历返回的多行结果，每次拿到一整行数据 简单来说游标就类似于集合的迭代器遍历 MySQL 中的游标只能用在存储过程和函数中 游标的语法 创建游标 1DECLARE 游标名称 CURSOR FOR 查询sql语句; 打开游标 1OPEN 游标名称; 使用游标获取数据 1FETCH 游标名称 INTO 变量名1,变量名2,...; 关闭游标 1CLOSE 游标名称; Mysql 通过一个 Error handler 声明来判断指针是否到尾部，并且必须和创建游标的 SQL 语句声明在一起： 1DECLARE EXIT HANDLER FOR NOT FOUND (do some action，一般是设置标志变量) 游标的基本使用 数据准备：表 student 12345id\tNAME\tage gender\tscore1\t张三 23 男 952\t李四 24 男 983\t王五 25 女 1004\t赵六 26 女 90 创建 stu_score 表 1234CREATE TABLE stu_score(\tid INT PRIMARY KEY AUTO_INCREMENT,\tscore INT); 将student表中所有的成绩保存到stu_score表中 12345678910111213141516171819202122232425262728293031323334DELIMITER $CREATE PROCEDURE pro_test12()BEGIN\t-- 定义成绩变量\tDECLARE s_score INT;\t-- 定义标记变量\tDECLARE flag INT DEFAULT 0; -- 创建游标，查询所有学生成绩数据\tDECLARE stu_result CURSOR FOR SELECT score FROM student;\t-- 游标结束后，将标记变量改为1 这两个必须声明在一起\tDECLARE EXIT HANDLER FOR NOT FOUND SET flag = 1; -- 开启游标\tOPEN stu_result;\t-- 循环使用游标\tREPEAT -- 使用游标，遍历结果,拿到数据 FETCH stu_result INTO s_score; -- 将数据保存到stu_score表中 INSERT INTO stu_score VALUES (NULL,s_score);\tUNTIL flag=1\tEND REPEAT;\t-- 关闭游标\tCLOSE stu_result;END$DELIMITER ;-- 调用pro_test12存储过程CALL pro_test12();-- 查询stu_score表SELECT * FROM stu_score; 存储函数存储函数和存储过程是非常相似的，存储函数可以做的事情，存储过程也可以做到 存储函数有返回值，存储过程没有返回值（参数的 out 其实也相当于是返回数据了） 创建存储函数 12345678910DELIMITER $-- 标准语法CREATE FUNCTION 函数名称(参数 数据类型)RETURNS 返回值类型BEGIN\t执行的sql语句;\tRETURN 结果;END$DELIMITER ; 调用存储函数，因为有返回值，所以使用 SELECT 调用 1SELECT 函数名称(实际参数); 删除存储函数 1DROP FUNCTION 函数名称; 定义存储函数，获取学生表中成绩大于95分的学生数量 1234567891011121314DELIMITER $CREATE FUNCTION fun_test()RETURN INTBEGIN\t-- 定义统计变量\tDECLARE result INT;\t-- 查询成绩大于95分的学生数量，给统计变量赋值\tSELECT COUNT(score) INTO result FROM student WHERE score &gt; 95;\t-- 返回统计结果\tSELECT result;ENDDELIMITER ;-- 调用fun_test存储函数SELECT fun_test(); 触发器基本介绍触发器是与表有关的数据库对象，在 insert&#x2F;update&#x2F;delete 之前或之后触发并执行触发器中定义的 SQL 语句 触发器的这种特性可以协助应用在数据库端确保数据的完整性 、日志记录 、数据校验等操作 使用别名 NEW 和 OLD 来引用触发器中发生变化的记录内容，这与其他的数据库是相似的 现在触发器还只支持行级触发，不支持语句级触发 触发器类型 OLD的含义 NEW的含义 INSERT 型触发器 无 (因为插入前状态无数据) NEW 表示将要或者已经新增的数据 UPDATE 型触发器 OLD 表示修改之前的数据 NEW 表示将要或已经修改后的数据 DELETE 型触发器 OLD 表示将要或者已经删除的数据 无 (因为删除后状态无数据) 基本操作 创建触发器 1234567891011DELIMITER $CREATE TRIGGER 触发器名称BEFORE|AFTER INSERT|UPDATE|DELETEON 表名[FOR EACH ROW] -- 行级触发器BEGIN\t触发器要执行的功能;END$DELIMITER ; 查看触发器的状态、语法等信息 1SHOW TRIGGERS; 删除触发器，如果没有指定 schema_name，默认为当前数据库 1DROP TRIGGER [schema_name.]trigger_name; 触发演示通过触发器记录账户表的数据变更日志。包含：增加、修改、删除 数据准备 1234-- 创建db9数据库CREATE DATABASE db9;-- 使用db9数据库USE db9; 12345678-- 创建账户表accountCREATE TABLE account(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 账户id\tNAME VARCHAR(20), -- 姓名\tmoney DOUBLE -- 余额);-- 添加数据INSERT INTO account VALUES (NULL,&#x27;张三&#x27;,1000),(NULL,&#x27;李四&#x27;,2000); 12345678-- 创建日志表account_logCREATE TABLE account_log(\tid INT PRIMARY KEY AUTO_INCREMENT,\t-- 日志id\toperation VARCHAR(20), -- 操作类型 (insert update delete)\toperation_time DATETIME, -- 操作时间\toperation_id INT, -- 操作表的id\toperation_params VARCHAR(200) -- 操作参数); 创建 INSERT 型触发器 1234567891011DELIMITER $CREATE TRIGGER account_insertAFTER INSERTON accountFOR EACH ROWBEGIN\tINSERT INTO account_log VALUES (NULL,&#x27;INSERT&#x27;,NOW(),new.id,CONCAT(&#x27;插入后&#123;id=&#x27;,new.id,&#x27;,name=&#x27;,new.name,&#x27;,money=&#x27;,new.money,&#x27;&#125;&#x27;));END$DELIMITER ; 123456789-- 向account表添加记录INSERT INTO account VALUES (NULL,&#x27;王五&#x27;,3000);-- 查询日志表SELECT * FROM account_log;/*id\toperation\toperation_time operation_id\toperation_params1\tINSERT 2021-01-26 19:51:11 3 插入后&#123;id=3,name=王五money=2000&#125;*/ 创建 UPDATE 型触发器 1234567891011DELIMITER $CREATE TRIGGER account_updateAFTER UPDATEON accountFOR EACH ROWBEGIN\tINSERT INTO account_log VALUES (NULL,&#x27;UPDATE&#x27;,NOW(),new.id,CONCAT(&#x27;修改前&#123;id=&#x27;,old.id,&#x27;,name=&#x27;,old.name,&#x27;,money=&#x27;,old.money,&#x27;&#125;&#x27;,&#x27;修改后&#123;id=&#x27;,new.id,&#x27;,name=&#x27;,new.name,&#x27;,money=&#x27;,new.money,&#x27;&#125;&#x27;));END$DELIMITER ; 12345678910-- 修改account表UPDATE account SET money=3500 WHERE id=3;-- 查询日志表SELECT * FROM account_log;/*id\toperation\toperation_time operation_id operation_params2\tUPDATE 2021-01-26 19:58:54 2 更新前&#123;id=2,name=李四money=1000&#125; 更新后&#123;id=2,name=李四money=200&#125;*/ 创建 DELETE 型触发器 1234567891011DELIMITER $CREATE TRIGGER account_deleteAFTER DELETEON accountFOR EACH ROWBEGIN\tINSERT INTO account_log VALUES (NULL,&#x27;DELETE&#x27;,NOW(),old.id,CONCAT(&#x27;删除前&#123;id=&#x27;,old.id,&#x27;,name=&#x27;,old.name,&#x27;,money=&#x27;,old.money,&#x27;&#125;&#x27;));END$DELIMITER ; 123456789-- 删除account表数据DELETE FROM account WHERE id=3;-- 查询日志表SELECT * FROM account_log;/*id\toperation\toperation_time operation_id\toperation_params3\tDELETE 2021-01-26 20:02:48 3 删除前&#123;id=3,name=王五money=2000&#125;*/ 存储引擎基本介绍对比其他数据库，MySQL 的架构可以在不同场景应用并发挥良好作用，主要体现在存储引擎，插件式的存储引擎架构将查询处理和其他的系统任务以及数据的存储提取分离，可以针对不同的存储需求可以选择最优的存储引擎 存储引擎的介绍： MySQL 数据库使用不同的机制存取表文件 , 机制的差别在于不同的存储方式、索引技巧、锁定水平等不同的功能和能力，在 MySQL 中，将这些不同的技术及配套的功能称为存储引擎 Oracle、SqlServer 等数据库只有一种存储引擎，MySQL 提供了插件式的存储引擎架构，所以 MySQL 存在多种存储引擎 , 就会让数据库采取了不同的处理数据的方式和扩展功能 在关系型数据库中数据的存储是以表的形式存进行，所以存储引擎也称为表类型（存储和操作此表的类型） 通过选择不同的引擎，能够获取最佳的方案, 也能够获得额外的速度或者功能，提高程序的整体效果。 MySQL 支持的存储引擎： MySQL 支持的引擎包括：InnoDB、MyISAM、MEMORY、Archive、Federate、CSV、BLACKHOLE 等 MySQL5.5 之前的默认存储引擎是 MyISAM，5.5 之后就改为了 InnoDB 引擎对比MyISAM 存储引擎： 特点：不支持事务和外键，读取速度快，节约资源 应用场景：适用于读多写少的场景，对事务的完整性要求不高，比如一些数仓、离线数据、支付宝的年度总结之类的场景，业务进行只读操作，查询起来会更快 存储方式： 每个 MyISAM 在磁盘上存储成 3 个文件，其文件名都和表名相同，拓展名不同 表的定义保存在 .frm 文件，表数据保存在 .MYD (MYData) 文件中，索引保存在 .MYI (MYIndex) 文件中 InnoDB 存储引擎：(MySQL5.5 版本后默认的存储引擎) 特点：支持事务和外键操作，支持并发控制。对比 MyISAM 的存储引擎，InnoDB 写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引 应用场景：对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，读写频繁的操作 存储方式： 使用共享表空间存储， 这种方式创建的表的表结构保存在 .frm 文件中， 数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path 定义的表空间中，可以是多个文件 使用多表空间存储，创建的表的表结构存在 .frm 文件中，每个表的数据和索引单独保存在 .ibd 中 MEMORY 存储引擎： 特点：每个 MEMORY 表实际对应一个磁盘文件 ，该文件中只存储表的结构，表数据保存在内存中，且默认使用 HASH 索引，所以数据默认就是无序的，但是在需要快速定位记录可以提供更快的访问，服务一旦关闭，表中的数据就会丢失，存储不安全 应用场景：缓存型存储引擎，通常用于更新不太频繁的小表，用以快速得到访问结果 存储方式：表结构保存在 .frm 中 MERGE 存储引擎： 特点： 是一组 MyISAM 表的组合，这些 MyISAM 表必须结构完全相同，通过将不同的表分布在多个磁盘上 MERGE 表本身并没有存储数据，对 MERGE 类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部的 MyISAM 表进行的 应用场景：将一系列等同的 MyISAM 表以逻辑方式组合在一起，并作为一个对象引用他们，适合做数据仓库 操作方式： 插入操作是通过 INSERT_METHOD 子句定义插入的表，使用 FIRST 或 LAST 值使得插入操作被相应地作用在第一或者最后一个表上；不定义这个子句或者定义为 NO，表示不能对 MERGE 表执行插入操作 对 MERGE 表进行 DROP 操作，但是这个操作只是删除 MERGE 表的定义，对内部的表是没有任何影响的 123456789CREATE TABLE order_1()ENGINE = MyISAM DEFAULT CHARSET=utf8;CREATE TABLE order_2()ENGINE = MyISAM DEFAULT CHARSET=utf8;CREATE TABLE order_all(\t-- 结构与MyISAM表相同)ENGINE = MERGE UNION = (order_1,order_2) INSERT_METHOD=LAST DEFAULT CHARSET=utf8; 特性 MyISAM InnoDB MEMORY 存储限制 有（平台对文件系统大小的限制） 64TB 有（平台的内存限制） 事务安全 不支持 支持 不支持 锁机制 表锁 表锁&#x2F;行锁 表锁 B+Tree 索引 支持 支持 支持 哈希索引 不支持 不支持 支持 全文索引 支持 支持 不支持 集群索引 不支持 支持 不支持 数据索引 不支持 支持 支持 数据缓存 不支持 支持 N&#x2F;A 索引缓存 支持 支持 N&#x2F;A 数据可压缩 支持 不支持 不支持 空间使用 低 高 N&#x2F;A 内存使用 低 高 中等 批量插入速度 高 低 高 外键 不支持 支持 不支持 只读场景 MyISAM 比 InnoDB 更快： 底层存储结构有差别，MyISAM 是非聚簇索引，叶子节点保存的是数据的具体地址，不用回表查询 InnoDB 每次查询需要维护 MVCC 版本状态，保证并发状态下的读写冲突问题 引擎操作 查询数据库支持的存储引擎 12SHOW ENGINES;SHOW VARIABLES LIKE &#x27;%storage_engine%&#x27;; -- 查看Mysql数据库默认的存储引擎 查询某个数据库中所有数据表的存储引擎 1SHOW TABLE STATUS FROM 数据库名称; 查询某个数据库中某个数据表的存储引擎 1SHOW TABLE STATUS FROM 数据库名称 WHERE NAME = &#x27;数据表名称&#x27;; 创建数据表，指定存储引擎 1234CREATE TABLE 表名(\t列名,数据类型, ...)ENGINE = 引擎名称; 修改数据表的存储引擎 1ALTER TABLE 表名 ENGINE = 引擎名称; 索引机制索引介绍基本介绍MySQL 官方对索引的定义为：索引（index）是帮助 MySQL 高效获取数据的一种数据结构，本质是排好序的快速查找数据结构。在表数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式指向数据， 这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引 索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样 索引使用：一张数据表，用于保存数据；一个索引配置文件，用于保存索引；每个索引都指向了某一个数据 左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快 Col2 的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据的物理地址的指针，这样就可以运用二叉查找快速获取到相应数据 索引的优点： 类似于书籍的目录索引，提高数据检索的效率，降低数据库的 IO 成本 通过索引列对数据进行排序，降低数据排序的成本，降低 CPU 的消耗 索引的缺点： 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上 虽然索引大大提高了查询效率，同时却也降低更新表的速度。对表进行 INSERT、UPDATE、DELETE 操作，MySQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，还会调整因为更新所带来的键值变化后的索引信息，但是更新数据也需要先从数据库中获取，索引加快了获取速度，所以可以相互抵消一下。 索引会影响到 WHERE 的查询条件和排序 ORDER BY 两大功能 索引分类索引一般的分类如下： 功能分类 主键索引：一种特殊的唯一索引，不允许有空值，一般在建表时同时创建主键索引 单列索引：一个索引只包含单个列，一个表可以有多个单列索引（普通索引） 联合索引：顾名思义，就是将单列索引进行组合 唯一索引：索引列的值必须唯一，允许有空值，如果是联合索引，则列值组合必须唯一 NULL 值可以出现多次，因为两个 NULL 比较的结果既不相等，也不不等，结果仍然是未知 可以声明不允许存储 NULL 值的非空唯一索引 外键索引：只有 InnoDB 引擎支持外键索引，用来保证数据的一致性、完整性和实现级联操作 结构分类 BTree 索引：MySQL 使用最频繁的一个索引数据结构，是 InnoDB 和 MyISAM 存储引擎默认的索引类型，底层基于 B+Tree Hash 索引：MySQL中 Memory 存储引擎默认支持的索引类型 R-tree 索引（空间索引）：空间索引是 MyISAM 引擎的一个特殊索引类型，主要用于地理空间数据类型 Full-text 索引（全文索引）：快速匹配全部文档的方式。MyISAM 支持， InnoDB 不支持 FULLTEXT 类型的索引，但是 InnoDB 可以使用 sphinx 插件支持全文索引，MEMORY 引擎不支持 索引 InnoDB MyISAM Memory BTREE 支持 支持 支持 HASH 不支持 不支持 支持 R-tree 不支持 支持 不支持 Full-text 5.6 版本之后支持 支持 不支持 联合索引图示：根据身高年龄建立的组合索引（height、age） 索引操作索引在创建表的时候可以同时创建， 也可以随时增加新的索引 创建索引：如果一个表中有一列是主键，那么会默认为其创建主键索引（主键列不需要单独创建索引） 12CREATE [UNIQUE|FULLTEXT] INDEX 索引名称 [USING 索引类型] ON 表名(列名...);-- 索引类型默认是 B+TREE 查看索引 1SHOW INDEX FROM 表名; 添加索引 1234567891011121314151617-- 单列索引ALTER TABLE 表名 ADD INDEX 索引名称(列名);-- 组合索引ALTER TABLE 表名 ADD INDEX 索引名称(列名1,列名2,...);-- 主键索引ALTER TABLE 表名 ADD PRIMARY KEY(主键列名); -- 外键索引(添加外键约束，就是外键索引)ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主键列名);-- 唯一索引ALTER TABLE 表名 ADD UNIQUE 索引名称(列名);-- 全文索引(mysql只支持文本类型)ALTER TABLE 表名 ADD FULLTEXT 索引名称(列名); 删除索引 1DROP INDEX 索引名称 ON 表名; 案例练习 数据准备：student 12345id\tNAME age\tscore1\t张三 23 992\t李四 24 953\t王五 25 984\t赵六 26 97 索引操作： 12345-- 为student表中姓名列创建一个普通索引CREATE INDEX idx_name ON student(NAME);-- 为student表中年龄列创建一个唯一索引CREATE UNIQUE INDEX idx_age ON student(age); 聚簇索引索引对比聚簇索引是一种数据存储方式，并不是一种单独的索引类型 聚簇索引的叶子节点存放的是主键值和数据行，支持覆盖索引 非聚簇索引的叶子节点存放的是主键值或指向数据行的指针（由存储引擎决定） 在 Innodb 下主键索引是聚簇索引，在 MyISAM 下主键索引是非聚簇索引 Innodb聚簇索引在 Innodb 存储引擎，B+ 树索引可以分为聚簇索引（也称聚集索引、clustered index）和辅助索引（也称非聚簇索引或二级索引、secondary index、non-clustered index） InnoDB 中，聚簇索引是按照每张表的主键构造一颗 B+ 树，叶子节点中存放的就是整张表的数据，将聚簇索引的叶子节点称为数据页 这个特性决定了数据也是索引的一部分，所以一张表只能有一个聚簇索引 辅助索引的存在不影响聚簇索引中数据的组织，所以一张表可以有多个辅助索引 聚簇索引的优点： 数据访问更快，聚簇索引将索引和数据保存在同一个 B+ 树中，因此从聚簇索引中获取数据比非聚簇索引更快 聚簇索引对于主键的排序查找和范围查找速度非常快 聚簇索引的缺点： 插入速度严重依赖于插入顺序，按照主键的顺序（递增）插入是最快的方式，否则将会出现页分裂，严重影响性能，所以对于 InnoDB 表，一般都会定义一个自增的 ID 列为主键 更新主键的代价很高，将会导致被更新的行移动，所以对于 InnoDB 表，一般定义主键为不可更新 二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据 辅助索引在聚簇索引之上创建的索引称之为辅助索引，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引等 辅助索引叶子节点存储的是主键值，而不是数据的物理地址，所以访问数据需要二次查找，推荐使用覆盖索引，可以减少回表查询 检索过程：辅助索引找到主键值，再通过聚簇索引（二分）找到数据页，最后通过数据页中的 Page Directory（二分）找到对应的数据分组，遍历组内所所有的数据找到数据行 补充：无索引走全表查询，查到数据页后和上述步骤一致 索引实现InnoDB 使用 B+Tree 作为索引结构，并且 InnoDB 一定有索引 主键索引： 在 InnoDB 中，表数据文件本身就是按 B+Tree 组织的一个索引结构，这个索引的 key 是数据表的主键，叶子节点 data 域保存了完整的数据记录 InnoDB 的表数据文件通过主键聚集数据，如果没有定义主键，会选择非空唯一索引代替，如果也没有这样的列，MySQL 会自动为 InnoDB 表生成一个隐含字段 row_id 作为主键，这个字段长度为 6 个字节，类型为长整形 辅助索引： InnoDB 的所有辅助索引（二级索引）都引用主键作为 data 域 InnoDB 表是基于聚簇索引建立的，因此 InnoDB 的索引能提供一种非常快速的主键查找性能。不过辅助索引也会包含主键列，所以不建议使用过长的字段作为主键，过长的主索引会令辅助索引变得过大 MyISAM非聚簇MyISAM 的主键索引使用的是非聚簇索引，索引文件和数据文件是分离的，索引文件仅保存数据的地址 主键索引 B+ 树的节点存储了主键，辅助键索引 B+ 树存储了辅助键，表数据存储在独立的地方，这两颗 B+ 树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别 由于索引树是独立的，通过辅助索引检索无需回表查询访问主键的索引树 索引实现MyISAM 的索引方式也叫做非聚集的，之所以这么称呼是为了与 InnoDB 的聚集索引区分 主键索引：MyISAM 引擎使用 B+Tree 作为索引结构，叶节点的 data 域存放的是数据记录的地址 辅助索引：MyISAM 中主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求 key 是唯一的，而辅助索引的 key 可以重复 参考文章：https://blog.csdn.net/lm1060891265/article/details/81482136 索引结构数据页文件系统的最小单元是块（block），一个块的大小是 4K，系统从磁盘读取数据到内存时是以磁盘块为基本单位的，位于同一个磁盘块中的数据会被一次性读取出来，而不是需要什么取什么 InnoDB 存储引擎中有页（Page）的概念，页是 MySQL 磁盘管理的最小单位 InnoDB 存储引擎中默认每个页的大小为 16KB，索引中一个节点就是一个数据页，所以会一次性读取 16KB 的数据到内存 InnoDB 引擎将若干个地址连接磁盘块，以此来达到页的大小 16KB 在查询数据时如果一个页中的每条数据都能有助于定位数据记录的位置，这将会减少磁盘 I&#x2F;O 次数，提高查询效率 超过 16KB 的一条记录，主键索引页只会存储部分数据和指向溢出页的指针，剩余数据都会分散存储在溢出页中 数据页物理结构，从上到下： File Header：上一页和下一页的指针、该页的类型（索引页、数据页、日志页等）、校验和、LSN（最近一次修改当前页面时的系统 lsn 值，事务持久性部分详解）等信息 Page Header：记录状态信息 Infimum + Supremum：当前页的最小记录和最大记录（头尾指针），Infimum 所在分组只有一条记录，Supremum 所在分组可以有 1 ~ 8 条记录，剩余的分组可以有 4 ~ 8 条记录 User Records：存储数据的记录 Free Space：尚未使用的存储空间 Page Directory：分组的目录，可以通过目录快速定位（二分法）数据的分组 File Trailer：检验和字段，在刷脏过程中，页首和页尾的校验和一致才能说明页面刷新成功，二者不同说明刷新期间发生了错误；LSN 字段，也是用来校验页面的完整性 数据页中包含数据行，数据的存储是基于数据行的，数据行有 next_record 属性指向下一个行数据，所以是可以遍历的，但是一组数据至多 8 个行，通过 Page Directory 先定位到组，然后遍历获取所需的数据行即可 数据行中有三个隐藏字段：trx_id、roll_pointer、row_id（在事务章节会详细介绍它们的作用） BTreeBTree 的索引类型是基于 B+Tree 树型数据结构的，B+Tree 又是 BTree 数据结构的变种，用在数据库和操作系统中的文件系统，特点是能够保持数据稳定有序 BTree 又叫多路平衡搜索树，一颗 m 叉的 BTree 特性如下： 树中每个节点最多包含 m 个孩子 除根节点与叶子节点外，每个节点至少有 [ceil(m&#x2F;2)] 个孩子 若根节点不是叶子节点，则至少有两个孩子 所有的叶子节点都在同一层 每个非叶子节点由 n 个 key 与 n+1 个指针组成，其中 [ceil(m&#x2F;2)-1] &lt;&#x3D; n &lt;&#x3D; m-1 5 叉，key 的数量 [ceil(m&#x2F;2)-1] &lt;&#x3D; n &lt;&#x3D; m-1 为 2 &lt;&#x3D; n &lt;&#x3D;4 ，当 n&gt;4 时中间节点分裂到父节点，两边节点分裂 插入 C N G A H E K Q M F W L T Z D P R X Y S 数据的工作流程： 插入前 4 个字母 C N G A 插入 H，n&gt;4，中间元素 G 字母向上分裂到新的节点 插入 E、K、Q 不需要分裂 插入 M，中间元素 M 字母向上分裂到父节点 G 插入 F，W，L，T 不需要分裂 插入 Z，中间元素 T 向上分裂到父节点中 插入 D，中间元素 D 向上分裂到父节点中，然后插入 P，R，X，Y 不需要分裂 最后插入 S，NPQR 节点 n&gt;5，中间节点 Q 向上分裂，但分裂后父节点 DGMT 的 n&gt;5，中间节点 M 向上分裂 BTree 树就已经构建完成了，BTree 树和二叉树相比， 查询数据的效率更高， 因为对于相同的数据量来说，BTree 的层级结构比二叉树少，所以搜索速度快 BTree 结构的数据可以让系统高效的找到数据所在的磁盘块，定义一条记录为一个二元组 [key, data] ，key 为记录的键值，对应表中的主键值，data 为一行记录中除主键外的数据。对于不同的记录，key 值互不相同，BTree 中的每个节点根据实际情况可以包含大量的关键字信息和分支 缺点：当进行范围查找时会出现回旋查找 B+Tree数据结构BTree 数据结构中每个节点中不仅包含数据的 key 值，还有 data 值。磁盘中每一页的存储空间是有限的，如果 data 数据较大时将会导致每个节点（即一个页）能存储的 key 的数量很小，当存储的数据量很大时同样会导致 B-Tree 的深度较大，增大查询时的磁盘 I&#x2F;O 次数，进而影响查询效率，所以引入 B+Tree B+Tree 为 BTree 的变种，B+Tree 与 BTree 的区别为： n 叉 B+Tree 最多含有 n 个 key（哈希值），而 BTree 最多含有 n-1 个 key 所有非叶子节点只存储键值 key 信息，只进行数据索引，使每个非叶子节点所能保存的关键字大大增加 所有数据都存储在叶子节点，所以每次数据查询的次数都一样 叶子节点按照 key 大小顺序排列，左边结尾数据都会保存右边节点开始数据的指针，形成一个链表 所有节点中的 key 在叶子节点中也存在（比如 5)，key 允许重复，B 树不同节点不存在重复的 key B* 树：是 B+ 树的变体，在 B+ 树的非根和非叶子结点再增加指向兄弟的指针 优化结构MySQL 索引数据结构对经典的 B+Tree 进行了优化，在原 B+Tree 的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的 B+Tree，提高区间访问的性能，防止回旋查找 区间访问的意思是访问索引为 5 - 15 的数据，可以直接根据相邻节点的指针遍历 B+ 树的叶子节点是数据页（page），一个页里面可以存多个数据行 通常在 B+Tree 上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构。可以对 B+Tree 进行两种查找运算： 有范围：对于主键的范围查找和分页查找 有顺序：从根节点开始，进行随机查找，顺序查找 InnoDB 中每个数据页的大小默认是 16KB， 索引行：一般表的主键类型为 INT（4 字节）或 BIGINT（8 字节），指针大小在 InnoDB 中设置为 6 字节节，也就是说一个页大概存储 16KB&#x2F;(8B+6B)&#x3D;1K 个键值（估值）。则一个深度为 3 的 B+Tree 索引可以维护 10^3 * 10^3 * 10^3 = 10亿 条记录 数据行：一行数据的大小可能是 1k，一个数据页可以存储 16 行 实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree 的高度一般都在 2-4 层。MySQL 的 InnoDB 存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要 1~3 次磁盘 I&#x2F;O 操作 B+Tree 优点：提高查询速度，减少磁盘的 IO 次数，树形结构较小 索引维护B+ 树为了保持索引的有序性，在插入新值的时候需要做相应的维护 每个索引中每个块存储在磁盘页中，可能会出现以下两种情况： 如果所在的数据页已经满了，这时候需要申请一个新的数据页，然后挪动部分数据过去，这个过程称为页分裂，原本放在一个页的数据现在分到两个页中，降低了空间利用率 当相邻两个页由于删除了数据，利用率很低之后，会将数据页做页合并，合并的过程可以认为是分裂过程的逆过程 这两个情况都是由 B+ 树的结构决定的 一般选用数据小的字段做索引，字段长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小 自增主键的插入数据模式，可以让主键索引尽量地保持递增顺序插入，不涉及到挪动其他记录，避免了页分裂，页分裂的目的就是保证后一个数据页中的所有行主键值比前一个数据页中主键值大 参考文章：https://developer.aliyun.com/article/919861 设计原则索引的设计可以遵循一些已有的原则，创建索引的时候请尽量考虑符合这些原则，便于提升索引的使用效率 创建索引时的原则： 对查询频次较高，且数据量比较大的表建立索引 使用唯一索引，区分度越高，使用索引的效率越高 索引字段的选择，最佳候选列应当从 where 子句的条件中提取，使用覆盖索引 使用短索引，索引创建之后也是使用硬盘来存储的，因此提升索引访问的 I&#x2F;O 效率，也可以提升总体的访问效率。假如构成索引的字段总长度比较短，那么在给定大小的存储块内可以存储更多的索引值，相应的可以有效的提升 MySQL 访问索引的 I&#x2F;O 效率 索引可以有效的提升查询数据的效率，但索引数量不是多多益善，索引越多，维护索引的代价越高。对于插入、更新、删除等 DML 操作比较频繁的表来说，索引过多，会引入相当高的维护代价，降低 DML 操作的效率，增加相应操作的时间消耗；另外索引过多的话，MySQL 也会犯选择困难病，虽然最终仍然会找到一个可用的索引，但提高了选择的代价 MySQL 建立联合索引时会遵守最左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配 N 个列组合而成的组合索引，相当于创建了 N 个索引，如果查询时 where 句中使用了组成该索引的前几个字段，那么这条查询 SQL 可以利用组合索引来提升查询效率 12345678910-- 对name、address、phone列建一个联合索引ALTER TABLE user ADD INDEX index_three(name,address,phone);-- 查询语句执行时会依照最左前缀匹配原则，检索时分别会使用索引进行数据匹配。(name,address,phone)(name,address)(name,phone)\t-- 只有name字段走了索引(name)-- 索引的字段可以是任意顺序的，优化器会帮助我们调整顺序，下面的SQL语句可以命中索引SELECT * FROM user WHERE address = &#x27;北京&#x27; AND phone = &#x27;12345&#x27; AND name = &#x27;张三&#x27;; 12-- 如果联合索引中最左边的列不包含在条件查询中，SQL语句就不会命中索引，比如：SELECT * FROM user WHERE address = &#x27;北京&#x27; AND phone = &#x27;12345&#x27;; 哪些情况不要建立索引： 记录太少的表 经常增删改的表 频繁更新的字段不适合创建索引 where 条件里用不到的字段不创建索引 索引优化覆盖索引覆盖索引：包含所有满足查询需要的数据的索引（SELECT 后面的字段刚好是索引字段），可以利用该索引返回 SELECT 列表的字段，而不必根据索引去聚簇索引上读取数据文件 回表查询：要查找的字段不在非主键索引树上时，需要通过叶子节点的主键值去主键索引上获取对应的行数据 使用覆盖索引，防止回表查询： 表 user 主键为 id，普通索引为 age，查询语句： 1SELECT * FROM user WHERE age = 30; 查询过程：先通过普通索引 age&#x3D;30 定位到主键值 id&#x3D;1，再通过聚集索引 id&#x3D;1 定位到行记录数据，需要两次扫描 B+ 树 使用覆盖索引： 123DROP INDEX idx_age ON user;CREATE INDEX idx_age_name ON user(age,name);SELECT id,age FROM user WHERE age = 30; 在一棵索引树上就能获取查询所需的数据，无需回表速度更快 使用覆盖索引，要注意 SELECT 列表中只取出需要的列，不可用 SELECT *，所有字段一起做索引会导致索引文件过大，查询性能下降 索引下推索引条件下推优化（Index Condition Pushdown，ICP）是 MySQL5.6 添加，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数 索引下推充分利用了索引中的数据，在查询出整行数据之前过滤掉无效的数据，再去主键索引树上查找 不使用索引下推优化时存储引擎通过索引检索到数据，然后回表查询记录返回给 Server 层，服务器判断数据是否符合条件 使用索引下推优化时，如果存在某些被索引的列的判断条件时，由存储引擎在索引遍历的过程中判断数据是否符合传递的条件，将符合条件的数据进行回表，检索出来返回给服务器，由此减少 IO 次数 适用条件： 需要存储引擎将索引中的数据与条件进行判断（所以条件列必须都在同一个索引中），所以优化是基于存储引擎的，只有特定引擎可以使用，适用于 InnoDB 和 MyISAM 存储引擎没有调用跨存储引擎的能力，跨存储引擎的功能有存储过程、触发器、视图，所以调用这些功能的不可以进行索引下推优化 对于 InnoDB 引擎只适用于二级索引，InnoDB 的聚簇索引会将整行数据读到缓冲区，不再需要去回表查询了 工作过程：用户表 user，(name, age) 是联合索引 1SELECT * FROM user WHERE name LIKE &#x27;张%&#x27; AND　age = 10;\t-- 头部模糊匹配会造成索引失效 优化前：在非主键索引树上找到满足第一个条件的行，然后通过叶子节点记录的主键值再回到主键索引树上查找到对应的行数据，再对比 AND 后的条件是否符合，符合返回数据，需要 4 次回表 优化后：检查索引中存储的列信息是否符合索引条件，然后交由存储引擎用剩余的判断条件判断此行数据是否符合要求，不满足条件的不去读取表中的数据，满足下推条件的就根据主键值进行回表查询，2 次回表 当使用 EXPLAIN 进行分析时，如果使用了索引条件下推，Extra 会显示 Using index condition 参考文章：https://blog.csdn.net/sinat_29774479/article/details/103470244 参考文章：https://time.geekbang.org/column/article/69636 前缀索引当要索引的列字符很多时，索引会变大变慢，可以只索引列开始的部分字符串，节约索引空间，提高索引效率 注意：使用前缀索引就系统就忽略覆盖索引对查询性能的优化了 优化原则：降低重复的索引值 比如地区表： 123456area gdp codechinaShanghai\t100 aaachinaDalian 200 bbbusaNewYork 300 cccchinaFuxin 400 dddchinaBeijing\t500 eee 发现 area 字段很多都是以 china 开头的，那么如果以前 1-5 位字符做前缀索引就会出现大量索引值重复的情况，索引值重复性越低，查询效率也就越高，所以需要建立前 6 位字符的索引： 1CREATE INDEX idx_area ON table_name(area(7)); 场景：存储身份证 直接创建完整索引，这样可能比较占用空间 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题（前 6 位相同的很多） 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描 索引合并使用多个索引来完成一次查询的执行方法叫做索引合并 index merge Intersection 索引合并： 1SELECT * FROM table_test WHERE key1 = &#x27;a&#x27; AND key3 = &#x27;b&#x27;; # key1 和 key3 列都是单列索引、二级索引 从不同索引中扫描到的记录的 id 值取交集（相同 id），然后执行回表操作，要求从每个二级索引获取到的记录都是按照主键值排序 Union 索引合并： 1SELECT * FROM table_test WHERE key1 = &#x27;a&#x27; OR key3 = &#x27;b&#x27;; 从不同索引中扫描到的记录的 id 值取并集，然后执行回表操作，要求从每个二级索引获取到的记录都是按照主键值排序 Sort-Union 索引合并 1SELECT * FROM table_test WHERE key1 &lt; &#x27;a&#x27; OR key3 &gt; &#x27;b&#x27;; 先将从不同索引中扫描到的记录的主键值进行排序，再按照 Union 索引合并的方式进行查询 索引合并算法的效率并不好，通过将其中的一个索引改成联合索引会优化效率 系统优化表优化分区表基本介绍分区表是将大表的数据按分区字段分成许多小的子集，建立一个以 ftime 年份为分区的表： 1234567891011CREATE TABLE `t` ( `ftime` datetime NOT NULL, `c` int(11) DEFAULT NULL, KEY (`ftime`)) ENGINE=InnoDB DEFAULT CHARSET=latin1PARTITION BY RANGE (YEAR(ftime))(PARTITION p_2017 VALUES LESS THAN (2017) ENGINE = InnoDB, PARTITION p_2018 VALUES LESS THAN (2018) ENGINE = InnoDB, PARTITION p_2019 VALUES LESS THAN (2019) ENGINE = InnoDB, PARTITION p_others VALUES LESS THAN MAXVALUE ENGINE = InnoDB);INSERT INTO t VALUES(&#x27;2017-4-1&#x27;,1),(&#x27;2018-4-1&#x27;,1);-- 这两行记录分别落在 p_2018 和 p_2019 这两个分区上 这个表包含了一个.frm 文件和 4 个.ibd 文件，每个分区对应一个.ibd 文件 对于引擎层来说，这是 4 个表，针对每个分区表的操作不会相互影响 对于 Server 层来说，这是 1 个表 分区策略打开表行为：第一次访问一个分区表时，MySQL 需要把所有的分区都访问一遍，如果分区表的数量很多，超过了 open_files_limit 参数（默认值 1024），那么就会在访问这个表时打开所有的文件，导致打开表文件的个数超过了上限而报错 通用分区策略：MyISAM 分区表使用的分区策略，每次访问分区都由 Server 层控制，在文件管理、表管理的实现上很粗糙，因此有比较严重的性能问题 本地分区策略：从 MySQL 5.7.9 开始，InnoDB 引擎内部自己管理打开分区的行为，InnoDB 引擎打开文件超过 innodb_open_files 时就会关掉一些之前打开的文件，所以即使分区个数大于 open_files_limit，也不会报错 从 MySQL 8.0 版本开始，就不允许创建 MyISAM 分区表，只允许创建已经实现了本地分区策略的引擎，目前只有 InnoDB 和 NDB 这两个引擎支持了本地分区策略 Server 层从 Server 层看一个分区表就只是一个表 Session A： 1SELECT * FROM t WHERE ftime = &#x27;2018-4-1&#x27;; Session B： 1ALTER TABLE t TRUNCATE PARTITION p_2017; -- blocked 现象：Session B 只操作 p_2017 分区，但是由于 Session A 持有整个表 t 的 MDL 读锁，就导致 B 的 ALTER 语句获取 MDL 写锁阻塞 分区表的特点： 第一次访问的时候需要访问所有分区 在 Server 层认为这是同一张表，因此所有分区共用同一个 MDL 锁 在引擎层认为这是不同的表，因此 MDL 锁之后的执行过程，会根据分区表规则，只访问需要的分区 应用场景分区表的优点： 对业务透明，相对于用户分表来说，使用分区表的业务代码更简洁 分区表可以很方便的清理历史数据。按照时间分区的分区表，就可以直接通过 alter table t drop partition 这个语法直接删除分区文件，从而删掉过期的历史数据，与使用 drop 语句删除数据相比，优势是速度快、对系统影响小 使用分区表，不建议创建太多的分区，注意事项： 分区并不是越细越好，单表或者单分区的数据一千万行，只要没有特别大的索引，对于现在的硬件能力来说都已经是小表 分区不要提前预留太多，在使用之前预先创建即可。比如是按月分区，每年年底时再把下一年度的 12 个新分区创建上即可，并且对于没有数据的历史分区，要及时的 drop 掉 参考文档：https://time.geekbang.org/column/article/82560 临时表基本介绍临时表分为内部临时表和用户临时表 内部临时表：系统执行 SQL 语句优化时产生的表，例如 Join 连接查询、去重查询等 用户临时表：用户主动创建的临时表 1CREATE TEMPORARY TABLE temp_t like table_1; 临时表可以是内存表，也可以是磁盘表（多表操作 → 嵌套查询章节提及） 内存表指的是使用 Memory 引擎的表，建立哈希索引，建表语法是 create table … engine=memory，这种表的数据都保存在内存里，系统重启时会被清空，但是表结构还在 磁盘表是使用 InnoDB 引擎或者 MyISAM 引擎的临时表，建立 B+ 树索引，写数据的时候是写到磁盘上的 临时表的特点： 一个临时表只能被创建它的 session 访问，对其他线程不可见，所以不同 session 的临时表是可以重名的 临时表可以与普通表同名，会话内有同名的临时表和普通表时，执行 show create 语句以及增删改查语句访问的都是临时表 show tables 命令不显示临时表 数据库发生异常重启不需要担心数据删除问题，临时表会自动回收 重名原理执行创建临时表的 SQL： 1create temporary table temp_t(id int primary key)engine=innodb; MySQL 给 InnoDB 表创建一个 frm 文件保存表结构定义，在 ibd 保存表数据。frm 文件放在临时文件目录下，文件名的后缀是 .frm，前缀是 #sql&#123;进程 id&#125;_&#123;线程 id&#125;_ 序列号，使用 select @@tmpdir 命令，来显示实例的临时文件目录 MySQL 维护数据表，除了物理磁盘上的文件外，内存里也有一套机制区别不同的表，每个表都对应一个 table_def_key 一个普通表的 table_def_key 的值是由 库名 + 表名 得到的，所以如果在同一个库下创建两个同名的普通表，创建第二个表的过程中就会发现 table_def_key 已经存在了 对于临时表，table_def_key 在 库名 + 表名 基础上，又加入了 server_id + thread_id，所以不同线程之间，临时表可以重名 实现原理：每个线程都维护了自己的临时表链表，每次 session 内操作表时，先遍历链表，检查是否有这个名字的临时表，如果有就优先操作临时表，如果没有再操作普通表；在 session 结束时对链表里的每个临时表，执行 DROP TEMPORARY TABLE + 表名 操作 执行 rename table 语句无法修改临时表，因为会按照 库名 / 表名.frm 的规则去磁盘找文件，但是临时表文件名的规则是 #sql&#123;进程 id&#125;_&#123;线程 id&#125;_ 序列号.frm，因此会报找不到文件名的错误 主备复制创建临时表的语句会传到备库执行，因此备库的同步线程就会创建这个临时表。主库在线程退出时会自动删除临时表，但备库同步线程是持续在运行的并不会退出，所以这时就需要在主库上再写一个 DROP TEMPORARY TABLE 传给备库执行 binlog 日志写入规则： binlog_format&#x3D;row，跟临时表有关的语句就不会记录到 binlog binlog_format&#x3D;statment&#x2F;mixed，binlog 中才会记录临时表的操作，也就会记录 DROP TEMPORARY TABLE 这条命令 主库上不同的线程创建同名的临时表是不冲突的，但是备库只有一个执行线程，所以 MySQL 在记录 binlog 时会把主库执行这个语句的线程 id 写到 binlog 中，在备库的应用线程就可以获取执行每个语句的主库线程 id，并利用这个线程 id 来构造临时表的 table_def_key session A 的临时表 t1，在备库的 table_def_key 就是：库名 + t1 +“M 的 serverid&quot; + &quot;session A 的 thread_id” session B 的临时表 t1，在备库的 table_def_key 就是 ：库名 + t1 +&quot;M 的 serverid&quot; + &quot;session B 的 thread_id&quot; MySQL 在记录 binlog 的时不论是 create table 还是 alter table 语句都是原样记录，但是如果执行 drop table，系统记录 binlog 就会被服务端改写 1DROP TABLE `t_normal` /* generated by server */ 跨库查询分库分表系统的跨库查询使用临时表不用担心线程之间的重名冲突，分库分表就是要把一个逻辑上的大表分散到不同的数据库实例上 比如将一个大表 ht，按照字段 f，拆分成 1024 个分表，分布到 32 个数据库实例上，一般情况下都有一个中间层 proxy 解析 SQL 语句，通过分库规则通过分表规则（比如 N%1024）确定将这条语句路由到哪个分表做查询 1select v from ht where f=N; 如果这个表上还有另外一个索引 k，并且查询语句： 1select v from ht where k &gt;= M order by t_modified desc limit 100; 查询条件里面没有用到分区字段 f，只能到所有的分区中去查找满足条件的所有行，然后统一做 order by 操作，两种方式： 在 proxy 层的进程代码中实现排序，拿到分库的数据以后，直接在内存中参与计算，但是对 proxy 端的压力比较大，很容易出现内存不够用和 CPU 瓶颈问题 把各个分库拿到的数据，汇总到一个 MySQL 实例的一个表中，然后在这个汇总实例上做逻辑操作，执行流程： 在汇总库上创建一个临时表 temp_ht，表里包含三个字段 v、k、t_modified 在各个分库执行：select v,k,t_modified from ht_x where k &gt;= M order by t_modified desc limit 100 把分库执行的结果插入到 temp_ht 表中 在临时表上执行：select v from temp_ht order by t_modified desc limit 100 优化步骤执行频率MySQL 客户端连接成功后，查询服务器状态信息： 123SHOW [SESSION|GLOBAL] STATUS LIKE &#x27;&#x27;;-- SESSION: 显示当前会话连接的统计结果，默认参数-- GLOBAL: 显示自数据库上次启动至今的统计结果 查看 SQL 执行频率： 1SHOW STATUS LIKE &#x27;Com_____&#x27;; Com_xxx 表示每种语句执行的次数 查询 SQL 语句影响的行数： 1SHOW STATUS LIKE &#x27;Innodb_rows_%&#x27;; Com_xxxx：这些参数对于所有存储引擎的表操作都会进行累计 Innodb_xxxx：这几个参数只是针对 InnoDB 存储引擎的，累加的算法也略有不同 参数 含义 Com_select 执行 SELECT 操作的次数，一次查询只累加 1 Com_insert 执行 INSERT 操作的次数，对于批量插入的 INSERT 操作，只累加一次 Com_update 执行 UPDATE 操作的次数 Com_delete 执行 DELETE 操作的次数 Innodb_rows_read 执行 SELECT 查询返回的行数 Innodb_rows_inserted 执行 INSERT 操作插入的行数 Innodb_rows_updated 执行 UPDATE 操作更新的行数 Innodb_rows_deleted 执行 DELETE 操作删除的行数 Connections 试图连接 MySQL 服务器的次数 Uptime 服务器工作时间 Slow_queries 慢查询的次数 定位低效SQL 执行慢有两种情况： 偶尔慢：DB 在刷新脏页（学完事务就懂了） redo log 写满了 内存不够用，要从 LRU 链表中淘汰 MySQL 认为系统空闲的时候 MySQL 关闭时 一直慢的原因：索引没有设计好、SQL 语句没写好、MySQL 选错了索引 通过以下两种方式定位执行效率较低的 SQL 语句 慢日志查询： 慢查询日志在查询结束以后才记录，执行效率出现问题时查询日志并不能定位问题 配置文件修改：修改 .cnf 文件 vim /etc/mysql/my.cnf，重启 MySQL 服务器 1234slow_query_log=ONslow_query_log_file=/usr/local/mysql/var/localhost-slow.loglong_query_time=1\t#记录超过long_query_time秒的SQL语句的日志log-queries-not-using-indexes = 1 使用命令配置： 12mysql&gt; SET slow_query_log=ON;mysql&gt; SET GLOBAL slow_query_log=ON; 查看是否配置成功： 1SHOW VARIABLES LIKE &#x27;%query%&#x27; SHOW PROCESSLIST：实时查看当前 MySQL 在进行的连接线程，包括线程的状态、是否锁表、SQL 的执行情况，同时对一些锁表操作进行优化 EXPLAIN执行计划通过 EXPLAIN 命令获取执行 SQL 语句的信息，包括在 SELECT 语句执行过程中如何连接和连接的顺序，执行计划在优化器优化完成后、执行器之前生成，然后执行器会调用存储引擎检索数据 查询 SQL 语句的执行计划： 1EXPLAIN SELECT * FROM table_1 WHERE id = 1; 字段 含义 id SELECT 的序列号 select_type 表示 SELECT 的类型 table 访问数据库中表名称，有时可能是简称或者临时表名称（） type 表示表的连接类型 possible_keys 表示查询时，可能使用的索引 key 表示实际使用的索引 key_len 索引字段的长度 ref 表示与索引列进行等值匹配的对象，常数、某个列、函数等，type 必须在（range, const] 之间，左闭右开 rows 扫描出的行数，表示 MySQL 根据表统计信息及索引选用情况，估算的找到所需的记录扫描的行数 filtered 条件过滤的行百分比，单表查询没意义，用于连接查询中对驱动表的扇出进行过滤，查询优化器预测所有扇出值满足剩余查询条件的百分比，相乘以后表示多表查询中还要对被驱动执行查询的次数 extra 执行情况的说明和描述 MySQL 执行计划的局限： 只是计划，不是执行 SQL 语句，可以随着底层优化器输入的更改而更改 EXPLAIN 不会告诉显示关于触发器、存储过程的信息对查询的影响情况， 不考虑各种 Cache EXPLAIN 不能显示 MySQL 在执行查询时的动态，因为执行计划在执行查询之前生成 EXPALIN 只能解释 SELECT 操作，其他操作要重写为 SELECT 后查看执行计划 EXPLAIN PLAN 显示的是在解释语句时数据库将如何运行 SQL 语句，由于执行环境和 EXPLAIN PLAN 环境的不同，此计划可能与 SQL 语句实际的执行计划不同，部分统计信息是估算的，并非精确值 SHOW WARINGS：在使用 EXPALIN 命令后执行该语句，可以查询与执行计划相关的拓展信息，展示出 Level、Code、Message 三个字段，当 Code 为 1003 时，Message 字段展示的信息类似于将查询语句重写后的信息，但是不是等价，不能执行复制过来运行 环境准备： idid 代表 SQL 执行的顺序的标识，每个 SELECT 关键字对应一个唯一 id，所以在同一个 SELECT 关键字中的表的 id 都是相同的。SELECT 后的 FROM 可以跟随多个表，每个表都会对应一条记录，这些记录的 id 都是相同的， id 相同时，执行顺序由上至下。连接查询的执行计划，记录的 id 值都是相同的，出现在前面的表为驱动表，后面为被驱动表 1EXPLAIN SELECT * FROM t_role r, t_user u, user_role ur WHERE r.id = ur.role_id AND u.id = ur.user_id ; id 不同时，id 值越大优先级越高，越先被执行 1EXPLAIN SELECT * FROM t_role WHERE id = (SELECT role_id FROM user_role WHERE user_id = (SELECT id FROM t_user WHERE username = &#x27;stu1&#x27;)) id 有相同也有不同时，id 相同的可以认为是一组，从上往下顺序执行；在所有的组中，id 的值越大的组，优先级越高，越先执行 1EXPLAIN SELECT * FROM t_role r , (SELECT * FROM user_role ur WHERE ur.`user_id` = &#x27;2&#x27;) a WHERE r.id = a.role_id ; id 为 NULL 时代表的是临时表 select表示查询中每个 select 子句的类型（简单 OR 复杂） select_type 含义 SIMPLE 简单的 SELECT 查询，查询中不包含子查询或者 UNION PRIMARY 查询中若包含任何复杂的子查询，最外层（也就是最左侧）查询标记为该标识 UNION 对于 UNION 或者 UNION ALL 的复杂查询，除了最左侧的查询，其余的小查询都是 UNION UNION RESULT UNION 需要使用临时表进行去重，临时表的是 UNION RESULT DEPENDENT UNION 对于 UNION 或者 UNION ALL 的复杂查询，如果各个小查询都依赖外层查询，是相关子查询，除了最左侧的小查询为 DEPENDENT SUBQUERY，其余都是 DEPENDENT UNION SUBQUERY 子查询不是相关子查询，该子查询第一个 SELECT 代表的查询就是这种类型，会进行物化（该子查询只需要执行一次） DEPENDENT SUBQUERY 子查询是相关子查询，该子查询第一个 SELECT 代表的查询就是这种类型，不会物化（该子查询需要执行多次） DERIVED 在 FROM 列表中包含的子查询，被标记为 DERIVED（衍生），也就是生成物化派生表的这个子查询 MATERIALIZED 将子查询物化后与与外层进行连接查询，生成物化表的子查询 子查询为 DERIVED：SELECT * FROM (SELECT key1 FROM t1) AS derived_1 WHERE key1 &gt; 10 子查询为 MATERIALIZED：SELECT * FROM t1 WHERE key1 IN (SELECT key1 FROM t2) type对表的访问方式，表示 MySQL 在表中找到所需行的方式，又称访问类型 type 含义 ALL 全表扫描，如果是 InnoDB 引擎是扫描聚簇索引 index 可以使用覆盖索引，但需要扫描全部索引 range 索引范围扫描，常见于 between、&lt;、&gt; 等的查询 index_subquery 子查询可以普通索引，则子查询的 type 为 index_subquery unique_subquery 子查询可以使用主键或唯一二级索引，则子查询的 type 为 index_subquery index_merge 索引合并 ref_or_null 非唯一性索引（普通二级索引）并且可以存储 NULL，进行等值匹配 ref 非唯一性索引与常量等值匹配 eq_ref 唯一性索引（主键或不存储 NULL 的唯一二级索引）进行等值匹配，如果二级索引是联合索引，那么所有联合的列都要进行等值匹配 const 通过主键或者唯一二级索引与常量进行等值匹配 system system 是 const 类型的特例，当查询的表只有一条记录的情况下，使用 system NULL MySQL 在优化过程中分解语句，执行时甚至不用访问表或索引 从上到下，性能从差到好，一般来说需要保证查询至少达到 range 级别， 最好达到 ref keypossible_keys： 指出 MySQL 能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用 如果该列是 NULL，则没有相关的索引 key： 显示 MySQL 在查询中实际使用的索引，若没有使用索引，显示为 NULL 查询中若使用了覆盖索引，则该索引可能出现在 key 列表，不出现在 possible_keys key_len： 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度 key_len 显示的值为索引字段的最大可能长度，并非实际使用长度，即 key_len 是根据表定义计算而得，不是通过表内检索出的 在不损失精确性的前提下，长度越短越好 Extra其他的额外的执行计划信息，在该列展示： No tables used：查询语句中使用 FROM dual 或者没有 FROM 语句 Impossible WHERE：查询语句中的 WHERE 子句条件永远为 FALSE，会导致没有符合条件的行 Using index：该值表示相应的 SELECT 操作中使用了覆盖索引（Covering Index） Using index condition：第一种情况是搜索条件中虽然出现了索引列，但是部分条件无法形成扫描区间（索引失效），会根据可用索引的条件先搜索一遍再匹配无法使用索引的条件，回表查询数据；第二种是使用了索引条件下推优化 Using where：搜索的数据需要在 Server 层判断，无法使用索引下推 Using join buffer：连接查询被驱动表无法利用索引，需要连接缓冲区来存储中间结果 Using filesort：无法利用索引完成排序（优化方向），需要对数据使用外部排序算法，将取得的数据在内存或磁盘中进行排序 Using temporary：表示 MySQL 需要使用临时表来存储结果集，常见于排序、去重（UNION）、分组等场景 Select tables optimized away：说明仅通过使用索引，优化器可能仅从聚合函数结果中返回一行 No tables used：Query 语句中使用 from dual 或不含任何 from 子句 参考文章：https://www.cnblogs.com/ggjucheng/archive/2012/11/11/2765237.html PROFILESSHOW PROFILES 能够在做 SQL 优化时分析当前会话中语句执行的资源消耗情况 通过 have_profiling 参数，能够看到当前 MySQL 是否支持 profile： 默认 profiling 是关闭的，可以通过 set 语句在 Session 级别开启 profiling： 1SET profiling=1; #开启profiling 开关； 执行 SHOW PROFILES 指令， 来查看 SQL 语句执行的耗时: 1SHOW PROFILES; 查看到该 SQL 执行过程中每个线程的状态和消耗的时间： 1SHOW PROFILE FOR QUERY query_id; 在获取到最消耗时间的线程状态后，MySQL 支持选择 all、cpu、block io 、context switch、page faults 等类型查看 MySQL 在使用什么资源上耗费了过高的时间。例如，选择查看 CPU 的耗费时间： Status：SQL 语句执行的状态 Durationsql：执行过程中每一个步骤的耗时 CPU_user：当前用户占有的 CPU CPU_system：系统占有的 CPU TRACEMySQL 提供了对 SQL 的跟踪， 通过 trace 文件可以查看优化器生成执行计划的过程 打开 trace 功能，设置格式为 JSON，并设置 trace 的最大使用内存，避免解析过程中因默认内存过小而不能够完整展示 12SET optimizer_trace=&quot;enabled=on&quot;,end_markers_in_json=ON;\t-- 会话内有效SET optimizer_trace_max_mem_size=1000000; 执行 SQL 语句： 1SELECT * FROM tb_item WHERE id &lt; 4; 检查 information_schema.optimizer_trace： 1SELECT * FROM information_schema.optimizer_trace \\G; -- \\G代表竖列展示 执行信息主要有三个阶段：prepare 阶段、optimize 阶段（成本分析）、execute 阶段（执行） 索引优化创建索引索引是数据库优化最重要的手段之一，通过索引通常可以帮助用户解决大多数的 MySQL 的性能优化问题 123456789101112CREATE TABLE `tb_seller` (\t`sellerid` varchar (100),\t`name` varchar (100),\t`nickname` varchar (50),\t`password` varchar (60),\t`status` varchar (1),\t`address` varchar (100),\t`createtime` datetime, PRIMARY KEY(`sellerid`))ENGINE=INNODB DEFAULT CHARSET=utf8mb4;INSERT INTO `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values(&#x27;xiaomi&#x27;,&#x27;小米科技&#x27;,&#x27;小米官方旗舰店&#x27;,&#x27;e10adc3949ba59abbe56e057f20f883e&#x27;,&#x27;1&#x27;,&#x27;西安市&#x27;,&#x27;2088-01-01 12:00:00&#x27;);CREATE INDEX idx_seller_name_sta_addr ON tb_seller(name, status, address); # 联合索引 避免失效语句错误 全值匹配：对索引中所有列都指定具体值，这种情况索引生效，执行效率高 1EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;小米科技&#x27; AND status=&#x27;1&#x27; AND address=&#x27;西安市&#x27;; 最左前缀法则：联合索引遵守最左前缀法则 匹配最左前缀法则，走索引： 12EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;小米科技&#x27;;EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;小米科技&#x27; AND status=&#x27;1&#x27;; 违法最左前缀法则 ， 索引失效： 12EXPLAIN SELECT * FROM tb_seller WHERE status=&#x27;1&#x27;;EXPLAIN SELECT * FROM tb_seller WHERE status=&#x27;1&#x27; AND address=&#x27;西安市&#x27;; 如果符合最左法则，但是出现跳跃某一列，只有最左列索引生效： 1EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;小米科技&#x27; AND address=&#x27;西安市&#x27;; 虽然索引列失效，但是系统会使用了索引下推进行了优化 范围查询右边的列，不能使用索引： 1EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;小米科技&#x27; AND status&gt;&#x27;1&#x27; AND address=&#x27;西安市&#x27;; 根据前面的两个字段 name ， status 查询是走索引的， 但是最后一个条件 address 没有用到索引，使用了索引下推 在索引列上函数或者运算（+ - 数值）操作， 索引将失效：会破坏索引值的有序性 1EXPLAIN SELECT * FROM tb_seller WHERE SUBSTRING(name,3,2) = &#x27;科技&#x27;; 字符串不加单引号，造成索引失效：隐式类型转换，当字符串和数字比较时会把字符串转化为数字 没有对字符串加单引号，查询优化器会调用 CAST 函数将 status 转换为 int 进行比较，造成索引失效 1EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;小米科技&#x27; AND status = 1; 如果 status 是 int 类型，SQL 为 SELECT * FROM tb_seller WHERE status = &#39;1&#39; 并不会造成索引失效，因为会将 &#39;1&#39; 转换为 1，并不会对索引列产生操作 多表连接查询时，如果两张表的字符集不同，会造成索引失效，因为会进行类型转换 解决方法：CONVERT 函数是加在输入参数上、修改表的字符集 用 OR 分割条件，索引失效，导致全表查询： OR 前的条件中的列有索引而后面的列中没有索引或 OR 前后两个列是同一个复合索引，都造成索引失效 12EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;阿里巴巴&#x27; OR createtime = &#x27;2088-01-01 12:00:00&#x27;;EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;小米科技&#x27; OR status=&#x27;1&#x27;; AND 分割的条件不影响： 1EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;阿里巴巴&#x27; AND createtime = &#x27;2088-01-01 12:00:00&#x27;; 以 % 开头的 LIKE 模糊查询，索引失效： 如果是尾部模糊匹配，索引不会失效；如果是头部模糊匹配，索引失效 1EXPLAIN SELECT * FROM tb_seller WHERE name like &#x27;%科技%&#x27;; 解决方案：通过覆盖索引来解决 1EXPLAIN SELECT sellerid,name,status FROM tb_seller WHERE name like &#x27;%科技%&#x27;; 原因：在覆盖索引的这棵 B+ 数上只需要进行 like 的匹配，或者是基于覆盖索引查询再进行 WHERE 的判断就可以获得结果 系统优化系统优化为全表扫描： 如果 MySQL 评估使用索引比全表更慢，则不使用索引，索引失效： 123CREATE INDEX idx_address ON tb_seller(address);EXPLAIN SELECT * FROM tb_seller WHERE address=&#x27;西安市&#x27;;EXPLAIN SELECT * FROM tb_seller WHERE address=&#x27;北京市&#x27;; 北京市的键值占 9&#x2F;10（区分度低），所以优化为全表扫描，type &#x3D; ALL IS NULL、IS NOT NULL 有时索引失效： 12EXPLAIN SELECT * FROM tb_seller WHERE name IS NULL;EXPLAIN SELECT * FROM tb_seller WHERE name IS NOT NULL; NOT NULL 失效的原因是 name 列全部不是 null，优化为全表扫描，当 NULL 过多时，IS NULL 失效 IN 肯定会走索引，但是当 IN 的取值范围较大时会导致索引失效，走全表扫描： 12EXPLAIN SELECT * FROM tb_seller WHERE sellerId IN (&#x27;alibaba&#x27;,&#x27;huawei&#x27;);-- 都走索引EXPLAIN SELECT * FROM tb_seller WHERE sellerId NOT IN (&#x27;alibaba&#x27;,&#x27;huawei&#x27;); MySQL 实战 45 讲该章节最后提出了一种慢查询场景，获取到数据以后 Server 层还会做判断 底层原理索引失效一般是针对联合索引，联合索引一般由几个字段组成，排序方式是先按照第一个字段进行排序，然后排序第二个，依此类推，图示（a, b）索引，a 相等的情况下 b 是有序的 最左前缀法则：当不匹配前面的字段的时候，后面的字段都是无序的。这种无序不仅体现在叶子节点，也会导致查询时扫描的非叶子节点也是无序的，因为索引树相当于忽略的第一个字段，就无法使用二分查找 范围查询右边的列，不能使用索引，比如语句： WHERE a &gt; 1 AND b = 1 ，在 a 大于 1 的时候，b 是无序的，a &gt; 1 是扫描时有序的，但是找到以后进行寻找 b 时，索引树就不是有序的了 以 % 开头的 LIKE 模糊查询，索引失效，比如语句：WHERE a LIKE &#39;%d&#39;，前面的不确定，导致不符合最左匹配，直接去索引中搜索以 d 结尾的节点，所以没有顺序 参考文章：https://mp.weixin.qq.com/s/B_M09dzLe9w7cT46rdGIeQ 查看索引12SHOW STATUS LIKE &#x27;Handler_read%&#x27;;\tSHOW GLOBAL STATUS LIKE &#x27;Handler_read%&#x27;; Handler_read_first：索引中第一条被读的次数，如果较高，表示服务器正执行大量全索引扫描（这个值越低越好） Handler_read_key：如果索引正在工作，这个值代表一个行被索引值读的次数，值越低表示索引不经常使用（这个值越高越好） Handler_read_next：按照键顺序读下一行的请求数，如果范围约束或执行索引扫描来查询索引列，值增加 Handler_read_prev：按照键顺序读前一行的请求数，该读方法主要用于优化 ORDER BY … DESC Handler_read_rnd：根据固定位置读一行的请求数，如果执行大量查询并对结果进行排序则该值较高，可能是使用了大量需要 MySQL 扫描整个表的查询或连接，这个值较高意味着运行效率低，应该建立索引来解决 Handler_read_rnd_next：在数据文件中读下一行的请求数，如果正进行大量的表扫描，该值较高，说明表索引不正确或写入的查询没有利用索引 SQL 优化自增主键自增机制自增主键可以让主键索引尽量地保持在数据页中递增顺序插入，不自增需要寻找其他页插入，导致随机 IO 和页分裂的情况 表的结构定义存放在后缀名为.frm 的文件中，但是并不会保存自增值，不同的引擎对于自增值的保存策略不同： MyISAM 引擎的自增值保存在数据文件中 InnoDB 引擎的自增值保存在了内存里，每次打开表都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为当前的自增值；8.0 版本后，才有了自增值持久化的能力，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值 在插入一行数据的时候，自增值的行为如下： 如果插入数据时 id 字段指定为 0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT 值填到自增字段 如果插入数据时 id 字段指定了具体的值，比如某次要插入的值是 X，当前的自增值是 Y 如果 X&lt;Y，那么这个表的自增值不变 如果 X≥Y，就需要把当前自增值修改为新的自增值 参数说明：auto_increment_offset 和 auto_increment_increment 分别表示自增的初始值和步长，默认值都是 1 语句执行失败也不回退自增 id，所以保证了自增 id 是递增的，但不保证是连续的（不能回退，所以有些回滚事务的自增 id 就不会重新使用，导致出现不连续） 自增 IDMySQL 不同的自增 id 在达到上限后的表现不同： 表的自增 id 如果是 int 类型，达到上限 2^32-1 后，再申请时值就不会改变，进而导致继续插入数据时报主键冲突的错误 row_id 长度为 6 个字节，达到上限后则会归 0 再重新递增，如果出现相同的 row_id，后写的数据会覆盖之前的数据，造成旧数据丢失，影响的是数据可靠性，所以应该在 InnoDB 表中主动创建自增主键报主键冲突，插入失败影响的是可用性，而一般情况下，可靠性优先于可用性 Xid 长度 8 字节，由 Server 层维护，只需要不在同一个 binlog 文件中出现重复值即可，虽然理论上会出现重复值，但是概率极小 InnoDB 的 max_trx_id 递增值每次 MySQL 重启都会被保存起来，重启也不会重置为 0，所以会导致一直增加到达上限，然后从 0 开始，这时原事务 0 修改的数据对当前事务就是可见的，产生脏读的现象 只读事务不分配 trx_id，所以 trx_id 的增加速度变慢了 thread_id 长度 4 个字节，到达上限后就会重置为 0，MySQL 设计了一个唯一数组的逻辑，给新线程分配 thread_id 时做判断，保证不会出现两个相同的 thread_id： 123do &#123;\tnew_id = thread_id_counter++;&#125; while (!thread_ids.insert_unique(new_id).second); 参考文章：https://time.geekbang.org/column/article/83183 覆盖索引复合索引叶子节点不仅保存了复合索引的值，还有主键索引，所以使用覆盖索引的时候，加上主键也会用到索引 尽量使用覆盖索引，避免 SELECT *： 1EXPLAIN SELECT name,status,address FROM tb_seller WHERE name=&#x27;小米科技&#x27; AND status=&#x27;1&#x27; AND address=&#x27;西安市&#x27;; 如果查询列，超出索引列，也会降低性能： 1EXPLAIN SELECT name,status,address,password FROM tb_seller WHERE name=&#x27;小米科技&#x27; AND status=&#x27;1&#x27; AND address=&#x27;西安市&#x27;; 减少访问避免对数据进行重复检索：能够一次连接就获取到结果的，就不用两次连接，这样可以大大减少对数据库无用的重复请求 查询数据： 1234SELECT id,name FROM tb_book;SELECT id,status FROM tb_book; -- 向数据库提交两次请求，数据库就要做两次查询操作-- &gt; 优化为:SELECT id,name,statu FROM tb_book; 插入数据： 12345INSERT INTO tb_test VALUES(1,&#x27;Tom&#x27;);INSERT INTO tb_test VALUES(2,&#x27;Cat&#x27;);INSERT INTO tb_test VALUES(3,&#x27;Jerry&#x27;);\t-- 连接三次数据库-- &gt;优化为INSERT INTO tb_test VALUES(1,&#x27;Tom&#x27;),(2,&#x27;Cat&#x27;)，(3,&#x27;Jerry&#x27;);\t-- 连接一次 在事务中进行数据插入： 12345start transaction;INSERT INTO tb_test VALUES(1,&#x27;Tom&#x27;);INSERT INTO tb_test VALUES(2,&#x27;Cat&#x27;);INSERT INTO tb_test VALUES(3,&#x27;Jerry&#x27;);commit;\t-- 手动提交，分段提交 数据有序插入： 123INSERT INTO tb_test VALUES(1,&#x27;Tom&#x27;);INSERT INTO tb_test VALUES(2,&#x27;Cat&#x27;);INSERT INTO tb_test VALUES(3,&#x27;Jerry&#x27;); 增加 cache 层：在应用中增加缓存层来达到减轻数据库负担的目的。可以部分数据从数据库中抽取出来放到应用端以文本方式存储，或者使用框架（Mybatis）提供的一级缓存 &#x2F; 二级缓存，或者使用 Redis 数据库来缓存数据 数据插入当使用 load 命令导入数据的时候，适当的设置可以提高导入的效率： ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-优化SQL load data.png) 1LOAD DATA LOCAL INFILE = &#x27;/home/seazean/sql1.log&#x27; INTO TABLE `tb_user_1` FIELD TERMINATED BY &#x27;,&#x27; LINES TERMINATED BY &#x27; &#x27;; -- 文件格式如上图 对于 InnoDB 类型的表，有以下几种方式可以提高导入的效率： 主键顺序插入：因为 InnoDB 类型的表是按照主键的顺序保存的，所以将导入的数据按照主键的顺序排列，可以有效的提高导入数据的效率，如果 InnoDB 表没有主键，那么系统会自动默认创建一个内部列作为主键 主键是否连续对性能影响不大，只要是递增的就可以，比如雪花算法产生的 ID 不是连续的，但是是递增的，因为递增可以让主键索引尽量地保持顺序插入，避免了页分裂，因此索引更紧凑 插入 ID 顺序排列数据： 插入 ID 无序排列数据： 关闭唯一性校验：在导入数据前执行 SET UNIQUE_CHECKS=0，关闭唯一性校验；导入结束后执行 SET UNIQUE_CHECKS=1，恢复唯一性校验，可以提高导入的效率。 手动提交事务：如果应用使用自动提交的方式，建议在导入前执行SET AUTOCOMMIT=0，关闭自动提交；导入结束后再打开自动提交，可以提高导入的效率。 事务需要控制大小，事务太大可能会影响执行的效率。MySQL 有 innodb_log_buffer_size 配置项，超过这个值的日志会写入磁盘数据，效率会下降，所以在事务大小达到配置项数据级前进行事务提交可以提高效率 分组排序ORDER数据准备： 123456789CREATE TABLE `emp` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `name` VARCHAR(100) NOT NULL, `age` INT(3) NOT NULL, `salary` INT(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;INSERT INTO `emp` (`id`, `name`, `age`, `salary`) VALUES(&#x27;1&#x27;,&#x27;Tom&#x27;,&#x27;25&#x27;,&#x27;2300&#x27;);-- ...CREATE INDEX idx_emp_age_salary ON emp(age, salary); 第一种是通过对返回数据进行排序，所有不通过索引直接返回结果的排序都叫 FileSort 排序，会在内存中重新排序 1EXPLAIN SELECT * FROM emp ORDER BY age DESC;\t-- 年龄降序 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-优化SQL ORDER BY排序1.png) 第二种通过有序索引顺序扫描直接返回有序数据，这种情况为 Using index，不需要额外排序，操作效率高 1EXPLAIN SELECT id, age, salary FROM emp ORDER BY age DESC; ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-优化SQL ORDER BY排序2.png) 多字段排序： 123EXPLAIN SELECT id,age,salary FROM emp ORDER BY age DESC, salary DESC;EXPLAIN SELECT id,age,salary FROM emp ORDER BY salary DESC, age DESC;EXPLAIN SELECT id,age,salary FROM emp ORDER BY age DESC, salary ASC; ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-优化SQL ORDER BY排序3.png) 尽量减少额外的排序，通过索引直接返回有序数据。需要满足 Order by 使用相同的索引、Order By 的顺序和索引顺序相同、Order by 的字段都是升序或都是降序，否则需要额外的操作，就会出现 FileSort ORDER BY RAND() 命令用来进行随机排序，会使用了临时内存表，临时内存表排序的时使用 rowid 排序方法 优化方式：创建合适的索引能够减少 Filesort 的出现，但是某些情况下条件限制不能让 Filesort 消失，就要加快 Filesort 的排序操作 内存临时表，MySQL 有两种 Filesort 排序算法： rowid 排序：首先根据条件取出排序字段和信息，然后在排序区 sort buffer（Server 层）中排序，如果 sort buffer 不够，则在临时表 temporary table 中存储排序结果。完成排序后再根据行指针回表读取记录，该操作可能会导致大量随机 I&#x2F;O 操作 说明：对于临时内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，不会导致多访问磁盘，优先选择该方式 全字段排序：一次性取出满足条件的所有数据，需要回表，然后在排序区 sort buffer 中排序后直接输出结果集。排序时内存开销较大，但是排序效率比两次扫描算法高 具体的选择方式： MySQL 通过比较系统变量 max_length_for_sort_data 的大小和 Query 语句取出的字段的大小，来判定使用哪种排序算法。如果前者大，则说明 sort buffer 空间足够，使用第二种优化之后的算法，否则使用第一种。 可以适当提高 sort_buffer_size 和 max_length_for_sort_data 系统变量，来增大排序区的大小，提高排序的效率 1234SET @@max_length_for_sort_data = 10000; -- 设置全局变量SET max_length_for_sort_data = 10240; -- 设置会话变量SHOW VARIABLES LIKE &#x27;max_length_for_sort_data&#x27;;\t-- 默认1024SHOW VARIABLES LIKE &#x27;sort_buffer_size&#x27;; -- 默认262114 磁盘临时表：排序使用优先队列（堆）的方式 GROUPGROUP BY 也会进行排序操作，与 ORDER BY 相比，GROUP BY 主要只是多了排序之后的分组操作，所以在 GROUP BY 的实现过程中，与 ORDER BY 一样也可以利用到索引 分组查询： 12DROP INDEX idx_emp_age_salary ON emp;EXPLAIN SELECT age,COUNT(*) FROM emp GROUP BY age; ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-优化SQL GROUP BY排序1.png) Using temporary：表示 MySQL 需要使用临时表（不是 sort buffer）来存储结果集，常见于排序和分组查询 查询包含 GROUP BY 但是用户想要避免排序结果的消耗， 则可以执行 ORDER BY NULL 禁止排序： 1EXPLAIN SELECT age,COUNT(*) FROM emp GROUP BY age ORDER BY NULL; ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-优化SQL GROUP BY排序2.png) 创建索引：索引本身有序，不需要临时表，也不需要再额外排序 1CREATE INDEX idx_emp_age_salary ON emp(age, salary); ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-优化SQL GROUP BY排序3.png) 数据量很大时，使用 SQL_BIG_RESULT 提示优化器直接使用直接用磁盘临时表 联合查询对于包含 OR 的查询子句，如果要利用索引，则 OR 之间的每个条件列都必须用到索引，而且不能使用到条件之间的复合索引，如果没有索引，则应该考虑增加索引 执行查询语句： 1EXPLAIN SELECT * FROM emp WHERE id = 1 OR age = 30;\t-- 两个索引，并且不是复合索引 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-优化SQL OR条件查询1.png) 1Extra: Using sort_union(idx_emp_age_salary,PRIMARY); Using where 使用 UNION 替换 OR，求并集： 注意：该优化只针对多个索引列有效，如果有列没有被索引，查询效率可能会因为没有选择 OR 而降低 1EXPLAIN SELECT * FROM emp WHERE id = 1 UNION SELECT * FROM emp WHERE age = 30; ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-优化SQL OR条件查询2.png) UNION 要优于 OR 的原因： UNION 语句的 type 值为 ref，OR 语句的 type 值为 range UNION 语句的 ref 值为 const，OR 语句的 ref 值为 null，const 表示是常量值引用，非常快 嵌套查询MySQL 4.1 版本之后，开始支持 SQL 的子查询 可以使用 SELECT 语句来创建一个单列的查询结果，然后把结果作为过滤条件用在另一个查询中 使用子查询可以一次性的完成逻辑上需要多个步骤才能完成的 SQL 操作，同时也可以避免事务或者表锁死 在有些情况下，子查询是可以被更高效的连接（JOIN）替代 例如查找有角色的所有的用户信息： 执行计划： 1EXPLAIN SELECT * FROM t_user WHERE id IN (SELECT user_id FROM user_role); 优化后： 1EXPLAIN SELECT * FROM t_user u , user_role ur WHERE u.id = ur.user_id; 连接查询之所以效率更高 ，是因为不需要在内存中创建临时表来完成逻辑上需要两个步骤的查询工作 分页查询一般分页查询时，通过创建覆盖索引能够比较好地提高性能 一个常见的问题是 LIMIT 200000,10，此时需要 MySQL 扫描前 200010 记录，仅仅返回 200000 - 200010 之间的记录，其他记录丢弃，查询排序的代价非常大 分页查询： 1EXPLAIN SELECT * FROM tb_user_1 LIMIT 200000,10; 优化方式一：内连接查询，在索引列 id 上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容 1EXPLAIN SELECT * FROM tb_user_1 t,(SELECT id FROM tb_user_1 ORDER BY id LIMIT 200000,10) a WHERE t.id = a.id; 优化方式二：方案适用于主键自增的表，可以把 LIMIT 查询转换成某个位置的查询 12EXPLAIN SELECT * FROM tb_user_1 WHERE id &gt; 200000 LIMIT 10; -- 写法 1EXPLAIN SELECT * FROM tb_user_1 WHERE id BETWEEN 200000 and 200010;\t-- 写法 2 使用提示SQL 提示，是优化数据库的一个重要手段，就是在 SQL 语句中加入一些提示来达到优化操作的目的 USE INDEX：在查询语句中表名的后面添加 USE INDEX 来提供 MySQL 去参考的索引列表，可以让 MySQL 不再考虑其他可用的索引 12CREATE INDEX idx_seller_name ON tb_seller(name);EXPLAIN SELECT * FROM tb_seller USE INDEX(idx_seller_name) WHERE name=&#x27;小米科技&#x27;; IGNORE INDEX：让 MySQL 忽略一个或者多个索引，则可以使用 IGNORE INDEX 作为提示 1EXPLAIN SELECT * FROM tb_seller IGNORE INDEX(idx_seller_name) WHERE name = &#x27;小米科技&#x27;; FORCE INDEX：强制 MySQL 使用一个特定的索引 1EXPLAIN SELECT * FROM tb_seller FORCE INDEX(idx_seller_name_sta_addr) WHERE NAME=&#x27;小米科技&#x27;; 统计计数在不同的 MySQL 引擎中，count(*) 有不同的实现方式： MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高，但不支持事务 show table status 命令通过采样估算可以快速获取，但是不准确 InnoDB 表执行 count(*) 会遍历全表，虽然结果准确，但会导致性能问题 解决方案： 计数保存在 Redis 中，但是更新 MySQL 和 Redis 的操作不是原子的，会存在数据一致性的问题 计数直接放到数据库里单独的一张计数表中，利用事务解决计数精确问题： 会话 B 的读操作在 T3 执行的，这时更新事务还没有提交，所以计数值加 1 这个操作对会话 B 还不可见，因此会话 B 查询的计数值和最近 100 条记录，返回的结果逻辑上就是一致的 并发系统性能的角度考虑，应该先插入操作记录再更新计数表，因为更新计数表涉及到行锁的竞争，先插入再更新能最大程度地减少事务之间的锁等待，提升并发度 count 函数的按照效率排序：count(字段) &lt; count(主键id) &lt; count(1) ≈ count(*)，所以建议尽量使用 count(*) count(主键 id)：InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来返回给 Server 层，Server 判断 id 不为空就按行累加 count(1)：InnoDB 引擎遍历整张表但不取值，Server 层对于返回的每一行，放一个数字 1 进去，判断不为空就按行累加 count(字段)：如果这个字段是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；如果这个字段定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加 count(*)：不取值，按行累加 参考文章：https://time.geekbang.org/column/article/72775 缓冲优化优化原则三个原则： 将尽量多的内存分配给 MySQL 做缓存，但也要给操作系统和其他程序预留足够内存 MyISAM 存储引擎的数据文件读取依赖于操作系统自身的 IO 缓存，如果有 MyISAM 表，就要预留更多的内存给操作系统做 IO 缓存 排序区、连接区等缓存是分配给每个数据库会话（Session）专用的，值的设置要根据最大连接数合理分配，如果设置太大，不但浪费资源，而且在并发数较高时会导致物理内存耗尽 缓冲内存Buffer Pool 本质上是 InnoDB 向操作系统申请的一段连续的内存空间。InnoDB 的数据是按数据页为单位来读写，每个数据页的大小默认是 16KB。数据是存放在磁盘中，每次读写数据都需要进行磁盘 IO 将数据读入内存进行操作，效率会很低，所以提供了 Buffer Pool 来暂存这些数据页，缓存中的这些页又叫缓冲页 工作原理： 从数据库读取数据时，会首先从缓存中读取，如果缓存中没有，则从磁盘读取后放入 Buffer Pool 向数据库写入数据时，会写入缓存，缓存中修改的数据会定期刷新到磁盘，这一过程称为刷脏 Buffer Pool 中每个缓冲页都有对应的控制信息，包括表空间编号、页号、偏移量、链表信息等，控制信息存放在占用的内存称为控制块，控制块与缓冲页是一一对应的，但并不是物理上相连的，都在缓冲池中 MySQL 提供了缓冲页的快速查找方式：哈希表，使用表空间号和页号作为 Key，缓冲页控制块的地址作为 Value 创建一个哈希表，获取数据页时根据 Key 进行哈希寻址： 如果不存在对应的缓存页，就从 free 链表中选一个空闲缓冲页，把磁盘中的对应页加载到该位置 如果存在对应的缓存页，直接获取使用，提高查询数据的效率 当内存数据页跟磁盘数据页内容不一致时，称这个内存页为脏页；内存数据写入磁盘后，内存和磁盘上的数据页一致，称为干净页 内存管理Free 链表MySQL 启动时完成对 Buffer Pool 的初始化，先向操作系统申请连续的内存空间，然后将内存划分为若干对控制块和缓冲页。为了区分空闲和已占用的数据页，将所有空闲缓冲页对应的控制块作为一个节点放入一个链表中，就是 Free 链表（空闲链表） 基节点：是一块单独申请的内存空间（占 40 字节），并不在 Buffer Pool 的那一大片连续内存空间里 磁盘加载页的流程： 从 Free 链表中取出一个空闲的缓冲页 把缓冲页对应的控制块的信息填上（页所在的表空间、页号之类的信息） 把缓冲页对应的 Free 链表节点（控制块）从链表中移除，表示该缓冲页已经被使用 参考文章：https://blog.csdn.net/li1325169021/article/details/121124440 Flush 链表Flush 链表是一个用来存储脏页的链表，对于已经修改过的缓冲脏页，第一次修改后加入到链表头部，以后每次修改都不会重新加入，只修改部分控制信息，出于性能考虑并不是直接更新到磁盘，而是在未来的某个时间进行刷脏 后台有专门的线程每隔一段时间把脏页刷新到磁盘： 从 Flush 链表中刷新一部分页面到磁盘： 后台线程定时从 Flush 链表刷脏，根据系统的繁忙程度来决定刷新速率，这种方式称为 BUF_FLUSH_LIST 线程刷脏的比较慢，导致用户线程加载一个新的数据页时发现没有空闲缓冲页，此时会尝试从 LRU 链表尾部寻找缓冲页直接释放，如果该页面是已经修改过的脏页就同步刷新到磁盘，速度较慢，这种方式称为 BUF_FLUSH_SINGLE_PAGE 从 LRU 链表的冷数据中刷新一部分页面到磁盘，即：BUF_FLUSH_LRU 后台线程会定时从 LRU 链表的尾部开始扫描一些页面，扫描的页面数量可以通过系统变量 innodb_lru_scan_depth 指定，如果在 LRU 链表中发现脏页，则把它们刷新到磁盘，这种方式称为 BUF_FLUSH_LRU 控制块里会存储该缓冲页是否被修改的信息，所以可以很容易的获取到某个缓冲页是否是脏页 参考文章：https://blog.csdn.net/li1325169021/article/details/121125765 LRU 链表Buffer Pool 需要保证缓存的命中率，所以 MySQL 创建了一个 LRU 链表，当访问某个页时： 如果该页不在 Buffer Pool 中，把该页从磁盘加载进来后会将该缓冲页对应的控制块作为节点放入 LRU 链表的头部，保证热点数据在链表头 如果该页在 Buffer Pool 中，则直接把该页对应的控制块移动到 LRU 链表的头部，所以 LRU 链表尾部就是最近最少使用的缓冲页 MySQL 基于局部性原理提供了预读功能： 线性预读：系统变量 innodb_read_ahead_threshold，如果顺序访问某个区（extent：16 KB 的页，连续 64 个形成一个区，一个区默认 1MB 大小）的页面数超过了该系统变量值，就会触发一次异步读取下一个区中全部的页面到 Buffer Pool 中 随机预读：如果某个区 13 个连续的页面都被加载到 Buffer Pool，无论这些页面是否是顺序读取，都会触发一次异步读取本区所有的其他页面到 Buffer Pool 中 预读会造成加载太多用不到的数据页，造成那些使用频率很高的数据页被挤到 LRU 链表尾部，所以 InnoDB 将 LRU 链表分成两段，冷热数据隔离： 一部分存储使用频率很高的数据页，这部分链表也叫热数据，young 区，靠近链表头部的区域 一部分存储使用频率不高的冷数据，old 区，靠近链表尾部，默认占 37%，可以通过系统变量 innodb_old_blocks_pct 指定 当磁盘上的某数据页被初次加载到 Buffer Pool 中会被放入 old 区，淘汰时优先淘汰 old 区 当对 old 区的数据进行访问时，会在控制块记录下访问时间，等待后续的访问时间与第一次访问的时间是否在某个时间间隔内，通过系统变量 innodb_old_blocks_time 指定时间间隔，默认 1000ms，成立就移动到 young 区的链表头部 innodb_old_blocks_time 为 0 时，每次访问一个页面都会放入 young 区的头部 参数优化InnoDB 用一块内存区做 IO 缓存池，该缓存池不仅用来缓存 InnoDB 的索引块，也用来缓存 InnoDB 的数据块，可以通过下面的指令查看 Buffer Pool 的状态信息： 1SHOW ENGINE INNODB STATUS\\G Buffer pool hit rate 字段代表内存命中率，表示 Buffer Pool 对查询的加速效果 核心参数： innodb_buffer_pool_size：该变量决定了 Innodb 存储引擎表数据和索引数据的最大缓存区大小，默认 128M 1SHOW VARIABLES LIKE &#x27;innodb_buffer_pool_size&#x27;; 在保证操作系统及其他程序有足够内存可用的情况下，innodb_buffer_pool_size 的值越大，缓存命中率越高，建议设置成可用物理内存的 60%~80% 1innodb_buffer_pool_size=512M innodb_log_buffer_size：该值决定了 Innodb 日志缓冲区的大小，保存要写入磁盘上的日志文件数据 对于可能产生大量更新记录的大事务，增加该值的大小，可以避免 Innodb 在事务提交前就执行不必要的日志写入磁盘操作，影响执行效率，通过配置文件修改： 1innodb_log_buffer_size=10M 在多线程下，访问 Buffer Pool 中的各种链表都需要加锁，所以将 Buffer Pool 拆成若干个小实例，每个线程对应一个实例，独立管理内存空间和各种链表（类似 ThreadLocal），多线程访问各自实例互不影响，提高了并发能力 MySQL 5.7.5 之前 innodb_buffer_pool_size 只支持在系统启动时修改，现在已经支持运行时修改 Buffer Pool 的大小，但是每次调整参数都会重新向操作系统申请一块连续的内存空间，将旧的缓冲池的内容拷贝到新空间非常耗时，所以 MySQL 开始以一个 chunk 为单位向操作系统申请内存，所以一个 Buffer Pool 实例可以由多个 chunk 组成 在系统启动时设置系统变量 innodb_buffer_pool_instance 可以指定 Buffer Pool 实例的个数，但是当 Buffer Pool 小于 1GB 时，设置多个实例时无效的 指定系统变量 innodb_buffer_pool_chunk_size 来改变 chunk 的大小，只能在启动时修改，运行中不能修改，而且该变量并不包含缓冲页的控制块的内存大小 innodb_buffer_pool_size 必须是 innodb_buffer_pool_chunk_size × innodb_buffer_pool_instance 的倍数，默认值是 128M × 16 = 2G，Buffer Pool 必须是 2G 的整数倍，如果指定 5G，会自动调整成 6G 如果启动时 chunk × instances &gt; pool_size，那么 chunk 的值会自动设置为 pool_size ÷ instances 内存优化ChangeInnoDB 管理的 Buffer Pool 中有一块内存叫 Change Buffer 用来对增删改操作提供缓存，可以通过参数来动态设置，设置为 50 时表示 Change Buffer 的大小最多占用 Buffer Pool 的 50% 唯一索引的更新不能使用 Change Buffer，需要将数据页读入内存，判断没有冲突在写入 普通索引可以使用 Change Buffer，直接写入 Buffer 就结束，不用校验唯一性 Change Buffer 并不是数据页，只是对操作的缓存，所以需要将 Change Buffer 中的操作应用到旧数据页，得到新的数据页（脏页）的过程称为 Merge 触发时机：访问数据页时会触发 Merge、后台有定时线程进行 Merge、在数据库正常关闭（shutdown）的过程中也会触发 工作流程：首先从磁盘读入数据页到内存（因为 Buffer Pool 中不一定存在对应的数据页），从 Change Buffer 中找到对应的操作应用到数据页，得到新的数据页即为脏页，然后写入 redo log，等待刷脏即可 说明：Change Buffer 中的记录，在事务提交时也会写入 redo log，所以是可以保证不丢失的 业务场景： 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 Change Buffer 的使用效果最好，常见的就是账单类、日志类的系统 一个业务的更新模式是写入后马上做查询，那么即使满足了条件，将更新先记录在 Change Buffer，但之后由于马上要访问这个数据页，会立即触发 Merge 过程，这样随机访问 IO 的次数不会减少，并且增加了 Change Buffer 的维护代价 补充：Change Buffer 的前身是 Insert Buffer，只能对 Insert 操作优化，后来增加了 Update&#x2F;Delete 的支持，改为 Change Buffer NetServer 层针对优化查询的内存为 Net Buffer，内存的大小是由参数 net_buffer_length定义，默认 16k，实现流程： 获取一行数据写入 Net Buffer，重复获取直到 Net Buffer 写满，调用网络接口发出去 若发送成功就清空 Net Buffer，然后继续取下一行；若发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，表示本地网络栈 socket send buffer 写满了，进入等待，直到网络栈重新可写再继续发送 MySQL 采用的是边读边发的逻辑，因此对于数据量很大的查询来说，不会在 Server 端保存完整的结果集，如果客户端读结果不及时，会堵住 MySQL 的查询过程，但是不会把内存打爆导致 OOM SHOW PROCESSLIST 获取线程信息后，处于 Sending to client 状态代表服务器端的网络栈写满，等待客户端接收数据 假设有一个业务的逻辑比较复杂，每读一行数据以后要处理很久的逻辑，就会导致客户端要过很久才会去取下一行数据，导致 MySQL 的阻塞，一直处于 Sending to client 的状态 解决方法：如果一个查询的返回结果很是很多，建议使用 mysql_store_result 这个接口，直接把查询结果保存到本地内存 参考文章：https://blog.csdn.net/qq_33589510/article/details/117673449 Readread_rnd_buffer 是 MySQL 的随机读缓冲区，当按任意顺序读取记录行时将分配一个随机读取缓冲区，进行排序查询时，MySQL 会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，大小是由 read_rnd_buffer_size 参数控制的 Multi-Range Read 优化，将随机 IO 转化为顺序 IO 以降低查询过程中 IO 开销，因为大多数的数据都是按照主键递增顺序插入得到，所以按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能 二级索引为 a，聚簇索引为 id，优化回表流程： 根据索引 a，定位到满足条件的记录，将 id 值放入 read_rnd_buffer 中 将 read_rnd_buffer 中的 id 进行递增排序 排序后的 id 数组，依次回表到主键 id 索引中查记录，并作为结果返回 说明：如果步骤 1 中 read_rnd_buffer 放满了，就会先执行步骤 2 和 3，然后清空 read_rnd_buffer，之后继续找索引 a 的下个记录 使用 MRR 优化需要设进行设置： 1SET optimizer_switch=&#x27;mrr_cost_based=off&#x27; KeyMyISAM 存储引擎使用 key_buffer 缓存索引块，加速 MyISAM 索引的读写速度。对于 MyISAM 表的数据块没有特别的缓存机制，完全依赖于操作系统的 IO 缓存 key_buffer_size：该变量决定 MyISAM 索引块缓存区的大小，直接影响到 MyISAM 表的存取效率 1SHOW VARIABLES LIKE &#x27;key_buffer_size&#x27;;\t-- 单位是字节 在 MySQL 配置文件中设置该值，建议至少将1&#x2F;4可用内存分配给 key_buffer_size： 12vim /etc/mysql/my.cnfkey_buffer_size=1024M read_buffer_size：如果需要经常顺序扫描 MyISAM 表，可以通过增大 read_buffer_size 的值来改善性能。但 read_buffer_size 是每个 Session 独占的，如果默认值设置太大，并发环境就会造成内存浪费 read_rnd_buffer_size：对于需要做排序的 MyISAM 表的查询，如带有 ORDER BY 子句的语句，适当增加该的值，可以改善此类的 SQL 的性能，但是 read_rnd_buffer_size 是每个 Session 独占的，如果默认值设置太大，就会造成内存浪费 存储优化数据存储系统表空间是用来放系统信息的，比如数据字典什么的，对应的磁盘文件是 ibdata，数据表空间是一个个的表数据文件，对应的磁盘文件就是表名.ibd 表数据既可以存在共享表空间里，也可以是单独的文件，这个行为是由参数 innodb_file_per_table 控制的： OFF：表示表的数据放在系统共享表空间，也就是跟数据字典放在一起 ON ：表示每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中（默认） 一个表单独存储为一个文件更容易管理，在不需要这个表时通过 drop table 命令，系统就会直接删除这个文件；如果是放在共享表空间中，即使表删掉了，空间也是不会回收的 数据删除MySQL 的数据删除就是移除掉某个记录后，该位置就被标记为可复用，如果有符合范围条件的数据可以插入到这里。符合范围条件的意思是假设删除记录 R4，之后要再插入一个 ID 在 300 和 600 之间的记录时，就会复用这个位置 InnoDB 的数据是按页存储的如果删掉了一个数据页上的所有记录，整个数据页就可以被复用了，如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用 删除命令其实只是把记录的位置，或者数据页标记为了可复用，但磁盘文件的大小是不会变的，这些可以复用还没有被使用的空间，看起来就像是空洞，造成数据库的稀疏，因此需要进行紧凑处理 重建数据重建表就是按照主键 ID 递增的顺序，把数据一行一行地从旧表中读出来再插入到新表中，让数据更加紧凑。重建表时 MySQL 会自动完成转存数据、交换表名、删除旧表的操作，线上操作会阻塞大量的线程增删改查的操作 重建命令： 1ALTER TABLE A ENGINE=InnoDB 工作流程：新建临时表 tmp_table B（在 Server 层创建的），把表 A 中的数据导入到表 B 中，操作完成后用表 B 替换表 A，完成重建 重建表的步骤需要 DDL 不是 Online 的，因为在导入数据的过程有新的数据要写入到表 A 的话，就会造成数据丢失 MySQL 5.6 版本开始引入的 Online DDL，重建表的命令默认执行此步骤： 建立一个临时文件 tmp_file（InnoDB 创建），扫描表 A 主键的所有数据页 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 用临时文件替换表 A 的数据文件 Online DDL 操作会先获取 MDL 写锁，再退化成 MDL 读锁。但 MDL 写锁持有时间比较短，所以可以称为 Online； 而 MDL 读锁，不阻止数据增删查改，但会阻止其它线程修改表结构（可以对比 ANALYZE TABLE t 命令） 问题：重建表可以收缩表空间，但是执行指令后整体占用空间增大 原因：在重建表后 InnoDB 不会把整张表占满，每个页留了 1&#x2F;16 给后续的更新使用。表在未整理之前页已经占用 15&#x2F;16 以上，收缩之后需要保持数据占用空间在 15&#x2F;16，所以文件占用空间更大才能保持 注意：临时文件也要占用空间，如果空间不足会重建失败 原地置换DDL 中的临时表 tmp_table 是在 Server 层创建的，Online DDL 中的临时文件 tmp_file 是 InnoDB 在内部创建出来的，整个 DDL 过程都在 InnoDB 内部完成，对于 Server 层来说，没有把数据挪动到临时表，是一个原地操作，这就是 inplace 两者的关系： DDL 过程如果是 Online 的，就一定是 inplace 的 inplace 的 DDL，有可能不是 Online 的，截止到 MySQL 8.0，全文索引（FULLTEXT）和空间索引（SPATIAL）属于这种情况 并发优化MySQL Server 是多线程结构，包括后台线程和客户服务线程。多线程可以有效利用服务器资源，提高数据库的并发性能。在 MySQL 中，控制并发连接和线程的主要参数： max_connections：控制允许连接到 MySQL 数据库的最大连接数，默认值是 151 如果状态变量 connection_errors_max_connections 不为零，并且一直增长，则说明不断有连接请求因数据库连接数已达到允许最大值而失败，这时可以考虑增大 max_connections 的值 MySQL 最大可支持的连接数取决于很多因素，包括操作系统平台的线程库的质量、内存大小、每个连接的负荷、CPU的处理速度、期望的响应时间等。在 Linux 平台下，性能好的服务器，可以支持 500-1000 个连接，需要根据服务器性能进行评估设定 innodb_thread_concurrency：并发线程数，代表系统内同时运行的线程数量（已经被移除） back_log：控制 MySQL 监听 TCP 端口时的积压请求栈的大小 如果 Mysql 的连接数达到 max_connections 时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即 back_log。如果等待连接的数量超过 back_log，将不被授予连接资源直接报错 5.6.6 版本之前默认值为 50，之后的版本默认为 50 + (max_connections/5)，但最大不超过900，如果需要数据库在较短的时间内处理大量连接请求， 可以考虑适当增大 back_log 的值 table_open_cache：控制所有 SQL 语句执行线程可打开表缓存的数量 在执行 SQL 语句时，每个执行线程至少要打开1个表缓存，该参数的值应该根据设置的最大连接数以及每个连接执行关联查询中涉及的表的最大数量来设定：max_connections * N thread_cache_size：可控制 MySQL 缓存客户服务线程的数量 为了加快连接数据库的速度，MySQL 会缓存一定数量的客户服务线程以备重用，池化思想 innodb_lock_wait_timeout：设置 InnoDB 事务等待行锁的时间，默认值是 50ms 对于需要快速反馈的业务系统，可以将行锁的等待时间调小，以避免事务被长时间挂起； 对于后台运行的批量处理程序来说，可以将行锁的等待时间调大，以避免发生大的回滚操作 事务机制基本介绍事务（Transaction）是访问和更新数据库的程序执行单元；事务中可能包含一个或多个 SQL 语句，这些语句要么都执行，要么都不执行，作为一个关系型数据库，MySQL 支持事务。 单元中的每条 SQL 语句都相互依赖，形成一个整体 如果某条 SQL 语句执行失败或者出现错误，那么整个单元就会回滚，撤回到事务最初的状态 如果单元中所有的 SQL 语句都执行成功，则事务就顺利执行 事务的四大特征：ACID 原子性 (atomicity) 一致性 (consistency) 隔离性 (isolaction) 持久性 (durability) 事务的几种状态： 活动的（active）：事务对应的数据库操作正在执行中 部分提交的（partially committed）：事务的最后一个操作执行完，但是内存还没刷新至磁盘 失败的（failed）：当事务处于活动状态或部分提交状态时，如果数据库遇到了错误或刷脏失败，或者用户主动停止当前的事务 中止的（aborted）：失败状态的事务回滚完成后的状态 提交的（committed）：当处于部分提交状态的事务刷脏成功，就处于提交状态 事务管理基本操作事务管理的三个步骤 开启事务：记录回滚点，并通知服务器，将要执行一组操作，要么同时成功、要么同时失败 执行 SQL 语句：执行具体的一条或多条 SQL 语句 结束事务（提交|回滚） 提交：没出现问题，数据进行更新 回滚：出现问题，数据恢复到开启事务时的状态 事务操作： 显式开启事务 12START TRANSACTION [READ ONLY|READ WRITE|WITH CONSISTENT SNAPSHOT]; #可以跟一个或多个状态，最后的是一致性读BEGIN [WORK]; 说明：不填状态默认是读写事务 回滚事务，用来手动中止事务 1ROLLBACK; 提交事务，显示执行是手动提交，MySQL 默认为自动提交 1COMMIT; 保存点：在事务的执行过程中设置的还原点，调用 ROLLBACK 时可以指定回滚到哪个点 123SAVEPOINT point_name; #设置保存点RELEASE point_name #删除保存点ROLLBACK [WORK] TO [SAVEPOINT] point_name\t#回滚至某个保存点，不填默认回滚到事务执行之前的状态 操作演示 1234567891011121314-- 开启事务START TRANSACTION;-- 张三给李四转账500元-- 1.张三账户-500UPDATE account SET money=money-500 WHERE NAME=&#x27;张三&#x27;;-- 2.李四账户+500UPDATE account SET money=money+500 WHERE NAME=&#x27;李四&#x27;;-- 回滚事务(出现问题)ROLLBACK;-- 提交事务(没出现问题)COMMIT; 提交方式提交方式的相关语法： 查看事务提交方式 12SELECT @@AUTOCOMMIT; -- 会话，1 代表自动提交 0 代表手动提交SELECT @@GLOBAL.AUTOCOMMIT;\t-- 系统 修改事务提交方式 12SET @@AUTOCOMMIT=数字;\t-- 系统SET AUTOCOMMIT=数字; -- 会话 系统变量的操作： 12SET [GLOBAL|SESSION] 变量名 = 值; -- 默认是会话SET @@[(GLOBAL|SESSION).]变量名 = 值; -- 默认是系统 1SHOW [GLOBAL|SESSION] VARIABLES [LIKE &#x27;变量%&#x27;]; -- 默认查看会话内系统变量值 工作原理： 自动提交：如果没有 START TRANSACTION 显式地开始一个事务，那么每条 SQL 语句都会被当做一个事务执行提交操作；显式开启事务后，会在本次事务结束（提交或回滚）前暂时关闭自动提交 手动提交：不需要显式的开启事务，所有的 SQL 语句都在一个事务中，直到执行了提交或回滚，然后进入下一个事务 隐式提交：存在一些特殊的命令，在事务中执行了这些命令会马上强制执行 COMMIT 提交事务 DDL 语句 (CREATE&#x2F;DROP&#x2F;ALTER)、LOCK TABLES 语句、LOAD DATA 导入数据语句、主从复制语句等 当一个事务还没提交或回滚，显式的开启一个事务会隐式的提交上一个事务 事务 ID事务在执行过程中对某个表执行了增删改操作或者创建表，就会为当前事务分配一个独一无二的事务 ID（对临时表并不会分配 ID），如果当前事务没有被分配 ID，默认是 0 说明：只读事务不能对普通的表进行增删改操作，但是可以对临时表增删改，读写事务可以对数据表执行增删改查操作 事务 ID 本质上就是一个数字，服务器在内存中维护一个全局变量： 每当需要为某个事务分配 ID，就会把全局变量的值赋值给事务 ID，然后变量自增 1 每当变量值为 256 的倍数时，就将该变量的值刷新到系统表空间的 Max Trx ID 属性中，该属性占 8 字节 系统再次启动后，会读取表空间的 Max Trx ID 属性到内存，加上 256 后赋值给全局变量，因为关机时的事务 ID 可能并不是 256 的倍数，会比 Max Trx ID 大，所以需要加上 256 保持事务 ID 是一个递增的数字 聚簇索引的行记录除了完整的数据，还会自动添加 trx_id、roll_pointer 隐藏列，如果表中没有主键并且没有非空唯一索引，也会添加一个 row_id 的隐藏列作为聚簇索引 隔离级别四种级别事务的隔离级别：多个客户端操作时，各个客户端的事务之间应该是隔离的，不同的事务之间不该互相影响，而如果多个事务操作同一批数据时，则需要设置不同的隔离级别，否则就会产生问题。 隔离级别分类： 隔离级别 名称 会引发的问题 数据库默认隔离级别 Read Uncommitted 读未提交 脏读、不可重复读、幻读 Read Committed 读已提交 不可重复读、幻读 Oracle &#x2F; SQL Server Repeatable Read 可重复读 幻读 MySQL Serializable 可串行化 无 一般来说，隔离级别越低，系统开销越低，可支持的并发越高，但隔离性也越差 脏写 (Dirty Write)：当两个或多个事务选择同一行，最初的事务修改的值被后面事务修改的值覆盖，所有的隔离级别都可以避免脏写（又叫丢失更新），因为有行锁 脏读 (Dirty Reads)：在一个事务处理过程中读取了另一个未提交的事务中修改过的数据 不可重复读 (Non-Repeatable Reads)：在一个事务处理过程中读取了另一个事务中修改并已提交的数据 可重复读的意思是不管读几次，结果都一样，可以重复的读，可以理解为快照读，要读的数据集不会发生变化 幻读 (Phantom Reads)：在事务中按某个条件先后两次查询数据库，后一次查询查到了前一次查询没有查到的行，数据条目发生了变化。比如查询某数据不存在，准备插入此记录，但执行插入时发现此记录已存在，无法插入 隔离级别操作语法： 查询数据库隔离级别 12SELECT @@TX_ISOLATION; -- 会话SELECT @@GLOBAL.TX_ISOLATION;\t-- 系统 修改数据库隔离级别 1SET GLOBAL TRANSACTION ISOLATION LEVEL 级别字符串; 加锁分析InnoDB 存储引擎支持事务，所以加锁分析是基于该存储引擎 Read Uncommitted 级别，任何操作都不会加锁 Read Committed 级别，增删改操作会加写锁（行锁），读操作不加锁 在 Server 层过滤条件时发现不满足的记录会调用 unlock_row 方法释放该记录的行锁，保证最后只有满足条件的记录加锁，但是扫表过程中每条记录的加锁操作不能省略。所以对数据量很大的表做批量修改时，如果无法使用相应的索引（全表扫描），在 Server 过滤数据时就会特别慢，出现虽然没有修改某些行的数据，但是还是被锁住了的现象（锁表），这种情况同样适用于 RR Repeatable Read 级别，增删改操作会加写锁，读操作不加锁。因为读写锁不兼容，加了读锁后其他事务就无法修改数据，影响了并发性能，为了保证隔离性和并发性，MySQL 通过 MVCC 解决了读写冲突。RR 级别下的锁有很多种，锁机制章节详解 Serializable 级别，读加共享锁，写加排他锁，读写互斥，使用的悲观锁的理论，实现简单，数据更加安全，但是并发能力非常差 串行化：让所有事务按顺序单独执行，写操作会加写锁，读操作会加读锁 可串行化：让所有操作相同数据的事务顺序执行，通过加锁实现 参考文章：https://tech.meituan.com/2014/08/20/innodb-lock.html 原子特性实现方式原子性是指事务是一个不可分割的工作单位，事务的操作如果成功就必须要完全应用到数据库，失败则不能对数据库有任何影响。比如事务中一个 SQL 语句执行失败，则已执行的语句也必须回滚，数据库退回到事务前的状态 InnoDB 存储引擎提供了两种事务日志：redo log（重做日志）和 undo log（回滚日志） redo log 用于保证事务持久性 undo log 用于保证事务原子性和隔离性 undo log 属于逻辑日志，根据每行操作进行记录，记录了 SQL 执行相关的信息，用来回滚行记录到某个版本 当事务对数据库进行修改时，InnoDB 会先记录对应的 undo log，如果事务执行失败或调用了 rollback 导致事务回滚，InnoDB 会根据 undo log 的内容做与之前相反的操作： 对于每个 insert，回滚时会执行 delete 对于每个 delete，回滚时会执行 insert 对于每个 update，回滚时会执行一个相反的 update，把数据修改回去 参考文章：https://www.cnblogs.com/kismetv/p/10331633.html DML 解析INSERT乐观插入：当前数据页的剩余空间充足，直接将数据进行插入 悲观插入：当前数据页的剩余空间不足，需要进行页分裂，申请一个新的页面来插入数据，会造成更多的 redo log，undo log 影响不大 当向某个表插入一条记录，实际上需要向聚簇索引和所有二级索引都插入一条记录，但是 undo log 只针对聚簇索引记录，在回滚时会根据聚簇索引去所有的二级索引进行回滚操作 roll_pointer 是一个指针，指向记录对应的 undo log 日志，一条记录就是一个数据行，行格式中的 roll_pointer 就指向 undo log DELETE插入到页面中的记录会根据 next_record 属性组成一个单向链表，这个链表称为正常链表，被删除的记录也会通过 next_record 组成一个垃圾链表，该链表中所占用的存储空间可以被重新利用，并不会直接清除数据 在页面 Page Header 中，PAGE_FREE 属性指向垃圾链表的头节点，删除的工作过程： 将要删除的记录的 delete_flag 位置为 1，其他不做修改，这个过程叫 delete mark 在事务提交前，delete_flag &#x3D; 1 的记录一直都会处于中间状态 事务提交后，有专门的线程将 delete_flag &#x3D; 1 的记录从正常链表移除并加入垃圾链表，这个过程叫 purge purge 线程在执行删除操作时会创建一个 ReadView，根据事务的可见性移除数据（隔离特性部分详解） 当有新插入的记录时，首先判断 PAGE_FREE 指向的头节点是否足够容纳新纪录： 如果可以容纳新纪录，就会直接重用已删除的记录的存储空间，然后让 PAGE_FREE 指向垃圾链表的下一个节点 如果不能容纳新纪录，就直接向页面申请新的空间存储，并不会遍历垃圾链表 重用已删除的记录空间，可能会造成空间碎片，当数据页容纳不了一条记录时，会判断将碎片空间加起来是否可以容纳，判断为真就会重新组织页内的记录： 开辟一个临时页面，将页内记录一次插入到临时页面，此时临时页面时没有碎片的 把临时页面的内容复制到本页，这样就解放出了内存碎片，但是会耗费很大的性能资源 UPDATE执行 UPDATE 语句，对于更新主键和不更新主键有两种不同的处理方式 不更新主键的情况： 就地更新（in-place update），如果更新后的列和更新前的列占用的存储空间一样大，就可以直接在原记录上修改 先删除旧纪录，再插入新纪录，这里的删除不是 delete mark，而是直接将记录加入垃圾链表，并且修改页面的相应的控制信息，执行删除的线程不是 purge，是执行更新的用户线程，插入新记录时可能造成页空间不足，从而导致页分裂 更新主键的情况： 将旧纪录进行 delete mark，在更新语句提交后由 purge 线程移入垃圾链表 根据更新的各列的值创建一条新纪录，插入到聚簇索引中 在对一条记录修改前会将记录的隐藏列 trx_id 和 roll_pointer 的旧值记录到当前 undo log 对应的属性中，这样当前记录的 roll_pointer 指向当前 undo log 记录，当前 undo log 记录的 roll_pointer 指向旧的 undo log 记录，形成一个版本链 UPDATE、DELETE 操作产生的 undo 日志会用于其他事务的 MVCC 操作，所以不能立即删除，INSERT 可以删除的原因是 MVCC 是对现有数据的快照 回滚日志undo log 是采用段的方式来记录，Rollback Segement 称为回滚段，本质上就是一个类型是 Rollback Segement Header 的页面 每个回滚段中有 1024 个 undo slot，每个 slot 存放 undo 链表页面的头节点页号，每个链表对应一个叫 undo log segment 的段 在以前老版本，只支持 1 个 Rollback Segement，只能记录 1024 个 undo log segment MySQL5.5 开始支持 128 个 Rollback Segement，支持 128*1024 个 undo 操作 工作流程： 事务执行前需要到系统表空间第 5 号页面中分配一个回滚段（页），获取一个 Rollback Segement Header 页面的地址 回滚段页面有 1024 个 undo slot，首先去回滚段的两个 cached 链表获取缓存的 slot，缓存中没有就在回滚段页面中找一个可用的 undo slot 分配给当前事务 如果是缓存中获取的 slot，则该 slot 对应的 undo log segment 已经分配了，需要重新分配，然后从 undo log segment 中申请一个页面作为日志链表的头节点，并填入对应的 slot 中 每个事务 undo 日志在记录的时候占用两个 undo 页面的组成链表，分别为 insert undo 链表和 update undo 链表，链表的头节点页面为 first undo page 会包含一些管理信息，其他页面为 normal undo page 说明：事务执行过程的临时表也需要两个 undo 链表，不和普通表共用，这些链表并不是事务开始就分配，而是按需分配 隔离特性实现方式隔离性是指，事务内部的操作与其他事务是隔离的，多个并发事务之间要相互隔离，不能互相干扰 严格的隔离性，对应了事务隔离级别中的 serializable，实际应用中对性能考虑很少使用可串行化 与原子性、持久性侧重于研究事务本身不同，隔离性研究的是不同事务之间的相互影响 隔离性让并发情形下的事务之间互不干扰： 一个事务的写操作对另一个事务的写操作（写写）：锁机制保证隔离性 一个事务的写操作对另一个事务的读操作（读写）：MVCC 保证隔离性 锁机制：事务在修改数据之前，需要先获得相应的锁，获得锁之后，事务便可以修改数据；该事务操作期间，这部分数据是锁定的，其他事务如果需要修改数据，需要等待当前事务提交或回滚后释放锁（详解见锁机制） 并发控制MVCC 全称 Multi-Version Concurrency Control，即多版本并发控制，用来解决读写冲突的无锁并发控制，可以在发生读写请求冲突时不用加锁解决，这个读是指的快照读（也叫一致性读或一致性无锁读），而不是当前读： 快照读：实现基于 MVCC，因为是多版本并发，所以快照读读到的数据不一定是当前最新的数据，有可能是历史版本的数据 当前读：又叫加锁读，读取数据库记录是当前最新的版本（产生幻读、不可重复读），可以对读取的数据进行加锁，防止其他事务修改数据，是悲观锁的一种操作，读写操作加共享锁或者排他锁和串行化事务的隔离级别都是当前读 数据库并发场景： 读-读：不存在任何问题，也不需要并发控制 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读 写-写：有线程安全问题，可能会存在脏写（丢失更新）问题 MVCC 的优点： 在并发读写数据库时，做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了并发读写的性能 可以解决脏读，不可重复读等事务隔离问题（加锁也能解决），但不能解决更新丢失问题（写锁会解决） 提高读写和写写的并发性能： MVCC + 悲观锁：MVCC 解决读写冲突，悲观锁解决写写冲突 MVCC + 乐观锁：MVCC 解决读写冲突，乐观锁解决写写冲突 参考文章：https://www.jianshu.com/p/8845ddca3b23 实现原理隐藏字段实现原理主要是隐藏字段，undo日志，Read View 来实现的 InnoDB 存储引擎，数据库中的聚簇索引每行数据，除了自定义的字段，还有数据库隐式定义的字段： DB_TRX_ID：最近修改事务 ID，记录创建该数据或最后一次修改该数据的事务 ID DB_ROLL_PTR：回滚指针，指向记录对应的 undo log 日志，undo log 中又指向上一个旧版本的 undo log DB_ROW_ID：隐含的自增 ID（隐藏主键），如果数据表没有主键，InnoDB 会自动以 DB_ROW_ID 作为聚簇索引 版本链undo log 是逻辑日志，记录的是每个事务对数据执行的操作，而不是记录的全部数据，要根据 undo log 逆推出以往事务的数据 undo log 的作用： 保证事务进行 rollback 时的原子性和一致性，当事务进行回滚的时候可以用 undo log 的数据进行恢复 用于 MVCC 快照读，通过读取 undo log 的历史版本数据可以实现不同事务版本号都拥有自己独立的快照数据 undo log 主要分为两种： insert undo log：事务在 insert 新记录时产生的 undo log，只在事务回滚时需要，并且在事务提交后可以被立即丢弃 update undo log：事务在进行 update 或 delete 时产生的 undo log，在事务回滚时需要，在快照读时也需要。不能随意删除，只有在当前读或事务回滚不涉及该日志时，对应的日志才会被 purge 线程统一清除 每次对数据库记录进行改动，都会产生的新版本的 undo log，随着更新次数的增多，所有的版本都会被 roll_pointer 属性连接成一个链表，把这个链表称之为版本链，版本链的头节点就是当前的最新的 undo log，链尾就是最早的旧 undo log 说明：因为 DELETE 删除记录，都是移动到垃圾链表中，不是真正的删除，所以才可以通过版本链访问原始数据 注意：undo 是逻辑日志，这里只是直观的展示出来 工作流程： 有个事务插入 persion 表一条新记录，name 为 Jerry，age 为 24 事务 1 修改该行数据时，数据库会先对该行加排他锁，然后先记录 undo log，然后修改该行 name 为 Tom，并且修改隐藏字段的事务 ID 为当前事务 1 的 ID（默认为 1 之后递增），回滚指针指向拷贝到 undo log 的副本记录，事务提交后，释放锁 以此类推 读视图Read View 是事务进行读数据操作时产生的读视图，该事务执行快照读的那一刻会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的 ID，用来做可见性判断，根据视图判断当前事务能够看到哪个版本的数据 注意：这里的快照并不是把所有的数据拷贝一份副本，而是由 undo log 记录的逻辑日志，根据库中的数据进行计算出历史数据 工作流程：将版本链的头节点的事务 ID（最新数据事务 ID，大概率不是当前线程）DB_TRX_ID 取出来，与系统当前活跃事务的 ID 对比进行可见性分析，不可见就通过 DB_ROLL_PTR 回滚指针去取出 undo log 中的下一个 DB_TRX_ID 比较，直到找到最近的满足可见性的 DB_TRX_ID，该事务 ID 所在的旧记录就是当前事务能看见的最新的记录 Read View 几个属性： m_ids：生成 Read View 时当前系统中活跃的事务 id 列表（未提交的事务集合，当前事务也在其中） min_trx_id：生成 Read View 时当前系统中活跃的最小的事务 id，也就是 m_ids 中的最小值（已提交的事务集合） max_trx_id：生成 Read View 时当前系统应该分配给下一个事务的 id 值，m_ids 中的最大值加 1（未开始事务） creator_trx_id：生成该 Read View 的事务的事务 id，就是判断该 id 的事务能读到什么数据 creator 创建一个 Read View，进行可见性算法分析：（解决了读未提交） db_trx_id &#x3D;&#x3D; creator_trx_id：表示这个数据就是当前事务自己生成的，自己生成的数据自己肯定能看见，所以此数据对 creator 是可见的 db_trx_id &lt; min_trx_id：该版本对应的事务 ID 小于 Read view 中的最小活跃事务 ID，则这个事务在当前事务之前就已经被提交了，对 creator 可见（因为比已提交的最大事务 ID 小的并不一定已经提交，所以应该判断是否在活跃事务列表） db_trx_id &gt;&#x3D; max_trx_id：该版本对应的事务 ID 大于 Read view 中当前系统的最大事务 ID，则说明该数据是在当前 Read view 创建之后才产生的，对 creator 不可见 min_trx_id&lt;&#x3D; db_trx_id &lt; max_trx_id：判断 db_trx_id 是否在活跃事务列表 m_ids 中 在列表中，说明该版本对应的事务正在运行，数据不能显示（不能读到未提交的数据） 不在列表中，说明该版本对应的事务已经被提交，数据可以显示（可以读到已经提交的数据） 工作流程表 user 数据 12id name age1 张三 18 Transaction 20： 123START TRANSACTION;\t-- 开启事务UPDATE user SET name = &#x27;李四&#x27; WHERE id = 1;UPDATE user SET name = &#x27;王五&#x27; WHERE id = 1; Transaction 60： 12START TRANSACTION;\t-- 开启事务-- 操作表的其他数据 ID 为 0 的事务创建 Read View： m_ids：20、60 min_trx_id：20 max_trx_id：61 creator_trx_id：0 只有红框部分才复合条件，所以只有张三对应的版本的数据可以被看到 参考视频：https://www.bilibili.com/video/BV1t5411u7Fg 二级索引只有在聚簇索引中才有 trx_id 和 roll_pointer 的隐藏列，对于二级索引判断可见性的方式： 二级索引页面的 Page Header 中有一个 PAGE_MAX_TRX_ID 属性，代表修改当前页面的最大的事务 ID，SELECT 语句访问某个二级索引时会判断 ReadView 的 min_trx_id 是否大于该属性，大于说明该页面的所有属性对 ReadView 可见 如果属性判断不可见，就需要利用二级索引获取主键值进行回表操作，得到聚簇索引后按照聚簇索引的可见性判断的方法操作 RC RRRead View 用于支持 RC（Read Committed，读已提交）和 RR（Repeatable Read，可重复读）隔离级别的实现，所以 SELECT 在 RC 和 RR 隔离级别使用 MVCC 读取记录 RR、RC 生成时机： RC 隔离级别下，每次读取数据前都会生成最新的 Read View（当前读） RR 隔离级别下，在第一次数据读取时才会创建 Read View（快照读） RC、RR 级别下的 InnoDB 快照读区别 RC 级别下，事务中每次快照读都会新生成一个 Read View，这就是在 RC 级别下的事务中可以看到别的事务提交的更新的原因 RR 级别下，某个事务的对某条记录的第一次快照读会创建一个 Read View， 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，使用的是同一个 Read View，所以一个事务的查询结果每次都是相同的 RR 级别下，通过 START TRANSACTION WITH CONSISTENT SNAPSHOT 开启事务，会在执行该语句后立刻生成一个 Read View，不是在执行第一条 SELECT 语句时生成（所以说 START TRANSACTION 并不是事务的起点，执行第一条语句才算起点） 解决幻读问题： 快照读：通过 MVCC 来进行控制的，在可重复读隔离级别下，普通查询是快照读，是不会看到别的事务插入的数据的，但是并不能完全避免幻读 场景：RR 级别，T1 事务开启，创建 Read View，此时 T2 去 INSERT 新的一行然后提交，然后 T1 去 UPDATE 该行会发现更新成功，并且把这条新记录的 trx_id 变为当前的事务 id，所以对当前事务就是可见的。因为 Read View 并不能阻止事务去更新数据，更新数据都是先读后写并且是当前读，读取到的是最新版本的数据 当前读：通过 next-key 锁（行锁 + 间隙锁）来解决问题 持久特性实现方式持久性是指一个事务一旦被提交了，那么对数据库中数据的改变就是永久性的，接下来的其他操作或故障不应该对其有任何影响。 Buffer Pool 的使用提高了读写数据的效率，但是如果 MySQL 宕机，此时 Buffer Pool 中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证，所以引入了 redo log 日志： redo log 记录数据页的物理修改，而不是某一行或某几行的修改，用来恢复提交后的数据页，只能恢复到最后一次提交的位置 redo log 采用的是 WAL（Write-ahead logging，预写式日志），所有修改要先写入日志，再更新到磁盘，保证了数据不会因 MySQL 宕机而丢失，从而满足了持久性要求 简单的 redo log 是纯粹的物理日志，复杂的 redo log 会存在物理日志和逻辑日志 工作过程：MySQL 发生了宕机，InnoDB 会判断一个数据页在崩溃恢复时丢失了更新，就会将它读到内存，然后根据 redo log 内容更新内存，更新完成后，内存页变成脏页，然后进行刷脏 缓冲池的刷脏策略： redo log 文件是固定大小的，如果写满了就要擦除以前的记录，在擦除之前需要把对应的更新持久化到磁盘中 Buffer Pool 内存不足，需要淘汰部分数据页（LRU 链表尾部），如果淘汰的是脏页，就要先将脏页写到磁盘（要避免大事务） 系统空闲时，后台线程会自动进行刷脏（Flush 链表部分已经详解） MySQL 正常关闭时，会把内存的脏页都刷新到磁盘上 重做日志日志缓冲服务器启动时会向操作系统申请一片连续内存空间作为 redo log buffer（重做日志缓冲区），可以通过 innodb_log_buffer_size 系统变量指定 redo log buffer 的大小，默认是 16MB log buffer 被划分为若干 redo log block（块，类似数据页的概念），每个默认大小 512 字节，每个 block 由 12 字节的 log block head、496 字节的 log block body、4 字节的 log block trailer 组成 当数据修改时，先修改 Change Buffer 中的数据，然后在 redo log buffer 记录这次操作，写入 log buffer 的过程是顺序写入的（先写入前面的 block，写满后继续写下一个） log buffer 中有一个指针 buf_free，来标识该位置之前都是填满的 block，该位置之后都是空闲区域 MySQL 规定对底层页面的一次原子访问称为一个 Mini-Transaction（MTR），比如在 B+ 树上插入一条数据就算一个 MTR 一个事务包含若干个 MTR，一个 MTR 对应一组若干条 redo log，一组 redo log 是不可分割的，在进行数据恢复时也把一组 redo log 当作一个不可分割的整体处理 不是每生成一条 redo 日志就将其插入到 log buffer 中，而是一个 MTR 结束后将一组 redo 日志写入 InnoDB 的 redo log 是固定大小的，redo 日志在磁盘中以文件组的形式存储，同一组中的每个文件大小一样格式一样 innodb_log_group_home_dir 代表磁盘存储 redo log 的文件目录，默认是当前数据目录 innodb_log_file_size 代表文件大小，默认 48M，innodb_log_files_in_group 代表文件个数，默认 2 最大 100，所以日志的文件大小为 innodb_log_file_size * innodb_log_files_in_group redo 日志文件也是由若干个 512 字节的 block 组成，日志文件的前 2048 个字节（前 4 个 block）用来存储一些管理信息，以后的用来存储 log buffer 中的 block 镜像 注意：block 并不代表一组 redo log，一组日志可能占用不到一个 block 或者几个 block，依赖于 MTR 的大小 日志刷盘redo log 需要在事务提交时将日志写入磁盘，但是比 Buffer Pool 修改的数据写入磁盘的速度快，原因： 刷脏是随机 IO，因为每次修改的数据位置随机；redo log 和 binlog 都是顺序写，磁盘的顺序 IO 比随机 IO 速度要快 刷脏是以数据页（Page）为单位的，一个页上的一个小修改都要整页写入；redo log 中只包含真正需要写入的部分，好几页的数据修改可能只记录在一个 redo log 页中，减少无效 IO 组提交机制，可以大幅度降低磁盘的 IO 消耗 InnoDB 引擎会在适当的时候，把内存中 redo log buffer 持久化（fsync）到磁盘，具体的刷盘策略： 在事务提交时需要进行刷盘，通过修改参数 innodb_flush_log_at_trx_commit 设置： 0：表示当提交事务时，并不将缓冲区的 redo 日志写入磁盘，而是等待后台线程每秒刷新一次 1：在事务提交时将缓冲区的 redo 日志同步写入到磁盘，保证一定会写入成功（默认值） 2：在事务提交时将缓冲区的 redo 日志异步写入到磁盘，不能保证提交时肯定会写入，只是有这个动作。日志已经在操作系统的缓存，如果操作系统没有宕机而 MySQL 宕机，也是可以恢复数据的 写入 redo log buffer 的日志超过了总容量的一半，就会将日志刷入到磁盘文件，这会影响执行效率，所以开发中应避免大事务 服务器关闭时 并行的事务提交（组提交）时，会将将其他事务的 redo log 持久化到磁盘。假设事务 A 已经写入 redo log buffer 中，这时另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么事务 B 要把 redo log buffer 里的日志全部持久化到磁盘，因为多个事务共用一个 redo log buffer，所以一次 fsync 可以刷盘多个事务的 redo log，提升了并发量 服务器启动后 redo 磁盘空间不变，所以 redo 磁盘中的日志文件是被循环使用的，采用循环写数据的方式，写完尾部重新写头部，所以要确保头部 log 对应的修改已经持久化到磁盘 日志序号lsn (log sequence number) 代表已经写入的 redo 日志量、flushed_to_disk_lsn 指刷新到磁盘中的 redo 日志量，两者都是全局变量，如果两者的值相同，说明 log buffer 中所有的 redo 日志都已经持久化到磁盘 工作过程：写入 log buffer 数据时，buf_free 会进行偏移，偏移量就会加到 lsn 上 MTR 的执行过程中修改过的页对应的控制块会加到 Buffer Pool 的 flush 链表中，链表中脏页是按照第一次修改的时间进行排序的（头插），控制块中有两个指针用来记录脏页被修改的时间： oldest_modification：第一次修改 Buffer Pool 中某个缓冲页时，将修改该页的 MTR 开始时对应的 lsn 值写入这个属性 newest_modification：每次修改页面，都将 MTR 结束时全局的 lsn 值写入这个属性，所以该值是该页面最后一次修改后的 lsn 值 全局变量 checkpoint_lsn 表示当前系统可以被覆盖的 redo 日志总量，当 redo 日志对应的脏页已经被刷新到磁盘后，该文件空间就可以被覆盖重用，此时执行一次 checkpoint 来更新 checkpoint_lsn 的值存入管理信息（刷脏和执行一次 checkpoint 并不是同一个线程），该值的增量就代表磁盘文件中当前位置向后可以被覆盖的文件的量，所以该值是一直增大的 checkpoint：从 flush 链表尾部中找出还未刷脏的页面，该页面是当前系统中最早被修改的脏页，该页面之前产生的脏页都已经刷脏，然后将该页 oldest_modification 值赋值给 checkpoint_lsn，因为 lsn 小于该值时产生的 redo 日志都可以被覆盖了 但是在系统忙碌时，后台线程的刷脏操作不能将脏页快速刷出，导致系统无法及时执行 checkpoint ，这时需要用户线程从 flush 链表中把最早修改的脏页刷新到磁盘中，然后执行 checkpoint 1write pos ------- checkpoint_lsn // 两值之间的部分表示可以写入的日志量，当 pos 追赶上 lsn 时必须执行 checkpoint 使用命令可以查看当前 InnoDB 存储引擎各种 lsn 的值： 1SHOW ENGINE INNODB STATUS\\G 崩溃恢复恢复的起点：在从 redo 日志文件组的管理信息中获取最近发生 checkpoint 的信息，从 checkpoint_lsn 对应的日志文件开始恢复 恢复的终点：扫描日志文件的 block，block 的头部记录着当前 block 使用了多少字节，填满的 block 总是 512 字节， 如果某个 block 不是 512 字节，说明该 block 就是需要恢复的最后一个 block 恢复的过程：按照 redo log 依次执行恢复数据，优化方式 使用哈希表：根据 redo log 的 space id 和 page number 属性计算出哈希值，将对同一页面的修改放入同一个槽里，可以一次性完成对某页的恢复，避免了随机 IO 跳过已经刷新到磁盘中的页面：数据页的 File Header 中的 FILE_PAGE_LSN 属性（类似 newest_modification）表示最近一次修改页面时的 lsn 值，数据页被刷新到磁盘中，那么该页 lsn 属性肯定大于 checkpoint_lsn 参考书籍：https://book.douban.com/subject/35231266/ 工作流程日志对比MySQL 中还存在 binlog（二进制日志）也可以记录写操作并用于数据的恢复，保证数据不丢失，二者的区别是： 作用不同：redo log 是用于 crash recovery （故障恢复），保证 MySQL 宕机也不会影响持久性；binlog 是用于 point-in-time recovery 的，保证服务器可以基于时间点恢复数据，此外 binlog 还用于主从复制 层次不同：redo log 是 InnoDB 存储引擎实现的，而 binlog 是MySQL的 Server 层实现的，同时支持 InnoDB 和其他存储引擎 内容不同：redo log 是物理日志，内容基于磁盘的 Page；binlog 的内容是二进制的，根据 binlog_format 参数的不同，可能基于SQL 语句、基于数据本身或者二者的混合（日志部分详解） 写入时机不同：binlog 在事务提交时一次写入；redo log 的写入时机相对多元 binlog 为什么不支持崩溃恢复？ binlog 记录的是语句，并不记录数据页级的数据（哪个页改了哪些地方），所以没有能力恢复数据页 binlog 是追加写，保存全量的日志，没有标志确定从哪个点开始的数据是已经刷盘了，而 redo log 只要在 checkpoint_lsn 后面的就是没有刷盘的 更新记录更新一条记录的过程：写之前一定先读 在 B+ 树中定位到该记录，如果该记录所在的页面不在 Buffer Pool 里，先将其加载进内存 首先更新该记录对应的聚簇索引，更新聚簇索引记录时： 更新记录前向 undo 页面写 undo 日志，由于这是更改页面，所以需要记录一下相应的 redo 日志 注意：修改 undo 页面也是在修改页面，事务只要修改页面就需要先记录相应的 redo 日志 然后记录对应的 redo 日志（等待 MTR 提交后写入 redo log buffer），最后进行真正的更新记录 更新其他的二级索引记录，不会再记录 undo log，只记录 redo log 到 buffer 中 在一条更新语句执行完成后（也就是将所有待更新记录都更新完了），就会开始记录该语句对应的 binlog 日志，此时记录的 binlog 并没有刷新到硬盘上，还在内存中，在事务提交时才会统一将该事务运行过程中的所有 binlog 日志刷新到硬盘 假设表中有字段 id 和 a，存在一条 id = 1, a = 2 的记录，此时执行更新语句： 1update table set a=2 where id=1; InnoDB 会真正的去执行把值修改成 (1,2) 这个操作，先加行锁，在去更新，并不会提前判断相同就不修改了 参考文章：https://mp.weixin.qq.com/s/wcJ2KisSaMnfP4nH5NYaQA 两段提交当客户端执行 COMMIT 语句或者在自动提交的情况下，MySQL 内部开启一个 XA 事务，分两阶段来完成 XA 事务的提交： 1update T set c=c+1 where ID=2; 流程说明：执行引擎将这行新数据读入到内存中（Buffer Pool）后，先将此次更新操作记录到 redo log buffer 里，然后更新记录。最后将 redo log 刷盘后事务处于 prepare 状态，执行器会生成这个操作的 binlog，并把 binlog 写入磁盘，完成提交 两阶段： Prepare 阶段：存储引擎将该事务的 redo 日志刷盘，并且将本事务的状态设置为 PREPARE，代表执行完成随时可以提交事务 Commit 阶段：先将事务执行过程中产生的 binlog 刷新到硬盘，再执行存储引擎的提交工作，引擎把 redo log 改成提交状态 存储引擎层的 redo log 和 server 层的 binlog 可以认为是一个分布式事务， 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致，也有利于主从复制，更好的保持主从数据的一致性 数据恢复系统崩溃前没有提交的事务的 redo log 可能已经刷盘（定时线程或者 checkpoint），怎么处理崩溃恢复？ 工作流程：获取 undo 链表首节点页面的 undo segement header 中的 TRX_UNDO_STATE 属性，表示当前链表的事务属性，事务状态是活跃（未提交）的就全部回滚，如果是 PREPARE 状态，就需要根据 binlog 的状态进行判断： 如果在时刻 A 发生了崩溃（crash），由于此时 binlog 还没完成，所以需要进行回滚 如果在时刻 B 发生了崩溃，redo log 和 binlog 有一个共同的数据字段叫 XID，崩溃恢复的时候，会按顺序扫描 redo log： 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，说明 binlog 也已经记录完整，直接从 redo log 恢复数据 如果 redo log 里面的事务只有 prepare，就根据 XID 去 binlog 中判断对应的事务是否存在并完整，如果完整可以恢复数据 判断一个事务的 binlog 是否完整的方法： statement 格式的 binlog，最后会有 COMMIT row 格式的 binlog，最后会有一个 XID event MySQL 5.6.2 版本以后，引入了 binlog-checksum 参数用来验证 binlog 内容的正确性（可能日志中间出错） 参考文章：https://time.geekbang.org/column/article/73161 刷脏优化系统在进行刷脏时会占用一部分系统资源，会影响系统的性能，产生系统抖动 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的 InnoDB 刷脏页的控制策略： innodb_io_capacity 参数代表磁盘的读写能力，建议设置成磁盘的 IOPS（每秒的 IO 次数） 刷脏速度参考两个因素：脏页比例和 redo log 写盘速度 参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%，InnoDB 会根据当前的脏页比例，算出一个范围在 0 到 100 之间的数字 InnoDB 每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间的差值，InnoDB 根据差值算出一个范围在 0 到 100 之间的数字 两者较大的值记为 R，执行引擎按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度 innodb_flush_neighbors 参数置为 1 代表控制刷脏时检查相邻的数据页，如果也是脏页就一起刷脏，并检查邻居的邻居，这个行为会一直蔓延直到不是脏页，在 MySQL 8.0 中该值的默认值是 0，不建议开启此功能 一致特性一致性是指事务执行前后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。 数据库的完整性约束包括但不限于：实体完整性（如行的主键存在且唯一）、列完整性（如字段的类型、大小、长度要符合要求）、外键约束、用户自定义完整性（如转账前后，两个账户余额的和应该不变） 实现一致性的措施： 保证原子性、持久性和隔离性，如果这些特性无法保证，事务的一致性也无法保证 数据库本身提供保障，例如不允许向整形列插入字符串值、字符串长度不能超过列的限制等 应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致 锁机制基本介绍锁机制：数据库为了保证数据的一致性，在共享的资源被并发访问时变得安全有序所设计的一种规则 利用 MVCC 性质进行读取的操作叫一致性读，读取数据前加锁的操作叫锁定读 锁的分类： 按操作分类： 共享锁：也叫读锁。对同一份数据，多个事务读操作可以同时加锁而不互相影响 ，但不能修改数据 排他锁：也叫写锁。当前的操作没有完成前，会阻断其他操作的读取和写入 按粒度分类： 表级锁：会锁定整个表，开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低，偏向 MyISAM 行级锁：会锁定当前操作行，开销大，加锁慢；会出现死锁；锁定力度小，发生锁冲突概率低，并发度高，偏向 InnoDB 页级锁：锁的力度、发生冲突的概率和加锁开销介于表锁和行锁之间，会出现死锁，并发性能一般 按使用方式分类： 悲观锁：每次查询数据时都认为别人会修改，很悲观，所以查询时加锁 乐观锁：每次查询数据时都认为别人不会修改，很乐观，但是更新时会判断一下在此期间别人有没有去更新这个数据 不同存储引擎支持的锁 存储引擎 表级锁 行级锁 页级锁 MyISAM 支持 不支持 不支持 InnoDB 支持 支持 不支持 MEMORY 支持 不支持 不支持 BDB 支持 不支持 支持 从锁的角度来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如 Web 应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并查询的应用，如一些在线事务处理系统 内存结构对一条记录加锁的本质就是在内存中创建一个锁结构与之关联，结构包括 事务信息：锁对应的事务信息，一个锁属于一个事务 索引信息：对于行级锁，需要记录加锁的记录属于哪个索引 表锁和行锁信息：表锁记录着锁定的表，行锁记录了 Space ID 所在表空间、Page Number 所在的页号、n_bits 使用了多少比特 type_mode：一个 32 比特的数，被分成 lock_mode、lock_type、rec_lock_type 三个部分 lock_mode：锁模式，记录是共享锁、排他锁、意向锁之类 lock_type：代表表级锁还是行级锁 rec_lock_type：代表行锁的具体类型和 is_waiting 属性，is_waiting &#x3D; true 时表示当前事务尚未获取到锁，处于等待状态。事务获取锁后的锁结构是 is_waiting 为 false，释放锁时会检查是否与当前记录关联的锁结构，如果有就唤醒对应事务的线程 一个事务可能操作多条记录，为了节省内存，满足下面条件的锁使用同一个锁结构： 在同一个事务中的加锁操作 被加锁的记录在同一个页面中 加锁的类型是一样的 加锁的状态是一样的 ServerMySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL) MDL 叫元数据锁，主要用来保护 MySQL 内部对象的元数据，保证数据读写的正确性，当对一个表做增删改查的时候，加 MDL 读锁；当要对表做结构变更操作 DDL 的时候，加 MDL 写锁，两种锁不相互兼容，所以可以保证 DDL、DML、DQL 操作的安全 说明：DDL 操作执行前会隐式提交当前会话的事务，因为 DDL 一般会在若干个特殊事务中完成，开启特殊事务前需要提交到其他事务 MDL 锁的特性： MDL 锁不需要显式使用，在访问一个表的时候会被自动加上，在事务开始时申请，整个事务提交后释放（执行完单条语句不释放） MDL 锁是在 Server 中实现，不是 InnoDB 存储引擎层能直接实现的锁 MDL 锁还能实现其他粒度级别的锁，比如全局锁、库级别的锁、表空间级别的锁 FLUSH TABLES WITH READ LOCK 简称（FTWRL），全局读锁，让整个库处于只读状态，DDL DML 都被阻塞，工作流程： 上全局读锁（lock_global_read_lock） 清理表缓存（close_cached_tables） 上全局 COMMIT 锁（make_global_read_lock_block_commit） 该命令主要用于备份工具做一致性备份，由于 FTWRL 需要持有两把全局的 MDL 锁，并且还要关闭所有表对象，因此杀伤性很大 MyISAM表级锁MyISAM 存储引擎只支持表锁，这也是 MySQL 开始几个版本中唯一支持的锁类型 MyISAM 引擎在执行查询语句之前，会自动给涉及到的所有表加读锁，在执行增删改之前，会自动给涉及的表加写锁，这个过程并不需要用户干预，所以用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁 加锁命令：（对 InnoDB 存储引擎也适用） 读锁：所有连接只能读取数据，不能修改 写锁：其他连接不能查询和修改数据 12345-- 读锁LOCK TABLE table_name READ;-- 写锁LOCK TABLE table_name WRITE; 解锁命令： 12-- 将当前会话所有的表进行解锁UNLOCK TABLES; 锁的兼容性： 对 MyISAM 表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求 对 MyISAM 表的写操作，则会阻塞其他用户对同一表的读和写操作 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-MyISAM 锁的兼容性.png) 锁调度：MyISAM 的读写锁调度是写优先，因为写锁后其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞，所以 MyISAM 不适合做写为主的表的存储引擎 锁操作读锁两个客户端操作 Client 1和 Client 2，简化为 C1、C2 数据准备： 12345678910CREATE TABLE `tb_book` ( `id` INT(11) AUTO_INCREMENT, `name` VARCHAR(50) DEFAULT NULL, `publish_time` DATE DEFAULT NULL, `status` CHAR(1) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=MYISAM DEFAULT CHARSET=utf8 ;INSERT INTO tb_book (id, NAME, publish_time, STATUS) VALUES(NULL,&#x27;java编程思想&#x27;,&#x27;2088-08-01&#x27;,&#x27;1&#x27;);INSERT INTO tb_book (id, NAME, publish_time, STATUS) VALUES(NULL,&#x27;mysql编程思想&#x27;,&#x27;2088-08-08&#x27;,&#x27;0&#x27;); C1、C2 加读锁，同时查询可以正常查询出数据 12LOCK TABLE tb_book READ;\t-- C1、C2SELECT * FROM tb_book; -- C1、C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-MyISAM 读锁1.png) C1 加读锁，C1、C2 查询未锁定的表，C1 报错，C2 正常查询 12LOCK TABLE tb_book READ;\t-- C1SELECT * FROM tb_user; -- C1、C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-MyISAM 读锁2.png) C1、C2 执行插入操作，C1 报错，C2 等待获取 1INSERT INTO tb_book VALUES(NULL,&#x27;Spring高级&#x27;,&#x27;2088-01-01&#x27;,&#x27;1&#x27;);\t-- C1、C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-MyISAM 读锁3.png) 当在 C1 中释放锁指令 UNLOCK TABLES，C2 中的 INSERT 语句立即执行 写锁两个客户端操作 Client 1和 Client 2，简化为 C1、C2 C1 加写锁，C1、C2查询表，C1 正常查询，C2 需要等待 12LOCK TABLE tb_book WRITE;\t-- C1SELECT * FROM tb_book; -- C1、C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-MyISAM 写锁1.png) 当在 C1 中释放锁指令 UNLOCK TABLES，C2 中的 SELECT 语句立即执行 C1、C2 同时加写锁 1LOCK TABLE tb_book WRITE; ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-MyISAM 写锁2.png) C1 加写锁，C1、C2查询未锁定的表，C1 报错，C2 正常查询 锁状态 查看锁竞争： 1SHOW OPEN TABLES; In_user：表当前被查询使用的次数，如果该数为零，则表是打开的，但是当前没有被使用 Name_locked：表名称是否被锁定，名称锁定用于取消表或对表进行重命名等操作 1LOCK TABLE tb_book READ;\t-- 执行命令 查看锁状态： 1SHOW STATUS LIKE &#x27;Table_locks%&#x27;; ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-MyISAM 锁状态.png) Table_locks_immediate：指的是能立即获得表级锁的次数，每立即获取锁，值加 1 Table_locks_waited：指的是不能立即获取表级锁而需要等待的次数，每等待一次，该值加 1，此值高说明存在着较为严重的表级锁争用情况 InnoDB行级锁记录锁InnoDB 与 MyISAM 的最大不同有两点：一是支持事务；二是采用了行级锁，InnoDB 同时支持表锁和行锁 行级锁，也称为记录锁（Record Lock），InnoDB 实现了以下两种类型的行锁： 共享锁 (S)：又称为读锁，简称 S 锁，多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改 排他锁 (X)：又称为写锁，简称 X 锁，不能与其他锁并存，获取排他锁的事务是可以对数据读取和修改 RR 隔离界别下，对于 UPDATE、DELETE 和 INSERT 语句，InnoDB 会自动给涉及数据集加排他锁（行锁），在 commit 时自动释放；对于普通 SELECT 语句，不会加任何锁（只是针对 InnoDB 层来说的，因为在 Server 层会加 MDL 读锁），通过 MVCC 防止并发冲突 在事务中加的锁，并不是不需要了就释放，而是在事务中止或提交时自动释放，这个就是两阶段锁协议。所以一般将更新共享资源（并发高）的 SQL 放到事务的最后执行，可以让其他线程尽量的减少等待时间 锁的兼容性： 共享锁和共享锁 兼容 共享锁和排他锁 冲突 排他锁和排他锁 冲突 排他锁和共享锁 冲突 显式给数据集加共享锁或排他锁：加锁读就是当前读，读取的是最新数据 12SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE\t-- 共享锁SELECT * FROM table_name WHERE ... FOR UPDATE -- 排他锁 注意：锁默认会锁聚簇索引（锁就是加在索引上），但是当使用覆盖索引时，加共享锁只锁二级索引，不锁聚簇索引 锁操作两个客户端操作 Client 1和 Client 2，简化为 C1、C2 环境准备 1234567891011CREATE TABLE test_innodb_lock(\tid INT(11),\tname VARCHAR(16),\tsex VARCHAR(1))ENGINE = INNODB DEFAULT CHARSET=utf8;INSERT INTO test_innodb_lock VALUES(1,&#x27;100&#x27;,&#x27;1&#x27;);-- ..........CREATE INDEX idx_test_innodb_lock_id ON test_innodb_lock(id);CREATE INDEX idx_test_innodb_lock_name ON test_innodb_lock(name); 关闭自动提交功能： 1SET AUTOCOMMIT=0;\t-- C1、C2 正常查询数据： 1SELECT * FROM test_innodb_lock;\t-- C1、C2 查询 id 为 3 的数据，正常查询： 1SELECT * FROM test_innodb_lock WHERE id=3;\t-- C1、C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-InnoDB 锁操作1.png) C1 更新 id 为 3 的数据，但不提交： 1UPDATE test_innodb_lock SET name=&#x27;300&#x27; WHERE id=3;\t-- C1 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-InnoDB 锁操作2.png) C2 查询不到 C1 修改的数据，因为隔离界别为 REPEATABLE READ，C1 提交事务，C2 查询： 1COMMIT;\t-- C1 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-InnoDB 锁操作3.png) 提交后仍然查询不到 C1 修改的数据，因为隔离级别可以防止脏读、不可重复读，所以 C2 需要提交才可以查询到其他事务对数据的修改： 12COMMIT;\t-- C2SELECT * FROM test_innodb_lock WHERE id=3;\t-- C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-InnoDB 锁操作4.png) C1 更新 id 为 3 的数据，但不提交，C2 也更新 id 为 3 的数据： 12UPDATE test_innodb_lock SET name=&#x27;3&#x27; WHERE id=3;\t-- C1UPDATE test_innodb_lock SET name=&#x27;30&#x27; WHERE id=3;\t-- C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-InnoDB 锁操作5.png) 当 C1 提交，C2 直接解除阻塞，直接更新 操作不同行的数据： 12UPDATE test_innodb_lock SET name=&#x27;10&#x27; WHERE id=1;\t-- C1UPDATE test_innodb_lock SET name=&#x27;30&#x27; WHERE id=3;\t-- C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-InnoDB 锁操作6.png) 由于 C1、C2 操作的不同行，获取不同的行锁，所以都可以正常获取行锁 锁分类间隙锁InnoDB 会对间隙（GAP）进行加锁，就是间隙锁 （RR 隔离级别下才有该锁）。间隙锁之间不存在冲突关系，多个事务可以同时对一个间隙加锁，但是间隙锁会阻止往这个间隙中插入一个记录的操作 InnoDB 加锁的基本单位是 next-key lock，该锁是行锁和 gap lock 的组合（X or S 锁），但是加锁过程是分为间隙锁和行锁两段执行 可以保护当前记录和前面的间隙，遵循左开右闭原则，单纯的间隙锁是左开右开 假设有 10、11、13，那么可能的间隙锁包括：(负无穷,10]、(10,11]、(11,13]、(13,正无穷) 几种索引的加锁情况： 唯一索引加锁在值存在时是行锁，next-key lock 会退化为行锁，值不存在会变成间隙锁 普通索引加锁会继续向右遍历到不满足条件的值为止，next-key lock 退化为间隙锁 范围查询无论是否是唯一索引，都需要访问到不满足条件的第一个值为止 对于联合索引且是唯一索引，如果 where 条件只包括联合索引的一部分，那么会加间隙锁 间隙锁优点：RR 级别下间隙锁可以解决事务的一部分的幻读问题，通过对间隙加锁，可以防止读取过程中数据条目发生变化。一部分的意思是不会对全部间隙加锁，只能加锁一部分的间隙 间隙锁危害： 当锁定一个范围的键值后，即使某些不存在的键值也会被无辜的锁定，造成在锁定的时候无法插入锁定键值范围内的任何数据，在某些场景下这可能会对性能造成很大的危害，影响并发度 事务 A B 同时锁住一个间隙后，A 往当前间隙插入数据时会被 B 的间隙锁阻塞，B 也执行插入间隙数据的操作时就会产生死锁 现场演示： 关闭自动提交功能： 1SET AUTOCOMMIT=0;\t-- C1、C2 查询数据表： 1SELECT * FROM test_innodb_lock; ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-InnoDB 间隙锁1.png) C1 根据 id 范围更新数据，C2 插入数据： 12UPDATE test_innodb_lock SET name=&#x27;8888&#x27; WHERE id &lt; 4;\t-- C1INSERT INTO test_innodb_lock VALUES(2,&#x27;200&#x27;,&#x27;2&#x27;); -- C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-InnoDB 间隙锁2.png) 出现间隙锁，C2 被阻塞，等待 C1 提交事务后才能更新 意向锁InnoDB 为了支持多粒度的加锁，允许行锁和表锁同时存在，支持在不同粒度上的加锁操作，InnoDB 增加了意向锁（Intention Lock） 意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度上进行加锁，意向锁分为两种： 意向共享锁（IS）：事务有意向对表加共享锁 意向排他锁（IX）：事务有意向对表加排他锁 IX，IS 是表级锁，不会和行级的 X，S 锁发生冲突，意向锁是在加表级锁之前添加，为了在加表级锁时可以快速判断表中是否有记录被上锁，比如向一个表添加表级 X 锁的时： 没有意向锁，则需要遍历整个表判断是否有锁定的记录 有了意向锁，首先判断是否存在意向锁，然后判断该意向锁与即将添加的表级锁是否兼容即可，因为意向锁的存在代表有表级锁的存在或者即将有表级锁的存在 兼容性如下所示： 插入意向锁 Insert Intention Lock 是在插入一行记录操作之前设置的一种间隙锁，是行级锁 插入意向锁释放了一种插入信号，即多个事务在相同的索引间隙插入时如果不是插入相同的间隙位置就不需要互相等待。假设某列有索引，只要两个事务插入位置不同，如事务 A 插入 3，事务 B 插入 4，那么就可以同时插入 自增锁系统会自动给 AUTO_INCREMENT 修饰的列进行递增赋值，实现方式： AUTO_INC 锁：表级锁，执行插入语句时会自动添加，在该语句执行完成后释放，并不是事务结束 轻量级锁：为插入语句生成 AUTO_INCREMENT 修饰的列时获取该锁，生成以后释放掉，不需要等到插入语句执行完后释放 系统变量 innodb_autoinc_lock_mode 控制采取哪种方式： 0：全部采用 AUTO_INC 锁 1：全部采用轻量级锁 2：混合使用，在插入记录的数量确定时采用轻量级锁，不确定时采用 AUTO_INC 锁 隐式锁一般情况下 INSERT 语句是不需要在内存中生成锁结构的，会进行隐式的加锁，保护的是插入后的安全 注意：如果插入的间隙被其他事务加了间隙锁，此次插入会被阻塞，并在该间隙插入一个插入意向锁 聚簇索引：索引记录有 trx_id 隐藏列，表示最后改动该记录的事务 id，插入数据后事务 id 就是当前事务。其他事务想获取该记录的锁时会判断当前记录的事务 id 是否是活跃的，如果不是就可以正常加锁；如果是就创建一个 X 的锁结构，该锁的 is_waiting 是 false，为自己的事务创建一个锁结构，is_waiting 是 true（类似 Java 中的锁升级） 二级索引：获取数据页 Page Header 中的 PAGE_MAX_TRX_ID 属性，代表修改当前页面的最大的事务 ID，如果小于当前活跃的最小事务 id，就证明插入该数据的事务已经提交，否则就需要获取到主键值进行回表操作 隐式锁起到了延迟生成锁的效果，如果其他事务与隐式锁没有冲突，就可以避免锁结构的生成，节省了内存资源 INSERT 在两种情况下会生成锁结构： 重复键：在插入主键或唯一二级索引时遇到重复的键值会报错，在报错前需要对对应的聚簇索引进行加锁 隔离级别 &lt;&#x3D; Read Uncommitted，加 S 型 Record Lock 隔离级别 &gt;&#x3D; Repeatable Read，加 S 型 next_key 锁 外键检查：如果待插入的记录在父表中可以找到，会对父表的记录加 S 型 Record Lock。如果待插入的记录在父表中找不到 隔离级别 &lt;&#x3D; Read Committed，不加锁 隔离级别 &gt;&#x3D; Repeatable Read，加间隙锁 锁优化优化锁InnoDB 存储引擎实现了行级锁定，虽然在锁定机制的实现方面带来了性能损耗可能比表锁会更高，但是在整体并发处理能力方面要远远优于 MyISAM 的表锁，当系统并发量较高的时候，InnoDB 的整体性能远远好于 MyISAM 但是使用不当可能会让 InnoDB 的整体性能表现不仅不能比 MyISAM 高，甚至可能会更差 优化建议： 尽可能让所有数据检索都能通过索引来完成，避免无索引行锁升级为表锁 合理设计索引，尽量缩小锁的范围 尽可能减少索引条件及索引范围，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 尽可使用低级别事务隔离（需要业务层面满足需求） 锁升级索引失效造成行锁升级为表锁，不通过索引检索数据，全局扫描的过程中 InnoDB 会将对表中的所有记录加锁，实际效果和表锁一样，实际开发过程应避免出现索引失效的状况 查看当前表的索引： 1SHOW INDEX FROM test_innodb_lock; 关闭自动提交功能： 1SET AUTOCOMMIT=0;\t-- C1、C2 执行更新语句： 12UPDATE test_innodb_lock SET sex=&#x27;2&#x27; WHERE name=10;\t-- C1UPDATE test_innodb_lock SET sex=&#x27;2&#x27; WHERE id=3; -- C2 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/DB/MySQL-InnoDB 锁升级.png) 索引失效：执行更新时 name 字段为 varchar 类型，造成索引失效，最终行锁变为表锁 死锁不同事务由于互相持有对方需要的锁而导致事务都无法继续执行的情况称为死锁 死锁情况：线程 A 修改了 id &#x3D; 1 的数据，请求修改 id &#x3D; 2 的数据，线程 B 修改了 id &#x3D; 2 的数据，请求修改 id &#x3D; 1 的数据，产生死锁 解决策略： 直接进入等待直到超时，超时时间可以通过参数 innodb_lock_wait_timeout 来设置，默认 50 秒，但是时间的设置不好控制，超时可能不是因为死锁，而是因为事务处理比较慢，所以一般不采取该方式 主动死锁检测，发现死锁后主动回滚死锁链条中较小的一个事务，让其他事务得以继续执行，将参数 innodb_deadlock_detect 设置为 on，表示开启该功能（事务较小的意思就是事务执行过程中插入、删除、更新的记录条数） 死锁检测并不是每个语句都要检测，只有在加锁访问的行上已经有锁时，当前事务被阻塞了才会检测，也是从当前事务开始进行检测 通过执行 SHOW ENGINE INNODB STATUS 可以查看最近发生的一次死循环，全局系统变量 innodb_print_all_deadlocks 设置为 on，就可以将每个死锁信息都记录在 MySQL 错误日志中 死锁一般是行级锁，当表锁发生死锁时，会在事务中访问其他表时直接报错，破坏了持有并等待的死锁条件 锁状态查看锁信息 1SHOW STATUS LIKE &#x27;innodb_row_lock%&#x27;; 参数说明： Innodb_row_lock_current_waits：当前正在等待锁定的数量 Innodb_row_lock_time：从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg：每次等待所花平均时长 Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花的时间 Innodb_row_lock_waits：系统启动后到现在总共等待的次数 当等待的次数很高，而且每次等待的时长也不短的时候，就需要分析系统中为什么会有如此多的等待，然后根据分析结果制定优化计划 查看锁状态： 12SELECT * FROM information_schema.innodb_locks;\t#锁的概况SHOW ENGINE INNODB STATUS\\G; #InnoDB整体状态，其中包括锁的情况 lock_id 是锁 id；lock_trx_id 为事务 id；lock_mode 为 X 代表排它锁（写锁）；lock_type 为 RECORD 代表锁为行锁（记录锁） 乐观锁悲观锁：在整个数据处理过程中，将数据处于锁定状态，为了保证事务的隔离性，就需要一致性锁定读。读取数据时给加锁，其它事务无法修改这些数据，修改删除数据时也加锁，其它事务同样无法读取这些数据 悲观锁和乐观锁使用前提： 对于读的操作远多于写的操作的时候，一个更新操作加锁会阻塞所有的读取操作，降低了吞吐量，最后需要释放锁，锁是需要一些开销的，这时候可以选择乐观锁 如果是读写比例差距不是非常大或者系统没有响应不及时，吞吐量瓶颈的问题，那就不要去使用乐观锁，它增加了复杂度，也带来了业务额外的风险，这时候可以选择悲观锁 乐观锁的实现方式：就是 CAS，比较并交换 版本号 给数据表中添加一个 version 列，每次更新后都将这个列的值加 1 读取数据时，将版本号读取出来，在执行更新的时候，比较版本号 如果相同则执行更新，如果不相同，说明此条数据已经发生了变化 用户自行根据这个通知来决定怎么处理，比如重新开始一遍，或者放弃本次更新 123456789101112131415-- 创建city表CREATE TABLE city(\tid INT PRIMARY KEY AUTO_INCREMENT, -- 城市id\tNAME VARCHAR(20), -- 城市名称\tVERSION INT -- 版本号);-- 添加数据INSERT INTO city VALUES (NULL,&#x27;北京&#x27;,1),(NULL,&#x27;上海&#x27;,1),(NULL,&#x27;广州&#x27;,1),(NULL,&#x27;深圳&#x27;,1);-- 修改北京为北京市-- 1.查询北京的versionSELECT VERSION FROM city WHERE NAME=&#x27;北京&#x27;;-- 2.修改北京为北京市，版本号+1。并对比版本号UPDATE city SET NAME=&#x27;北京市&#x27;,VERSION=VERSION+1 WHERE NAME=&#x27;北京&#x27; AND VERSION=1; 时间戳 和版本号方式基本一样，给数据表中添加一个列，名称无所谓，数据类型需要是 timestamp 每次更新后都将最新时间插入到此列 读取数据时，将时间读取出来，在执行更新的时候，比较时间 如果相同则执行更新，如果不相同，说明此条数据已经发生了变化 乐观锁的异常情况：如果 version 被其他事务抢先更新，则在当前事务中更新失败，trx_id 没有变成当前事务的 ID，当前事务再次查询还是旧值，就会出现值没变但是更新不了的现象（anomaly） 解决方案：每次 CAS 更新不管成功失败，就结束当前事务；如果失败则重新起一个事务进行查询更新 主从基本介绍主从复制是指将主数据库的 DDL 和 DML 操作通过二进制日志传到从库服务器中，然后在从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步 MySQL 支持一台主库同时向多台从库进行复制，从库同时也可以作为其他从服务器的主库，实现链状复制 MySQL 复制的优点主要包含以下三个方面： 主库出现问题，可以快速切换到从库提供服务 可以在从库上执行查询操作，从主库中更新，实现读写分离 可以在从库中执行备份，以避免备份期间影响主库的服务（备份时会加全局读锁） 主从复制主从结构MySQL 的主从之间维持了一个长连接。主库内部有一个线程，专门用于服务从库的长连接，连接过程： 从库执行 change master 命令，设置主库的 IP、端口、用户名、密码以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量 从库执行 start slave 命令，这时从库会启动两个线程，就是图中的 io_thread 和 sql_thread，其中 io_thread 负责与主库建立连接 主库校验完用户名、密码后，开始按照从传过来的位置，从本地读取 binlog 发给从库，开始主从复制 主从复制原理图： 主从复制主要依赖的是 binlog，MySQL 默认是异步复制，需要三个线程： binlog thread：在主库事务提交时，把数据变更记录在日志文件 binlog 中，并通知 slave 有数据更新 I&#x2F;O thread：负责从主服务器上拉取二进制日志，并将 binlog 日志内容依次写到 relay log 中转日志的最末端，并将新的 binlog 文件名和 offset 记录到 master-info 文件中，以便下一次读取日志时从指定 binlog 日志文件及位置开始读取新的 binlog 日志内容 SQL thread：监测本地 relay log 中新增了日志内容，读取中继日志并重做其中的 SQL 语句，从库在 relay-log.info 中记录当前应用中继日志的文件名和位点以便下一次执行 同步与异步： 异步复制有数据丢失风险，例如数据还未同步到从库，主库就给客户端响应，然后主库挂了，此时从库晋升为主库的话数据是缺失的 同步复制，主库需要将 binlog 复制到所有从库，等所有从库响应了之后主库才进行其他逻辑，这样的话性能很差，一般不会选择 MySQL 5.7 之后出现了半同步复制，有参数可以选择成功同步几个从库就返回响应 主主结构主主结构就是两个数据库之间总是互为主从关系，这样在切换的时候就不用再修改主从关系 循环复制：在库 A 上更新了一条语句，然后把生成的 binlog 发给库 B，库 B 执行完这条更新语句后也会生成 binlog，会再发给 A 解决方法： 两个库的 server id 必须不同，如果相同则它们之间不能设定为主主关系 一个库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog 每个库在收到从主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志 主从延迟延迟原因正常情况主库执行更新生成的所有 binlog，都可以传到从库并被正确地执行，从库就能达到跟主库一致的状态，这就是最终一致性 主从延迟是主从之间是存在一定时间的数据不一致，就是同一个事务在从库执行完成的时间和主库执行完成的时间的差值，即 T2-T1 主库 A 执行完成一个事务，写入 binlog，该时刻记为 T1 日志传给从库 B，从库 B 执行完这个事务，该时刻记为 T2 通过在从库执行 show slave status 命令，返回结果会显示 seconds_behind_master 表示当前从库延迟了多少秒 每一个事务的 binlog 都有一个时间字段，用于记录主库上写入的时间 从库取出当前正在执行的事务的时间字段，跟系统的时间进行相减，得到的就是 seconds_behind_master 主从延迟的原因： 从库的机器性能比主库的差，导致从库的复制能力弱 从库的查询压力大，建立一主多从的结构 大事务的执行，主库必须要等到事务完成之后才会写入 binlog，导致从节点出现应用 binlog 延迟 主库的 DDL，从库与主库的 DDL 同步是串行进行，DDL 在主库执行时间很长，那么从库也会消耗同样的时间 锁冲突问题也可能导致从节点的 SQL 线程执行慢 主从同步问题永远都是一致性和性能的权衡，需要根据实际的应用场景，可以采取下面的办法： 优化 SQL，避免慢 SQL，减少批量操作 降低多线程大事务并发的概率，优化业务逻辑 业务中大多数情况查询操作要比更新操作更多，搭建一主多从结构，让这些从库来分担读的压力 尽量采用短的链路，主库和从库服务器的距离尽量要短，提升端口带宽，减少 binlog 传输的网络延时 实时性要求高的业务读强制走主库，从库只做备份 并行复制MySQL5.6高并发情况下，主库的会产生大量的 binlog，在从库中有两个线程 IO Thread 和 SQL Thread 单线程执行，会导致主库延迟变大。为了改善复制延迟问题，MySQL 5.6 版本增加了并行复制功能，以采用多线程机制来促进执行 coordinator 就是原来的 SQL Thread，并行复制中它不再直接更新数据，只负责读取中转日志和分发事务： 线程分配完成并不是立即执行，为了防止造成更新覆盖，更新同一 DB 的两个事务必须被分发到同一个工作线程 同一个事务不能被拆开，必须放到同一个工作线程 MySQL 5.6 版本的策略：每个线程对应一个 hash 表，用于保存当前这个线程的执行队列里的事务所涉及的表，hash 表的 key 是数据库名，value 是一个数字，表示队列中有多少个事务修改这个库，适用于主库上有多个 DB 的情况 每个事务在分发的时候，跟线程的冲突（事务操作的是同一个库）关系包括以下三种情况： 如果跟所有线程都不冲突，coordinator 线程就会把这个事务分配给最空闲的线程 如果只跟一个线程冲突，coordinator 线程就会把这个事务分配给这个存在冲突关系的线程 如果跟多于一个线程冲突，coordinator 线程就进入等待状态，直到和这个事务存在冲突关系的线程只剩下 1 个 优缺点： 构造 hash 值的时候很快，只需要库名，而且一个实例上 DB 数也不会很多，不会出现需要构造很多项的情况 不要求 binlog 的格式，statement 格式的 binlog 也可以很容易拿到库名（日志章节详解了 binlog） 主库上的表都放在同一个 DB 里面，这个策略就没有效果了；或者不同 DB 的热点不同，比如一个是业务逻辑库，一个是系统配置库，那也起不到并行的效果，需要把相同热度的表均匀分到这些不同的 DB 中，才可以使用这个策略 MySQL5.7MySQL 5.7 由参数 slave-parallel-type 来控制并行复制策略： 配置为 DATABASE，表示使用 MySQL 5.6 版本的按库（DB）并行策略 配置为 LOGICAL_CLOCK，表示的按提交状态并行执行 按提交状态并行复制策略的思想是： 所有处于 commit 状态的事务可以并行执行；同时处于 prepare 状态的事务，在从库执行时是可以并行的 处于 prepare 状态的事务，与处于 commit 状态的事务之间，在从库执行时也是可以并行的 MySQL 5.7.22 版本里，MySQL 增加了一个新的并行复制策略，基于 WRITESET 的并行复制，新增了一个参数 binlog-transaction-dependency-tracking，用来控制是否启用这个新策略： COMMIT_ORDER：表示根据同时进入 prepare 和 commit 来判断是否可以并行的策略 WRITESET：表示的是对于每个事务涉及更新的每一行，计算出这一行的 hash 值，组成该事务的 writeset 集合，如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行（按行并行） 为了唯一标识，这个 hash 表的值是通过 库名 + 表名 + 索引名 + 值（表示的是某一行）计算出来的 WRITESET_SESSION：是在 WRITESET 的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序 MySQL 5.7.22 按行并发的优势： writeset 是在主库生成后直接写入到 binlog 里面的，这样在备库执行的时候，不需要解析 binlog 内容，节省了计算量 不需要把整个事务的 binlog 都扫一遍才能决定分发到哪个线程，更省内存 从库的分发策略不依赖于 binlog 内容，所以 binlog 是 statement 格式也可以，更节约内存（因为 row 才记录更改的行） MySQL 5.7.22 的并行复制策略在通用性上是有保证的，但是对于表上没主键、唯一和外键约束的场景，WRITESET 策略也是没法并行的，也会暂时退化为单线程模型 参考文章：https://time.geekbang.org/column/article/77083 读写分离读写延迟读写分离：可以降低主库的访问压力，提高系统的并发能力 主库不建查询的索引，从库建查询的索引。因为索引需要维护的，比如插入一条数据，不仅要在聚簇索引上面插入，对应的二级索引也得插入 将读操作分到从库了之后，可以在主库把查询要用的索引删了，减少写操作对主库的影响 读写分离产生了读写延迟，造成数据的不一致性。假如客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，可能读到的还是以前的数据，叫过期读 解决方案： 强制将写之后立刻读的操作转移到主库，比如刚注册的用户，直接登录从库查询可能查询不到，先走主库登录 二次查询，如果从库查不到数据，则再去主库查一遍，由 API 封装，比较简单，但导致主库压力大 更新主库后，读从库之前先 sleep 一下，类似于执行一条 select sleep(1) 命令，大多数情况下主备延迟在 1 秒之内 确保机制无延迟确保主备无延迟的方法： 每次从库执行查询请求前，先判断 seconds_behind_master 是否已经等于 0，如果不等于那就等到参数变为 0 执行查询请求 对比位点，Master_Log_File 和 Read_Master_Log_Pos 表示的是读到的主库的最新位点，Relay_Master_Log_File 和 Exec_Master_Log_Pos 表示的是备库执行的最新位点，这两组值完全相同就说明接收到的日志已经同步完成 对比 GTID 集合，Retrieved_Gtid_Set 是备库收到的所有日志的 GTID 集合，Executed_Gtid_Set 是备库所有已经执行完成的 GTID 集合，如果这两个集合相同也表示备库接收到的日志都已经同步完成 半同步半同步复制就是 semi-sync replication，适用于一主一备的场景，工作流程： 事务提交的时候，主库把 binlog 发给从库 从库收到 binlog 以后，发回给主库一个 ack，表示收到了 主库收到这个 ack 以后，才能给客户端返回事务完成的确认 在一主多从场景中，主库只要等到一个从库的 ack，就开始给客户端返回确认，这时在从库上执行查询请求，有两种情况： 如果查询是落在这个响应了 ack 的从库上，是能够确保读到最新数据 如果查询落到其他从库上，它们可能还没有收到最新的日志，就会产生过期读的问题 在业务更新的高峰期，主库的位点或者 GTID 集合更新很快，导致从库来不及处理，那么两个位点等值判断就会一直不成立，很可能出现从库上迟迟无法响应查询请求的情况 等位点在从库执行判断位点的命令，参数 file 和 pos 指的是主库上的文件名和位置，timeout 可选，设置为正整数 N 表示最多等待 N 秒 1SELECT master_pos_wait(file, pos[, timeout]); 命令正常返回的结果是一个正整数 M，表示从命令开始执行，到应用完 file 和 pos 表示的 binlog 位置，执行了多少事务 如果执行期间，备库同步线程发生异常，则返回 NULL 如果等待超过 N 秒，就返回 -1 如果刚开始执行的时候，就发现已经执行过这个位置了，则返回 0 工作流程：先执行 trx1，再执行一个查询请求的逻辑，要保证能够查到正确的数据 trx1 事务更新完成后，马上执行 show master status 得到当前主库执行到的 File 和 Position 选定一个从库执行判断位点语句，如果返回值是 &gt;&#x3D;0 的正整数，说明从库已经同步完事务，可以在这个从库执行查询语句 如果出现其他情况，需要到主库执行查询语句 注意：如果所有的从库都延迟超过 timeout 秒，查询压力就都跑到主库上，所以需要进行权衡 等GTID数据库开启了 GTID 模式，MySQL 提供了判断 GTID 的命令 1SELECT wait_for_executed_gtid_set(gtid_set [, timeout]) 等待直到这个库执行的事务中包含传入的 gtid_set，返回 0 超时返回 1 工作流程：先执行 trx1，再执行一个查询请求的逻辑，要保证能够查到正确的数据 trx1 事务更新完成后，从返回包直接获取这个事务的 GTID，记为 gtid 选定一个从库执行查询语句，如果返回值是 0，则在这个从库执行查询语句，否则到主库执行查询语句 对比等待位点方法，减少了一次 show master status 的方法，将参数 session_track_gtids 设置为 OWN_GTID，然后通过 API 接口 mysql_session_track_get_first 从返回包解析出 GTID 的值即可 总结：所有的等待无延迟的方法，都需要根据具体的业务场景去判断实施 参考文章：https://time.geekbang.org/column/article/77636 负载均衡负载均衡是应用中使用非常普遍的一种优化方法，机制就是利用某种均衡算法，将固定的负载量分布到不同的服务器上，以此来降低单台服务器的负载，达到优化的效果 分流查询：通过 MySQL 的主从复制，实现读写分离，使增删改操作走主节点，查询操作走从节点，从而可以降低单台服务器的读写压力 分布式数据库架构：适合大数据量、负载高的情况，具有良好的拓展性和高可用性。通过在多台服务器之间分布数据，可以实现在多台服务器之间的负载均衡，提高访问效率 主从搭建master 在master 的配置文件（&#x2F;etc&#x2F;mysql&#x2F;my.cnf）中，配置如下内容： 1234567891011121314151617181920212223242526#mysql 服务ID,保证整个集群环境中唯一server-id=1#mysql binlog 日志的存储路径和文件名log-bin=/var/lib/mysql/mysqlbin#错误日志,默认已经开启#log-err#mysql的安装目录#basedir#mysql的临时目录#tmpdir#mysql的数据存放目录#datadir#是否只读,1 代表只读, 0 代表读写read-only=0#忽略的数据, 指不需要同步的数据库binlog-ignore-db=mysql#指定同步的数据库#binlog-do-db=db01 执行完毕之后，需要重启 MySQL 创建同步数据的账户，并且进行授权操作： 12GRANT REPLICATION SLAVE ON *.* TO &#x27;seazean&#x27;@&#x27;192.168.0.137&#x27; IDENTIFIED BY &#x27;123456&#x27;;FLUSH PRIVILEGES; 查看 master 状态： 1SHOW MASTER STATUS; File：从哪个日志文件开始推送日志文件 Position：从哪个位置开始推送日志 Binlog_Ignore_DB：指定不需要同步的数据库 slave 在 slave 端配置文件中，配置如下内容： 12345#mysql服务端ID,唯一server-id=2#指定binlog日志log-bin=/var/lib/mysql/mysqlbin 执行完毕之后，需要重启 MySQL 指定当前从库对应的主库的IP地址、用户名、密码，从哪个日志文件开始的那个位置开始同步推送日志 1CHANGE MASTER TO MASTER_HOST= &#x27;192.168.0.138&#x27;, MASTER_USER=&#x27;seazean&#x27;, MASTER_PASSWORD=&#x27;seazean&#x27;, MASTER_LOG_FILE=&#x27;mysqlbin.000001&#x27;, MASTER_LOG_POS=413; 开启同步操作： 12START SLAVE;SHOW SLAVE STATUS; 停止同步操作： 1STOP SLAVE; 验证 在主库中创建数据库，创建表并插入数据： 123456789101112CREATE DATABASE db01;USE db01;CREATE TABLE user(\tid INT(11) NOT NULL AUTO_INCREMENT,\tname VARCHAR(50) NOT NULL,\tsex VARCHAR(1),\tPRIMARY KEY (id))ENGINE=INNODB DEFAULT CHARSET=utf8;INSERT INTO user(id,NAME,sex) VALUES(NULL,&#x27;Tom&#x27;,&#x27;1&#x27;);INSERT INTO user(id,NAME,sex) VALUES(NULL,&#x27;Trigger&#x27;,&#x27;0&#x27;);INSERT INTO user(id,NAME,sex) VALUES(NULL,&#x27;Dawn&#x27;,&#x27;1&#x27;); 在从库中查询数据，进行验证： 在从库中，可以查看到刚才创建的数据库： 在该数据库中，查询表中的数据： 主从切换正常切换正常切换步骤： 在开始切换之前先对主库进行锁表 flush tables with read lock，然后等待所有语句执行完成，切换完成后可以释放锁 检查 slave 同步状态，在 slave 执行 show processlist 停止 slave io 线程，执行命令 STOP SLAVE IO_THREAD 提升 slave 为 master 1234Stop slave;Reset master;Reset slave all;set global read_only=off;\t-- 设置为可更新状态 将原来 master 变为 slave（参考搭建流程中的 slave 方法） 可靠性优先策略： 判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步 把主库 A 改成只读状态，即把 readonly 设置为 true 判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止（该步骤比较耗时，所以步骤 1 中要尽量等待该值变小） 把备库 B 改成可读写状态，也就是把 readonly 设置为 false 把业务请求切到备库 B 可用性优先策略：先做最后两步，会造成主备数据不一致的问题 参考文章：https://time.geekbang.org/column/article/76795 健康检测主库发生故障后从库会上位，其他从库指向新的主库，所以需要一个健康检测的机制来判断主库是否宕机 select 1 判断，但是高并发下检测不出线程的锁等待的阻塞问题 查表判断，在系统库（mysql 库）里创建一个表，比如命名为 health_check，里面只放一行数据，然后定期执行。但是当 binlog 所在磁盘的空间占用率达到 100%，所有的更新和事务提交语句都被阻塞，查询语句可以继续运行 更新判断，在健康检测表中放一个 timestamp 字段，用来表示最后一次执行检测的时间 1UPDATE mysql.health_check SET t_modified=now(); 节点可用性的检测都应该包含主库和备库，为了让主备之间的更新不产生冲突，可以在 mysql.health_check 表上存入多行数据，并用主备的 server_id 做主键，保证主、备库各自的检测命令不会发生冲突 基于位点主库上位后，从库 B 执行 CHANGE MASTER TO 命令，指定 MASTER_LOG_FILE、MASTER_LOG_POS 表示从新主库 A 的哪个文件的哪个位点开始同步，这个位置就是同步位点，对应主库的文件名和日志偏移量 寻找位点需要找一个稍微往前的，然后再通过判断跳过那些在从库 B 上已经执行过的事务，获取位点方法： 等待新主库 A 把中转日志（relay log）全部同步完成 在 A 上执行 show master status 命令，得到当前 A 上最新的 File 和 Position 取原主库故障的时刻 T，用 mysqlbinlog 工具解析新主库 A 的 File，得到 T 时刻的位点 通常情况下该值并不准确，在切换的过程中会发生错误，所以要先主动跳过这些错误： 切换过程中，可能会重复执行一个事务，所以需要主动跳过所有重复的事务 12SET GLOBAL sql_slave_skip_counter=1;START SLAVE; 设置 slave_skip_errors 参数，直接设置跳过指定的错误，保证主从切换的正常进行 1062 错误是插入数据时唯一键冲突 1032 错误是删除数据时找不到行 该方法针对的是主备切换时，由于找不到精确的同步位点，只能采用这种方法来创建从库和新主库的主备关系。等到主备间的同步关系建立完成并稳定执行一段时间后，还需要把这个参数设置为空，以免真的出现了主从数据不一致也跳过了 基于GTIDGTIDGTID 的全称是 Global Transaction Identifier，全局事务 ID，是一个事务在提交时生成的，是这个事务的唯一标识，组成： 1GTID=source_id:transaction_id source_id：是一个实例第一次启动时自动生成的，是一个全局唯一的值 transaction_id：初始值是 1，每次提交事务的时候分配给这个事务，并加 1，是连续的（区分事务 ID，事务 ID 是在执行时生成） 启动 MySQL 实例时，加上参数 gtid_mode=on 和 enforce_gtid_consistency=on 就可以启动 GTID 模式，每个事务都会和一个 GTID 一一对应，每个 MySQL 实例都维护了一个 GTID 集合，用来存储当前实例执行过的所有事务 GTID 有两种生成方式，使用哪种方式取决于 session 变量 gtid_next： gtid_next=automatic：使用默认值，把 source_id:transaction_id （递增）分配给这个事务，然后加入本实例的 GTID 集合 1@@SESSION.GTID_NEXT = &#x27;source_id:transaction_id&#x27;; gtid_next=GTID：指定的 GTID 的值，如果该值已经存在于实例的 GTID 集合中，接下来执行的事务会直接被系统忽略；反之就将该值分配给接下来要执行的事务，系统不需要给这个事务生成新的 GTID，也不用加 1 注意：一个 GTID 只能给一个事务使用，所以执行下一个事务，要把 gtid_next 设置成另外一个 GTID 或者 automatic 业务场景： 主库 X 和从库 Y 执行一条相同的指令后进行事务同步 1INSERT INTO t VALUES(1,1); 当 Y 同步 X 时，会出现主键冲突，导致实例 X 的同步线程停止，解决方法： 12345SET gtid_next=&#x27;(这里是主库 X 的 GTID 值)&#x27;;BEGIN;COMMIT;SET gtid_next=automatic;START SLAVE; 前三条语句通过提交一个空事务，把 X 的 GTID 加到实例 Y 的 GTID 集合中，实例 Y 就会直接跳过这个事务 切换在 GTID 模式下，CHANGE MASTER TO 不需要指定日志名和日志偏移量，指定 master_auto_position=1 代表使用 GTID 模式 新主库实例 A 的 GTID 集合记为 set_a，从库实例 B 的 GTID 集合记为 set_b，主备切换逻辑： 实例 B 指定主库 A，基于主备协议建立连接，实例 B 并把 set_b 发给主库 A 实例 A 算出 set_a 与 set_b 的差集，就是所有存在于 set_a 但不存在于 set_b 的 GTID 的集合，判断 A 本地是否包含了这个差集需要的所有 binlog 事务 如果不包含，表示 A 已经把实例 B 需要的 binlog 给删掉了，直接返回错误 如果确认全部包含，A 从自己的 binlog 文件里面，找出第一个不在 set_b 的事务，发给 B 实例 A 之后就从这个事务开始，往后读文件，按顺序取 binlog 发给 B 去执行 参考文章：https://time.geekbang.org/column/article/77427 日志日志分类在任何一种数据库中，都会有各种各样的日志，记录着数据库工作的过程，可以帮助数据库管理员追踪数据库曾经发生过的各种事件 MySQL日志主要包括六种： 重做日志（redo log） 回滚日志（undo log） 归档日志（binlog）（二进制日志） 错误日志（errorlog） 慢查询日志（slow query log） 一般查询日志（general log） 中继日志（relay log） 错误日志错误日志是 MySQL 中最重要的日志之一，记录了当 mysqld 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，可以首先查看此日志 该日志是默认开启的，默认位置是：/var/log/mysql/error.log 查看指令： 1SHOW VARIABLES LIKE &#x27;log_error%&#x27;; 查看日志内容： 1tail -f /var/log/mysql/error.log 归档日志基本介绍归档日志（BINLOG）也叫二进制日志，是因为采用二进制进行存储，记录了所有的 DDL（数据定义语言）语句和 DML（数据操作语言）语句，但不包括数据查询语句，在事务提交前的最后阶段写入 作用：灾难时的数据恢复和 MySQL 的主从复制 归档日志默认情况下是没有开启的，需要在 MySQL 配置文件中开启，并配置 MySQL 日志的格式： 1234567cd /etc/mysqlvim my.cnf# 配置开启binlog日志， 日志的文件前缀为 mysqlbin -----&gt; 生成的文件名如: mysqlbin.000001log_bin=mysqlbin# 配置二进制日志的格式binlog_format=STATEMENT 日志存放位置：配置时给定了文件名但是没有指定路径，日志默认写入MySQL 的数据目录 日志格式： STATEMENT：该日志格式在日志文件中记录的都是 SQL 语句，每一条对数据进行修改的 SQL 都会记录在日志文件中，通过 mysqlbinlog 工具，可以查看到每条语句的文本。主从复制时，从库会将日志解析为原语句，并在从库重新执行一遍 缺点：可能会导致主备不一致，因为记录的 SQL 在不同的环境中可能选择的索引不同，导致结果不同 ROW：该日志格式在日志文件中记录的是每一行的数据变更，而不是记录 SQL 语句。比如执行 SQL 语句 update tb_book set status=&#39;1&#39;，如果是 STATEMENT，在日志中会记录一行 SQL 语句； 如果是 ROW，由于是对全表进行更新，就是每一行记录都会发生变更，ROW 格式的日志中会记录每一行的数据变更 缺点：记录的数据比较多，占用很多的存储空间 MIXED：这是 MySQL 默认的日志格式，混合了STATEMENT 和 ROW 两种格式，MIXED 格式能尽量利用两种模式的优点，而避开它们的缺点 日志刷盘事务执行过程中，先将日志写（write）到 binlog cache，事务提交时再把 binlog cache 写（fsync）到 binlog 文件中，一个事务的 binlog 是不能被拆开的，所以不论这个事务多大也要确保一次性写入 事务提交时执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache write 和 fsync 的时机由参数 sync_binlog 控制的： sync_binlog&#x3D;0：表示每次提交事务都只 write，不 fsync sync_binlog&#x3D;1：表示每次提交事务都会执行 fsync sync_binlog&#x3D;N(N&gt;1)：表示每次提交事务都 write，但累积 N 个事务后才 fsync，但是如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志 日志读取日志文件存储位置：&#x2F;var&#x2F;lib&#x2F;mysql 由于日志以二进制方式存储，不能直接读取，需要用 mysqlbinlog 工具来查看，语法如下： 1mysqlbinlog log-file; 查看 STATEMENT 格式日志： 执行插入语句： 1INSERT INTO tb_book VALUES(NULL,&#x27;Lucene&#x27;,&#x27;2088-05-01&#x27;,&#x27;0&#x27;); cd /var/lib/mysql： 12-rw-r----- 1 mysql mysql 177 5月 23 21:08 mysqlbin.000001-rw-r----- 1 mysql mysql 18 5月 23 21:04 mysqlbin.index mysqlbin.index：该文件是日志索引文件 ， 记录日志的文件名； mysqlbing.000001：日志文件 查看日志内容： 1mysqlbinlog mysqlbing.000001; 日志结尾有 COMMIT 查看 ROW 格式日志： 修改配置： 12# 配置二进制日志的格式binlog_format=ROW 插入数据： 1INSERT INTO tb_book VALUES(NULL,&#x27;SpringCloud实战&#x27;,&#x27;2088-05-05&#x27;,&#x27;0&#x27;); 查看日志内容：日志格式 ROW，直接查看数据是乱码，可以在 mysqlbinlog 后面加上参数 -vv 1mysqlbinlog -vv mysqlbin.000002 日志删除对于比较繁忙的系统，生成日志量大，这些日志如果长时间不清除，将会占用大量的磁盘空间，需要删除日志 Reset Master 指令删除全部 binlog 日志，删除之后，日志编号将从 xxxx.000001重新开始 1Reset Master\t-- MySQL指令 执行指令 PURGE MASTER LOGS TO &#39;mysqlbin.***，该命令将删除 *** 编号之前的所有日志 执行指令 PURGE MASTER LOGS BEFORE &#39;yyyy-mm-dd hh:mm:ss&#39; ，该命令将删除日志为 yyyy-mm-dd hh:mm:ss 之前产生的日志 设置参数 --expire_logs_days=#，此参数的含义是设置日志的过期天数，过了指定的天数后日志将会被自动删除，这样做有利于减少管理日志的工作量，配置 my.cnf 文件： 123log_bin=mysqlbinbinlog_format=ROW--expire_logs_days=3 数据恢复误删库或者表时，需要根据 binlog 进行数据恢复 一般情况下数据库有定时的全量备份，假如每天 0 点定时备份，12 点误删了库，恢复流程： 取最近一次全量备份，用备份恢复出一个临时库 从日志文件中取出凌晨 0 点之后的日志 把除了误删除数据的语句外日志，全部应用到临时库 跳过误删除语句日志的方法： 如果原实例没有使用 GTID 模式，只能在应用到包含 12 点的 binlog 文件的时候，先用 –stop-position 参数执行到误操作之前的日志，然后再用 –start-position 从误操作之后的日志继续执行 如果实例使用了 GTID 模式，假设误操作命令的 GTID 是 gtid1，那么只需要提交一个空事务先将这个 GTID 加到临时实例的 GTID 集合，之后按顺序执行 binlog 的时就会自动跳过误操作的语句 查询日志查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的 SQL 语句 默认情况下，查询日志是未开启的。如果需要开启查询日志，配置 my.cnf： 1234# 该选项用来开启查询日志，可选值0或者1，0代表关闭，1代表开启 general_log=1# 设置日志的文件名，如果没有指定，默认的文件名为host_name.log，存放在/var/lib/mysqlgeneral_log_file=mysql_query.log 配置完毕之后，在数据库执行以下操作： 1234SELECT * FROM tb_book;SELECT * FROM tb_book WHERE id = 1;UPDATE tb_book SET name = &#x27;lucene入门指南&#x27; WHERE id = 5;SELECT * FROM tb_book WHERE id &lt; 8 执行完毕之后， 再次来查询日志文件： 慢日志慢查询日志记录所有执行时间超过 long_query_time 并且扫描记录数不小于 min_examined_row_limit 的所有的 SQL 语句的日志long_query_time 默认为 10 秒，最小为 0， 精度到微秒 慢查询日志默认是关闭的，可以通过两个参数来控制慢查询日志，配置文件 /etc/mysql/my.cnf： 12345678# 该参数用来控制慢查询日志是否开启，可选值0或者1，0代表关闭，1代表开启 slow_query_log=1 # 该参数用来指定慢查询日志的文件名，存放在 /var/lib/mysqlslow_query_log_file=slow_query.log# 该选项用来配置查询的时间限制，超过这个时间将认为值慢查询，将需要进行日志记录，默认10slong_query_time=10 日志读取： 直接通过 cat 指令查询该日志文件： 1cat slow_query.log 如果慢查询日志内容很多，直接查看文件比较繁琐，可以借助 mysql 自带的 mysqldumpslow 工具对慢查询日志进行分类汇总： 1mysqldumpslow slow_query.log 范式第一范式建立科学的，规范的数据表就需要满足一些规则来优化数据的设计和存储，这些规则就称为范式 1NF：数据库表的每一列都是不可分割的原子数据项，不能是集合、数组等非原子数据项。即表中的某个列有多个值时，必须拆分为不同的列。简而言之，第一范式每一列不可再拆分，称为原子性 基本表： 第一范式表： 第二范式2NF：在满足第一范式的基础上，非主属性完全依赖于主码（主关键字、主键），消除非主属性对主码的部分函数依赖。简而言之，表中的每一个字段 （所有列）都完全依赖于主键，记录的唯一性 作用：遵守第二范式减少数据冗余，通过主键区分相同数据。 函数依赖：A → B，如果通过 A 属性(属性组)的值，可以确定唯一 B 属性的值，则称 B 依赖于 A 学号 → 姓名；(学号，课程名称) → 分数 完全函数依赖：A → B，如果A是一个属性组，则 B 属性值的确定需要依赖于 A 属性组的所有属性值 (学号，课程名称) → 分数 部分函数依赖：A → B，如果 A 是一个属性组，则 B 属性值的确定只需要依赖于 A 属性组的某些属性值 (学号，课程名称) → 姓名 传递函数依赖：A → B，B → C，如果通过A属性(属性组)的值，可以确定唯一 B 属性的值，在通过 B 属性(属性组)的值，可以确定唯一 C 属性的值，则称 C 传递函数依赖于 A 学号 → 系名，系名 → 系主任 码：如果在一张表中，一个属性或属性组，被其他所有属性所完全依赖，则称这个属性(属性组)为该表的码 该表中的码：(学号，课程名称) 主属性：码属性组中的所有属性 非主属性：除码属性组以外的属性 第三范式3NF：在满足第二范式的基础上，表中的任何属性不依赖于其它非主属性，消除传递依赖。简而言之，非主键都直接依赖于主键，而不是通过其它的键来间接依赖于主键。 作用：可以通过主键 id 区分相同数据，修改数据的时候只需要修改一张表（方便修改），反之需要修改多表。 总结 RedisNoSQL概述NoSQL（Not-Only SQL）：泛指非关系型的数据库，作为关系型数据库的补充 MySQL 支持 ACID 特性，保证可靠性和持久性，读取性能不高，因此需要缓存的来减缓数据库的访问压力 作用：应对基于海量用户和海量数据前提下的数据处理问题 特征： 可扩容，可伸缩，SQL 数据关系过于复杂，Nosql 不存关系，只存数据 大数据量下高性能，数据不存取在磁盘 IO，存取在内存 灵活的数据模型，设计了一些数据存储格式，能保证效率上的提高 高可用，集群 常见的 NoSQL：Redis、memcache、HBase、MongoDB 参考书籍：https://book.douban.com/subject/25900156/ 参考视频：https://www.bilibili.com/video/BV1CJ411m7Gc RedisRedis (REmote DIctionary Server) ：用 C 语言开发的一个开源的高性能键值对（key-value）数据库 特征： 数据间没有必然的关联关系，不存关系，只存数据 数据存储在内存，存取速度快，解决了磁盘 IO 速度慢的问题 内部采用单线程机制进行工作 高性能，官方测试数据，50 个并发执行 100000 个请求，读的速度是 110000 次&#x2F;s，写的速度是 81000 次&#x2F;s 多数据类型支持 字符串类型：string（String） 列表类型：list（LinkedList） 散列类型：hash（HashMap） 集合类型：set（HashSet） 有序集合类型：zset&#x2F;sorted_set（TreeSet） 支持持久化，可以进行数据灾难恢复 安装启动安装： Redis 5.0 被包含在默认的 Ubuntu 20.04 软件源中 12sudo apt updatesudo apt install redis-server 检查 Redis 状态 1sudo systemctl status redis-server 启动： 启动服务器——参数启动 12redis-server [--port port]#redis-server --port 6379 启动服务器——配置文件启动 12redis-server config_file_name#redis-server /etc/redis/conf/redis-6397.conf 启动客户端： 12redis-cli [-h host] [-p port]#redis-cli -h 192.168.2.185 -p 6397 注意：服务器启动指定端口使用的是–port，客户端启动指定端口使用的是-p 基本配置系统目录 创建文件结构 创建配置文件存储目录 1mkdir conf 创建服务器文件存储目录（包含日志、数据、临时配置文件等） 1mkdir data 创建配置文件副本放入 conf 目录，Ubuntu 系统配置文件 redis.conf 在目录 /etc/redis 中 1cat redis.conf | grep -v &quot;#&quot; | grep -v &quot;^$&quot; -&gt; /conf/redis-6379.conf 去除配置文件的注释和空格，输出到新的文件，命令方式采用 redis-port.conf 服务器 设置服务器以守护进程的方式运行，关闭后服务器控制台中将打印服务器运行信息（同日志内容相同）： 1daemonize yes|no 绑定主机地址，绑定本地IP地址，否则SSH无法访问： 1bind ip 设置服务器端口： 1port port 设置服务器文件保存地址： 1dir path 设置数据库的数量： 1databases 16 多服务器快捷配置： 导入并加载指定配置文件信息，用于快速创建 redis 公共配置较多的 redis 实例配置文件，便于维护 1include /path/conf_name.conf 客户端 服务器允许客户端连接最大数量，默认 0，表示无限制，当客户端连接到达上限后，Redis 会拒绝新的连接： 1maxclients count 客户端闲置等待最大时长，达到最大值后关闭对应连接，如需关闭该功能，设置为 0： 1timeout seconds 日志配置设置日志记录 设置服务器以指定日志记录级别 1loglevel debug|verbose|notice|warning 日志记录文件名 1logfile filename 注意：日志级别开发期设置为 verbose 即可，生产环境中配置为 notice，简化日志输出量，降低写日志 IO 的频度 配置文件： 1234567bind 192.168.2.185port 6379#timeout 0daemonize nologfile /etc/redis/data/redis-6379.logdir /etc/redis/datadbfilename &quot;dump-6379.rdb&quot; 基本指令帮助信息： 获取命令帮助文档 12help [command]#help set 获取组中所有命令信息名称 12help [@group-name]#help @string 退出服务 退出客户端： 12quitexit 退出客户端服务器快捷键： 1Ctrl+C 数据库服务器Redis 服务器将所有数据库保存在服务器状态 redisServer 结构的 db 数组中，数组的每一项都是 redisDb 结构，代表一个数据库，每个数据库之间相互独立，**共用 **Redis 内存，不区分大小。在初始化服务器时，根据 dbnum 属性决定创建数据库的数量，该属性由服务器配置的 database 选项决定，默认 16 1234567struct redisServer &#123; // 保存服务器所有的数据库 redisDB *db; // 服务器数据库的数量 int dbnum;&#125;; 在服务器内部，客户端状态 redisClient 结构的 db 属性记录了目标数据库，是一个指向 redisDb 结构的指针 1234struct redisClient &#123; // 记录客户端正在使用的数据库，指向 redisServer.db 数组中的某一个 db redisDB *db;&#125;; 每个 Redis 客户端都有目标数据库，执行数据库读写命令时目标数据库就会成为这些命令的操作对象，默认情况下 Redis 客户端的目标数据库为 0 号数据库，客户端可以执行 SELECT 命令切换目标数据库，原理是通过修改 redisClient.db 指针指向服务器中不同数据库 命令操作： 1234select index\t#切换数据库，index从0-15取值move key db #数据移动到指定数据库，db是数据库编号ping #测试数据库是否连接正常，返回PONGecho message\t#控制台输出信息 Redis 没有可以返回客户端目标数据库的命令，但是 redis-cli 客户端旁边会提示当前所使用的目标数据库 123redis&gt; SELECT 1 OK redis[1]&gt; 键空间key spaceRedis 是一个键值对（key-value pair）数据库服务器，每个数据库都由一个 redisDb 结构表示，redisDb.dict 字典中保存了数据库的所有键值对，将这个字典称为键空间（key space） 1234typedef struct redisDB &#123; // 数据库键空间，保存所有键值对 dict *dict&#125; redisDB; 键空间和用户所见的数据库是直接对应的： 键空间的键就是数据库的键，每个键都是一个字符串对象 键空间的值就是数据库的值，每个值可以是任意一种 Redis 对象 当使用 Redis 命令对数据库进行读写时，服务器不仅会对键空间执行指定的读写操作，还会进行一些维护操作： 在读取一个键后（读操作和写操作都要对键进行读取），服务器会根据键是否存在来更新服务器的键空间命中 hit 次数或键空间不命中 miss 次数，这两个值可以在 INFO stats 命令的 keyspace_hits 属性和 keyspace_misses 属性中查看 更新键的 LRU（最后使用）时间，该值可以用于计算键的闲置时间，使用 OBJECT idletime key 查看键 key 的闲置时间 如果在读取一个键时发现该键已经过期，服务器会先删除过期键，再执行其他操作 如果客户端使用 WATCH 命令监视了某个键，那么服务器在对被监视的键进行修改之后，会将这个键标记为脏（dirty），从而让事务注意到这个键已经被修改过 服务器每次修改一个键之后，都会对 dirty 键计数器的值增1，该计数器会触发服务器的持久化以及复制操作 如果服务器开启了数据库通知功能，那么在对键进行修改之后，服务器将按配置发送相应的数据库通知 读写指令常见键操作指令： 增加指令 1234567 set key value #添加一个字符串类型的键值对* 删除指令 ```sh del key #删除指定key unlink key #非阻塞删除key，真正的删除会在后续异步操作 更新指令 12rename key newkey #改名renamenx key newkey #改名 值得更新需要参看具体得 Redis 对象得操作方式，比如字符串对象执行 SET key value 就可以完成修改 查询指令 123exists key #获取key是否存在randomkey #随机返回一个键keys pattern #查询key KEYS 命令需要遍历存储的键值对，操作延时高，一般不被建议用于生产环境中 查询模式规则：* 匹配任意数量的任意符号、? 配合一个任意符号、[] 匹配一个指定符号 123456keys * #查询所有keykeys aa* #查询所有以aa开头keys *bb #查询所有以bb结尾keys ??cc #查询所有前面两个字符任意，后面以cc结尾 keys user:? #查询所有以user:开头，最后一个字符任意keys u[st]er:1 #查询所有以u开头，以er:1结尾，中间包含一个字母，s或t 其他指令 1234type key #获取key的类型dbsize #获取当前数据库的数据总量，即key的个数flushdb #清除当前数据库的所有数据(慎用)flushall #清除所有数据(慎用) 在执行 FLUSHDB 这样的危险命令之前，最好先执行一个 SELECT 命令，保证当前所操作的数据库是目标数据库 时效设置客户端可以以秒或毫秒的精度为数据库中的某个键设置生存时间（TimeTo Live, TTL），在经过指定时间之后，服务器就会自动删除生存时间为 0 的键；也可以以 UNIX 时间戳的方式设置过期时间（expire time），当键的过期时间到达，服务器会自动删除这个键 1234expire key seconds #为指定key设置生存时间，单位为秒pexpire key milliseconds\t#为指定key设置生存时间，单位为毫秒expireat key timestamp #为指定key设置过期时间，单位为时间戳pexpireat key mil-timestamp\t#为指定key设置过期时间，单位为毫秒时间戳 实际上 EXPIRE、EXPIRE、EXPIREAT 三个命令底层都是转换为 PEXPIREAT 命令来实现的 SETEX 命令可以在设置一个字符串键的同时为键设置过期时间，但是该命令是一个类型限定命令 redisDb 结构的 expires 字典保存了数据库中所有键的过期时间，字典称为过期字典： 键是一个指针，指向键空间中的某个键对象（复用键空间的对象，不会产生内存浪费） 值是一个 long long 类型的整数，保存了键的过期时间，是一个毫秒精度的 UNIX 时间戳 1234typedef struct redisDB &#123; // 过期字典，保存所有键的过期时间 dict *expires&#125; redisDB; 客户端执行 PEXPIREAT 命令，服务器会在数据库的过期字典中关联给定的数据库键和过期时间： 12345678910def PEXPIREAT(key, expire_time_in_ms):\t# 如果给定的键不存在于键空间，那么不能设置过期时间\tif key not in redisDb.dict: return 0 # 在过期字典中关联键和过期时间\tredisDB.expires[key] = expire_time_in_ms # 过期时间设置成功\treturn 1 时效状态TTL 和 PTTL 命令通过计算键的过期时间和当前时间之间的差，返回这个键的剩余生存时间 返回正数代表该数据在内存中还能存活的时间 返回 -1 代表永久性，返回 -2 代表键不存在 12ttl key #获取key的剩余时间，每次获取会自动变化(减小)，类似于倒计时pttl key #获取key的剩余时间，单位是毫秒，每次获取会自动变化(减小) PERSIST 是 PEXPIREAT 命令的反操作，在过期字典中查找给定的键，并解除键和值（过期时间）在过期字典中的关联 1persist key #切换key从时效性转换为永久性 Redis 通过过期字典可以检查一个给定键是否过期： 检查给定键是否存在于过期字典：如果存在，那么取得键的过期时间 检查当前 UNIX 时间戳是否大于键的过期时间：如果是那么键已经过期，否则键未过期 补充：AOF、RDB 和复制功能对过期键的处理 RDB ： 生成 RDB 文件，程序会对数据库中的键进行检查，已过期的键不会被保存到新创建的 RDB 文件中 载入 RDB 文件，如果服务器以主服务器模式运行，那么在载入时会对键进行检查，过期键会被忽略；如果服务器以从服务器模式运行，会载入所有键，包括过期键，但是主从服务器进行数据同步时就会删除这些键 AOF： 写入 AOF 文件，如果数据库中的某个键已经过期，但还没有被删除，那么 AOF 文件不会因为这个过期键而产生任何影响；当该过期键被删除，程序会向 AOF 文件追加一条 DEL 命令，显式的删除该键 AOF 重写，会对数据库中的键进行检查，忽略已经过期的键 复制：当服务器运行在复制模式下时，从服务器的过期键删除动作由主服务器控制 主服务器在删除一个过期键之后，会显式地向所有从服务器发送一个 DEL 命令，告知从服务器删除这个过期键 从服务器在执行客户端发送的读命令时，即使碰到过期键也不会将过期键删除，会当作未过期键处理，只有在接到主服务器发来的 DEL 命令之后，才会删除过期键（数据不一致） 过期删除删除策略删除策略就是针对已过期数据的处理策略，已过期的数据不一定被立即删除，在不同的场景下使用不同的删除方式会有不同效果，在内存占用与 CPU 占用之间寻找一种平衡，顾此失彼都会造成整体 Redis 性能的下降，甚至引发服务器宕机或内存泄露 针对过期数据有三种删除策略： 定时删除 惰性删除（被动删除） 定期删除 Redis 采用惰性删除和定期删除策略的结合使用 定时删除在设置键的过期时间的同时，创建一个定时器（timer），让定时器在键的过期时间到达时，立即执行对键的删除操作 优点：节约内存，到时就删除，快速释放掉不必要的内存占用 缺点：对 CPU 不友好，无论 CPU 此时负载多高均占用 CPU，会影响 Redis 服务器响应时间和指令吞吐量 总结：用处理器性能换取存储空间（拿时间换空间） 创建一个定时器需要用到 Redis 服务器中的时间事件，而时间事件的实现方式是无序链表，查找一个事件的时间复杂度为 O(N)，并不能高效地处理大量时间事件，所以采用这种方式并不现实 惰性删除数据到达过期时间不做处理，等下次访问到该数据时执行 expireIfNeeded() 判断： 如果输入键已经过期，那么 expireIfNeeded 函数将输入键从数据库中删除，接着访问就会返回空 如果输入键未过期，那么 expireIfNeeded 函数不做动作 所有的 Redis 读写命令在执行前都会调用 expireIfNeeded 函数进行检查，该函数就像一个过滤器，在命令真正执行之前过滤掉过期键 惰性删除的特点： 优点：节约 CPU 性能，删除的目标仅限于当前处理的键，不会在删除其他无关的过期键上花费任何 CPU 时间 缺点：内存压力很大，出现长期占用内存的数据，如果过期键永远不被访问，这种情况相当于内存泄漏 总结：用存储空间换取处理器性能（拿空间换时间） 定期删除定期删除策略是每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响 如果删除操作执行得太频繁，或者执行时间太长，就会退化成定时删除策略，将 CPU 时间过多地消耗在删除过期键上 如果删除操作执行得太少，或者执行时间太短，定期删除策略又会和惰性删除策略一样，出现浪费内存的情况 定期删除是周期性轮询 Redis 库中的时效性数据，从过期字典中随机抽取一部分键检查，利用过期数据占比的方式控制删除频度 Redis 启动服务器初始化时，读取配置 server.hz 的值，默认为 10，执行指令 info server 可以查看，每秒钟执行 server.hz 次 serverCron() → activeExpireCycle() activeExpireCycle() 对某个数据库中的每个 expires 进行检测，工作模式： 轮询每个数据库，从数据库中取出一定数量的随机键进行检查，并删除其中的过期键，如果过期 key 的比例超过了 25%，则继续重复此过程，直到过期 key 的比例下降到 25% 以下，或者这次任务的执行耗时超过了 25 毫秒 全局变量 current_db 用于记录 activeExpireCycle() 的检查进度（哪一个数据库），下一次调用时接着该进度处理 随着函数的不断执行，服务器中的所有数据库都会被检查一遍，这时将 current_db 重置为 0，然后再次开始新一轮的检查 定期删除特点： CPU 性能占用设置有峰值，检测频度可自定义设置 内存压力不是很大，长期占用内存的冷数据会被持续清理 周期性抽查存储空间（随机抽查，重点抽查） 数据淘汰逐出算法数据淘汰策略：当新数据进入 Redis 时，在执行每一个命令前，会调用 freeMemoryIfNeeded() 检测内存是否充足。如果内存不满足新加入数据的最低存储要求，Redis 要临时删除一些数据为当前指令清理存储空间，清理数据的策略称为逐出算法 逐出数据的过程不是 100% 能够清理出足够的可使用的内存空间，如果不成功则反复执行，当对所有数据尝试完毕，如不能达到内存清理的要求，出现 Redis 内存打满异常： 1(error) OOM command not allowed when used memory &gt;&#x27;maxmemory&#x27; 策略配置Redis 如果不设置最大内存大小或者设置最大内存大小为 0，在 64 位操作系统下不限制内存大小，在 32 位操作系统默认为 3GB 内存，一般推荐设置 Redis 内存为最大物理内存的四分之三 内存配置方式： 通过修改文件配置（永久生效）：修改配置文件 maxmemory 字段，单位为字节 通过命令修改（重启失效）： config set maxmemory 104857600：设置 Redis 最大占用内存为 100MB config get maxmemory：获取 Redis 最大占用内存 info ：可以查看 Redis 内存使用情况，used_memory_human 字段表示实际已经占用的内存，maxmemory 表示最大占用内存 影响数据淘汰的相关配置如下，配置 conf 文件： 每次选取待删除数据的个数，采用随机获取数据的方式作为待检测删除数据，防止全库扫描，导致严重的性能消耗，降低读写性能 1maxmemory-samples count 达到最大内存后的，对被挑选出来的数据进行删除的策略 1maxmemory-policy policy 数据删除的策略 policy：3 类 8 种 第一类：检测易失数据（可能会过期的数据集 server.db[i].expires）： 1234volatile-lru\t# 对设置了过期时间的 key 选择最近最久未使用使用的数据淘汰volatile-lfu\t# 对设置了过期时间的 key 选择最近使用次数最少的数据淘汰volatile-ttl\t# 对设置了过期时间的 key 选择将要过期的数据淘汰volatile-random\t# 对设置了过期时间的 key 选择任意数据淘汰 第二类：检测全库数据（所有数据集 server.db[i].dict ）： 123allkeys-lru # 对所有 key 选择最近最少使用的数据淘汰allkeLyRs-lfu\t# 对所有 key 选择最近使用次数最少的数据淘汰allkeys-random\t# 对所有 key 选择任意数据淘汰，相当于随机 第三类：放弃数据驱逐 1no-enviction\t#禁止驱逐数据(redis4.0中默认策略)，会引发OOM(Out Of Memory) 数据淘汰策略配置依据：使用 INFO 命令输出监控信息，查询缓存 hit 和 miss 的次数，根据需求调优 Redis 配置 排序机制基本介绍Redis 的 SORT 命令可以对列表键、集合键或者有序集合键的值进行排序，并不更改集合中的数据位置，只是查询 12SORT key [ASC/DESC] #对key中数据排序，默认对数字排序，并不更改集合中的数据位置，只是查询SORT key ALPHA #对key中字母排序，按照字典序 SORTSORT &lt;key&gt; 命令可以对一个包含数字值的键 key 进行排序 假设 RPUSH numbers 3 1 2，执行 SORT numbers 的详细步骤： 创建一个和 key 列表长度相同的数组，数组每项都是 redisSortObject 结构 123456789101112typedef struct redisSortObject &#123; // 被排序键的值 robj *obj; // 权重 union &#123; // 排序数字值时使用 double score; // 排序带有 BY 选项的字符串 robj *cmpobj; &#125; u;&#125; 遍历数组，将各个数组项的 obj 指针分别指向 numbers 列表的各个项 遍历数组，将 obj 指针所指向的列表项转换成一个 double 类型的浮点数，并将浮点数保存在对应数组项的 u.score 属性里 根据数组项 u.score 属性的值，对数组进行数字值排序，排序后的数组项按 u.score 属性的值从小到大排列 遍历数组，将各个数组项的 obj 指针所指向的值作为排序结果返回给客户端，程序首先访问数组的索引 0，依次向后访问 对于 SORT key [ASC/DESC] 函数： 在执行升序排序时，排序算法使用的对比函数产生升序对比结果 在执行降序排序时，排序算法使用的对比函数产生降序对比结果 BYSORT 命令默认使用被排序键中包含的元素作为排序的权重，元素本身决定了元素在排序之后所处的位置，通过使用 BY 选项，SORT 命令可以指定某些字符串键，或者某个哈希键所包含的某些域（field）来作为元素的权重，对一个键进行排序 12SORT &lt;key&gt; BY &lt;pattern&gt; # 数值SORT &lt;key&gt; BY &lt;pattern&gt; ALPHA\t# 字符 123456redis&gt; SADD fruits &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; (integer) 3redis&gt; SORT fruits ALPHA1)\t&quot;apple&quot;2)\t&quot;banana&quot;3)\t&quot;cherry&quot; 1234567redis&gt; MSET apple-price 8 banana-price 5.5 cherry-price 7 OK# 使用水果的价钱进行排序redis&gt; SORT fruits BY *-price1)\t&quot;banana&quot;2)\t&quot;cherry&quot;3)\t&quot;apple&quot; 实现原理：排序时的 u.score 属性就会被设置为对应的权重 LIMITSORT 命令默认会将排序后的所有元素都返回给客户端，通过 LIMIT 选项可以让 SORT 命令只返回其中一部分已排序的元素 1LIMIT &lt;offset&gt; &lt;count&gt; offset 参数表示要跳过的已排序元素数量 count 参数表示跳过给定数量的元素后，要返回的已排序元素数量 12345# 对应 a b c d e f gredis&gt; SORT alphabet ALPHA LIMIT 2 31) &quot;c&quot;2) &quot;d&quot;3) &quot;e&quot; 实现原理：在排序后的 redisSortObject 结构数组中，将指针移动到数组的索引 2 上，依次访问 array[2]、array[3]、array[4] 这 3 个数组项，并将数组项的 obj 指针所指向的元素返回给客户端 GETSORT 命令默认在对键进行排序后，返回被排序键本身所包含的元素，通过使用 GET 选项， 可以在对键进行排序后，根据被排序的元素以及 GET 选项所指定的模式，查找并返回某些键的值 1SORT &lt;key&gt; GET &lt;pattern&gt; 12345678redis&gt; SADD students &quot;tom&quot; &quot;jack&quot; &quot;sea&quot;#设置全名redis&gt; SET tom-name &quot;Tom Li&quot; OK redis&gt; SET jack-name &quot;Jack Wang&quot; OK redis&gt; SET sea-name &quot;Sea Zhang&quot;OK 1234redis&gt; SORT students ALPHA GET *-name1)\t&quot;Jack Wang&quot;2)\t&quot;Sea Zhang&quot;3) &quot;Tom Li&quot; 实现原理：对 students 进行排序后，对于 jack 元素和 *-name 模式，查找程序返回键 jack-name，然后获取 jack-name 键对应的值 STORESORT 命令默认只向客户端返回排序结果，而不保存排序结果，通过使用 STORE 选项可以将排序结果保存在指定的键里面 1SORT &lt;key&gt; STORE &lt;sort_key&gt; 1234redis&gt; SADD students &quot;tom&quot; &quot;jack&quot; &quot;sea&quot;(integer) 3 redis&gt; SORT students ALPHA STORE sorted_students (integer) 3 实现原理：排序后，检查 sorted_students 键是否存在，如果存在就删除该键，设置 sorted_students 为空白的列表键，遍历排序数组将元素依次放入 执行顺序调用 SORT 命令，除了 GET 选项之外，改变其他选项的摆放顺序并不会影响命令执行选项的顺序 1SORT &lt;key&gt; ALPHA [ASC/DESC] BY &lt;by-pattern&gt; LIMIT &lt;offset&gt; &lt;count&gt; GET &lt;get-pattern&gt; STORE &lt;store_key&gt; 执行顺序： 排序：命令会使用 ALPHA 、ASC 或 DESC、BY 这几个选项，对输入键进行排序，并得到一个排序结果集 限制排序结果集的长度：使用 LIMIT 选项，对排序结果集的长度进行限制 获取外部键：根据排序结果集中的元素以及 GET 选项指定的模式，查找并获取指定键的值，并用这些值来作为新的排序结果集 保存排序结果集：使用 STORE 选项，将排序结果集保存到指定的键上面去 向客户端返回排序结果集：最后一步命令遍历排序结果集，并依次向客户端返回排序结果集中的元素 通知机制数据库通知是可以让客户端通过订阅给定的频道或者模式，来获知数据库中键的变化，以及数据库中命令的执行情况 关注某个键执行了什么命令的通知称为键空间通知（key-space notification） 关注某个命令被什么键执行的通知称为键事件通知（key-event notification） 图示订阅 0 号数据库 message 键： 服务器配置的 notify-keyspace-events 选项决定了服务器所发送通知的类型 AKE 代表服务器发送所有类型的键空间通知和键事件通知 AK 代表服务器发送所有类型的键空间通知 AE 代表服务器发送所有类型的键事件通知 K$ 代表服务器只发送和字符串键有关的键空间通知 EL 代表服务器只发送和列表键有关的键事件通知 ….. 发送数据库通知的功能是由 notifyKeyspaceEvent 函数实现的： 如果给定的通知类型 type 不是服务器允许发送的通知类型，那么函数会直接返回 如果给定的通知是服务器允许发送的通知 检测服务器是否允许发送键空间通知，允许就会构建并发送事件通知 检测服务器是否允许发送键事件通知，允许就会构建并发送事件通知 体系架构事件驱动基本介绍Redis 服务器是一个事件驱动程序，服务器需要处理两类事件 文件事件 (file event)：服务器通过套接字与客户端（或其他 Redis 服务器）进行连接，而文件事件就是服务器对套接字操作的抽象。服务器与客户端的通信会产生相应的文件事件，服务器通过监听并处理这些事件完成一系列网络通信操作 时间事件 (time event)：Redis 服务器中的一些操作（比如 serverCron 函数）需要在指定时间执行，而时间事件就是服务器对这类定时操作的抽象 文件事件基本组成Redis 基于 Reactor 模式开发了网络事件处理器，这个处理器被称为文件事件处理器 (file event handler) 使用 I&#x2F;O 多路复用 (multiplexing) 程序来同时监听多个套接字，并根据套接字执行的任务来为套接字关联不同的事件处理器 当被监听的套接字准备好执行连接应答 (accept)、 读取 (read)、 写入 (write)、 关闭 (close) 等操作时，与操作相对应的文件事件就会产生，这时文件事件分派器会调用套接字关联好的事件处理器来处理事件 文件事件处理器以单线程方式运行，但通过使用 I&#x2F;O 多路复用程序来监听多个套接字， 既实现了高性能的网络通信模型，又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，保持了 Redis 内部单线程设计的简单性 文件事件处理器的组成结构： I&#x2F;O 多路复用程序将所有产生事件的套接字处理请求放入一个单线程的执行队列中，通过队列有序、同步的向文件事件分派器传送套接字，上一个套接字产生的事件处理完后，才会继续向分派器传送下一个 Redis 单线程也能高效的原因： 纯内存操作 核心是基于非阻塞的 IO 多路复用机制，单线程可以高效处理多个请求 底层使用 C 语言实现，C 语言实现的程序距离操作系统更近，执行速度相对会更快 单线程同时也避免了多线程的上下文频繁切换问题，预防了多线程可能产生的竞争问题 多路复用Redis 的 I&#x2F;O 多路复用程序的所有功能都是通过包装常见的 select 、epoll、 evport 和 kqueue 这些函数库来实现的，Redis 在 I&#x2F;O 多路复用程序的实现源码中用 #include 宏定义了相应的规则，编译时自动选择系统中性能最高的多路复用函数来作为底层实现 I&#x2F;O 多路复用程序监听多个套接字的 AE_READABLE 事件和 AE_WRITABLE 事件，这两类事件和套接字操作之间的对应关系如下： 当套接字变得可读时（客户端对套接字执行 write 操作或者 close 操作），或者有新的可应答（acceptable）套接字出现时（客户端对服务器的监听套接字执行 connect 连接操作），套接字产生 AE_READABLE 事件 当套接字变得可写时（客户端对套接字执行 read 操作，对于服务器来说就是可以写了），套接字产生 AE_WRITABLE 事件 I&#x2F;O 多路复用程序允许服务器同时监听套接字的 AE_READABLE 和 AE_WRITABLE 事件， 如果一个套接字同时产生了这两种事件，那么文件事件分派器会优先处理 AE_READABLE 事件， 等 AE_READABLE 事件处理完之后才处理 AE_WRITABLE 事件 处理器Redis 为文件事件编写了多个处理器，这些事件处理器分别用于实现不同的网络通信需求： 连接应答处理器，用于对连接服务器的各个客户端进行应答，Redis 服务器初始化时将该处理器与 AE_READABLE 事件关联 命令请求处理器，用于接收客户端传来的命令请求，执行套接字的读入操作，与 AE_READABLE 事件关联 命令回复处理器，用于向客户端返回命令的执行结果，执行套接字的写入操作，与 AE_WRITABLE 事件关联 复制处理器，当主服务器和从服务器进行复制操作时，主从服务器都需要关联该处理器 Redis 客户端与服务器进行连接并发送命令的整个过程： Redis 服务器正在运作监听套接字的 AE_READABLE 事件，关联连接应答处理器 当 Redis 客户端向服务器发起连接，监听套接字将产生 AE_READABLE 事件，触发连接应答处理器执行，对客户端的连接请求进行应答，创建客户端套接字以及客户端状态，并将客户端套接字的 AE_READABLE 事件与命令请求处理器进行关联 客户端向服务器发送命令请求，客户端套接字产生 AE_READABLE 事件，引发命令请求处理器执行，读取客户端的命令内容传给相关程序去执行 执行命令会产生相应的命令回复，为了将这些命令回复传送回客户端，服务器会将客户端套接字的 AE_WRITABLE 事件与命令回复处理器进行关联 当客户端尝试读取命令回复时，客户端套接字产生 AE_WRITABLE 事件，触发命令回复处理器执行，在命令回复全部写入套接字后，服务器就会解除客户端套接字的 AE_WRITABLE 事件与命令回复处理器之间的关联 时间事件Redis 的时间事件分为以下两类： 定时事件：在指定的时间之后执行一次（Redis 中暂时未使用） 周期事件：每隔指定时间就执行一次 一个时间事件主要由以下三个属性组成： id：服务器为时间事件创建的全局唯一 ID（标识号），从小到大顺序递增，新事件的 ID 比旧事件的 ID 号要大 when：毫秒精度的 UNIX 时间戳，记录了时间事件的到达（arrive）时间 timeProc：时间事件处理器，当时间事件到达时，服务器就会调用相应的处理器来处理事件 时间事件是定时事件还是周期性事件取决于时间事件处理器的返回值： 定时事件：事件处理器返回 AE_NOMORE，该事件在到达一次后就会被删除 周期事件：事件处理器返回非 AE_NOMORE 的整数值，服务器根据该值对事件的 when 属性更新，让该事件在一段时间后再次交付 服务器将所有时间事件都放在一个无序链表中，新的时间事件插入到链表的表头： 无序链表指是链表不按 when 属性的大小排序，每当时间事件执行器运行时就必须遍历整个链表，查找所有已到达的时间事件，并调用相应的事件处理器处理 无序链表并不影响时间事件处理器的性能，因为正常模式下的 Redis 服务器只使用 serverCron 一个时间事件，在 benchmark 模式下服务器也只使用两个时间事件，所以无序链表不会影响服务器的性能，几乎可以按照一个指针处理 事件调度服务器中同时存在文件事件和时间事件两种事件类型，调度伪代码： 1234567891011121314151617181920# 事件调度伪代码def aeProcessEvents():\t# 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTime() # 计算最接近的时间事件距离到达还有多少亳秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，设置为 0 if remaind_ms &lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval 结构\ttimeval = create_timeval_with_ms(remaind_ms) # 【阻塞并等待文件事件】产生，最大阻塞时间由传入的timeval结构决定，remaind_ms的值为0时调用后马上返回，不阻塞 aeApiPoll(timeval) # 处理所有已产生的文件事件\tprocessFileEvents() # 处理所有已到达的时间事件\tprocessTimeEvents() 事件的调度和执行规则： aeApiPoll 函数的最大阻塞时间由到达时间最接近当前时间的时间事件决定，可以避免服务器对时间事件进行频繁的轮询（忙等待），也可以确保 aeApiPoll 函数不会阻塞过长时间 对文件事件和时间事件的处理都是同步、有序、原子地执行，服务器不会中途中断事件处理，也不会对事件进行抢占，所以两种处理器都要尽可地减少程序的阻塞时间，并在有需要时主动让出执行权，从而降低事件饥饿的可能性 命令回复处理器在写入字节数超过了某个预设常量，就会主动用 break 跳出写入循环，将余下的数据留到下次再写 时间事件也会将非常耗时的持久化操作放到子线程或者子进程执行 时间事件在文件事件之后执行，并且事件之间不会出现抢占，所以时间事件的实际处理时间通常会比设定的到达时间稍晚 多线程Redis6.0 引入多线程主要是为了提高网络 IO 读写性能，因为这是 Redis 的一个性能瓶颈（Redis 的瓶颈主要受限于内存和网络），多线程只是用来处理网络数据的读写和协议解析， 执行命令仍然是单线程顺序执行，因此不需要担心线程安全问题。 Redis6.0 的多线程默认是禁用的，只使用主线程。如需开启需要修改 redis 配置文件 redis.conf ： 1io-threads-do-reads yesCopy to clipboardErrorCopied 开启多线程后，还需要设置线程数，否则是不生效的，同样需要修改 redis 配置文件 : 1io-threads 4 #官网建议4核的机器建议设置为2或3个线程，8核的建议设置为6个线程 参考文章：https://mp.weixin.qq.com/s/dqmiR0ECf4lB6Y2OyK-dyA 客户端基本介绍Redis 服务器是典型的一对多程序，一个服务器可以与多个客户端建立网络连接，服务器对每个连接的客户端建立了相应的 redisClient 结构（客户端状态，在服务器端的存储结构），保存了客户端当前的状态信息，以及执行相关功能时需要用到的数据结构 Redis 服务器状态结构的 clients 属性是一个链表，这个链表保存了所有与服务器连接的客户端的状态结构： 123456struct redisServer &#123; // 一个链表，保存了所有客户端状态 list *clients; //...&#125;; 数据结构redisClient客户端的数据结构： 123456789101112131415161718192021222324252627282930313233343536typedef struct redisClient &#123; //... // 套接字 int fd; // 名字 robj *name; // 标志 int flags; // 输入缓冲区 sds querybuf; // 输出缓冲区 buf 数组 char buf[REDIS_REPLY_CHUNK_BYTES]; // 记录了 buf 数组目前已使用的字节数量 int bufpos; // 可变大小的输出缓冲区，链表 + 字符串对象 list *reply; // 命令数组 rboj **argv; // 命令数组的长度 int argc; // 命令的信息 struct redisCommand *cmd; // 是否通过身份验证 int authenticated; // 创建客户端的时间 time_t ctime; // 客户端与服务器最后一次进行交互的时间 time_t lastinteraction; // 输出缓冲区第一次到达软性限制 (soft limit) 的时间 time_t obuf_soft_limit_reached_time;&#125; 客户端状态包括两类属性 一类是比较通用的属性，这些属性很少与特定功能相关，无论客户端执行的是什么工作，都要用到这些属性 另一类是和特定功能相关的属性，比如操作数据库时用到的 db 属性和 dict id 属性，执行事务时用到的 mstate 属性，以及执行 WATCH 命令时用到的 watched_keys 属性等，代码中没有列出 套接字客户端状态的 fd 属性记录了客户端正在使用的套接字描述符，根据客户端类型的不同，fd 属性的值可以是 -1 或者大于 -1 的整数： 伪客户端 (fake client) 的 fd 属性的值为 -1，命令请求来源于 AOF 文件或者 Lua 脚本，而不是网络，所以不需要套接字连接 普通客户端的 fd 属性的值为大于 -1 的整数，因为合法的套接字描述符不能是 -1 执行 CLIENT list 命令可以列出目前所有连接到服务器的普通客户端，不包括伪客户端 名字在默认情况下，一个连接到服务器的客户端是没有名字的，使用 CLIENT setname 命令可以为客户端设置一个名字 标志客户端的标志属性 flags 记录了客户端的角色以及客户端目前所处的状态，每个标志使用一个常量表示 flags 的值可以是单个标志：flags = &lt;flag&gt; flags 的值可以是多个标志的二进制：flags = &lt;flagl&gt; | &lt;flag2&gt; | ... 一部分标志记录客户端的角色： REDIS_MASTER 表示客户端是一个从服务器，REDIS_SLAVE 表示客户端是一个从服务器，在主从复制时使用 REDIS_PRE_PSYNC 表示客户端是一个版本低于 Redis2.8 的从服务器，主服务器不能使用 PSYNC 命令与该从服务器进行同步，这个标志只能在 REDIS_ SLAVE 标志处于打开状态时使用 REDIS_LUA_CLIENT 表示客户端是专门用于处理 Lua 脚本里面包含的 Redis 命令的伪客户端 一部分标志记录目前客户端所处的状态： REDIS_UNIX_SOCKET 表示服务器使用 UNIX 套接字来连接客户端 REDIS_BLOCKED 表示客户端正在被 BRPOP、BLPOP 等命令阻塞 REDIS_UNBLOCKED 表示客户端已经从 REDIS_BLOCKED 所表示的阻塞状态脱离，在 REDIS_BLOCKED 标志打开的情况下使用 REDIS_MULTI 标志表示客户端正在执行事务 REDIS_DIRTY_CAS 表示事务使用 WATCH 命令监视的数据库键已经被修改 ….. 缓冲区客户端状态的输入缓冲区用于保存客户端发送的命令请求，输入缓冲区的大小会根据输入内容动态地缩小或者扩大，但最大大小不能超过 1GB，否则服务器将关闭这个客户端，比如执行 SET key value ，那么缓冲区 querybuf 的内容： 1*3\\r $3\\r SET\\r $3\\r key\\r $5\\r value\\r # 输出缓冲区是服务器用于保存执行客户端命令所得的命令回复，每个客户端都有两个输出缓冲区可用： 一个是固定大小的缓冲区，保存长度比较小的回复，比如 OK、简短的字符串值、整数值、错误回复等 一个是可变大小的缓冲区，保存那些长度比较大的回复， 比如一个非常长的字符串值或者一个包含了很多元素的集合等 buf 是一个大小为 REDIS_REPLY_CHUNK_BYTES (常量默认 16*1024 &#x3D; 16KB) 字节的字节数组，bufpos 属性记录了 buf 数组目前已使用的字节数量，当 buf 数组的空间已经用完或者回复数据太大无法放进 buf 数组里，服务器就会开始使用可变大小的缓冲区 通过使用 reply 链表连接多个字符串对象，可以为客户端保存一个非常长的命令回复，而不必受到固定大小缓冲区 16KB 大小的限制 命令服务器对 querybuf 中的命令请求的内容进行分析，得出的命令参数以及参数的数量分别保存到客户端状态的 argv 和 argc 属性 argv 属性是一个数组，数组中的每项都是字符串对象，其中 argv[0] 是要执行的命令，而之后的其他项则是命令的参数 argc 属性负责记录 argv 数组的长度 服务器将根据项 argv[0] 的值，在命令表中查找命令所对应的命令的 redisCommand，将客户端状态的 cmd 指向该结构 命令表是一个字典结构，键是 SDS 结构保存命令的名字；值是命令所对应的 redisCommand 结构，保存了命令的实现函数、命令标志、 命令应该给定的参数个数、命令的总执行次数和总消耗时长等统计信息 验证客户端状态的 authenticated 属性用于记录客户端是否通过了身份验证 authenticated 值为 0，表示客户端未通过身份验证 authenticated 值为 1，表示客户端已通过身份验证 当客户端 authenticated &#x3D; 0 时，除了 AUTH 命令之外， 客户端发送的所有其他命令都会被服务器拒绝执行 123456redis&gt; PING (error) NOAUTH Authentication required.redis&gt; AUTH 123321 OKredis&gt; PING PONG 时间ctime 属性记录了创建客户端的时间，这个时间可以用来计算客户端与服务器已经连接了多少秒，CLIENT list 命令的 age 域记录了这个秒数 lastinteraction 属性记录了客户端与服务器最后一次进行互动 (interaction) 的时间，互动可以是客户端向服务器发送命令请求，也可以是服务器向客户端发送命令回复。该属性可以用来计算客户端的空转 (idle) 时长， 就是距离客户端与服务器最后一次进行互动已经过去了多少秒，CLIENT list 命令的 idle 域记录了这个秒数 obuf_soft_limit_reached_time 属性记录了输出缓冲区第一次到达软性限制 (soft limit) 的时间 生命周期创建服务器使用不同的方式来创建和关闭不同类型的客户端 如果客户端是通过网络连接与服务器进行连接的普通客户端，那么在客户端使用 connect 函数连接到服务器时，服务器就会调用连接应答处理器为客户端创建相应的客户端状态，并将这个新的客户端状态添加到服务器状态结构 clients 链表的末尾 服务器会在初始化时创建负责执行 Lua 脚本中包含的 Redis 命令的伪客户端，并将伪客户端关联在服务器状态的 lua_client 属性 123456struct redisServer &#123; // 保存伪客户端 redisClient *lua_client； //...&#125;; lua_client 伪客户端在服务器运行的整个生命周期会一直存在，只有服务器被关闭时，这个客户端才会被关闭 载入 AOF 文件时， 服务器会创建用于执行 AOF 文件包含的 Redis 命令的伪客户端，并在载入完成之后，关闭这个伪客户端 关闭一个普通客户端可以因为多种原因而被关闭： 客户端进程退出或者被杀死，那么客户端与服务器之间的网络连接将被关闭，从而造成客户端被关闭 客户端向服务器发送了带有不符合协议格式的命令请求，那么这个客户端会被服务器关闭 客户端是 CLIENT KILL 命令的目标 如果用户为服务器设置了 timeout 配置选项，那么当客户端的空转时间超过该值时将被关闭，特殊情况不会被关闭： 客户端是主服务器（REDIS_MASTER ）或者从服务器（打开了 REDIS_SLAVE 标志） 正在被 BLPOP 等命令阻塞（REDIS_BLOCKED） 正在执行 SUBSCRIBE、PSUBSCRIBE 等订阅命令 客户端发送的命令请求的大小超过了输入缓冲区的限制大小（默认为 1GB） 发送给客户端的命令回复的大小超过了输出缓冲区的限制大小 理论上来说，可变缓冲区可以保存任意长的命令回复，但是为了回复过大占用过多的服务器资源，服务器会时刻检查客户端的输出缓冲区的大小，并在缓冲区的大小超出范围时，执行相应的限制操作： 硬性限制 (hard limit)：输出缓冲区的大小超过了硬性限制所设置的大小，那么服务器会关闭客户端（serverCron 函数中执行），积存在输出缓冲区中的所有内容会被直接释放，不会返回给客户端 软性限制 (soft limit)：输出缓冲区的大小超过了软性限制所设置的大小，小于硬性限制的大小，服务器的操作： 用属性 obuf_soft_limit_reached_time 记录下客户端到达软性限制的起始时间，继续监视客户端 如果输出缓冲区的大小一直超出软性限制，并且持续时间超过服务器设定的时长，那么服务器将关闭客户端 如果在指定时间内不再超出软性限制，那么客户端就不会被关闭，并且 o_s_l_r_t 属性清零 使用 client-output-buffer-limit 选项可以为普通客户端、从服务器客户端、执行发布与订阅功能的客户端分别设置不同的软性限制和硬性限制，格式： 12345client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 第一行：将普通客户端的硬性限制和软性限制都设置为 0，表示不限制客户端的输出缓冲区大小 第二行：将从服务器客户端的硬性限制设置为 256MB，软性限制设置为 64MB，软性限制的时长为 60 秒 第三行：将执行发布与订阅功能的客户端的硬性限制设置为 32MB，软性限制设置为 8MB，软性限制的时长为 60 秒 服务器执行流程Redis 服务器与多个客户端建立网络连接，处理客户端发送的命令请求，在数据库中保存客户端执行命令所产生的数据，并通过资源管理来维持服务器自身的运转，所以一个命令请求从发送到获得回复的过程中，客户端和服务器需要完成一系列操作 命令请求Redis 服务器的命令请求来自 Redis 客户端，当用户在客户端中键入一个命令请求时，客户端会将这个命令请求转换成协议格式，通过连接到服务器的套接字，将协议格式的命令请求发送给服务器 12SET KEY VALUE -&gt;\t# 命令*3\\r S3\\r SET\\r $3\\r KEY\\r $5\\r VALUE\\r # 协议格式 当客户端与服务器之间的连接套接字因为客户端的写入而变得可读，服务器调用命令请求处理器来执行以下操作： 读取套接字中协议格式的命令请求，并将其保存到客户端状态的输入缓冲区里面 对输入缓冲区中的命令请求进行分析，提取出命令请求中包含的命令参数以及命令参数的个数，然后分别将参数和参数个数保存到客户端状态的 argv 属性和 argc 属性里 调用命令执行器，执行客户端指定的命令 最后客户端接收到协议格式的命令回复之后，会将这些回复转换成用户可读的格式打印给用户观看，至此整体流程结束 命令执行命令执行器开始对命令操作： 查找命令：首先根据客户端状态的 argv[0] 参数，在命令表 (command table) 中查找参数所指定的命令，并将找到的命令保存到客户端状态的 cmd 属性里面，是一个 redisCommand 结构 命令查找算法与字母的大小写无关，所以命令名字的大小写不影响命令表的查找结果 执行预备操作： 检查客户端状态的 cmd 指针是否指向 NULL，根据 redisCommand 检查请求参数的数量是否正确 检查客户端是否通过身份验证 如果服务器打开了 maxmemory 功能，执行命令之前要先检查服务器的内存占用，在有需要时进行内存回收（逐出算法） 如果服务器上一次执行 BGSAVE 命令出错，并且服务器打开了 stop-writes-on-bgsave-error 功能，那么如果本次执行的是写命令，服务会拒绝执行，并返回错误 如果客户端当前正在用 SUBSCRIBE 或 PSUBSCRIBE 命令订阅频道，那么服务器会拒绝除了 SUBSCRIBE、SUBSCRIBE、 UNSUBSCRIBE、PUNSUBSCRIBE 之外的其他命令 如果服务器正在进行载入数据，只有 sflags 带有 1 标识（比如 INFO、SHUTDOWN、PUBLISH等）的命令才会被执行 如果服务器执行 Lua 脚本而超时并进入阻塞状态，那么只会执行客户端发来的 SHUTDOWN nosave 和 SCRIPT KILL 命令 如果客户端正在执行事务，那么服务器只会执行客户端发来的 EXEC、DISCARD、MULTI、WATCH 四个命令，其他命令都会被放进事务队列中 如果服务器打开了监视器功能，那么会将要执行的命令和参数等信息发送给监视器 调用命令的实现函数：被调用的函数会执行指定的操作并产生相应的命令回复，回复会被保存在客户端状态的输出缓冲区里面（buf 和 reply 属性），然后实现函数还会为客户端的套接字关联命令回复处理器，这个处理器负责将命令回复返回给客户端 执行后续工作： 如果服务器开启了慢查询日志功能，那么慢查询日志模块会检查是否需要为刚刚执行完的命令请求添加一条新的慢查询日志 根据执行命令所耗费的时长，更新命令的 redisCommand 结构的 milliseconds 属性，并将命令 calls 计数器的值增一 如果服务器开启了 AOF 持久化功能，那么 AOF 持久化模块会将执行的命令请求写入到 AOF 缓冲区里面 如果有其他从服务器正在复制当前这个服务器，那么服务器会将执行的命令传播给所有从服务器 将命令回复发送给客户端：客户端套接字变为可写状态时，服务器就会执行命令回复处理器，将客户端输出缓冲区中的命令回复发送给客户端，发送完毕之后回复处理器会清空客户端状态的输出缓冲区，为处理下一个命令请求做好准备 Command每个 redisCommand 结构记录了一个Redis 命令的实现信息，主要属性 1234567891011121314151617181920212223242526struct redisCommand &#123; // 命令的名字，比如&quot;set&quot; char *name; // 函数指针，指向命令的实现函数，比如setCommand // redisCommandProc 类型的定义为 typedef void redisCommandProc(redisClient *c) redisCommandProc *proc; // 命令参数的个数，用于检查命令请求的格式是否正确。如果这个值为负数-N, 那么表示参数的数量大于等于N。 // 注意命令的名字本身也是一个参数，比如 SET msg &quot;hello&quot;，命令的参数是&quot;SET&quot;、&quot;msg&quot;、&quot;hello&quot; 三个\tint arity; // 字符串形式的标识值，这个值记录了命令的属性，， // 比如这个命令是写命令还是读命令，这个命令是否允许在载入数据时使用，是否允许在Lua脚本中使用等等 char *sflags; // 对sflags标识进行分析得出的二进制标识，由程序自动生成。服务器对命令标识进行检查时使用的都是 flags 属性 // 而不是sflags属性，因为对二进制标识的检查可以方便地通过&amp; ^ ~ 等操作来完成 int flags; // 服务器总共执行了多少次这个命令 long long calls; // 服务器执行这个命令所耗费的总时长 long long milliseconds;&#125;; serverCron基本介绍Redis 服务器以周期性事件的方式来运行 serverCron 函数，服务器初始化时读取配置 server.hz 的值，默认为 10，代表每秒钟执行 10 次，即每隔 100 毫秒执行一次，执行指令 info server 可以查看 serverCron 函数负责定期对自身的资源和状态进行检查和调整，从而确保服务器可以长期、稳定地运行 更新服务器的各类统计信息，比如时间、内存占用、 数据库占用情况等 清理数据库中的过期键值对 关闭和清理连接失效的客户端 进行 AOF 或 RDB 持久化操作 如果服务器是主服务器，那么对从服务器进行定期同步 如果处于集群模式，对集群进行定期同步和连接测试 时间缓存Redis 服务器中有很多功能需要获取系统的当前时间，而每次获取系统的当前时间都需要执行一次系统调用，为了减少系统调用的执行次数，服务器状态中的 unixtime 属性和 mstime 属性被用作当前时间的缓存 1234567struct redisServer &#123; // 保存了秒级精度的系统当前UNIX时间戳 time_t unixtime;\t// 保存了毫秒级精度的系统当前UNIX时间戳 long long mstime; &#125;; serverCron 函数默认以每 100 毫秒一次的频率更新两个属性，所以属性记录的时间的精确度并不高 服务器只会在打印日志、更新服务器的 LRU 时钟、决定是否执行持久化任务、计算服务器上线时间（uptime）这类对时间精确度要求不高的功能上 对于为键设置过期时间、添加慢查询日志这种需要高精确度时间的功能来说，服务器还是会再次执行系统调用，从而获得最准确的系统当前时间 LRU 时钟服务器状态中的 lruclock 属性保存了服务器的 LRU 时钟 1234struct redisServer &#123; // 默认每10秒更新一次的时钟缓存，用于计算键的空转(idle)时长。 unsigned lruclock:22; &#125;; 每个 Redis 对象都会有一个 lru 属性， 这个 lru 属性保存了对象最后一次被命令访问的时间 123typedef struct redisObiect &#123;\tunsigned lru:22; &#125; robj; 当服务器要计算一个数据库键的空转时间（即数据库键对应的值对象的空转时间），程序会用服务器的 lruclock 属性记录的时间减去对象的 lru 属性记录的时间 serverCron 函数默认以每 100 毫秒一次的频率更新这个属性，所以得出的空转时间也是模糊的 命令次数serverCron 中的 trackOperationsPerSecond 函数以每 100 毫秒一次的频率执行，函数功能是以抽样计算的方式，估算并记录服务器在最近一秒钟处理的命令请求数量，这个值可以通过 INFO status 命令的 instantaneous_ops_per_sec 域查看： 123redis&gt; INFO stats# Stats instantaneous_ops_per_sec:6 根据上一次抽样时间 ops_sec_last_sample_time 和当前系统时间，以及上一次已执行的命令数 ops_sec_last_sample_ops 和服务器当前已经执行的命令数，计算出两次函数调用期间，服务器平均每毫秒处理了多少个命令请求，该值乘以 1000 得到每秒内的执行命令的估计值，放入 ops_sec_samples 环形数组里 12345678910struct redisServer &#123; // 上一次进行抽样的时间\tlong long ops_sec_last_sample_time; // 上一次抽样时，服务器已执行命令的数量 long long ops_sec_last_sample_ops; // REDIS_OPS_SEC_SAMPLES 大小（默认值为16)的环形数组，数组的每一项记录一次的抽样结果 long long ops_sec_samples[REDIS_OPS_SEC_SAMPLES]; // ops_sec_samples数组的索引值，每次抽样后将值自增一，值为16时重置为0，让数组成为一个环形数组 int ops_sec_idx;&#125;; 内存峰值服务器状态里的 stat_peak_memory 属性记录了服务器内存峰值大小，循环函数每次执行时都会查看服务器当前使用的内存数量，并与 stat_peak_memory 保存的数值进行比较，设置为较大的值 1234struct redisServer &#123; // 已使用内存峰值 size_t stat_peak_memory;&#125;; INFO memory 命令的 used_memory_peak 和 used_memory_peak_human 两个域分别以两种格式记录了服务器的内存峰值： 12345redis&gt; INFO memory # Memory ...used_memory_peak:501824 used_memory_peak_human:490.06K SIGTERM服务器启动时，Redis 会为服务器进程的 SIGTERM 信号关联处理器 sigtermHandler 函数，该信号处理器负责在服务器接到 SIGTERM 信号时，打开服务器状态的 shutdown_asap 标识 1234struct redisServer &#123; // 关闭服务器的标识：值为1时关闭服务器，值为0时不做操作 int shutdown_asap;&#125;; 每次 serverCron 函数运行时，程序都会对服务器状态的 shutdown_asap 属性进行检查，并根据属性的值决定是否关闭服务器 服务器在接到 SIGTERM 信号之后，关闭服务器并打印相关日志的过程： 12345[6794 | signal handler] (1384435690) Received SIGTERM, scheduling shutdown ... [6794] 14 Nov 21:28:10.108 # User requested shutdown ... [6794] 14 Nov 21:28:10.108 * Saving the final RDB snapshot before exiting. [6794) 14 Nov 21:28:10.161 * DB saved on disk [6794) 14 Nov 21:28:10.161 # Redisis now ready to exit, bye bye ... 管理资源serverCron 函数每次执行都会调用 clientsCron 和 databasesCron 函数，进行管理客户端资源和数据库资源 clientsCron 函数对一定数量的客户端进行以下两个检查： 如果客户端与服务器之间的连接巳经超时（很长一段时间客户端和服务器都没有互动），那么程序释放这个客户端 如果客户端在上一次执行命令请求之后，输入缓冲区的大小超过了一定的长度，那么程序会释放客户端当前的输入缓冲区，并重新创建一个默认大小的输入缓冲区，从而防止客户端的输入缓冲区耗费了过多的内存 databasesCron 函数会对服务器中的一部分数据库进行检查，删除其中的过期键，并在有需要时对字典进行收缩操作 持久状态服务器状态中记录执行 BGSAVE 命令和 BGREWRITEAOF 命令的子进程的 ID 123456struct redisServer &#123; // 记录执行BGSAVE命令的子进程的ID，如果服务器没有在执行BGSAVE，那么这个属性的值为-1 pid_t rdb_child_pid; // 记录执行BGREWRITEAOF命令的子进程的ID，如果服务器没有在执行那么这个属性的值为-1 pid_t aof_child_pid&#125;; serverCron 函数执行时，会检查两个属性的值，只要其中一个属性的值不为 -1，程序就会执行一次 wait3 函数，检查子进程是否有信号发来服务器进程： 如果有信号到达，那么表示新的 RDB 文件已经生成或者 AOF 重写完毕，服务器需要进行相应命令的后续操作，比如用新的 RDB 文件替换现有的 RDB 文件，用重写后的 AOF 文件替换现有的 AOF 文件 如果没有信号到达，那么表示持久化操作未完成，程序不做动作 如果两个属性的值都为 -1，表示服务器没有进行持久化操作 查看是否有 BGREWRITEAOF 被延迟，然后执行 AOF 后台重写 查看服务器的自动保存条件是否已经被满足，并且服务器没有在进行持久化，就开始一次新的 BGSAVE 操作 因为条件 1 可能会引发一次 AOF，所以在这个检查中会再次确认服务器是否已经在执行持久化操作 检查服务器设置的 AOF 重写条件是否满足，条件满足并且服务器没有进行持久化，就进行一次 AOF 重写 如果服务器开启了 AOF 持久化功能，并且 AOF 缓冲区里还有待写入的数据， 那么 serverCron 函数会调用相应的程序，将 AOF 缓冲区中的内容写入到 AOF 文件里 延迟执行在服务器执行 BGSAVE 命令的期间，如果客户端发送 BGREWRITEAOF 命令，那么服务器会将 BGREWRITEAOF 命令的执行时间延迟到 BGSAVE 命令执行完毕之后，用服务器状态的 aof_rewrite_scheduled 属性标识延迟与否 1234struct redisServer &#123; // 如果值为1，那么表示有 BGREWRITEAOF命令被延迟了 int aof_rewrite_scheduled;&#125;; serverCron 函数会检查 BGSAVE 或者 BGREWRITEAOF 命令是否正在执行，如果这两个命令都没在执行，并且 aof_rewrite_scheduled 属性的值为 1，那么服务器就会执行之前被推延的 BGREWRITEAOF 命令 执行次数服务器状态的 cronloops 属性记录了 serverCron 函数执行的次数 1234struct redisServer &#123; // serverCron 函数每执行一次，这个属性的值就增 1 int cronloops;&#125;; 缓冲限制服务器会关闭那些输入或者输出缓冲区大小超出限制的客户端 初始化初始结构一个 Redis 服务器从启动到能够接受客户端的命令请求，需要经过一系列的初始化和设置过程 第一步：创建一个 redisServer 类型的实例变量 server 作为服务器的状态，并为结构中的各个属性设置默认值，由 initServerConfig 函数进行初始化一般属性： 设置服务器的运行 ID、默认运行频率、默认配置文件路径、默认端口号、默认 RDB 持久化条件和 AOF 持久化条件 初始化服务器的 LRU 时钟，创建命令表 第二步：载入配置选项，用户可以通过给定配置参数或者指定配置文件，对 server 变量相关属性的默认值进行修改 第三步：初始化服务器数据结构（除了命令表之外），因为服务器必须先载入用户指定的配置选项才能正确地对数据结构进行初始化，所以载入配置完成后才进性数据结构的初始化，服务器将调用 initServer 函数： server.clients 链表，记录了的客户端的状态结构；server.db 数组，包含了服务器的所有数据库 用于保存频道订阅信息的 server.pubsub_channels 字典， 以及保存模式订阅信息的 server.pubsub_patterns 链表 用于执行 Lua 脚本的 Lua 环境 server.lua 保存慢查询日志的 server.slowlog 属性 initServer 还进行了非常重要的设置操作： 为服务器设置进程信号处理器 创建共享对象，包含 OK、ERR、整数 1 到 10000 的字符串对象等 打开服务器的监听端口 为 serverCron 函数创建时间事件， 等待服务器正式运行时执行 serverCron 函数 如果 AOF 持久化功能已经打开，那么打开现有的 AOF 文件，如果 AOF 文件不存在，那么创建并打开一个新的 AOF 文件 ，为 AOF 写入做好准备 初始化服务器的后台 I&#x2F;O 模块（BIO）, 为将来的 I&#x2F;O 操作做好准备 当 initServer 函数执行完毕之后， 服务器将用 ASCII 字符在日志中打印出 Redis 的图标， 以及 Redis 的版本号信息 还原状态在完成了对服务器状态的初始化之后，服务器需要载入RDB文件或者AOF 文件， 并根据文件记录的内容来还原服务器的数据库状态： 如果服务器启用了 AOF 持久化功能，那么服务器使用 AOF 文件来还原数据库状态 如果服务器没有启用 AOF 持久化功能，那么服务器使用 RDB 文件来还原数据库状态 当服务器完成数据库状态还原工作之后，服务器将在日志中打印出载入文件并还原数据库状态所耗费的时长 1[7171] 22 Nov 22:43:49.084 * DB loaded from disk: 0.071 seconds 驱动循环在初始化的最后一步，服务器将打印出以下日志，并开始执行服务器的事件循环（loop） 1[7171] 22 Nov 22:43:49.084 * The server is now ready to accept connections on pert 6379 服务器现在开始可以接受客户端的连接请求，并处理客户端发来的命令请求了 慢日志基本介绍Redis 的慢查询日志功能用于记录执行时间超过给定时长的命令请求，通过产生的日志来监视和优化查询速度 服务器配置有两个和慢查询日志相关的选项： slowlog-log-slower-than 选项指定执行时间超过多少微秒的命令请求会被记录到日志上 slowlog-max-len 选项指定服务器最多保存多少条慢查询日志 服务器使用先进先出 FIFO 的方式保存多条慢查询日志，当服务器存储的慢查询日志数量等于 slowlog-max-len 选项的值时，在添加一条新的慢查询日志之前，会先将最旧的一条慢查询日志删除 配置选项可以通过 CONFIG SET option value 命令进行设置 常用命令： 123SLOWLOG GET [n]\t# 查看 n 条服务器保存的慢日志SLOWLOG LEN # 查看日志数量SLOWLOG RESET\t# 清除所有慢查询日志 日志保存服务器状态中包含了慢查询日志功能有关的属性： 123456789101112struct redisServer &#123;\t// 下一条慢查询日志的ID\tlong long slowlog_entry_id; // 保存了所有慢查询日志的链表\tlist *slowlog; // 服务器配置选项的值 long long slowlog-log-slower-than;\t// 服务器配置选项的值\tunsigned long slowlog_max_len;&#125; slowlog_entry_id 属性的初始值为 0，每当创建一条新的慢查询日志时，这个属性就会用作新日志的 id 值，之后该属性增一 slowlog 链表保存了服务器中的所有慢查询日志，链表中的每个节点是一个 slowlogEntry 结构， 代表一条慢查询日志： 123456789101112typedef struct slowlogEntry &#123; // 唯一标识符 long long id; // 命令执行时的时间，格式为UNIX时间戳 time_t time;\t// 执行命令消耗的时间，以微秒为单位 long long duration;\t// 命令与命令参数\trobj **argv;\t// 命令与命令参数的数量\tint argc;&#125; 添加日志在每次执行命令的前后，程序都会记录微秒格式的当前 UNIX 时间戳，两个时间之差就是执行命令所耗费的时长，函数会检查命令的执行时长是否超过 slowlog-log-slower-than 选项所设置： 如果是的话，就为命令创建一个新的日志，并将新日志添加到 slowlog 链表的表头 检查慢查询日志的长度是否超过 slowlog-max-len 选项所设置的长度，如果是将多出来的日志从 slowlog 链表中删除掉 将 redisServer. slowlog_entry_id 的值增 1 数据结构字符串SDSRedis 构建了简单动态字符串（SDS）的数据类型，作为 Redis 的默认字符串表示，包含字符串的键值对在底层都是由 SDS 实现 12345678910struct sdshdr &#123; // 记录buf数组中已使用字节的数量，等于 SDS 所保存字符串的长度 int len; // 记录buf数组中未使用字节的数量 int free; // 【字节】数组，用于保存字符串（不是字符数组） char buf[];&#125;; SDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 1 字节不计算在 len 属性，SDS 会自动为空字符分配额外的 1 字节空间和添加空字符到字符串末尾，所以空字符对于 SDS 的使用者来说是完全透明的 对比常数复杂度获取字符串长度： C 字符串不记录自身的长度，获取时需要遍历整个字符串，遇到空字符串为止，时间复杂度为 O(N) SDS 获取字符串长度的时间复杂度为 O(1)，设置和更新 SDS 长度由函数底层自动完成 杜绝缓冲区溢出： C 字符串调用 strcat 函数拼接字符串时，如果字符串内存不够容纳目标字符串，就会造成缓冲区溢出（Buffer Overflow） s1 和 s2 是内存中相邻的字符串，执行 strcat(s1, &quot; Cluster&quot;)（有空格）： SDS 空间分配策略：当对 SDS 进行修改时，首先检查 SDS 的空间是否满足修改所需的要求， 如果不满足会自动将 SDS 的空间扩展至执行修改所需的大小，然后执行实际的修改操作， 避免了缓冲区溢出的问题 二进制安全： C 字符串中的字符必须符合某种编码（比如 ASCII）方式，除了字符串末尾以外其他位置不能包含空字符，否则会被误认为是字符串的结尾，所以只能保存文本数据 SDS 的 API 都是二进制安全的，使用字节数组 buf 保存一系列的二进制数据，使用 len 属性来判断数据的结尾，所以可以保存图片、视频、压缩文件等二进制数据 兼容 C 字符串的函数：SDS 会在为 buf 数组分配空间时多分配一个字节来保存空字符，所以可以重用一部分 C 字符串函数库的函数 内存C 字符串每次增长或者缩短都会进行一次内存重分配，拼接操作通过重分配扩展底层数组空间，截断操作通过重分配释放不使用的内存空间，防止出现内存泄露 SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联，在 SDS 中 buf 数组的长度不一定就是字符数量加一， 数组里面可以包含未使用的字节，字节的数量由 free 属性记录 内存重分配涉及复杂的算法，需要执行系统调用，是一个比较耗时的操作，SDS 的两种优化策略： 空间预分配：当 SDS 需要进行空间扩展时，程序不仅会为 SDS 分配修改所必需的空间， 还会为 SDS 分配额外的未使用空间 对 SDS 修改之后，SDS 的长度（len 属性）小于 1MB，程序分配和 len 属性同样大小的未使用空间，此时 len 和 free 相等 s 为 Redis，执行 sdscat(s, &quot; Cluster&quot;) 后，len 变为 13 字节，所以也分配了 13 字节的 free 空间，总长度变为 27 字节（额外的一字节保存空字符，13 + 13 + 1 &#x3D; 27） 对 SDS 修改之后，SDS 的长度大于等于 1MB，程序会分配 1MB 的未使用空间 在扩展 SDS 空间前，API 会先检查 free 空间是否足够，如果足够就无需执行内存重分配，所以通过预分配策略，SDS 将连续增长 N 次字符串所需内存的重分配次数从必定 N 次降低为最多 N 次 惰性空间释放：当 SDS 缩短字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 free 属性将这些字节的数量记录起来，并等待将来复用 SDS 提供了相应的 API 来真正释放 SDS 的未使用空间，所以不用担心空间惰性释放策略造成的内存浪费问题 链表链表提供了高效的节点重排能力，C 语言并没有内置这种数据结构，所以 Redis 构建了链表数据类型 链表节点： 12345678910typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value&#125; listNode; 多个 listNode 通过 prev 和 next 指针组成双端链表： list 链表结构：提供了表头指针 head 、表尾指针 tail 以及链表长度计数器 len 12345678910111213141516typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 链表所包含的节点数量 unsigned long len; // 节点值复制函数，用于复制链表节点所保存的值 void *(*dup) (void *ptr); // 节点值释放函数，用于释放链表节点所保存的值 void (*free) (void *ptr); // 节点值对比函数，用于对比链表节点所保存的值和另一个输入值是否相等 int (*match) (void *ptr, void *key);&#125; list; Redis 链表的特性： 双端：链表节点带有 prev 和 next 指针，获取某个节点的前置节点和后置节点的时间复杂度都是 O(1) 无环：表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL，对链表的访问以 NULL 为终点 带表头指针和表尾指针： 通过 list 结构的 head 指针和 tail 指针，获取链表的表头节点和表尾节点的时间复杂度为 O(1) 带链表长度计数器：使用 len 属性来对 list 持有的链表节点进行计数，获取链表中节点数量的时间复杂度为 O(1) 多态：链表节点使用 void * 指针来保存节点值， 并且可以通过 dup、free 、match 三个属性为节点值设置类型特定函数，所以链表可以保存各种不同类型的值 字典哈希表Redis 字典使用的哈希表结构： 12345678910111213typedef struct dictht &#123; // 哈希表数组，数组中每个元素指向 dictEntry 结构\tdictEntry **table; // 哈希表大小，数组的长度\tunsigned long size; // 哈希表大小掩码，用于计算索引值，总是等于 【size-1】\tunsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used;&#125; dictht; 哈希表节点结构： 1234567891011121314typedef struct dictEntry &#123; // 键\tvoid *key; // 值，可以是一个指针，或者整数\tunion &#123; void *val;\t// 指针 uint64_t u64; int64_t s64; &#125; // 指向下个哈希表节点，形成链表，用来解决冲突问题 struct dictEntry *next;&#125; dictEntry; 字典结构字典，又称为符号表、关联数组、映射（Map），用于保存键值对的数据结构，字典中的每个键都是独一无二的。底层采用哈希表实现，一个哈希表包含多个哈希表节点，每个节点保存一个键值对 1234567891011121314typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表，数组中的每个项都是一个dictht哈希表， // 一般情况下字典只使用 ht[0] 哈希表， ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用 dictht ht[2]; // rehash 索引，当 rehash 不在进行时，值为 -1 int rehashidx;&#125; dict; type 属性和 privdata 属性是针对不同类型的键值对， 为创建多态字典而设置的： type 属性是指向 dictType 结构的指针， 每个 dictType 结构保存了一簇用于操作特定类型键值对的函数， Redis 会为用途不同的字典设置不同的类型特定函数 privdata 属性保存了需要传给那些类型特定函数的可选参数 哈希冲突Redis 使用 MurmurHash 算法来计算键的哈希值，这种算法的优点在于，即使输入的键是有规律的，算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快 将一个新的键值对添加到字典里，需要先根据键 key 计算出哈希值，然后进行取模运算（取余）： 1index = hash &amp; dict-&gt;ht[x].sizemask 当有两个或以上数量的键被分配到了哈希表数组的同一个索引上时，就称这些键发生了哈希冲突（collision） Redis 的哈希表使用链地址法（separate chaining）来解决键哈希冲突， 每个哈希表节点都有一个 next 指针，多个节点通过 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来，这就解决了键冲突的问题 dictEntry 节点组成的链表没有指向链表表尾的指针，为了速度考虑，程序总是将新节点添加到链表的表头位置（头插法），时间复杂度为 O(1) 负载因子负载因子的计算方式：哈希表中的节点数量 &#x2F; 哈希表的大小（长度） 1load_factor = ht[0].used / ht[0].size 为了让哈希表的负载因子（load factor）维持在一个合理的范围之内，当哈希表保存的键值对数量太多或者太少时 ，程序会自动对哈希表的大小进行相应的扩展或者收缩 哈希表执行扩容的条件： 服务器没有执行 BGSAVE 或者 BGREWRITEAOF 命令，哈希表的负载因子大于等于 1 服务器正在执行 BGSAVE 或者 BGREWRITEAOF 命令，哈希表的负载因子大于等于 5 原因：执行该命令的过程中，Redis 需要创建当前服务器进程的子进程，而大多数操作系统都采用写时复制（copy-on­-write）技术来优化子进程的使用效率，通过提高执行扩展操作的负载因子，尽可能地避免在子进程存在期间进行哈希表扩展操作，可以避免不必要的内存写入操作，最大限度地节约内存 哈希表执行收缩的条件：负载因子小于 0.1（自动执行，servreCron 中检测） 重新散列扩展和收缩哈希表的操作通过 rehash（重新散列）来完成，步骤如下： 为字典的 ht[1] 哈希表分配空间，空间大小的分配情况： 如果执行的是扩展操作，ht[1] 的大小为第一个大于等于 $ht[0].used * 2$ 的 $2^n$ 如果执行的是收缩操作，ht[1] 的大小为第一个大于等于 $ht[0].used$ 的 $2^n$ 将保存在 ht[0] 中所有的键值对重新计算哈希值和索引值，迁移到 ht[1] 上 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后（ht[0] 变为空表），释放 ht[0]，将 ht[1] 设置为 ht[0]，并在 ht[1] 创建一个新的空白哈希表，为下一次 rehash 做准备 如果哈希表里保存的键值对数量很少，rehash 就可以在瞬间完成，但是如果哈希表里数据很多，那么要一次性将这些键值对全部 rehash 到 ht[1] 需要大量计算，可能会导致服务器在一段时间内停止服务 Redis 对 rehash 做了优化，使 rehash 的动作并不是一次性、集中式的完成，而是分多次，渐进式的完成，又叫渐进式 rehash 为 ht[1] 分配空间，此时字典同时持有 ht[0] 和 ht[1] 两个哈希表 在字典中维护了一个索引计数器变量 rehashidx，并将变量的值设为 0，表示 rehash 正式开始 在 rehash 进行期间，每次对字典执行增删改查操作时，程序除了执行指定的操作以外，还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1]，rehash 完成之后将 rehashidx 属性的值增一 随着字典操作的不断执行，最终在某个时间点 ht[0] 的所有键值对都被 rehash 至 ht[1]，将 rehashidx 属性的值设为 -1 渐进式 rehash 采用分而治之的方式，将 rehash 键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式 rehash 带来的庞大计算量 渐进式 rehash 期间的哈希表操作： 字典的查找、删除、更新操作会在两个哈希表上进行，比如查找一个键会先在 ht[0] 上查找，查找不到就去 ht[1] 继续查找 字典的添加操作会直接在 ht[1] 上添加，不在 ht[0] 上进行任何添加 跳跃表底层结构跳跃表（skiplist）是一种有序（默认升序）的数据结构，在链表的基础上增加了多级索引以提升查找的效率，索引是占内存的，所以是一个空间换时间的方案，跳表平均 O(logN)、最坏 O(N) 复杂度的节点查找，效率与平衡树相当但是实现更简单 原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，因此当节点本身比较大或者元素数量比较多的时候，其优势可以被放大，而缺点（占内存）则可以忽略 Redis 只在两个地方应用了跳跃表，一个是实现有序集合键，另一个是在集群节点中用作内部数据结构 12345678910typedef struct zskiplist &#123; // 表头节点和表尾节点，O(1) 的时间复杂度定位头尾节点 struct skiplistNode *head, *tail; // 表的长度，也就是表内的节点数量 (表头节点不计算在内) unsigned long length; // 表中层数最大的节点的层数 (表头节点的层高不计算在内) int level&#125; zskiplist; 123456789101112131415161718typedef struct zskiplistNode &#123; // 层 struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; &#125; level[]; // 后退指针 struct zskiplistNode *backward; // 分值 double score; // 成员对象 robj *obj;&#125; zskiplistNode; 属性分析层：level 数组包含多个元素，每个元素包含指向其他节点的指针。根据幕次定律（power law，越大的数出现的概率越小）随机生成一个介于 1 和 32 之间的值（Redis5 之后最大为 64）作为 level 数组的大小，这个大小就是层的高度，节点的第一层是 level[0] &#x3D; L1 前进指针：forward 用于从表头到表尾方向正序（升序）遍历节点，遇到 NULL 停止遍历 跨度：span 用于记录两个节点之间的距离，用来计算排位（rank）： 两个节点之间的跨度越大相距的就越远，指向 NULL 的所有前进指针的跨度都为 0 在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来，结果就是目标节点在跳跃表中的排位，按照上图所示： 查找分值为 3.0 的节点，沿途经历的层：查找的过程只经过了一个层，并且层的跨度为 3，所以目标节点在跳跃表中的排位为 3 查找分值为 2.0 的节点，沿途经历的层：经过了两个跨度为 1 的节点，因此可以计算出目标节点在跳跃表中的排位为 2 后退指针：backward 用于从表尾到表头方向逆序（降序）遍历节点 分值：score 属性一个 double 类型的浮点数，跳跃表中的所有节点都按分值从小到大来排序 成员对象：obj 属性是一个指针，指向一个 SDS 字符串对象。同一个跳跃表中，各个节点保存的成员对象必须是唯一的，但是多个节点保存的分值可以是相同的，分值相同的节点将按照成员对象在字典序中的大小来进行排序（从小到大） 个人笔记：JUC → 并发包 → ConcurrentSkipListMap 详解跳跃表 整数集合底层结构整数集合（intset）是用于保存整数值的集合数据结构，是 Redis 集合键的底层实现之一 12345678910typedef struct intset &#123;\t// 编码方式\tuint32_t encoding; // 集合包含的元素数量，也就是 contents 数组的长度\tuint32_t length; // 保存元素的数组 int8_t contents[];&#125; intset; encoding 取值为三种：INTSET_ENC_INT16、INTSET_ENC_INT32、INTSET_ENC_INT64 整数集合的每个元素都是 contents 数组的一个数组项（item），在数组中按值的大小从小到大有序排列，并且数组中不包含任何重复项。虽然 contents 属性声明为 int8_t 类型，但实际上数组并不保存任何 int8_t 类型的值， 真正类型取决于 encoding 属性 说明：底层存储结构是数组，所以为了保证有序性和不重复性，每次添加一个元素的时间复杂度是 O(N) 类型升级整数集合添加的新元素的类型比集合现有所有元素的类型都要长时，需要先进行升级（upgrade），升级流程： 根据新元素的类型长度以及集合元素的数量（包括新元素在内），扩展整数集合底层数组的空间大小 将底层数组现有的所有元素都转换成与新元素相同的类型，并将转换后的元素放入正确的位置，放置过程保证数组的有序性 图示 32 * 4 &#x3D; 128 位，首先将 3 放入索引 2（64 位 - 95 位），然后将 2 放置索引 1，将 1 放置在索引 0，从后向前依次放置在对应的区间，最后放置 65535 元素到索引 3（96 位- 127 位），修改 length 属性为 4 将新元素添加到底层数组里 每次向整数集合添加新元素都可能会引起升级，而每次升级都需要对底层数组中的所有元素进行类型转换，所以向整数集合添加新元素的时间复杂度为 O(N) 引发升级的新元素的长度总是比整数集合现有所有元素的长度都大，所以这个新元素的值要么就大于所有现有元素，要么就小于所有现有元素，升级之后新元素的摆放位置： 在新元素小于所有现有元素的情况下，新元素会被放置在底层数组的最开头（索引 0） 在新元素大于所有现有元素的情况下，新元素会被放置在底层数组的最末尾（索引 length-1） 整数集合升级策略的优点： 提升整数集合的灵活性：C 语言是静态类型语言，为了避免类型错误通常不会将两种不同类型的值放在同一个数据结构里面，整数集合可以自动升级底层数组来适应新元素，所以可以随意的添加整数 节约内存：要让数组可以同时保存 int16、int32、int64 三种类型的值，可以直接使用 int64_t 类型的数组作为整数集合的底层实现，但是会造成内存浪费，整数集合可以确保升级操作只会在有需要的时候进行，尽量节省内存 整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态 压缩列表底层结构压缩列表（ziplist）是 Redis 为了节约内存而开发的，是列表键和哈希键的底层实现之一。是由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构，一个压缩列表可以包含任意多个节点（entry），每个节点可以保存一个字节数组或者一个整数值 zlbytes：uint32_t 类型 4 字节，记录整个压缩列表占用的内存字节数，在对压缩列表进行内存重分配或者计算 zlend 的位置时使用 zltail：uint32_t 类型 4 字节，记录压缩列表表尾节点距离起始地址有多少字节，通过这个偏移量程序无须遍历整个压缩列表就可以确定表尾节点的地址 zllen：uint16_t 类型 2 字节，记录了压缩列表包含的节点数量，当该属性的值小于 UINT16_MAX (65535) 时，该值就是压缩列表中节点的数量；当这个值等于 UINT16_MAX 时节点的真实数量需要遍历整个压缩列表才能计算得出 entryX：列表节点，压缩列表中的各个节点，节点的长度由节点保存的内容决定 zlend：uint8_t 类型 1 字节，是一个特殊值 0xFF (255)，用于标记压缩列表的末端 列表 zlbytes 属性的值为 0x50 (十进制 80)，表示压缩列表的总长为 80 字节，列表 zltail 属性的值为 0x3c (十进制 60)，假设表的起始地址为 p，计算得出表尾节点 entry3 的地址 p + 60 列表节点列表节点 entry 的数据结构： previous_entry_length：以字节为单位记录了压缩列表中前一个节点的长度，程序可以通过指针运算，根据当前节点的起始地址来计算出前一个节点的起始地址，完成从表尾向表头遍历操作 如果前一节点的长度小于 254 字节，该属性的长度为 1 字节，前一节点的长度就保存在这一个字节里 如果前一节点的长度大于等于 254 字节，该属性的长度为 5 字节，其中第一字节会被设置为 0xFE（十进制 254），之后的四个字节则用于保存前一节点的长度 encoding：记录了节点的 content 属性所保存的数据类型和长度 长度为 1 字节、2 字节或者 5 字节，值的最高位为 00、01 或者 10 的是字节数组编码，数组的长度由编码除去最高两位之后的其他位记录，下划线 _ 表示留空，而 b、x 等变量则代表实际的二进制数据 长度为 1 字节，值的最高位为 11 的是整数编码，整数值的类型和长度由编码除去最高两位之后的其他位记录 content：每个压缩列表节点可以保存一个字节数组或者一个整数值 字节数组可以是以下三种长度的其中一种： 长度小于等于 $63 (2^6-1)$ 字节的字节数组 长度小于等于 $16383(2^{14}-1)$ 字节的字节数组 长度小于等于 $4294967295(2^{32}-1)$ 字节的字节数组 整数值则可以是以下六种长度的其中一种： 4 位长，介于 0 至 12 之间的无符号整数 1 字节长的有符号整数 3 字节长的有符号整数 int16_t 类型整数 int32_t 类型整数 int64_t 类型整数 连锁更新Redis 将在特殊情况下产生的连续多次空间扩展操作称之为连锁更新（cascade update） 假设在一个压缩列表中，有多个连续的、长度介于 250 到 253 字节之间的节点 e1 至 eN。将一个长度大于等于 254 字节的新节点 new 设置为压缩列表的头节点，new 就成为 e1 的前置节点。e1 的 previous_entry_length 属性仅为 1 字节，无法保存新节点 new 的长度，所以要对压缩列表执行空间重分配操作，并将 e1 节点的 previous_entry_length 属性从 1 字节长扩展为 5 字节长。由于 e1 原本的长度介于 250 至 253 字节之间，所以扩展后 e1 的长度就变成了 254 至 257 字节之间，导致 e2 的 previous_entry_length 属性无法保存 e1 的长度，程序需要不断地对压缩列表执行空间重分配操作，直到 eN 为止 删除节点也可能会引发连锁更新，big.length &gt;&#x3D; 254，small.length &lt; 254，删除 small 节点 连锁更新在最坏情况下需要对压缩列表执行 N 次空间重分配，每次重分配的最坏复杂度为 O(N)，所以连锁更新的最坏复杂度为 O(N^2) 说明：尽管连锁更新的复杂度较高，但出现的记录是非常低的，即使出现只要被更新的节点数量不多，就不会对性能造成影响 数据类型redisObj对象系统Redis 使用对象来表示数据库中的键和值，当在 Redis 数据库中新创建一个键值对时至少会创建两个对象，一个对象用作键值对的键（键对象），另一个对象用作键值对的值（值对象） Redis 中对象由一个 redisObject 结构表示，该结构中和保存数据有关的三个属性分别是 type、 encoding、ptr： 12345678910typedef struct redisObiect &#123;\t// 类型\tunsigned type:4;\t// 编码\tunsigned encoding:4;\t// 指向底层数据结构的指针\tvoid *ptr; // ....&#125; robj; Redis 并没有直接使用数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统，包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象，而每种对象又通过不同的编码映射到不同的底层数据结构 Redis 是一个 Map 类型，其中所有的数据都是采用 key : value 的形式存储，键对象都是字符串对象，而值对象有五种基本类型和三种高级类型对象 对一个数据库键执行 TYPE 命令，返回的结果为数据库键对应的值对象的类型，而不是键对象的类型 对一个数据库键执行 OBJECT ENCODING 命令，查看数据库键对应的值对象的编码 命令多态Redis 中用于操作键的命令分为两种类型： 一种命令可以对任何类型的键执行，比如说 DEL 、EXPIRE、RENAME、 TYPE 等（基于类型的多态） 只能对特定类型的键执行，比如 SET 只能对字符串键执行、HSET 对哈希键执行、SADD 对集合键执行，如果类型步匹配会报类型错误： (error) WRONGTYPE Operation against a key holding the wrong kind of value Redis 为了确保只有指定类型的键可以执行某些特定的命令，在执行类型特定的命令之前，先通过值对象 redisObject 结构 type 属性检查操作类型是否正确，然后再决定是否执行指定的命令 对于多态命令，比如列表对象有 ziplist 和 linkedlist 两种实现方式，通过 redisObject 结构 encoding 属性确定具体的编码类型，底层调用对应的 API 实现具体的操作（基于编码的多态） 内存回收对象的整个生命周期可以划分为创建对象、 操作对象、 释放对象三个阶段 C 语言没有自动回收内存的功能，所以 Redis 在对象系统中构建了引用计数（reference counting）技术实现的内存回收机制，程序可以跟踪对象的引用计数信息，在适当的时候自动释放对象并进行内存回收 1234typedef struct redisObiect &#123;\t// 引用计数\tint refcount;&#125; robj; 对象的引用计数信息会随着对象的使用状态而不断变化，创建时引用计数 refcount 初始化为 1，每次被一个新程序使用时引用计数加 1，当对象不再被一个程序使用时引用计数值会被减 1，当对象的引用计数值变为 0 时，对象所占用的内存会被释放 对象共享对象的引用计数属性带有对象共享的作用，共享对象机制更节约内存，数据库中保存的相同值对象越多，节约的内存就越多 让多个键共享一个对象的步骤： 将数据库键的值指针指向一个现有的值对象 将被共享的值对象的引用计数增一 Redis 在初始化服务器时创建一万个（配置文件可以修改）字符串对象，包含了从 0 到 9999 的所有整数值，当服务器需要用到值为 0 到 9999 的字符串对象时，服务器就会使用这些共享对象，而不是新创建对象 比如创建一个值为 100 的键 A，并使用 OBJECT REFCOUNT 命令查看键 A 的值对象的引用计数，会发现值对象的引用计数为 2，引用这个值对象的两个程序分别是持有这个值对象的服务器程序，以及共享这个值对象的键 A 共享对象在嵌套了字符串对象的对象（linkedlist 编码的列表、hashtable 编码的哈希、zset 编码的有序集合）中也能使用 Redis 不共享包含字符串对象的原因：验证共享对象和目标对象是否相同的复杂度越高，消耗的 CPU 时间也会越多 整数值的字符串对象， 验证操作的复杂度为 O(1) 字符串值的字符串对象， 验证操作的复杂度为 O(N) 如果共享对象是包含了多个值（或者对象的）对象，比如列表对象或者哈希对象，验证操作的复杂度为 O(N^2) 空转时长redisObject 结构包含一个 lru 属性，该属性记录了对象最后一次被命令程序访问的时间 123typedef struct redisObiect &#123;\tunsigned lru:22; &#125; robj; OBJECT IDLETIME 命令可以打印出给定键的空转时长，该值就是通过将当前时间减去键的值对象的 lru 时间计算得出的，这个命令在访问键的值对象时，不会修改值对象的 lru 属性 1234567891011redis&gt; OBJECT IDLETIME msg(integer) 10# 等待一分钟redis&gt; OBJECT IDLETIME msg(integer) 70# 访问 msgredis&gt; GET msg&quot;hello world&quot;# 键处于活跃状态，空转时长为 0redis&gt; OBJECT IDLETIME msg(integer) 0 空转时长的作用：如果服务器开启 maxmemory 选项，并且回收内存的算法为 volatile-lru 或者 allkeys-lru，那么当服务器占用的内存数超过了 maxmemory 所设置的上限值时，空转时长较高的那部分键会优先被服务器释放，从而回收内存（LRU 算法） string简介存储的数据：单个数据，最简单的数据存储类型，也是最常用的数据存储类型，实质上是存一个字符串，string 类型是二进制安全的，可以包含任何数据，比如图片或者序列化的对象 存储数据的格式：一个存储空间保存一个数据，每一个空间中只能保存一个字符串信息 存储内容：通常使用字符串，如果字符串以整数的形式展示，可以作为数字操作使用 Redis 所有操作都是原子性的，采用单线程机制，命令是单个顺序执行，无需考虑并发带来影响，原子性就是有一个失败则都失败 字符串对象可以是 int、raw、embstr 三种实现方式 操作指令操作： 数据操作： 12345set key value #添加/修改数据添加/修改数据del key #删除数据setnx key value #判定性添加数据，键值为空则设添加mset k1 v1 k2 v2... #添加/修改多个数据，m：Multipleappend key value #追加信息到原始信息后部（如果原始信息存在就追加，否则新建） 查询操作 123get key #获取数据，如果不存在，返回空（nil）mget key1 key2... #获取多个数据strlen key #获取数据字符个数（字符串长度） 设置数值数据增加&#x2F;减少指定范围的值 12345incr key #key++incrby key increment #key+incrementincrbyfloat key increment\t#对小数操作decr key #key--decrby key increment #key-increment 设置数据具有指定的生命周期 12setex key seconds value #设置key-value存活时间，seconds单位是秒psetex key milliseconds value\t#毫秒级 注意事项： 数据操作不成功的反馈与数据正常操作之间的差异 表示运行结果是否成功 (integer) 0 → false ，失败 (integer) 1 → true，成功 表示运行结果值 (integer) 3 → 3 个 (integer) 1 → 1 个 数据未获取到时，对应的数据为（nil），等同于null 数据最大存储量：512MB string 在 Redis 内部存储默认就是一个字符串，当遇到增减类操作 incr，decr 时会转成数值型进行计算 按数值进行操作的数据，如果原始数据不能转成数值，或超越了Redis 数值上限范围，将报错9223372036854775807（java 中 Long 型数据最大值，Long.MAX_VALUE） Redis 可用于控制数据库表主键 ID，为数据库表主键提供生成策略，保障数据库表的主键唯一性 单数据和多数据的选择： 单数据执行 3 条指令的过程：3 次发送 + 3 次处理 + 3 次返回 多数据执行 1 条指令的过程：1 次发送 + 3 次处理 + 1 次返回（发送和返回的事件略高于单数据） 实现字符串对象的编码可以是 int、raw、embstr 三种 int：字符串对象保存的是整数值，并且整数值可以用 long 类型来表示，那么对象会将整数值保存在字符串对象结构的 ptr 属性面（将 void * 转换成 long)，并将字符串对象的编码设置为 int（浮点数用另外两种方式） raw：字符串对象保存的是一个字符串值，并且值的长度大于 39 字节，那么对象将使用简单动态字符串（SDS）来保存该值，并将对象的编码设置为 raw embstr：字符串对象保存的是一个字符串值，并且值的长度小于等于 39 字节，那么对象将使用 embstr 编码的方式来保存这个字符串值，并将对象的编码设置为 embstr 上图所示，embstr 与 raw 都使用了 redisObject 和 sdshdr 来表示字符串对象，但是 raw 需要调用两次内存分配函数分别创建两种结构，embstr 只需要一次内存分配来分配一块连续的空间 embstr 是用于保存短字符串的一种编码方式，对比 raw 的优点： 内存分配次数从两次降低为一次，同样释放内存的次数也从两次变为一次 embstr 编码的字符串对象的数据都保存在同一块连续内存，所以比 raw 编码能够更好地利用缓存优势（局部性原理） int 和 embstr 编码的字符串对象在条件满足的情况下，会被转换为 raw 编码的字符串对象： int 编码的整数值，执行 APPEND 命令追加一个字符串值，先将整数值转为字符串然后追加，最后得到一个 raw 编码的对象 Redis 没有为 embstr 编码的字符串对象编写任何相应的修改程序，所以 embstr 对象实际上是只读的，执行修改命令会将对象的编码从 embstr 转换成 raw，操作完成后得到一个 raw 编码的对象 某些情况下，程序会将字符串对象里面的字符串值转换回浮点数值，执行某些操作后再将浮点数值转换回字符串值： 12345678redis&gt; SET pi 3.14 OK redis&gt; OBJECT ENCODING pi&quot;embstr&quot; redis&gt; INCRBYFLOAT pi 2.0 # 转为浮点数执行增加的操作&quot;5. 14&quot; redis&gt; OBJECT ENCODING pi &quot;embstr&quot; 应用主页高频访问信息显示控制，例如新浪微博大 V 主页显示粉丝数与微博数量 在 Redis 中为大 V 用户设定用户信息，以用户主键和属性值作为 key，后台设定定时刷新策略 123set user:id:3506728370:fans 12210947set user:id:3506728370:blogs 6164set user:id:3506728370:focuses 83 使用 JSON 格式保存数据 1user:id:3506728370 → &#123;&quot;fans&quot;:12210947,&quot;blogs&quot;:6164,&quot;focuses&quot;:83&#125; key的设置约定：表名 : 主键名 : 主键值 : 字段名 表名 主键名 主键值 字段名 order id 29437595 name equip id 390472345 type news id 202004150 title hash简介数据存储需求：对一系列存储的数据进行编组，方便管理，典型应用存储对象信息 数据存储结构：一个存储空间保存多个键值对数据 hash 类型：底层使用哈希表结构实现数据存储 Redis 中的 hash 类似于 Java 中的 Map&lt;String, Map&lt;Object,object&gt;&gt;，左边是 key，右边是值，中间叫 field 字段，本质上 hash 存了一个 key-value 的存储空间 hash 是指的一个数据类型，并不是一个数据 如果 field 数量较少，存储结构优化为压缩列表结构（有序） 如果 field 数量较多，存储结构使用 HashMap 结构（无序） 操作指令操作： 数据操作 1234hset key field value #添加/修改数据hdel key field1 [field2]\t#删除数据，[]代表可选hsetnx key field value #设置field的值，如果该field存在则不做任何操作hmset key f1 v1 f2 v2...\t#添加/修改多个数据 查询操作 12345hget key field #获取指定field对应数据hgetall key #获取指定key所有数据hmget key field1 field2...\t#获取多个数据hexists key field #获取哈希表中是否存在指定的字段hlen key #获取哈希表中字段的数量 获取哈希表中所有的字段名或字段值 12hkeys key #获取所有的field\thvals key #获取所有的value 设置指定字段的数值数据增加指定范围的值 12hincrby key field increment #指定字段的数值数据增加指定的值，increment为负数则减少hincrbyfloat key field increment#操作小数 注意事项 hash 类型中 value 只能存储字符串，不允许存储其他数据类型，不存在嵌套现象，如果数据未获取到，对应的值为（nil） 每个 hash 可以存储 2^32 - 1 个键值对 hash 类型和对象的数据存储形式相似，并且可以灵活添加删除对象属性。但 hash 设计初衷不是为了存储大量对象而设计的，不可滥用，不可将 hash 作为对象列表使用 hgetall 操作可以获取全部属性，如果内部 field 过多，遍历整体数据效率就很会低，有可能成为数据访问瓶颈 实现哈希对象的内部编码有两种：ziplist（压缩列表）、hashtable（哈希表、字典） 压缩列表实现哈希对象：同一键值对的节点总是挨在一起，保存键的节点在前，保存值的节点在后 字典实现哈希对象：字典的每一个键都是一个字符串对象，每个值也是 当存储的数据量比较小的情况下，Redis 才使用压缩列表来实现字典类型，具体需要满足两个条件： 当键值对数量小于 hash-max-ziplist-entries 配置（默认 512 个） 所有键和值的长度都小于 hash-max-ziplist-value 配置（默认 64 字节） 以上两个条件的上限值是可以通过配置文件修改的，当两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行 ziplist 使用更加紧凑的结构实现多个元素的连续存储，所以在节省内存方面比 hashtable 更加优秀，当 ziplist 无法满足哈希类型时，Redis 会使用 hashtable 作为哈希的内部实现，因为此时 ziplist 的读写效率会下降，而 hashtable 的读写时间复杂度为 O(1) 应用1user:id:3506728370 → &#123;&quot;name&quot;:&quot;春晚&quot;,&quot;fans&quot;:12210862,&quot;blogs&quot;:83&#125; 对于以上数据，使用单条去存的话，存的条数会很多。但如果用 json 格式，存一条数据就够了。 假如现在粉丝数量发生了变化，要把整个值都改变，但是用单条存就不存在这个问题，只需要改其中一个就可以 可以实现购物车的功能，key 对应着每个用户，存储空间存储购物车的信息 list简介数据存储需求：存储多个数据，并对数据进入存储空间的顺序进行区分 数据存储结构：一个存储空间保存多个数据，且通过数据可以体现进入顺序，允许重复元素 list 类型：保存多个数据，底层使用双向链表存储结构实现，类似于 LinkedList 如果两端都能存取数据的话，这就是双端队列，如果只能从一端进一端出，这个模型叫栈 操作指令操作： 数据操作 12345lpush key value1 [value2]...#从左边添加/修改数据(表头)rpush key value1 [value2]...#从右边添加/修改数据(表尾)lpop key #从左边获取并移除第一个数据，类似于出栈/出队rpop key #从右边获取并移除第一个数据lrem key count value #删除指定数据，count=2删除2个，该value可能有多个(重复数据) 查询操作 123lrange key start stop #从左边遍历数据并指定开始和结束索引，0是第一个索引，-1是终索引lindex key index #获取指定索引数据，没有则为nil，没有索引越界llen key #list中数据长度/个数 规定时间内获取并移除数据 1234b #代表阻塞blpop key1 [key2] timeout\t#在指定时间内获取指定key(可以多个)的数据，超时则为(nil) #可以从其他客户端写数据，当前客户端阻塞读取数据brpop key1 [key2] timeout\t#从右边操作 复制操作 1brpoplpush source destination timeout\t#从source获取数据放入destination，假如在指定时间内没有任何元素被弹出，则返回一个nil和等待时长。反之，返回一个含有两个元素的列表，第一个元素是被弹出元素的值，第二个元素是等待时长 注意事项 list 中保存的数据都是 string 类型的，数据总容量是有限的，最多 2^32 - 1 个元素（4294967295） list 具有索引的概念，但操作数据时通常以队列的形式进行入队出队，或以栈的形式进行入栈出栈 获取全部数据操作结束索引设置为 -1 list 可以对数据进行分页操作，通常第一页的信息来自于 list，第 2 页及更多的信息通过数据库的形式加载 实现在 Redis3.2 版本以前列表对象的内部编码有两种：ziplist（压缩列表）和 linkedlist（链表） 压缩列表实现的列表对象：PUSH 1、three、5 三个元素 链表实现的列表对象：为了简化字符串对象的表示，使用了 StringObject 的结构，底层其实是 sdshdr 结构 列表中存储的数据量比较小的时候，列表就会使用一块连续的内存存储，采用压缩列表的方式实现的条件： 列表对象保存的所有字符串元素的长度都小于 64 字节 列表对象保存的元素数量小于 512 个 以上两个条件的上限值是可以通过配置文件修改的，当两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行 在 Redis3.2 版本 以后对列表数据结构进行了改造，使用 quicklist（快速列表）代替了 linkedlist，quicklist 实际上是 ziplist 和 linkedlist 的混合体，将 linkedlist 按段切分，每一段使用 ziplist 来紧凑存储，多个 ziplist 之间使用双向指针串接起来，既满足了快速的插入删除性能，又不会出现太大的空间冗余 应用企业运营过程中，系统将产生出大量的运营数据，如何保障多台服务器操作日志的统一顺序输出？ 依赖 list 的数据具有顺序的特征对信息进行管理，右进左查或者左近左查 使用队列模型解决多路信息汇总合并的问题 使用栈模型解决最新消息的问题 微信文章订阅公众号： 比如订阅了两个公众号，它们发布了两篇文章，文章 ID 分别为 666 和 888，可以通过执行 LPUSH key 666 888 命令推送给我 set简介数据存储需求：存储大量的数据，在查询方面提供更高的效率 数据存储结构：能够保存大量的数据，高效的内部存储机制，便于查询 set 类型：与 hash 存储结构哈希表完全相同，只是仅存储键不存储值（nil），所以添加，删除，查找的复杂度都是 O(1)，并且值是不允许重复且无序的 操作指令操作： 数据操作 12sadd key member1 [member2]\t#添加数据srem key member1 [member2]\t#删除数据 查询操作 123smembers key #获取全部数据scard key #获取集合数据总量sismember key member #判断集合中是否包含指定数据 随机操作 12345678910111213 spop key [count] #随机获取集中的某个数据并将该数据移除集合 srandmember key [count] #随机获取集合中指定(数量)的数据* 集合的交、并、差 ```sh sinter key1 [key2...] #两个集合的交集，不存在为(empty list or set) sunion key1 [key2...] #两个集合的并集 sdiff key1 [key2...] #两个集合的差集 sinterstore destination key1 [key2...]\t#两个集合的交集并存储到指定集合中 sunionstore destination key1 [key2...]\t#两个集合的并集并存储到指定集合中 sdiffstore destination key1 [key2...]\t#两个集合的差集并存储到指定集合中 复制 1smove source destination member #将指定数据从原始集合中移动到目标集合中 注意事项 set 类型不允许数据重复，如果添加的数据在 set 中已经存在，将只保留一份 set 虽然与 hash 的存储结构相同，但是无法启用 hash 中存储值的空间 实现集合对象的内部编码有两种：intset（整数集合）、hashtable（哈希表、字典） 整数集合实现的集合对象： 字典实现的集合对象：键值对的值为 NULL 当集合对象可以同时满足以下两个条件时，对象使用 intset 编码： 集合中的元素都是整数值 集合中的元素数量小于 set-maxintset-entries配置（默认 512 个） 以上两个条件的上限值是可以通过配置文件修改的 应用应用场景： 黑名单：资讯类信息类网站追求高访问量，但是由于其信息的价值，往往容易被不法分子利用，通过爬虫技术，快速获取信息，个别特种行业网站信息通过爬虫获取分析后，可以转换成商业机密。 注意：爬虫不一定做摧毁性的工作，有些小型网站需要爬虫为其带来一些流量。 白名单：对于安全性更高的应用访问，仅仅靠黑名单是不能解决安全问题的，此时需要设定可访问的用户群体， 依赖白名单做更为苛刻的访问验证 随机操作可以实现抽奖功能 集合的交并补可以实现微博共同关注的查看，可以根据共同关注或者共同喜欢推荐相关内容 zset简介数据存储需求：数据排序有利于数据的有效展示，需要提供一种可以根据自身特征进行排序的方式 数据存储结构：新的存储模型，可以保存可排序的数据 操作指令操作： 数据操作 123456zadd key score1 member1 [score2 member2]\t#添加数据zrem key member [member ...] #删除数据zremrangebyrank key start stop #删除指定索引范围的数据zremrangebyscore key min max #删除指定分数区间内的数据zscore key member #获取指定值的分数zincrby key increment member #指定值的分数增加increment 查询操作 12345678910zrange key start stop [WITHSCORES] #获取指定范围的数据，升序，WITHSCORES 代表显示分数zrevrange key start stop [WITHSCORES]\t#获取指定范围的数据，降序zrangebyscore key min max [WITHSCORES] [LIMIT offset count]\t#按条件获取数据，从小到大zrevrangebyscore key max min [WITHSCORES] [...] #从大到小zcard key #获取集合数据的总量zcount key min max #获取指定分数区间内的数据总量zrank key member #获取数据对应的索引（排名）升序zrevrank key member #获取数据对应的索引（排名）降序 min 与 max 用于限定搜索查询的条件 start 与 stop 用于限定查询范围，作用于索引，表示开始和结束索引 offset 与 count 用于限定查询范围，作用于查询结果，表示开始位置和数据总量 集合的交、并操作 12zinterstore destination numkeys key [key ...]\t#两个集合的交集并存储到指定集合中zunionstore destination numkeys key [key ...]\t#两个集合的并集并存储到指定集合中 注意事项： score 保存的数据存储空间是 64 位，如果是整数范围是 -9007199254740992~9007199254740992 score 保存的数据也可以是一个双精度的 double 值，基于双精度浮点数的特征可能会丢失精度，慎重使用 sorted_set 底层存储还是基于 set 结构的，因此数据不能重复，如果重复添加相同的数据，score 值将被反复覆盖，保留最后一次修改的结果 实现有序集合对象的内部编码有两种：ziplist（压缩列表）和 skiplist（跳跃表） 压缩列表实现有序集合对象：ziplist 本身是有序、不可重复的，符合有序集合的特性 跳跃表实现有序集合对象：底层是 zset 结构，zset 同时包含字典和跳跃表的结构，图示字典和跳跃表中重复展示了各个元素的成员和分值，但实际上两者会通过指针来共享相同元素的成员和分值，不会产生空间浪费 1234typedef struct zset &#123; zskiplist *zsl; dict *dict;&#125; zset; 使用字典加跳跃表的优势： 字典为有序集合创建了一个从成员到分值的映射，用 O(1) 复杂度查找给定成员的分值 排序操作使用跳跃表完成，节省每次重新排序带来的时间成本和空间成本 使用 ziplist 格式存储需要满足以下两个条件： 有序集合保存的元素个数要小于 128 个； 有序集合保存的所有元素大小都小于 64 字节 当元素比较多时，此时 ziplist 的读写效率会下降，时间复杂度是 O(n)，跳表的时间复杂度是 O(logn) 为什么用跳表而不用平衡树？ 在做范围查找的时候，跳表操作简单（前进指针或后退指针），平衡树需要回旋查找 跳表比平衡树实现简单，平衡树的插入和删除操作可能引发子树的旋转调整，而跳表的插入和删除只需要修改相邻节点的指针 应用 排行榜 对于基于时间线限定的任务处理，将处理时间记录为 score 值，利用排序功能区分处理的先后顺序 当任务或者消息待处理，形成了任务队列或消息队列时，对于高优先级的任务要保障对其优先处理，采用 score 记录权重 Bitmaps基本操作Bitmaps 是二进制位数组（bit array），底层使用 SDS 字符串表示，因为 SDS 是二进制安全的 buf 数组的每个字节用一行表示，buf[1] 是 &#39;\\0&#39;，保存位数组的顺序和书写位数组的顺序是完全相反的，图示的位数组 0100 1101 数据结构的详解查看 Java → Algorithm → 位图 命令实现GETBITGETBIT 命令获取位数组 bitarray 在 offset 偏移量上的二进制位的值 1GETBIT &lt;bitarray&gt; &lt;offset&gt; 执行过程： 计算 byte = offset/8（向下取整）, byte 值记录数据保存在位数组中的索引 计算 bit = (offset mod 8) + 1，bit 值记录数据在位数组中的第几个二进制位 根据 byte 和 bit 值，在位数组 bitarray 中定位 offset 偏移量指定的二进制位，并返回这个位的值 GETBIT 命令执行的所有操作都可以在常数时间内完成，所以时间复杂度为 O(1) SETBITSETBIT 将位数组 bitarray 在 offset 偏移量上的二进制位的值设置为 value，并向客户端返回二进制位的旧值 1SETBIT &lt;bitarray&gt; &lt;offset&gt; &lt;value&gt; 执行过程： 计算 len = offset/8 + 1，len 值记录了保存该数据至少需要多少个字节 检查 bitarray 键保存的位数组的长度是否小于 len，成立就会将 SDS 扩展为 len 字节（注意空间预分配机制），所有新扩展空间的二进制位的值置为 0 计算 byte = offset/8（向下取整）, byte 值记录数据保存在位数组中的索引 计算 bit = (offset mod 8) + 1，bit 值记录数据在位数组中的第几个二进制位 根据 byte 和 bit 值，在位数组 bitarray 中定位 offset 偏移量指定的二进制位，首先将指定位现存的值保存在 oldvalue 变量，然后将新值 value 设置为这个二进制位的值 向客户端返回 oldvalue 变量的值 BITCOUNTBITCOUNT 命令用于统计给定位数组中，值为 1 的二进制位的数量 1BITCOUNT &lt;bitarray&gt; [start end] 二进制位统计算法： 遍历法：遍历位数组中的每个二进制位 查表算法：读取每个字节（8 位）的数据，查表获取数值对应的二进制中有几个 1 variable-precision SWAR算法：计算汉明距离 Redis 实现： 如果二进制位的数量大于等于 128 位， 那么使用 variable-precision SWAR 算法来计算二进制位的汉明重量 如果二进制位的数量小于 128 位，那么使用查表算法来计算二进制位的汉明重量 BITOPBITOP 命令对指定 key 按位进行交、并、非、异或操作，并将结果保存到指定的键中 1BITOP OPTION destKey key1 [key2...] OPTION 有 AND（与）、OR（或）、 XOR（异或）和 NOT（非）四个选项 AND、OR、XOR 三个命令可以接受多个位数组作为输入，需要遍历输入的每个位数组的每个字节来进行计算，所以命令的复杂度为 O(n^2)；与此相反，NOT 命令只接受一个位数组输入，所以时间复杂度为 O(n) 应用场景 解决 Redis 缓存穿透，判断给定数据是否存在， 防止缓存穿透 垃圾邮件过滤，对每一个发送邮件的地址进行判断是否在布隆的黑名单中，如果在就判断为垃圾邮件 爬虫去重，爬给定网址的时候对已经爬取过的 URL 去重 信息状态统计 Hyper基数是数据集去重后元素个数，HyperLogLog 是用来做基数统计的，运用了 LogLog 的算法 12&#123;1, 3, 5, 7, 5, 7, 8&#125; 基数集： &#123;1, 3, 5 ,7, 8&#125; 基数：5&#123;1, 1, 1, 1, 1, 7, 1&#125; 基数集： &#123;1,7&#125; 基数：2 相关指令： 添加数据 1pfadd key element [element ...] 统计数据 1pfcount key [key ...] 合并数据 1pfmerge destkey sourcekey [sourcekey...] 应用场景： 用于进行基数统计，不是集合不保存数据，只记录数量而不是具体数据，比如网站的访问量 核心是基数估算算法，最终数值存在一定误差 误差范围：基数估计的结果是一个带有 0.81% 标准错误的近似值 耗空间极小，每个 hyperloglog key 占用了12K的内存用于标记基数 pfadd 命令不是一次性分配12K内存使用，会随着基数的增加内存逐渐增大 Pfmerge 命令合并后占用的存储空间为12K，无论合并之前数据量多少 GEOGeoHash 是一种地址编码方法，把二维的空间经纬度数据编码成一个字符串 添加坐标点 12geoadd key longitude latitude member [longitude latitude member ...]georadius key longitude latitude radius m|km|ft|mi [withcoord] [withdist] [withhash] [count count] 获取坐标点 12geopos key member [member ...]georadiusbymember key member radius m|km|ft|mi [withcoord] [withdist] [withhash] [count count] 计算距离 12geodist key member1 member2 [unit]\t#计算坐标点距离geohash key member [member ...] #计算经纬度 Redis 应用于地理位置计算 持久机制概述持久化：利用永久性存储介质将数据进行保存，在特定的时间将保存的数据进行恢复的工作机制称为持久化 作用：持久化用于防止数据的意外丢失，确保数据安全性，因为 Redis 是内存级，所以需要持久化到磁盘 计算机中的数据全部都是二进制，保存一组数据有两种方式 RDB：将当前数据状态进行保存，快照形式，存储数据结果，存储格式简单 AOF：将数据的操作过程进行保存，日志形式，存储操作过程，存储格式复杂 RDB文件创建RDB 持久化功能所生成的 RDB 文件是一个经过压缩的紧凑二进制文件，通过该文件可以还原生成 RDB 文件时的数据库状态，有两个 Redis 命令可以生成 RDB 文件，一个是 SAVE，另一个是 BGSAVE SAVESAVE 指令：手动执行一次保存操作，该指令的执行会阻塞当前 Redis 服务器，客户端发送的所有命令请求都会被拒绝，直到当前 RDB 过程完成为止，有可能会造成长时间阻塞，线上环境不建议使用 工作原理：Redis 是个单线程的工作模式，会创建一个任务队列，所有的命令都会进到这个队列排队执行。当某个指令在执行的时候，队列后面的指令都要等待，所以这种执行方式会非常耗时 配置 redis.conf： 1234dir path #设置存储.rdb文件的路径，通常设置成存储空间较大的目录中，目录名称datadbfilename &quot;x.rdb&quot; #设置本地数据库文件名，默认值为dump.rdb，通常设置为dump-端口号.rdbrdbcompression yes|no\t#设置存储至本地数据库时是否压缩数据，默认yes，设置为no节省CPU运行时间rdbchecksum yes|no #设置读写文件过程是否进行RDB格式校验，默认yes BGSAVEBGSAVE：bg 是 background，代表后台执行，命令的完成需要两个进程，进程之间不相互影响，所以持久化期间 Redis 正常工作 工作原理： 流程：客户端发出 BGSAVE 指令，Redis 服务器使用 fork 函数创建一个子进程，然后响应后台已经开始执行的信息给客户端。子进程会异步执行持久化的操作，持久化过程是先将数据写入到一个临时文件中，持久化操作结束再用这个临时文件替换上次持久化的文件 12345678910111213# 创建子进程pid = fork()if pid == 0: # 子进程负责创建 RDB 文件 rdbSave() # 完成之后向父进程发送信号 signal_parent()elif pid &gt; 0: # 父进程继续处理命令请求，并通过轮询等待子进程的信号 handle_request_and_wait_signal()else: # 处理出错恃况 handle_fork_error() 配置 redis.conf 12345stop-writes-on-bgsave-error yes|no\t#后台存储过程中如果出现错误，是否停止保存操作，默认yesdbfilename filename dir path rdbcompression yes|no rdbchecksum yes|no 注意：BGSAVE 命令是针对 SAVE 阻塞问题做的优化，Redis 内部所有涉及到 RDB 操作都采用 BGSAVE 的方式，SAVE 命令放弃使用 在 BGSAVE 命令执行期间，服务器处理 SAVE、BGSAVE、BGREWRITEAOF 三个命令的方式会和平时有所不同 SAVE 命令会被服务器拒绝，服务器禁止 SAVE 和 BGSAVE 命令同时执行是为了避免父进程（服务器进程）和子进程同时执行两个 rdbSave 调用，产生竞争条件 BGSAVE 命令也会被服务器拒绝，也会产生竞争条件 BGREWRITEAOF 和 BGSAVE 两个命令不能同时执行 如果 BGSAVE 命令正在执行，那么 BGREWRITEAOF 命令会被延迟到 BGSAVE 命令执行完毕之后执行 如果 BGREWRITEAOF 命令正在执行，那么 BGSAVE 命令会被服务器拒绝 特殊指令RDB 特殊启动形式的指令（客户端输入） 服务器运行过程中重启 1debug reload 关闭服务器时指定保存数据 1shutdown save 默认情况下执行 shutdown 命令时，自动执行 bgsave（如果没有开启 AOF 持久化功能） 全量复制：主从复制部分详解 文件载入RDB 文件的载入工作是在服务器启动时自动执行，期间 Redis 会一直处于阻塞状态，直到载入完成 Redis 并没有专门用于载入 RDB 文件的命令，只要服务器在启动时检测到 RDB 文件存在，就会自动载入 RDB 文件 1[7379] 30 Aug 21:07:01.289 * DB loaded from disk: 0.018 seconds # 服务器在成功载入 RDB 文件之后打印 AOF 文件的更新频率通常比 RDB 文件的更新频率高： 如果服务器开启了 AOF 持久化功能，那么会优先使用 AOF 文件来还原数据库状态 只有在 AOF 持久化功能处于关闭状态时，服务器才会使用 RDB 文件来还原数据库状态 自动保存配置文件Redis 支持通过配置服务器的 save 选项，让服务器每隔一段时间自动执行一次 BGSAVE 命令 配置 redis.conf： 1save second changes #设置自动持久化条件，满足限定时间范围内key的变化数量就进行持久化(bgsave) second：监控时间范围 changes：监控 key 的变化量 默认三个条件： 123save 900 1 # 900s内1个key发生变化就进行持久化save 300 10save 60 10000 判定 key 变化的依据： 对数据产生了影响，不包括查询 不进行数据比对，比如 name 键存在，重新 set name seazean 也算一次变化 save 配置要根据实际业务情况进行设置，频度过高或过低都会出现性能问题，结果可能是灾难性的 自动原理服务器状态相关的属性： 12345678910struct redisServer &#123; // 记录了保存条件的数组 struct saveparam *saveparams; // 修改计数器 long long dirty; // 上一次执行保存的时间 time_t lastsave;&#125;; Redis 服务器启动时，可以通过指定配置文件或者传入启动参数的方式设置 save 选项， 如果没有自定义就设置为三个默认值（上节提及），设置服务器状态 redisServe.saveparams 属性，该数组每一项为一个 saveparam 结构，代表 save 的选项设置 123456struct saveparam &#123; // 秒数 time_t seconds // 修改数 int changes;&#125;; dirty 计数器记录距离上一次成功执行 SAVE 或者 BGSAVE 命令之后，服务器中的所有数据库进行了多少次修改（包括写入、删除、更新等操作），当服务器成功执行一个修改指令，该命令修改了多少次数据库， dirty 的值就增加多少 lastsave 属性是一个 UNIX 时间戳，记录了服务器上一次成功执行 SAVE 或者 BGSAVE 命令的时间 Redis 的服务器周期性操作函数 serverCron 默认每隔 100 毫秒就会执行一次，该函数用于对正在运行的服务器进行维护 serverCron 函数的其中一项工作是检查 save 选项所设置的保存条件是否满足，会遍历 saveparams 数组中的所有保存条件，只要有任意一个条件被满足服务器就会执行 BGSAVE 命令 文件结构RDB 的存储结构：图示全大写单词标示常量，用全小写单词标示变量和数据 REDIS：长度为 5 字节，保存着 REDIS 五个字符，是 RDB 文件的开头，在载入文件时可以快速检查所载入的文件是否 RDB 文件 db_version：长度为 4 字节，是一个用字符串表示的整数，记录 RDB 的版本号 database：包含着零个或任意多个数据库，以及各个数据库中的键值对数据 EOF：长度为 1 字节的常量，标志着 RDB 文件正文内容的结束，当读入遇到这个值时，代表所有数据库的键值对都已经载入完毕 check_sum：长度为 8 字节的无符号整数，保存着一个校验和，该值是通过 REDIS、db_version、databases、EOF 四个部分的内容进行计算得出。服务器在载入 RDB 文件时，会将载入数据所计算出的校验和与 check_sum 所记录的校验和进行对比，来检查 RDB 文件是否有出错或者损坏 Redis 本身带有 RDB 文件检查工具 redis-check-dump AOF基本概述AOF（append only file）持久化：以独立日志的方式记录每次写命令（不记录读）来记录数据库状态，增量保存只许追加文件但不可以改写文件，与 RDB 相比可以理解为由记录数据改为记录数据的变化 AOF 主要作用是解决了数据持久化的实时性，目前已经是 Redis 持久化的主流方式 AOF 写数据过程： Redis 只会将对数据库进行了修改的命令写入到 AOF 文件，并复制到各个从服务器，但是 PUBSUB 和 SCRIPT LOAD 命令例外： PUBSUB 命令虽然没有修改数据库，但 PUBSUB 命令向频道的所有订阅者发送消息这一行为带有副作用，接收到消息的所有客户端的状态都会因为这个命令而改变，所以服务器需要使用 REDIS_FORCE_AOF 标志强制将这个命令写入 AOF 文件。这样在将来载入 AOF 文件时，服务器就可以再次执行相同的 PUBSUB 命令，并产生相同的副作用 SCRIPT LOAD 命令虽然没有修改数据库，但它修改了服务器状态，所以也是一个带有副作用的命令，需要使用 REDIS_FORCE_AOF 持久实现AOF 持久化功能的实现可以分为命令追加（append）、文件写入、文件同步（sync）三个步骤 命令追加启动 AOF 的基本配置： 123appendonly yes|no #开启AOF持久化功能，默认no，即不开启状态appendfilename filename #AOF持久化文件名，默认appendonly.aof，建议设置appendonly-端口号.aofdir #AOF持久化文件保存路径，与RDB持久化文件路径保持一致即可 当 AOF 持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的 aof_buf 缓冲区的末尾 1234struct redisServer &#123; // AOF 缓冲区 sds aof_buf;&#125;; 文件写入服务器在处理文件事件时会执行写命令，追加一些内容到 aof_buf 缓冲区里，所以服务器每次结束一个事件循环之前，就会执行 flushAppendOnlyFile 函数，判断是否需要将 aof_buf 缓冲区中的内容写入和保存到 AOF 文件里 flushAppendOnlyFile 函数的行为由服务器配置的 appendfsync 选项的值来决定 1appendfsync always|everysec|no\t#AOF写数据策略：默认为everysec always：每次写入操作都将 aof_buf 缓冲区中的所有内容写入并同步到 AOF 文件 特点：安全性最高，数据零误差，但是性能较低，不建议使用 everysec：先将 aof_buf 缓冲区中的内容写入到操作系统缓存，判断上次同步 AOF 文件的时间距离现在超过一秒钟，再次进行同步 fsync，这个同步操作是由一个（子）线程专门负责执行的 特点：在系统突然宕机的情况下丢失 1 秒内的数据，准确性较高，性能较高，建议使用，也是默认配置 no：将 aof_buf 缓冲区中的内容写入到操作系统缓存，但并不进行同步，何时同步由操作系统来决定 特点：整体不可控，服务器宕机会丢失上次同步 AOF 后的所有写指令 文件同步在现代操作系统中，当用户调用 write 函数将数据写入文件时，操作系统通常会将写入数据暂时保存在一个内存缓冲区空间，等到缓冲区写满或者到达特定时间周期，才真正地将缓冲区中的数据写入到磁盘里面（刷脏） 优点：提高文件的写入效率 缺点：为写入数据带来了安全问题，如果计算机发生停机，那么保存在内存缓冲区里面的写入数据将会丢失 系统提供了 fsync 和 fdatasync 两个同步函数做强制硬盘同步，可以让操作系统立即将缓冲区中的数据写入到硬盘里面，函数会阻塞到写入硬盘完成后返回，保证了数据持久化 异常恢复：AOF 文件损坏，通过 redis-check-aof–fix appendonly.aof 进行恢复，重启 Redis，然后重新加载 文件载入AOF 文件里包含了重建数据库状态所需的所有写命令，所以服务器只要读入并重新执行一遍 AOF 文件里的命令，就还原服务器关闭之前的数据库状态，服务器在启动时，还原数据库状态打印的日志： 1[8321] 05 Sep 11:58:50.449 * DB loaded from append only file: 0.000 seconds AOF 文件里面除了用于指定数据库的 SELECT 命令是服务器自动添加的，其他都是通过客户端发送的命令 123* 2\\r $6\\r SELECT\\r $1\\r 0\\r # 服务器自动添加* 3\\r $3\\r SET\\r $3\\r msg\\r $5\\r hello\\r * 5\\r $4\\r SADD\\r $6\\r fruits\\r $5\\r apple\\r $6\\r banana\\r $6\\r cherry\\r Redis 读取 AOF 文件并还原数据库状态的步骤： 创建一个不带网络连接的伪客户端（fake client）执行命令，因为 Redis 的命令只能在客户端上下文中执行， 而载入 AOF 文件时所使用的命令来源于本地 AOF 文件而不是网络连接 从 AOF 文件分析并读取一条写命令 使用伪客户端执行被读出的写命令，然后重复上述步骤 重写实现重写策略AOF 重写：读取服务器当前的数据库状态，生成新 AOF 文件来替换旧 AOF 文件，不会对现有的 AOF 文件进行任何读取、分析或者写入操作，而是直接原子替换。新 AOF 文件不会包含任何浪费空间的冗余命令，所以体积通常会比旧 AOF 文件小得多 AOF 重写规则： 进程内具有时效性的数据，并且数据已超时将不再写入文件 对同一数据的多条写命令合并为一条命令，因为会读取当前的状态，所以直接将当前状态转换为一条命令即可。为防止数据量过大造成客户端缓冲区溢出，对 list、set、hash、zset 等集合类型，单条指令最多写入 64 个元素 如 lpushlist1 a、lpush list1 b、lpush list1 c 可以转化为：lpush list1 a b c 非写入类的无效指令将被忽略，只保留最终数据的写入命令，但是 select 指令虽然不更改数据，但是更改了数据的存储位置，此类命令同样需要记录 AOF 重写作用： 降低磁盘占用量，提高磁盘利用率 提高持久化效率，降低持久化写时间，提高 IO 性能 降低数据恢复的用时，提高数据恢复效率 重写原理AOF 重写程序 aof_rewrite 函数可以创建一个新 AOF 文件， 但是该函数会进行大量的写入操作，调用这个函数的线程将被长时间阻塞，所以 Redis 将 AOF 重写程序放到 fork 的子进程里执行，不会阻塞父进程，重写命令： 1bgrewriteaof 子进程进行 AOF 重写期间，服务器进程（父进程）可以继续处理命令请求 子进程带有服务器进程的数据副本，使用子进程而不是线程，可以在避免使用锁的情况下， 保证数据安全性 子进程在进行 AOF 重写期间，服务器进程还需要继续处理命令请求，而新命令可能会对现有的数据库状态进行修改，从而使得服务器当前的数据库状态和重写后的 AOF 文件所保存的数据库状态不一致，所以 Redis 设置了 AOF 重写缓冲区 工作流程： Redis 服务器执行完一个写命令，会同时将该命令追加到 AOF 缓冲区和 AOF 重写缓冲区（从创建子进程后才开始写入） 当子进程完成 AOF 重写工作之后，会向父进程发送一个信号，父进程在接到该信号之后， 会调用一个信号处理函数，该函数执行时会对服务器进程（父进程）造成阻塞（影响很小，类似 JVM STW），主要工作： 将 AOF 重写缓冲区中的所有内容写入到新 AOF 文件中， 这时新 AOF 文件所保存的状态将和服务器当前的数据库状态一致 对新的 AOF 文件进行改名，原子地（atomic）覆盖现有的 AOF 文件，完成新旧两个 AOF 文件的替换 自动重写触发时机：Redis 会记录上次重写时的 AOF 大小，默认配置是当 AOF 文件大小是上次重写后大小的一倍且文件大于 64M 时触发 12auto-aof-rewrite-min-size size #设置重写的基准值，最小文件 64MB，达到这个值开始重写auto-aof-rewrite-percentage percent\t#触发AOF文件执行重写的增长率，当前AOF文件大小超过上一次重写的AOF文件大小的百分之多少才会重写，比如文件达到 100% 时开始重写就是两倍时触发 自动重写触发比对参数（ 运行指令 info Persistence 获取具体信息 ）： 12aof_current_size #AOF文件当前尺寸大小（单位:字节）aof_base_size #AOF文件上次启动和重写时的尺寸大小（单位:字节） 自动重写触发条件公式： aof_current_size &gt; auto-aof-rewrite-min-size (aof_current_size - aof_base_size) &#x2F; aof_base_size &gt;&#x3D; auto-aof-rewrite-percentage 对比RDB 的特点 RDB 优点： RDB 是一个紧凑压缩的二进制文件，存储效率较高，但存储数据量较大时，存储效率较低 RDB 内部存储的是 Redis 在某个时间点的数据快照，非常适合用于数据备份，全量复制、灾难恢复 RDB 恢复数据的速度要比 AOF 快很多，因为是快照，直接恢复 RDB 缺点： BGSAVE 指令每次运行要执行 fork 操作创建子进程，会牺牲一些性能 RDB 方式无论是执行指令还是利用配置，无法做到实时持久化，具有丢失数据的可能性，最后一次持久化后的数据可能丢失 Redis 的众多版本中未进行 RDB 文件格式的版本统一，可能出现各版本之间数据格式无法兼容 AOF 特点： AOF 的优点：数据持久化有较好的实时性，通过 AOF 重写可以降低文件的体积 AOF 的缺点：文件较大时恢复较慢 AOF 和 RDB 同时开启，系统默认取 AOF 的数据（数据不会存在丢失） 应用场景： 对数据非常敏感，建议使用默认的 AOF 持久化方案，AOF 持久化策略使用 everysecond，每秒钟 fsync 一次，该策略 Redis 仍可以保持很好的处理性能 注意：AOF 文件存储体积较大，恢复速度较慢，因为要执行每条指令 数据呈现阶段有效性，建议使用 RDB 持久化方案，可以做到阶段内无丢失，且恢复速度较快 注意：利用 RDB 实现紧凑的数据持久化，存储数据量较大时，存储效率较低 综合对比： RDB 与 AOF 的选择实际上是在做一种权衡，每种都有利有弊 灾难恢复选用 RDB 如不能承受数分钟以内的数据丢失，对业务数据非常敏感，选用 AOF；如能承受数分钟以内的数据丢失，且追求大数据集的恢复速度，选用 RDB 双保险策略，同时开启 RDB 和 AOF，重启后 Redis 优先使用 AOF 来恢复数据，降低丢失数据的量 不建议单独用 AOF，因为可能会出现 Bug，如果只是做纯内存缓存，可以都不用 fork介绍fork() 函数创建一个子进程，子进程与父进程几乎是完全相同的进程，系统先给子进程分配资源，然后把父进程的所有数据都复制到子进程中，只有少数值与父进程的值不同，相当于克隆了一个进程 在完成对其调用之后，会产生 2 个进程，且每个进程都会从 fork() 的返回处开始执行，这两个进程将执行相同的程序段，但是拥有各自不同的堆段，栈段，数据段，每个子进程都可修改各自的数据段，堆段，和栈段 123#include&lt;unistd.h&gt;pid_t fork(void);// 父进程返回子进程的pid，子进程返回0，错误返回负值，根据返回值的不同进行对应的逻辑处理 fork 调用一次，却能够返回两次，可能有三种不同的返回值： 在父进程中，fork 返回新创建子进程的进程 ID 在子进程中，fork 返回 0 如果出现错误，fork 返回一个负值，错误原因： 当前的进程数已经达到了系统规定的上限，这时 errno 的值被设置为 EAGAIN 系统内存不足，这时 errno 的值被设置为 ENOMEM fpid 的值在父子进程中不同：进程形成了链表，父进程的 fpid 指向子进程的进程 id，因为子进程没有子进程，所以其 fpid 为0 创建新进程成功后，系统中出现两个基本完全相同的进程，这两个进程执行没有固定的先后顺序，哪个进程先执行要看系统的调度策略 每个进程都有一个独特（互不相同）的进程标识符 process ID，可以通过 getpid() 函数获得；还有一个记录父进程 pid 的变量，可以通过 getppid() 函数获得变量的值 使用基本使用： 1234567891011121314151617181920212223242526#include &lt;unistd.h&gt; #include &lt;stdio.h&gt; int main () &#123; pid_t fpid; // fpid表示fork函数返回的值 int count = 0; fpid = fork(); if (fpid &lt; 0) printf(&quot;error in fork!&quot;); else if (fpid == 0) &#123; printf(&quot;i am the child process, my process id is %d/n&quot;, getpid()); count++; &#125; else &#123; printf(&quot;i am the parent process, my process id is %d/n&quot;, getpid()); count++; &#125; printf(&quot;count: %d/n&quot;,count);// 1 return 0; &#125; /* 输出内容： i am the child process, my process id is 5574 count: 1 i am the parent process, my process id is 5573 count: 1*/ 进阶使用： 1234567891011121314151617181920212223242526#include &lt;unistd.h&gt; #include &lt;stdio.h&gt; int main(void) &#123; int i = 0; // ppid 指当前进程的父进程pid // pid 指当前进程的pid, // fpid 指fork返回给当前进程的值，在这可以表示子进程 for(i = 0; i &lt; 2; i++)&#123; pid_t fpid = fork(); if(fpid == 0) printf(&quot;%d child %4d %4d %4d/n&quot;,i, getppid(), getpid(), fpid); else printf(&quot;%d parent %4d %4d %4d/n&quot;,i, getppid(), getpid(),fpid); &#125; return 0; &#125; /*输出内容：\ti 父id id 子id\t0 parent 2043 3224 3225 0 child 3224 3225 0 1 parent 2043 3224 3226 1 parent 3224 3225 3227 1 child 1 3227 0 1 child 1 3226 0 */ 在 p3224 和 p3225 执行完第二个循环后，main 函数退出，进程死亡。所以 p3226，p3227 就没有父进程了，成为孤儿进程，所以 p3226 和 p3227 的父进程就被置为 ID 为 1 的 init 进程（笔记 Tool → Linux → 进程管理详解） 参考文章：https://blog.csdn.net/love_gaohz/article/details/41727415 内存fork() 调用之后父子进程的内存关系 早期 Linux 的 fork() 实现时，就是全部复制，这种方法效率太低，而且造成了很大的内存浪费，现在 Linux 实现采用了两种方法： 父子进程的代码段是相同的，所以代码段是没必要复制的，只需内核将代码段标记为只读，父子进程就共享此代码段。fork() 之后在进程创建代码段时，子进程的进程级页表项都指向和父进程相同的物理页帧 对于父进程的数据段，堆段，栈段中的各页，由于父子进程相互独立，采用写时复制 COW 的技术，来提高内存以及内核的利用率 在 fork 之后两个进程用的是相同的物理空间（内存区），子进程的代码段、数据段、堆栈都是指向父进程的物理空间，两者的虚拟空间不同，但其对应的物理空间是同一个，当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间。如果两者的代码完全相同，代码段继续共享父进程的物理空间；而如果两者执行的代码不同，子进程的代码段也会分配单独的物理空间。 fork 之后内核会将子进程放在队列的前面，让子进程先执行，以免父进程执行导致写时复制，而后子进程再执行，因无意义的复制而造成效率的下降 补充知识： vfork（虚拟内存 fork virtual memory fork）：调用 vfork() 父进程被挂起，子进程使用父进程的地址空间。不采用写时复制，如果子进程修改父地址空间的任何页面，这些修改过的页面对于恢复的父进程是可见的 参考文章：https://blog.csdn.net/Shreck66/article/details/47039937 事务机制事务特征Redis 事务就是将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务去执行其他的命令请求，会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求，Redis 事务的特性： Redis 事务没有隔离级别的概念，队列中的命令在事务没有提交之前都不会实际被执行 Redis 单条命令式保存原子性的，但是事务不保证原子性，事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚 工作流程事务的执行流程分为三个阶段： 事务开始：MULTI 命令的执行标志着事务的开始，通过在客户端状态的 flags 属性中打开 REDIS_MULTI 标识，将执行该命令的客户端从非事务状态切换至事务状态 1MULTI\t# 设定事务的开启位置，此指令执行后，后续的所有指令均加入到事务中 命令入队：事务队列以先进先出（FIFO）的方式保存入队的命令，每个 Redis 客户端都有事务状态，包含着事务队列： 123456789101112typedef struct redisClient &#123;\t// 事务状态 multiState mstate;\t/* MULTI/EXEC state */ &#125;typedef struct multiState &#123; // 事务队列，FIFO顺序 multiCmd *commands; // 已入队命令计数 int count；&#125; 如果命令为 EXEC、DISCARD、WATCH、MULTI 四个命中的一个，那么服务器立即执行这个命令 其他命令服务器不执行，而是将命令放入一个事务队列里面，然后向客户端返回 QUEUED 回复 事务执行：EXEC 提交事务给服务器执行，服务器会遍历这个客户端的事务队列，执行队列中的命令并将执行结果返回 1EXEC\t# Commit 提交，执行事务，与multi成对出现，成对使用 事务取消的方法： 取消事务： 1DISCARD\t# 终止当前事务的定义，发生在multi之后，exec之前 一般用于事务执行过程中输入了错误的指令，直接取消这次事务，类似于回滚 WATCH监视机制WATCH 命令是一个乐观锁（optimistic locking），可以在 EXEC 命令执行之前，监视任意数量的数据库键，并在 EXEC 命令执行时，检查被监视的键是否至少有一个已经被修改过了，如果是服务器将拒绝执行事务，并向客户端返回代表事务执行失败的空回复 添加监控锁 1WATCH key1 [key2……]\t#可以监控一个或者多个key 取消对所有 key 的监视 1UNWATCH 实现原理每个 Redis 数据库都保存着一个 watched_keys 字典，键是某个被 WATCH 监视的数据库键，值则是一个链表，记录了所有监视相应数据库键的客户端： 1234typedef struct redisDb &#123;\t// 正在被 WATCH 命令监视的键 dict *watched_keys;&#125; 所有对数据库进行修改的命令，在执行后都会调用 multi.c/touchWatchKey 函数对 watched_keys 字典进行检查，是否有客户端正在监视刚被命令修改过的数据库键，如果有的话函数会将监视被修改键的客户端的 REDIS_DIRTY_CAS 标识打开，表示该客户端的事务安全性已经被破坏 服务器接收到个客户端 EXEC 命令时，会根据这个客户端是否打开了 REDIS_DIRTY_CAS 标识，如果打开了说明客户端提交事务不安全，服务器会拒绝执行 ACID原子性事务具有原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability） 原子性指事务队列中的命令要么就全部都执行，要么一个都不执行，但是在命令执行出错时，不会保证原子性（下一节详解） Redis 不支持事务回滚机制（rollback），即使事务队列中的某个命令在执行期间出现了错误，整个事务也会继续执行下去，直到将事务队列中的所有命令都执行完毕为止 回滚需要程序员在代码中实现，应该尽可能避免： 事务操作之前记录数据的状态 单数据：string 多数据：hash、list、set、zset 设置指令恢复所有的被修改的项 单数据：直接 set（注意周边属性，例如时效） 多数据：修改对应值或整体克隆复制 一致性事务具有一致性指的是，数据库在执行事务之前是一致的，那么在事务执行之后，无论事务是否执行成功，数据库也应该仍然是一致的 一致是数据符合数据库的定义和要求，没有包含非法或者无效的错误数据，Redis 通过错误检测和简单的设计来保证事务的一致性： 入队错误：命令格式输入错误，出现语法错误造成，整体事务中所有命令均不会执行，包括那些语法正确的命令 执行错误：命令执行出现错误，例如对字符串进行 incr 操作，事务中正确的命令会被执行，运行错误的命令不会被执行 服务器停机： 如果服务器运行在无持久化的内存模式下，那么重启之后的数据库将是空白的，因此数据库是一致的 如果服务器运行在持久化模式下，重启之后将数据库还原到一致的状态 隔离性Redis 是一个单线程的执行原理，所以对于隔离性，分以下两种情况： 并发操作在 EXEC 命令前执行，隔离性的保证要使用 WATCH 机制来实现，否则隔离性无法保证 并发操作在 EXEC 命令后执行，隔离性可以保证 持久性Redis 并没有为事务提供任何额外的持久化功能，事务的持久性由 Redis 所使用的持久化模式决定 配置选项 no-appendfsync-on-rewrite 可以配合 appendfsync 选项在 AOF 持久化模式使用： 选项打开时在执行 BGSAVE 或者 BGREWRITEAOF 期间，服务器会暂时停止对 AOF 文件进行同步，从而尽可能地减少 I&#x2F;O 阻塞 选项打开时运行在 always 模式的 AOF 持久化，事务也不具有持久性，所以该选项默认关闭 在一个事务的最后加上 SAVE 命令总可以保证事务的耐久性 Lua 脚本环境创建基本介绍Redis 对 Lua 脚本支持，通过在服务器中嵌入 Lua 环境，客户端可以使用 Lua 脚本直接在服务器端原子地执行多个命令 12EVAL &lt;script&gt; &lt;numkeys&gt; [key ...] [arg ...]EVALSHA &lt;sha1&gt; &lt;numkeys&gt; [key ...] [arg ...] EVAL 命令可以直接对输入的脚本计算： 12redis&gt; EVAL &quot;return 1 + 1&quot; 0\t# 0代表需要的参数(integer) 2 EVALSHA 命令根据脚本的 SHA1 校验和来对脚本计算： 12redis&gt; EVALSHA &quot;2f3lba2bb6d6a0f42ccl59d2e2dad55440778de3&quot; 0(integer) 2 应用场景：Redis 只保证单条命令的原子性，所以为了实现原子操作，将多条的对 Redis 的操作整合到一个脚本里，但是避免把不需要做并发控制的操作写入脚本中 Lua 语法特点： 声明变量的时候无需指定数据类型，而是用 local 来声明变量为局部变量 数组下标是从 1 开始 创建过程Redis 服务器创建并修改 Lua 环境的整个过程： 创建一个基础的 Lua 环境，调用 Lua 的 API 函数 lua_open 载入多个函数库到 Lua 环境里面，让 Lua 脚本可以使用这些函数库来进行数据操作，包括基础核心函数 创建全局变量 redis 表格，表格包含以下函数： 执行 Redis 命令的 redis.call 和 redis.pcall 函数 记录 Redis 日志的 redis.log 函数，以及相应的日志级别 (level) 常量 redis.LOG_DEBUG 等 计算 SHAl 校验和的 redis.shalhex 函数 返回错误信息的 redis.error_reply 函数和 redis.status_reply 函数 使用 Redis 自制的随机函数来替换 Lua 原有的带有副作用的随机函数，从而避免在脚本中引入副作用 Redis 要求所有传入服务器的 Lua 脚本，以及 Lua 环境中的所有函数，都必须是无副作用（side effect）的纯函数（pure function），所以对有副作用的随机函数 math.random 和 math.randornseed 进行替换 创建排序辅助函数 _redis_compare_helper，使用辅助函数来对一部分 Redis 命令的结果进行排序，从而消除命令的不确定性 比如集合元素的排列是无序的， 所以即使两个集合的元素完全相同，输出结果也不一定相同，Redis 将 SMEMBERS 这类在相同数据集上产生不同输出的命令称为带有不确定性的命令 创建 redis.pcall 函数的错误报告辅助函数 _redis_err_handler ，这个函数可以打印出错代码的来源和发生错误的行数 对 Lua 环境中的全局环境进行保护，确保传入服务器的脚本不会因忘记使用 local 关键字，而将额外的全局变量添加到 Lua 环境 将完成修改的 Lua 环境保存到服务器状态的 lua 属性中，等待执行服务器传来的 Lua 脚本 123struct redisServer &#123; Lua *lua;&#125;; Redis 使用串行化的方式来执行 Redis 命令，所以在任何时间里最多都只会有一个脚本能够被放进 Lua 环境里面运行，因此整个 Redis 服务器只需要创建一个 Lua 环境即可 协作组件伪客户端Redis 服务器为 Lua 环境创建了一个伪客户端负责处理 Lua 脚本中包含的所有 Redis 命令，工作流程： Lua 环境将 redis.call 或者 redis.pcall 函数想要执行的命令传给伪客户端 伪客户端将命令传给命令执行器 命令执行器执行命令并将命令的执行结果返回给伪客户端 伪客户端接收命令执行器返回的命令结果，并将结果返回给 Lua 环境 Lua 将命令结果返回给 redis.call 函数或者 redis.pcall 函数 redis.call 函数或者 redis.pcall 函数会将命令结果作为返回值返回给脚本的调用者 脚本字典Redis 服务器为 Lua 环境创建 lua_scripts 字典，键为某个 Lua 脚本的 SHA1 校验和（checksum），值则是校验和对应的 Lua 脚本 123struct redisServer &#123; dict *lua_scripts;&#125;; 服务器会将所有被 EVAL 命令执行过的 Lua 脚本，以及所有被 SCRIPT LOAD 命令载入过的 Lua 脚本都保存到 lua_scripts 字典 12redis&gt; SCRIPT LOAD &quot;return &#x27;hi&#x27;&quot;&quot;2f3lba2bb6d6a0f42ccl59d2e2dad55440778de3&quot; # 字典的键，SHA1 校验和 命令实现脚本函数EVAL 命令的执行的第一步是为传入的脚本定义一个相对应的 Lua 函数，Lua 函数的名字由 f_ 前缀加上脚本的 SHA1 校验和（四十个字符长）组成，而函数的体（body）则是脚本本身 12345EVAL &quot;return &#x27;hello world&#x27;&quot; 0 # 命令将会定义以下的函数function f_533203lc6b470dc5a0dd9b4bf2030dea6d65de91() &#123;\treturn &#x27;hello world&#x27;&#125; 使用函数来保存客户端传入的脚本有以下优点： 通过函数的局部性来让 Lua 环境保持清洁，减少了垃圾回收的工作最， 并且避免了使用全局变量 如果某个脚本在 Lua 环境中被定义过至少一次，那么只需要 SHA1 校验和，服务器就可以在不知道脚本本身的情况下，直接通过调用 Lua 函数来执行脚本 EVAL 命令第二步是将客户端传入的脚本保存到服务器的 lua_scripts 字典里，在字典中新添加一个键值对 执行函数EVAL 命令第三步是执行脚本函数 将 EVAL 命令中传入的键名参数和脚本参数分别保存到 KEYS 数组和 ARGV 数组，将这两个数组作为全局变量传入到 Lua 环境 为 Lua 环境装载超时处理钩子（hook），这个钩子可以在脚本出现超时运行情况时，让客户端通过 SCRIPT KILL 命令停止脚本，或者通过 SHUTDOWN 命令直接关闭服务器 因为 Redis 是单线程的执行命令，当 Lua 脚本阻塞时需要兜底策略，可以中断执行 执行脚本函数 移除之前装载的超时钩子 将执行脚本函数的结果保存到客户端状态的输出缓冲区里，等待服务器将结果返回给客户端 EVALSHAEVALSHA 命令的实现原理就是根据脚本的 SHA1 校验和来调用脚本对应的函数，如果函数在 Lua 环境中不存在，找不到 f_ 开头的函数，就会返回 SCRIPT NOT FOUND 管理命令Redis 中与 Lua 脚本有关的管理命令有四个： SCRIPT FLUSH：用于清除服务器中所有和 Lua 脚本有关的信息，会释放并重建 lua_scripts 字典，关闭现有的 Lua 环境并重新创建一个新的 Lua 环境 SCRIPT EXISTS：根据输入的 SHA1 校验和（允许一次传入多个校验和），检查校验和对应的脚本是否存在于服务器中，通过检查 lua_scripts 字典实现 SCRIPT LOAD：在 Lua 环境中为脚本创建相对应的函数，然后将脚本保存到 lua_scripts字典里 12redis&gt; SCRIPT LOAD &quot;return &#x27;hi&#x27;&quot;&quot;2f3lba2bb6d6a0f42ccl59d2e2dad55440778de3&quot; SCRIPT KILL：停止脚本 如果服务器配置了 lua-time-li­mit 选项，那么在每次执行 Lua 脚本之前，都会设置一个超时处理的钩子。钩子会在脚本运行期间会定期检查运行时间是否超过配置时间，如果超时钩子将定期在脚本运行的间隙中，查看是否有 SCRIPT KILL 或者 SHUTDOWN 到达： 如果超时运行的脚本没有执行过写入操作，客户端可以通过 SCRIPT KILL 来停止这个脚本 如果执行过写入操作，客户端只能用 SHUTDOWN nosave 命令来停止服务器，防止不合法的数据被写入数据库中 脚本复制命令复制当服务器运行在复制模式时，具有写性质的脚本命令也会被复制到从服务器，包括 EVAL、EVALSHA、SCRIPT FLUSH，以及 SCRIPT LOAD 命令 Redis 复制 EVAL、SCRIPT FLUSH、SCRIPT LOAD 三个命令的方法和复制普通 Redis 命令的方法一样，当主服务器执行完以上三个命令的其中一个时，会直接将被执行的命令传播（propagate）给所有从服务器，在从服务器中产生相同的效果 EVALSHAEVALSHA 命令的复制操作相对复杂，因为多个从服务器之间载入 Lua 脚本的清况各有不同，一个在主服务器被成功执行的 EVALSHA 命令，在从服务器执行时可能会出现脚本未找到（not found）错误 Redis 要求主服务器在传播 EVALSHA 命令时，必须确保 EVALSHA 命令要执行的脚本已经被所有从服务器载入过，如果不能确保主服务器会将 EVALSHA 命令转换成一个等价的 EVAL 命令，然后通过传播 EVAL 命令来代替 EVALSHA 命令 主服务器使用服务器状态的 repl_scriptcache_dict 字典记录已经将哪些脚本传播给了所有从服务器，当一个校验和出现在字典时，说明校验和对应的 Lua 脚本已经传播给了所有从服务器，主服务器可以直接传播 EVALSHA 命令 1234struct redisServer &#123; // 键是一个个 Lua 脚本的 SHA1 校验和，值则全部都是 NULL dict *repl_scriptcache_dict;&#125; 注意：每当主服务器添加一个新的从服务器时，都会清空 repl_scriptcache_dict 字典，因为字典里面记录的脚本已经不再被所有从服务器载入过，所以服务器以清空字典的方式，强制重新向所有从服务器传播脚本 通过使用 EVALSHA 命令指定的 SHA1 校验和，以及 lua_scripts 字典保存的 Lua 脚本，可以将一个 EVALSHA 命令转化为 EVAL 命令 123EVALSHA &quot;533203lc6b470dc5a0dd9b4bf2030dea6d65de91&quot; 0 # -&gt; 转换EVAL &quot;return&#x27;hello world&#x27;&quot; 0 脚本内容 &quot;return&#39;hello world&#39;&quot; 来源于 lua_scripts 字典 533203lc6b470dc5a0dd9b4bf2030dea6d65de91 键的值 分布式锁基本操作在分布式场景下，锁变量需要由一个共享存储系统来维护，多个客户端才可以通过访问共享存储系统来访问锁变量，加锁和释放锁的操作就变成了读取、判断和设置共享存储系统中的锁变量值多步操作 Redis 分布式锁的基本使用，悲观锁 使用 SETNX 设置一个公共锁 1SETNX lock-key value\t# value任意数，返回为1设置成功，返回为0设置失败 NX：只在键不存在时，才对键进行设置操作，SET key value NX 效果等同于 SETNX key value XX：只在键已经存在时，才对键进行设置操作 EX：设置键 key 的过期时间，单位时秒 PX：设置键 key 的过期时间，单位时毫秒 说明：由于 SET 命令加上选项已经可以完全取代 SETNX、SETEX、PSETEX 的功能，Redis 不推荐使用这几个命令 操作完毕通过 DEL 操作释放锁 1DEL lock-key 使用 EXPIRE 为锁 key 添加存活（持有）时间，过期自动删除（放弃）锁，防止线程出现异常，无法释放锁 12EXPIRE lock-key second PEXPIRE lock-key milliseconds 通过 EXPIRE 设置过期时间缺乏原子性，如果在 SETNX 和 EXPIRE 之间出现异常，锁也无法释放 在 SET 时指定过期时间，保证原子性 1234567891011121314151617181920 SET key value NX [EX seconds | PX milliseconds]****### 防误删场景描述：线程 A 正在执行，但是业务阻塞，在锁的过期时间内未执行完成，过期删除后线程 B 重新获取到锁，此时线程 A 执行完成，删除锁，导致线程 B 的锁被线程 A 误删SETNX 获取锁时，设置一个指定的唯一值（UUID），释放前获取这个值，判断是否自己的锁，防止出现线程之间误删了其他线程的锁```java// 加锁, unique_value作为客户端唯一性的标识，// PX 10000 则表示 lock_key 会在 10s 后过期，以免客户端在这期间发生异常而无法释放锁SET lock_key unique_value NX PX 10000 Lua 脚本（unlock.script）实现的释放锁操作的伪代码：key 类型参数会放入 KEYS 数组，其它参数会放入 ARGV 数组，在脚本中通过 KEYS 和 ARGV 传递参数，保证判断标识和释放锁这两个操作的原子性 1EVAL &quot;return redis.call(&#x27;set&#x27;, KEYS[1], ARGV[1])&quot; 1 lock_key unique_value # 1 代表需要一个参数 1234567// 释放锁，KEYS[1] 就是锁的 key，ARGV[1] 就是标识值，避免误释放// 获取标识值，判断是否与当前线程标示一致if redis.call(&quot;get&quot;, KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;, KEYS[1])else return 0end 优化锁不可重入不可重入：同一个线程无法多次获取同一把锁 使用 hash 键，filed 是加锁的线程标识， value 是锁重入次数 1234| key | value || | filed | value ||-------------------------------|| lock_key | thread1 | 1 | 锁重入： 加锁时判断锁的 filed 属性是否是当前线程，如果是将 value 加 1 解锁时判断锁的 filed 属性是否是当前线程，首先将 value 减一，如果 value 为 0 直接释放锁 使用 Lua 脚本保证多条命令的原子性 不可重试不可重试：获取锁只尝试一次就返回 false，没有重试机制 利用 Lua 脚本尝试获取锁，获取失败获取锁的剩余超时时间 ttl，或者通过参数传入线程抢锁允许等待的时间 利用订阅功能订阅锁释放的信息，然后线程挂起等待 ttl 时间 利用 Lua 脚本在释放锁时，发布一条锁释放的消息 超时释放超时释放：锁超时释放可以避免死锁，但如果是业务执行耗时较长，需要进行锁续时，防止业务未执行完提前释放锁 看门狗 Watch Dog 机制： 获取锁成功后，提交周期任务，每隔一段时间（Redisson 中默认为过期时间 &#x2F; 3），重置一次超时时间 如果服务宕机，Watch Dog 机制线程就停止，就不会再延长 key 的过期时间 释放锁后，终止周期任务 主从一致主从一致性：集群模式下，主从同步存在延迟，当加锁后主服务器宕机时，从服务器还没同步主服务器中的锁数据，此时从服务器升级为主服务器，其他线程又可以获取到锁 将服务器升级为多主多从： 获取锁需要从所有主服务器 SET 成功才算获取成功 某个 master 宕机，slave 还没有同步锁数据就升级为 master，其他线程尝试加锁会加锁失败，因为其他 master 上已经存在该锁 主从复制基本操作主从介绍主从复制：一个服务器去复制另一个服务器，被复制的服务器为主服务器 master，复制的服务器为从服务器 slave master 用来写数据，执行写操作时，将出现变化的数据自动同步到 slave，很少会进行读取操作 slave 用来读数据，禁止在 slave 服务器上进行读操作 进行复制中的主从服务器双方的数据库将保存相同的数据，将这种现象称作数据库状态一致 主从复制的特点： 薪火相传：一个 slave 可以是下一个 slave 的 master，slave 同样可以接收其他 slave 的连接和同步请求，那么该 slave 作为了链条中下一个的 master，可以有效减轻 master 的写压力，去中心化降低风险 注意：主机挂了，从机还是从机，无法写数据了 反客为主：当一个 master 宕机后，后面的 slave 可以立刻升为 master，其后面的 slave 不做任何修改 主从复制的作用： 读写分离：master 写、slave 读，提高服务器的读写负载能力 负载均衡：基于主从结构，配合读写分离，由 slave 分担 master 负载，并根据需求的变化，改变 slave 的数量，通过多个从节点分担数据读取负载，大大提高 Redis 服务器并发量与数据吞吐量 故障恢复：当 master 出现问题时，由 slave 提供服务，实现快速的故障恢复 数据冗余：实现数据热备份，是持久化之外的一种数据冗余方式 高可用基石：基于主从复制，构建哨兵模式与集群，实现 Redis 的高可用方案 三高架构： 高并发：应用提供某一业务要能支持很多客户端同时访问的能力，称为并发 高性能：性能最直观的感受就是速度快，时间短 高可用： 可用性：应用服务在全年宕机的时间加在一起就是全年应用服务不可用的时间 业界可用性目标 5 个 9，即 99.999%，即服务器年宕机时长低于 315 秒，约 5.25 分钟 操作指令系统状态指令： 1INFO replication master 和 slave 互连： 方式一：客户端发送命令，设置 slaveof 选项，产生主从结构 1slaveof masterip masterport 方式二：服务器带参启动 1redis-server --slaveof masterip masterport 方式三：服务器配置（主流方式） 1slaveof masterip masterport 主从断开连接： slave 断开连接后，不会删除已有数据，只是不再接受 master 发送的数据，可以作为从服务器升级为主服务器的指令 1slaveof no one 授权访问：master 有服务端和客户端，slave 也有服务端和客户端，不仅服务端之间可以发命令，客户端也可以 master 客户端发送命令设置密码： 1requirepass password master 配置文件设置密码： 12config set requirepass passwordconfig get requirepass slave 客户端发送命令设置密码： 1auth password slave 配置文件设置密码： 1masterauth password slave 启动服务器设置密码： 1redis-server –a password 复制流程旧版复制Redis 的复制功能分为同步（sync）和命令传播（command propagate）两个操作，主从库间的复制是异步进行的 同步操作用于将从服务器的数据库状态更新至主服务器当前所处的数据库状态，该过程又叫全量复制： 从服务器向主服务器发送 SYNC 命令来进行同步 收到 SYNC 的主服务器执行 BGSAVE 命令，在后台生成一个 RDB 文件，并使用一个缓冲区记录从现在开始执行的所有写命令 当 BGSAVE 命令执行完毕时，主服务器会将 RDB 文件发送给从服务器 从服务接收并载入 RDB 文件（从服务器会清空原有数据） 缓冲区记录了 RDB 文件所在状态后的所有写命令，主服务器将在缓冲区的所有命令发送给从服务器，从服务器执行这些写命令 至此从服务器的数据库状态和主服务器一致 命令传播用于在主服务器的数据库状态被修改，导致主从数据库状态出现不一致时， 让主从服务器的数据库重新回到一致状态 主服务器会将自己执行的写命令，也即是造成主从服务器不一致的那条写命令，发送给从服务器 从服务器接受命令并执行，主从服务器将再次回到一致状态 功能缺陷SYNC 本身就是一个非常消耗资源的操作，每次执行 SYNC 命令，都需要执行以下动作： 生成 RDB 文件，耗费主服务器大量 CPU 、内存和磁盘 I&#x2F;O 资源 RDB 文件发送给从服务器，耗费主从服务器大量的网络资源（带宽和流量），并对主服务器响应命令请求的时间产生影响 从服务器载入 RDB 文件，期间会因为阻塞而没办法处理命令请求 SYNC 命令下的从服务器对主服务器的复制分为两种情况： 初次复制：从服务器没有复制过任何主服务器，或者从服务器当前要复制的主服务器和上一次复制的主服务器不同 断线后重复制：处于命令传播阶段的主从服务器因为网络原因而中断了复制，自动重连后并继续复制主服务器 旧版复制在断线后重复制时，也会创建 RDB 文件进行全量复制，但是从服务器只需要断线时间内的这部分数据，所以旧版复制的实现方式非常浪费资源 新版复制Redis 从 2.8 版本开始，使用 PSYNC 命令代替 SYNC 命令来执行复制时的同步操作（命令传播阶段相同），解决了旧版复制在处理断线重复制情况的低效问题 PSYNC 命令具有完整重同步（full resynchronization）和部分重同步（partial resynchronization）两种模式： 完整重同步：处理初次复制情况，执行步骤和 SYNC命令基本一样 部分重同步：处理断线后重复制情况，主服务器可以将主从连接断开期间执行的写命令发送给从服务器，从服务器只要接收并执行这些写命令，就可以将数据库更新至主服务器当前所处的状态，该过程又叫部分复制 部分同步部分重同步功能由以下三个部分构成： 主服务器的复制偏移量（replication offset）和从服务器的复制偏移量 主服务器的复制积压缓冲区（replication backlog） 服务器的运行 ID (run ID) 偏移量主服务器和从服务器会分别维护一个复制偏移量： 主服务器每次向从服务器传播 N 个字节的数据时，就将自己的复制偏移量的值加上 N 从服务器每次收到主服务器传播来的 N 个字节的数据时，就将自己的复制偏移量的值加上 N 通过对比主从服务器的复制偏移量，可以判断主从服务器是否处于一致状态 主从服务器的偏移量是相同的，说明主从服务器处于一致状态 主从服务器的偏移量是不同的，说明主从服务器处于不一致状态 缓冲区复制积压缓冲区是由主服务器维护的一个固定长度（fixed-size）先进先出（FIFO）队列，默认大小为 1MB 出队规则跟普通的先进先出队列一样 入队规则是当入队元素的数量大于队列长度时，最先入队的元素会被弹出，然后新元素才会被放入队列 当主服务器进行命令传播时，不仅会将写命令发送给所有从服务器，还会将写命令入队到复制积压缓冲区，缓冲区会保存着一部分最近传播的写命令，并且缓冲区会为队列中的每个字节记录相应的复制偏移量 从服务器会通过 PSYNC 命令将自己的复制偏移量 offset 发送给主服务器，主服务器会根据这个复制偏移量来决定对从服务器执行何种同步操作： offset 之后的数据（即 offset+1）仍然存在于复制积压缓冲区里，那么主服务器将对从服务器执行部分重同步操作 offset 之后的数据已经不在复制积压缓冲区，说明部分数据已经丢失，那么主服务器将对从服务器执行完整重同步操作 复制缓冲区大小设定不合理，会导致数据溢出。比如主服务器需要执行大量写命令，又或者主从服务器断线后重连接所需的时间较长，导致缓冲区中的数据已经丢失，则必须进行完整重同步 1repl-backlog-size ?mb 建议设置如下，这样可以保证绝大部分断线情况都能用部分重同步来处理： 从服务器断线后重新连接上主服务器所需的平均时间 second 获取 master 平均每秒产生写命令数据总量 write_size_per_second 最优复制缓冲区空间 &#x3D; 2 * second * write_size_per_second 运行ID服务器运行 ID（run ID）：是每一台服务器每次运行的身份识别码，在服务器启动时自动生成，由 40 位随机的十六进制字符组成，一台服务器多次运行可以生成多个运行 ID 作用：服务器间进行传输识别身份，如果想两次操作均对同一台服务器进行，每次必须操作携带对应的运行 ID，用于对方识别 从服务器对主服务器进行初次复制时，主服务器将自己的运行 ID 传送给从服务器，然后从服务器会将该运行 ID 保存。当从服务器断线并重新连上一个主服务器时，会向当前连接的主服务器发送之前保存的运行 ID： 如果运行 ID 和当前连接的主服务器的运行 ID 相同，说明从服务器断线之前复制的就是当前连接的这个主服务器，执行部分重同步 如果不同，需要执行完整重同步操作 PSYNCPSYNC 命令的调用方法有两种 如果从服务器之前没有复制过任何主服务器，或者执行了 SLAVEOF no one，开始一次新的复制时将向主服务器发送 PSYNC ? -1 命令，主动请求主服务器进行完整重同步 如果从服务器已经复制过某个主服务器，那么从服务器在开始一次新的复制时将向主服务器发送 PSYNC &lt;runid&gt; &lt;offset&gt; 命令，runid 是上一次复制的主服务器的运行 ID，offset 是复制的偏移量 接收到 PSYNC 命令的主服务器会向从服务器返回以下三种回复的其中一种： 执行完整重同步操作：返回 +FULLRESYNC &lt;runid&gt; &lt;offset&gt;，runid 是主服务器的运行 ID，offset 是主服务器的复制偏移量 执行部分重同步操作：返回 +CONTINUE，从服务器收到该回复说明只需要等待主服务器发送缺失的部分数据即可 主服务器的版本低于 Redis2.8：返回 -ERR，版本过低识别不了 PSYNC，从服务器将向主服务器发送 SYNC 命令 复制实现实现流程通过向从服务器发送 SLAVEOF 命令，可以让从服务器去复制一个主服务器 设置主服务器的地址和端口：将 SLAVEOF 命令指定的 ip 和 port 保存到服务器状态 redisServer 123456struct redisServer &#123;\t// 主服务器的地址 char *masterhost; //主服务器的端口 int masterport; &#125;; SLAVEOF 命令是一个异步命令，在完成属性的设置后服务器直接返回 OK，而实际的复制工作将在 OK 返回之后才真正开始执行 建立套接字连接： 从服务器 connect 主服务器建立套接字连接，成功后从服务器将为这个套接字关联一个用于复制工作的文件事件处理器，负责执行后续的复制工作，如接收 RDB 文件、接收主服务器传播来的写命令等 主服务器在接受 accept 从务器的套接字连接后，将为该套接字创建相应的客户端状态，将从服务器看作一个客户端，从服务器将同时具有 server 和 client（可以发命令）两个身份 发送 PING 命令：从服务器向主服务器发送一个 PING 命令，检查主从之间的通信是否正常、主服务器处理命令的能力是否正常 返回错误，表示主服务器无法处理从服务器的命令请求（忙碌），从服务器断开并重新创建连向主服务器的套接字 返回命令回复，但从服务器不能在规定的时间内读取出命令回复的内容，表示主从之间的网络状态不佳，需要断开重连 读取到 PONG，表示一切状态正常，可以执行复制 身份验证：如果从服务器设置了 masterauth 选项就进行身份验证，将向主服务器发送一条 AUTH 命令，命令参数为从服务器 masterauth 选项的值，如果主从设置的密码不相同，那么主将返回一个 invalid password 错误 发送端口信息：身份验证后 从服务器执行命令 REPLCONF listening-port &lt;port­number&gt;， 向主服务器发送从服务器的监听端口号 主服务器在接收到这个命令后，会将端口号记录在对应的客户端状态 redisClient.slave_listening_port 属性中： 同步：从服务器将向主服务器发送 PSYNC 命令，在同步操作执行之后，主从服务器双方都是对方的客户端，可以相互发送命令 完整重同步：主服务器需要成为从服务器的客户端，才能将保存在缓冲区里面的写命令发送给从服务器执行 部分重同步：主服务器需要成为从服务器的客户端，才能向从服务器发送保存在复制积压缓冲区里面的写命令 命令传播：主服务器将写命令发送给从服务器，保持数据库的状态一致 复制图示 心跳检测心跳机制心跳机制：进入命令传播阶段，从服务器默认会以每秒一次的频率，向主服务器发送命令：REPLCONF ACK &lt;replication_offset&gt;，replication_offset 是从服务器当前的复制偏移量 心跳的作用： 检测主从服务器的网络连接状态 辅助实现 min-slaves 选项 检测命令丢失 网络状态如果主服务器超过一秒钟没有收到从服务器发来的 REPLCONF ACK 命令，主服务就认为主从服务器之间的连接出现问题 向主服务器发送 INFO replication 命令，lag 一栏表示从服务器最后一次向主服务器发送 ACK 命令距离现在多少秒： 123456127.0.0.1:6379&gt; INFO replication # Replication role:master connected_slaves:2 slave0: ip=127.0.0.1,port=11111,state=online,offset=123,lag=0 # 刚刚发送过 REPLCONF ACK slavel: ip=127.0.0.1,port=22222,state=online,offset=456,lag=3 # 3秒之前发送过REPLCONF ACK 在一般情况下，lag 的值应该在 0 或者 1 秒之间跳动，如果超过 1 秒说明主从服务器之间的连接出现了故障 配置选项Redis 的 min-slaves-to-write 和 min-slaves-max-lag 两个选项可以防止主服务器在不安全的情况下拒绝执行写命令 比如向主服务器设置： min-slaves-to-write：主库最少有 N 个健康的从库存活才能执行写命令，没有足够的从库直接拒绝写入 min-slaves-max-lag：从库和主库进行数据复制时的 ACK 消息延迟的最大时间 12min-slaves-to-write 5min-slaves-max-lag 10 那么在从服务器的数少于 5 个，或者 5 个从服务器的延迟（lag）值都大于或等于10 秒时，主服务器将拒绝执行写命令 命令丢失检测命令丢失：由于网络或者其他原因，主服务器传播给从服务器的写命令丢失，那么当从服务器向主服务器发送 REPLCONF ACK 命令时，主服务器会检查从服务器的复制偏移量是否小于自己的，然后在复制积压缓冲区里找到从服务器缺少的数据，并将这些数据重新发送给从服务器 说明：REPLCONF ACK 命令和复制积压缓冲区都是 Redis 2.8 版本新增的，在 Redis 2.8 版本以前，即使命令在传播过程中丢失，主从服务器都不会注意到，也不会向从服务器补发丢失的数据，所以为了保证主从复制的数据一致性，最好使用 2.8 或以上版本的 Redis 常见问题重启恢复系统不断运行，master 的数据量会越来越大，一旦 master 重启，runid 将发生变化，会导致全部 slave 的全量复制操作 解决方法：本机保存上次 runid，重启后恢复该值，使所有 slave 认为还是之前的 master 优化方案： master 内部创建 master_replid 变量，使用 runid 相同的策略生成，并发送给所有 slave 在 master 关闭时执行命令 shutdown save，进行 RDB 持久化，将 runid 与 offset 保存到 RDB 文件中 redis-check-rdb dump.rdb 命令可以查看该信息，保存为 repl-id 和 repl-offset master 重启后加载 RDB 文件，恢复数据，将 RDB 文件中保存的 repl-id 与 repl-offset 加载到内存中，master_repl_id &#x3D; repl-id，master_repl_offset &#x3D; repl-offset 通过 info 命令可以查看该信息 网络中断master 的 CPU 占用过高或 slave 频繁断开连接 出现的原因： slave 每 1 秒发送 REPLCONF ACK 命令到 master 当 slave 接到了慢查询时（keys * ，hgetall 等），会大量占用 CPU 性能 master 每 1 秒调用复制定时函数 replicationCron()，比对 slave 发现长时间没有进行响应 最终导致 master 各种资源（输出缓冲区、带宽、连接等）被严重占用 解决方法：通过设置合理的超时时间，确认是否释放 slave 1repl-timeout\t# 该参数定义了超时时间的阈值（默认60秒），超过该值，释放slave slave 与 master 连接断开 出现的原因： master 发送 ping 指令频度较低 master 设定超时时间较短 ping 指令在网络中存在丢包 解决方法：提高 ping 指令发送的频度 1repl-ping-slave-period 超时时间 repl-time 的时间至少是 ping 指令频度的5到10倍，否则 slave 很容易判定超时 一致性网络信息不同步，数据发送有延迟，导致多个 slave 获取相同数据不同步 解决方案： 优化主从间的网络环境，通常放置在同一个机房部署，如使用阿里云等云服务器时要注意此现象 监控主从节点延迟（通过offset）判断，如果 slave 延迟过大，暂时屏蔽程序对该 slave 的数据访问 1slave-serve-stale-data yes|no 开启后仅响应 info、slaveof 等少数命令（慎用，除非对数据一致性要求很高） 多个 slave 同时对 master 请求数据同步，master 发送的 RDB 文件增多，会对带宽造成巨大冲击，造成 master 带宽不足，因此数据同步需要根据业务需求，适量错峰 哨兵模式哨兵概述Sentinel（哨兵）是 Redis 的高可用性（high availability）解决方案，由一个或多个 Sentinel 实例 instance 组成的 Sentinel 系统可以监视任意多个主服务器，以及这些主服务器的所有从服务器，并在被监视的主服务器下线时进行故障转移 双环图案表示主服务器 单环图案表示三个从服务器 哨兵的作用： 监控：监控 master 和 slave，不断的检查 master 和 slave 是否正常运行，master 存活检测、master 与 slave 运行情况检测 通知：当被监控的服务器出现问题时，向其他哨兵发送通知 自动故障转移：断开 master 与 slave 连接，选取一个 slave 作为 master，将其他 slave 连接新的 master，并告知客户端新的服务器地址 启用哨兵配置方式配置三个哨兵 sentinel.conf：一般多个哨兵配置相同、端口不同，特殊需求可以配置不同的属性 1234567port 26401dir &quot;/redis/data&quot;sentinel monitor mymaster 127.0.0.1 6401 2sentinel down-after-milliseconds mymaster 5000sentinel failover-timeout mymaster 20000sentinel parallel-sync mymaster 1sentinel deny-scripts-reconfig yes 配置说明： 设置哨兵监听的主服务器信息，判断主观下线所需要的票数 1sentinel monitor &lt;master-name&gt; &lt;master_ip&gt; &lt;master_port&gt; &lt;quorum&gt; 指定哨兵在监控 Redis 服务时，设置判定服务器宕机的时长，该设置控制是否进行主从切换 1sentinel down-after-milliseconds &lt;master-name&gt; &lt;million_seconds&gt; 出现故障后，故障切换的最大超时时间，超过该值，认定切换失败，默认 3 分钟 1sentinel failover-timeout &lt;master_name&gt; &lt;million_seconds&gt; 故障转移时，同时进行主从同步的 slave 数量，数值越大，要求网络资源越高 1sentinel parallel-syncs &lt;master_name&gt; &lt;sync_slave_number&gt; 启动哨兵：服务端命令（Linux 命令） 1redis-sentinel filename 初始化Sentinel 本质上只是一个运行在特殊模式下的 Redis 服务器，当一个 Sentinel 启动时，首先初始化 Redis 服务器，但是初始化过程和普通 Redis 服务器的初始化过程并不完全相同，哨兵不提供数据相关服务，所以不会载入 RDB、AOF 文件 整体流程： 初始化服务器 将普通 Redis 服务器使用的代码替换成 Sentinel 专用代码 初始化 Sentinel 状态 根据给定的配置文件，初始化 Sentinel 的监视主服务器列表 创建连向主服务器的网络连接 代码替换将一部分普通 Redis 服务器使用的代码替换成 Sentinel 专用代码 Redis 服务器端口： 12# define REDIS_SERVERPORT 6379 // 普通服务器端口# define REDIS_SENTINEL_PORT 26379 // 哨兵端口 服务器的命令表： 12345678910111213// 普通 Redis 服务器struct redisCommand redisCommandTable[] = &#123; &#123;&quot;get&quot;, getCommand, 2, &quot;r&quot;, 0, NULL, 1, 1, 1, 0, 0&#125;, &#123;&quot;set&quot;, setCommand, -3, &quot;wm&quot;, 0, noPreloadGetKeys, 1, 1, 1, 0, 0&#125;, //....&#125;// 哨兵struct redisCommand sentinelcmds[] = &#123; &#123;&quot;ping&quot;, pingCommand, 1, &quot;&quot;, 0, NULL, 0, 0, 0, 0, 0&#125;, &#123;&quot;sentinel&quot;, sentinelCommand, -2,&quot;&quot;,0,NULL,0,0,0,0,0&#125;, &#123;&quot;subscribe&quot;,...&#125;, &#123;&quot;unsubscribe&quot;,...O&#125;, &#123;&quot;psubscribe&quot;,...&#125;, &#123;&quot;punsubscribe&quot;,...&#125;, &#123;&quot;info&quot;,...&#125;&#125;; 上述表是哨兵模式下客户端可以执行的命令，所以对于 GET、SET 等命令，服务器根本就没有载入 哨兵状态服务器会初始化一个 sentinelState 结构，又叫 Sentinel 状态，结构保存了服务器中所有和 Sentinel 功能有关的状态（服务器的一般状态仍然由 redisServer 结构保存） 123456789101112131415161718192021struct sentinelState &#123; // 当前纪元，用于实现故障转移 uint64_t current_epoch; // 【保存了所有被这个sentinel监视的主服务器】 dict *masters; // 是否进入了 TILT 模式 int tilt; // 进入 TILT 模式的时间 mstime_t tilt_start_time; // 最后一次执行时间处理的事件 mstime_t previous_time; // 目前正在执行的脚本数量 int running_scripts; // 一个FIFO队列，包含了所有需要执行的用户脚本 list *scripts_queue; &#125; sentinel; 监控列表Sentinel 状态的初始化将 masters 字典的初始化，根据被载入的 Sentinel 配置文件 conf 来进行属性赋值 Sentinel 状态中的 masters 字典记录了所有被 Sentinel 监视的主服务器的相关信息，字典的键是被监视主服务器的名字，值是主服务器对应的实例结构 实例结构是一个 sentinelRedisinstance 数据类型，代表被 Sentinel 监视的实例，这个实例可以是主、从服务器，或者其他 Sentinel 1234567891011121314151617181920212223242526272829303132333435typedef struct sentinelRedisinstance &#123; // 标识值，记录了实例的类型，以及该实例的当前状态 int flags; // 实例的名字，主服务器的名字由用户在配置文件中设置， // 从服务器和哨兵的名字由 Sentinel 自动设置，格式为 ip:port，例如 127.0.0.1:6379 char *name; // 实例运行的 ID char *runid; // 配置纪元，用于实现故障转移 uint64_t config_epoch; // 实例地址 sentinelAddr *addr; // 如果当前实例时主服务器，该字段保存从服务器信息，键是名字格式为 ip:port，值是实例结构 dict *slaves; // 所有监视当前服务器的 Sentinel 实例，键是名字格式为 ip:port，值是实例结构 dict *sentinels; // sentinel down-after-milliseconds 的值，表示实例无响应多少毫秒后会被判断为主观下线(subjectively down) mstime_t down_after_period; // sentinel monitor 选项中的quorum参数，判断这个实例为客观下线(objectively down)所需的支持投票数量 int quorum; // sentinel parallel-syncs 的值，在执行故障转移操作时，可以同时对新的主服务器进行同步的从服务器数量 int parallel-syncs; // sentinel failover-timeout的值，刷新故障迁移状态的最大时限 mstime_t failover_timeout;&#125; addr 属性是一个指向 sentinelAddr 的指针： 1234typedef struct sentinelAddr &#123; char *ip; int port;&#125; 网络连接初始化 Sentinel 的最后一步是创建连向被监视主服务器的网络连接，Sentinel 将成为主服务器的客户端，可以向主服务器发送命令，并从命令回复中获取相关的信息 每个被 Sentinel 监视的主服务器，Sentinel 会创建两个连向主服务器的异步网络连接： 命令连接：用于向主服务器发送命令，并接收命令回复 订阅连接：用于订阅主服务器的 _sentinel_:hello 频道 建立两个连接的原因： 在 Redis 目前的发布与订阅功能中，被发送的信息都不会保存在 Redis 服务器里， 如果在信息发送时接收信息的客户端离线或断线，那么这个客户端就会丢失这条信息，为了不丢失 hello 频道的任何信息，Sentinel 必须用一个订阅连接来接收该频道的信息 Sentinel 还必须向主服务器发送命令，以此来与主服务器进行通信，所以 Sentinel 还必须向主服务器创建命令连接 说明：断线的意思就是网络连接断开 信息交互获取信息主服务器Sentinel 默认会以每十秒一次的频率，通过命令连接向被监视的主服务器发送 INFO 命令，来获取主服务器的信息 一部分是主服务器本身的信息，包括 runid 域记录的服务器运行 ID，以及 role 域记录的服务器角色 另一部分是服务器属下所有从服务器的信息，每个从服务器都由一个 slave 字符串开头的行记录，根据这些 IP 地址和端口号，Sentinel 无须用户提供从服务器的地址信息，就可以自动发现从服务器 123456789# Server run_id:76llc59dc3a29aa6fa0609f84lbb6al019008a9c...# Replication role:master ...slave0: ip=l27.0.0.1, port=11111, state=online, offset=22, lag=0slave1: ip=l27.0.0.1, port=22222, state=online, offset=22, lag=0... 根据 run_id 和 role 记录的信息 Sentinel 将对主服务器的实例结构进行更新，比如主服务器重启之后，运行 ID 就会和实例结构之前保存的运行 ID 不同，哨兵检测到这一情况之后就会对实例结构的运行 ID 进行更新 对于主服务器返回的从服务器信息，用实例结构的 slaves 字典记录了从服务器的信息： 如果从服务器对应的实例结构已经存在，那么 Sentinel 对从服务器的实例结构进行更新 如果不存在，为这个从服务器新创建一个实例结构加入字典，字典键为 ip:port 从服务器当 Sentinel 发现主服务器有新的从服务器出现时，会为这个新的从服务器创建相应的实例结构，还会创建到从服务器的命令连接和订阅连接，所以 Sentinel 对所有的从服务器之间都可以进行命令操作 Sentinel 默认会以每十秒一次的频率，向从服务器发送 INFO 命令： 123456789101112# Server run_id:76llc59dc3a29aa6fa0609f84lbb6al019008a9c\t#从服务器的运行 id...# Replication role:slave # 从服务器角色...master_host:127.0.0.1 # 主服务器的 ipmaster_port:6379 # 主服务器的 portmaster_link_status:up # 主从服务器的连接状态slave_repl_offset:11111\t# 从服务器的复制偏移蜇slave_priority:100 # 从服务器的优先级... 优先级属性在故障转移时会用到 根据这些信息，Sentinel 会对从服务器的实例结构进行更新 发送信息Sentinel 在默认情况下，会以每两秒一次的频率，通过命令连接向所有被监视的主服务器和从服务器发送以下格式的命令： 1PUBLISH _sentinel_:hello &quot;&lt;s_ip&gt;, &lt;s_port&gt;, &lt;s_runid&gt;, &lt;s_epoch&gt;, &lt;m_name&gt;, &lt;m_ip&gt;, &lt;m_port&gt;, &lt;m_epoch&gt; 这条命令向服务器的 _sentinel_:hello 频道发送了一条信息，信息的内容由多个参数组成： 以 s_ 开头的参数记录的是 Sentinel 本身的信息 以 m_ 开头的参数记录的则是主服务器的信息 说明：通过命令连接发送的频道信息 接受信息订阅频道Sentinel 与一个主或从服务器建立起订阅连接之后，就会通过订阅连接向服务器发送订阅命令，频道的订阅会一直持续到 Sentinel 与服务器的连接断开为止 1SUBSCRIBE _sentinel_:hello 订阅成功后，Sentinel 就可以通过订阅连接从服务器的 _sentinel_:hello 频道接收信息，对消息分析： 如果信息中记录的 Sentinel 运行 ID 与自己的相同，不做进一步处理 如果不同，将根据信息中的各个参数，对相应主服务器的实例结构进行更新 Sentinel 为主服务器创建的实例结构的 sentinels 字典保存所有同样监视这个主服务器的 Sentinel 信息（包括 Sentinel 自己），字典的键是 Sentinel 的名字，格式为 ip:port，值是键所对应 Sentinel 的实例结构 监视同一个服务器的 Sentinel 订阅的频道相同，Sentinel 发送的信息会被其他 Sentinel 接收到（发送信息的为源 Sentinel，接收信息的为目标 Sentinel），目标 Sentinel 在自己的 sentinelState.masters 中查找源 Sentinel 服务器的实例结构进行添加或更新 因为 Sentinel 可以接收到的频道信息来感知其他 Sentinel 的存在，并通过发送频道信息来让其他 Sentinel 知道自己的存在，所以用户在使用 Sentinel 时并不需要提供各个 Sentinel 的地址信息，监视同一个主服务器的多个 Sentinel 可以相互发现对方 哨兵实例之间可以相互发现，要归功于 Redis 提供发布订阅机制 命令连接Sentinel 通过频道信息发现新的 Sentinel，除了创建实例结构，还会创建一个连向新 Sentinel 的命令连接，而新 Sentinel 也同样会创建连向这个 Sentinel 的命令连接，最终监视同一主服务器的多个 Sentinel 将形成相互连接的网络 作用：通过命令连接相连的各个 Sentinel 可以向其他 Sentinel 发送命令请求来进行信息交换 Sentinel 之间不会创建订阅连接： Sentinel 需要通过接收主服务器或者从服务器发来的频道信息来发现未知的新 Sentinel，所以才创建订阅连接 相互已知的 Sentinel 只要使用命令连接来进行通信就足够了 下线检测主观下线Sentinel 在默认情况下会以每秒一次的频率向所有与它创建了命令连接的实例（包括主从服务器、其他 Sentinel）发送 PING 命令，通过实例返回的 PING 命令回复来判断实例是否在线 有效回复：实例返回 +PONG、-LOADING、-MASTERDOWN 三种回复的其中一种 无效回复：实例返回除上述三种以外的任何数据 Sentinel 配置文件中 down-after-milliseconds 选项指定了判断实例进入主观下线所需的时长，如果主服务器在该时间内一直向 Sentinel 返回无效回复，Sentinel 就会在该服务器对应实例结构的 flags 属性打开 SRI_S_DOWN 标识，表示该主服务器进入主观下线状态 配置的 down-after-milliseconds 值不仅适用于主服务器，还会被用于当前 Sentinel 判断主服务器属下的所有从服务器，以及所有同样监视这个主服务器的其他 Sentinel 的主观下线状态 注意：对于监视同一个主服务器的多个 Sentinel 来说，设置的 down-after-milliseconds 选项的值可能不同，所以当一个 Sentinel 将主服务器判断为主观下线时，其他 Sentinel 可能仍然会认为主服务器处于在线状态 客观下线当 Sentinel 将一个主服务器判断为主观下线之后，会向同样监视这一主服务器的其他 Sentinel 进行询问 Sentinel 使用命令询问其他 Sentinel 是否同意主服务器已下线： 1SENTINEL is-master-down-by-addr &lt;ip&gt; &lt;port&gt; &lt;current_epoch&gt; &lt;runid&gt; ip：被 Sentinel 判断为主观下线的主服务器的 IP 地址 port：被 Sentinel 判断为主观下线的主服务器的端口号 current_epoch：Sentinel 当前的配置纪元，用于选举领头 Sentinel runid：取值为 * 符号代表命令仅仅用于检测主服务器的客观下线状态；取值为 Sentinel 的运行 ID 则用于选举领头 Sentinel 目标 Sentinel 接收到源 Sentinel 的命令时，会根据参数的 lP 和端口号，检查主服务器是否已下线，然后返回一条包含三个参数的 Multi Bulk 回复： down_state：返回目标 Sentinel 对服务器的检查结果，1 代表主服务器已下线，0 代表未下线 leader_runid：取值为 * 符号代表命令仅用于检测服务器的下线状态；而局部领头 Sentinel 的运行 ID 则用于选举领头 Sentinel leader_epoch：目标 Sentinel 的局部领头 Sentinel 的配置纪元 源 Sentinel 将统计其他 Sentinel 同意主服务器已下线的数量，当这一数量达到配置指定的判断客观下线所需的数量（quorum）时，Sentinel 会将主服务器对应实例结构 flags 属性的 SRI_O_DOWN 标识打开，代表客观下线，并对主服务器执行故障转移操作 注意：不同 Sentinel 判断客观下线的条件可能不同，因为载入的配置文件中的属性 quorum 可能不同 领头选举主服务器被判断为客观下线时，监视该主服务器的各个 Sentinel 会进行协商，选举出一个领头 Sentinel 对下线服务器执行故障转移 Redis 选举领头 Sentinel 的规则： 所有在线的 Sentinel 都有被选为领头 Sentinel 的资格 每个发现主服务器进入客观下线的 Sentinel 都会要求其他 Sentinel 将自己设置为局部领头 Sentinel 在一个配置纪元里，所有 Sentinel 都只有一次将某个 Sentinel 设置为局部领头 Sentinel 的机会，并且局部领头一旦设置，在这个配置纪元里就不能再更改 Sentinel 设置局部领头 Sentinel 的规则是先到先得，最先向目标 Sentinel 发送设置要求的源 Sentinel 将成为目标 Sentinel 的局部领头 Sentinel，之后接收到的所有设置要求都会被目标 Sentinel 拒绝 领头 Sentinel 的产生需要半数以上 Sentinel 的支持，并且每个 Sentinel 只有一票，所以一个配置纪元只会出现一个领头 Sentinel，比如 10 个 Sentinel 的系统中，至少需要 10/2 + 1 = 6 票 选举过程： 一个 Sentinel 向目标 Sentinel 发送 SENTINEL is-master-down-by-addr 命令，命令中的 runid 参数不是＊符号而是源 Sentinel 的运行 ID，表示源 Sentinel 要求目标 Sentinel 将自己设置为它的局部领头 Sentinel 目标 Sentinel 接受命令处理完成后，将返回一条命令回复，回复中的 leader_runid 和 leader_epoch 参数分别记录了目标 Sentinel 的局部领头 Sentinel 的运行 ID 和配置纪元 源 Sentinel 接收目标 Sentinel 命令回复之后，会判断 leader_epoch 是否和自己的相同，相同就继续判断 leader_runid 是否和自己的运行 ID 一致，成立表示目标 Sentinel 将源 Sentinel 设置成了局部领头 Sentinel，即获得一票 如果某个 Sentinel 被半数以上的 Sentinel 设置成了局部领头 Sentinel，那么这个 Sentinel 成为领头 Sentinel 如果在给定时限内，没有一个 Sentinel 被选举为领头 Sentinel，那么各个 Sentinel 将在一段时间后再次选举，直到选出领头 每次进行领头 Sentinel 选举之后，不论选举是否成功，所有 Sentinel 的配置纪元（configuration epoch）都要自增一次 Sentinel 集群至少 3 个节点的原因： 如果 Sentinel 集群只有 2 个 Sentinel 节点，则领头选举需要 2/2 + 1 = 2 票，如果一个节点挂了，那就永远选不出领头 Sentinel 集群允许 1 个 Sentinel 节点故障则需要 3 个节点的集群，允许 2 个节点故障则需要 5 个节点集群 如何获取哨兵节点的半数数量？ 客观下线是通过配置文件获取的数量，达到 quorum 就客观下线 哨兵数量是通过主节点是实例结构中，保存着监视该主节点的所有哨兵信息，从而获取得到 故障转移执行流程领头 Sentinel 将对已下线的主服务器执行故障转移操作，该操作包含以下三个步骤 从下线主服务器属下的所有从服务器里面，挑选出一个从服务器，执行 SLAVEOF no one，将从服务器升级为主服务器 在发送 SLAVEOF no one 命令后，领头 Sentinel 会以每秒一次的频率（一般是 10s&#x2F;次）向被升级的从服务器发送 INFO 命令，观察命令回复中的角色信息，当被升级服务器的 role 从 slave 变为 master 时，说明从服务器已经顺利升级为主服务器 将已下线的主服务器的所有从服务器改为复制新的主服务器，通过向从服务器发送 SLAVEOF 命令实现 将已经下线的主服务器设置为新的主服务器的从服务器，设置是保存在服务器对应的实例结构中，当旧的主服务器重新上线时，Sentinel 就会向它发送 SLAVEOF 命令，成为新的主服务器的从服务器 示例：sever1 是主，sever2、sever3、sever4 是从服务器，sever1 故障后选中 sever2 升级 选择算法领头 Sentinel 会将已下线主服务器的所有从服务器保存到一个列表里，然后按照以下规则对列表进行过滤，最后挑选出一个状态良好、数据完整的从服务器 删除列表中所有处于下线或者断线状态的从服务器，保证列表中的从服务器都是正常在线的 删除列表中所有最近五秒内没有回复过领头 Sentinel 的 INFO 命令的从服务器，保证列表中的从服务器最近成功进行过通信 删除所有与已下线主服务器连接断开超过 down-after-milliseconds * 10 毫秒的从服务器，保证列表中剩余的从服务器都没有过早地与主服务器断开连接，保存的数据都是比较新的 down-after-milliseconds 时间用来判断是否主观下线，其余的时间完全可以完成客观下线和领头选举 根据从服务器的优先级，对列表中剩余的从服务器进行排序，并选出其中优先级最高的从服务器 如果有多个具有相同最高优先级的从服务器，领头 Sentinel 将对这些相同优先级的服务器按照复制偏移量进行排序，选出其中偏移量最大的从服务器，也就是保存着最新数据的从服务器 如果还没选出来，就按照运行 ID 对这些从服务器进行排序，并选出其中运行 ID 最小的从服务器 集群模式集群节点节点概述Redis 集群是 Redis 提供的分布式数据库方案，集群通过分片（sharding）来进行数据共享， 并提供复制和故障转移功能，一个 Redis 集群通常由多个节点（node）组成，将各个独立的节点连接起来，构成一个包含多节点的集群 一个节点就是一个运行在集群模式下的 Redis 服务器，Redis 在启动时会根据配置文件中的 cluster-enabled 配置选项是否为 yes 来决定是否开启服务器的集群模式 节点会继续使用所有在单机模式中使用的服务器组件，使用 redisServer 结构来保存服务器的状态，使用 redisClient 结构来保存客户端的状态，也有集群特有的数据结构 数据结构每个节点都保存着一个集群状态 clusterState 结构，这个结构记录了在当前节点的视角下，集群目前所处的状态 1234567891011121314151617typedef struct clusterState &#123; // 指向当前节点的指针\tclusterNode *myself; // 集群当前的配置纪元，用于实现故障转移\tuint64_t currentEpoch; // 集群当前的状态，是在线还是下线\tint state; // 集群中至少处理着一个槽的（主）节点的数量，为0表示集群目前没有任何节点在处理槽 // 【选举时投票数量超过半数，从这里获取的】\tint size; // 集群节点名单（包括 myself 节点），字典的键为节点的名字，字典的值为节点对应的clusterNode结构 dict *nodes;&#125; 每个节点都会使用 clusterNode 结构记录当前状态，并为集群中的所有其他节点（包括主节点和从节点）都创建一个相应的 clusterNode 结构，以此来记录其他节点的状态 12345678910111213141516171819202122struct clusterNode &#123; // 创建节点的时间 mstime_t ctime; // 节点的名字，由 40 个十六进制字符组成 char name[REDIS_CLUSTER_NAMELEN]; // 节点标识，使用各种不同的标识值记录节点的角色（比如主节点或者从节点）以及节点目前所处的状态（比如在线或者下线） int flags; // 节点当前的配置纪元，用于实现故障转移 uint64_t configEpoch; // 节点的IP地址 char ip[REDIS_IP_STR_LEN]; // 节点的端口号 int port; // 保存连接节点所需的有关信息 clusterLink *link;&#125; clusterNode 结构的 link 属性是一个 clusterLink 结构，该结构保存了连接节点所需的有关信息 12345678910111213141516typedef struct clusterLink &#123; // 连接的创建时间 mstime_t ctime; // TCP套接字描述符\tint fd; // 输出缓冲区，保存着等待发送给其他节点的消息(message)。 sds sndbuf; // 输入缓冲区，保存着从其他节点接收到的消息。\tsds rcvbuf; // 与这个连接相关联的节点，如果没有的话就为NULL\tstruct clusterNode *node; &#125; redisClient 结构中的套接宇和缓冲区是用于连接客户端的 clusterLink 结构中的套接宇和缓冲区则是用于连接节点的 MEETCLUSTER MEET 命令用来将 ip 和 port 所指定的节点添加到接受命令的节点所在的集群中 1CLUSTER MEET &lt;ip&gt; &lt;port&gt; 假设向节点 A 发送 CLUSTER MEET 命令，让节点 A 将另一个节点 B 添加到节点 A 当前所在的集群里，收到命令的节点 A 将与根据 ip 和 port 向节点 B 进行握手（handshake）： 节点 A 会为节点 B 创建一个 clusterNode 结构，并将该结构添加到自己的 clusterState.nodes 字典里，然后节点 A 向节点 B 发送 MEET 消息（message） 节点 B 收到 MEET 消息后，节点 B 会为节点 A 创建一个 clusterNode 结构，并将该结构添加到自己的 clusterState.nodes 字典里，之后节点 B 将向节点 A 返回一条 PONG 消息 节点 A 收到 PONG 消息后，代表节点 A 可以知道节点 B 已经成功地接收到了自已发送的 MEET 消息，此时节点 A 将向节点 B 返回一条 PING 消息 节点 B 收到 PING 消息后， 代表节点 B 可以知道节点 A 已经成功地接收到了自己返回的 PONG 消息，握手完成 节点 A 会将节点 B 的信息通过 Gossip 协议传播给集群中的其他节点，让其他节点也与节点 B 进行握手，最终经过一段时间之后，节点 B 会被集群中的所有节点认识 槽指派基本操作Redis 集群通过分片的方式来保存数据库中的键值对，集群的整个数据库被分为 16384 个槽（slot），数据库中的每个键都属于 16384 个槽中的一个，集群中的每个节点可以处理 0 个或最多 16384 个槽（每个主节点存储的数据并不一样） 当数据库中的 16384 个槽都有节点在处理时，集群处于上线状态（ok） 如果数据库中有任何一个槽得到处理，那么集群处于下线状态（fail） 通过向节点发送 CLUSTER ADDSLOTS 命令，可以将一个或多个槽指派（assign）给节点负责 1CLUSTER ADDSLOTS &lt;slot&gt; [slot ... ] 12127.0.0.1:7000&gt; CLUSTER ADDSLOTS 0 1 2 3 4 ... 5000 # 将槽0至槽5000指派给节点7000负责OK 命令执行细节： 如果命令参数中有一个槽已经被指派给了某个节点，那么会向客户端返回错误，并终止命令执行 将 slots 数组中的索引 i 上的二进制位设置为 1，就代表指派成功 节点指派clusterNode 结构的 slots 属性和 numslot 属性记录了节点负责处理哪些槽： 123456struct clusterNode &#123; // 处理信息，一字节等于 8 位 unsigned char slots[l6384/8]; // 记录节点负责处理的槽的数量，就是 slots 数组中值为 1 的二进制位数量 int numslots;&#125; slots 是一个二进制位数组（bit array），长度为 16384/8 = 2048 个字节，包含 16384 个二进制位，Redis 以 0 为起始索引，16383 为终止索引，对 slots 数组的 16384 个二进制位进行编号，并根据索引 i 上的二进制位的值来判断节点是否负责处理槽 i： 在索引 i 上的二进制位的值为 1，那么表示节点负责处理槽 i 在索引 i 上的二进制位的值为 0，那么表示节点不负责处理槽 i 取出和设置 slots 数组中的任意一个二进制位的值的**复杂度仅为 O(1)**，所以对于一个给定节点的 slots 数组来说，检查节点是否负责处理某个槽或者将某个槽指派给节点负责，这两个动作的复杂度都是 O(1) 传播节点的槽指派信息：一个节点除了会将自己负责处理的槽记录在 clusterNode 中，还会将自己的 slots 数组通过消息发送给集群中的其他节点，每个接收到 slots 数组的节点都会将数组保存到相应节点的 clusterNode 结构里面，因此集群中的每个节点都会知道数据库中的 16384 个槽分别被指派给了集群中的哪些节点 集群指派集群状态 clusterState 结构中的 slots 数组记录了集群中所有 16384 个槽的指派信息，数组每一项都是一个指向 clusterNode 的指针 1234typedef struct clusterState &#123; // ... clusterNode *slots[16384];&#125; 如果 slots[i] 指针指向 NULL，那么表示槽 i 尚未指派给任何节点 如果 slots[i] 指针指向一个 clusterNode 结构，那么表示槽 i 已经指派给该节点所代表的节点 通过该节点，程序检查槽 i 是否已经被指派或者取得负责处理槽 i 的节点，只需要访问 clusterState. slots[i] 即可，时间复杂度仅为 O(1) 集群数据集群节点保存键值对以及键值对过期时间的方式，与单机 Redis 服务器保存键值对以及键值对过期时间的方式完全相同，但是集群节点只能使用 0 号数据库，单机服务器可以任意使用 除了将键值对保存在数据库里面之外，节点还会用 clusterState 结构中的 slots_to_keys 跳跃表来保存槽和键之间的关系 1234typedef struct clusterState &#123; // ... zskiplist *slots_to_keys;&#125; slots_to_keys 跳跃表每个节点的分值（score）都是一个槽号，而每个节点的成员（member）都是一个数据库键（按槽号升序） 当节点往数据库中添加一个新的键值对时，节点就会将这个键以及键的槽号关联到 slots_to_keys 跳跃表 当节点删除数据库中的某个键值对时，节点就会在 slots_to_keys 跳跃表解除被删除键与槽号的关联 通过在 slots_to_keys 跳跃表中记录各个数据库键所属的槽，可以很方便地对属于某个或某些槽的所有数据库键进行批量操作，比如 CLUSTER GETKEYSINSLOT &lt;slot&gt; &lt;count&gt; 命令返回最多 count 个属于槽 slot 的数据库键，就是通过该跳表实现 集群命令执行命令集群处于上线状态，客户端就可以向集群中的节点发送命令（16384 个槽全部指派就进入上线状态） 当客户端向节点发送与数据库键有关的命令时，接收命令的节点会计算出命令该键属于哪个槽，并检查这个槽是否指派给了自己 如果键所在的槽正好就指派给了当前节点，那么节点直接执行这个命令 反之，节点会向客户端返回一个 MOVED 错误，指引客户端转向（redirect）至正确的节点，再次发送该命令 计算键归属哪个槽的寻址算法： 12def slot_number(key): // CRC16(key) 语句计算键 key 的 CRC-16 校验和\treturn CRC16(key) &amp; 16383;\t// 取模，十进制对16384的取余 使用 CLUSTER KEYSLOT &lt;key&gt; 命令可以查看一个给定键属于哪个槽，底层实现： 12345def CLUSTER_KEYSLOT(key):\t// 计算槽号\tslot = slot_number(key);\t// 将槽号返回给客户端\treply_client(slot); 判断槽是否由当前节点负责处理：如果 clusterState.slots[i] 不等于 clusterState.myself，那么说明槽 i 并非由当前节点负责，节点会根据 clusterState.slots[i] 指向的 clusterNode 结构所记录的节点 IP 和端口号，向客户端返回 MOVED 错误 MOVEDMOVED 错误的格式为： 1MOVED &lt;slot&gt; &lt;ip&gt;:&lt;port＞ 参数 slot 为键所在的槽，ip 和 port 是负责处理槽 slot 的节点的 ip 地址和端口号 1MOVED 12345 127.0.0.1:6380 # 表示槽 12345 正由 IP地址为 127.0.0.1, 端口号为 6380 的节点负责 当客户端接收到节点返回的 MOVED 错误时，客户端会根据 MOVED 错误中提供的 IP 地址和端口号，转至负责处理槽 slot 的节点重新发送执行的命令 一个集群客户端通常会与集群中的多个节点创建套接字连接，节点转向实际上就是换一个套接字来发送命令 如果客户端尚未与转向的节点创建套接字连接，那么客户端会先根据 IP 地址和端口号来连接节点，然后再进行转向 集群模式的 redis-cli 在接收到 MOVED 错误时，并不会打印出 MOVED 错误，而是根据错误自动进行节点转向，并打印出转向信息： 123456$ redis-cli -c -p 6379 #集群模式127.0.0.1:6379&gt; SET msg &quot;happy&quot; -&gt; Redirected to slot [6257] located at 127.0.0.1:6380OK 127.0.0.1:6379&gt; 使用单机（stand alone）模式的 redis-cli 会打印错误，因为单机模式客户端不清楚 MOVED 错误的作用，不会进行自动转向： 12345$ redis-cli -c -p 6379 #集群模式127.0.0.1:6379&gt; SET msg &quot;happy&quot; (error) MOVED 6257 127.0.0.1:6380127.0.0.1:6379&gt; 重新分片实现原理Redis 集群的重新分片操作可以将任意数量已经指派给某个节点（源节点）的槽改为指派给另一个节点（目标节点），并且相关槽的键值对也会从源节点被移动到目标节点，该操作是可以在线（online）进行，在重新分片的过程中源节点和目标节点都可以处理命令请求 Redis 的集群管理软件 redis-trib 负责执行重新分片操作，redis-trib 通过向源节点和目标节点发送命令来进行重新分片操作 向目标节点发送 CLUSTER SETSLOT &lt;slot&gt; IMPORTING &lt;source_id&gt; 命令，准备好从源节点导入属于槽 slot 的键值对 向源节点发送 CLUSTER SETSLOT &lt;slot&gt; MIGRATING &lt;target_id&gt; 命令，让源节点准备好将属于槽 slot 的键值对迁移 redis-trib 向源节点发送 CLUSTER GETKEYSINSLOT &lt;slot&gt; &lt;count&gt; 命令，获得最多 count 个属于槽 slot 的键值对的键名 对于每个 key，redis-trib 都向源节点发送一个 MIGRATE &lt;target_ip&gt; &lt;target_port&gt; &lt;key_name&gt; 0 &lt;timeout＞ 命令，将被选中的键原子地从源节点迁移至目标节点 重复上述步骤，直到源节点保存的所有槽 slot 的键值对都被迁移至目标节点为止 redis-trib 向集群中的任意一个节点发送 CLUSTER SETSLOT &lt;slot&gt; NODE &lt;target _id&gt; 命令，将槽 slot 指派给目标节点，这一指派信息会通过消息传播至整个集群，最终集群中的所有节点都直到槽 slot 已经指派给了目标节点 如果重新分片涉及多个槽，那么 redis-trib 将对每个给定的槽分别执行上面给出的步骤 命令原理clusterState 结构的 importing_slots_from 数组记录了当前节点正在从其他节点导入的槽，migrating_slots_to 数组记录了当前节点正在迁移至其他节点的槽： 12345678typedef struct clusterState &#123; // 如果 importing_slots_from[i] 的值不为 NULL，而是指向一个 clusterNode 结构， // 那么表示当前节点正在从 clusterNode 所代表的节点导入槽 i clusterNode *importing_slots_from[16384]; // 表示当前节点正在将槽 i 迁移至 clusterNode 所代表的节点 clusterNode *migrating_slots_to[16384];&#125; CLUSTER SETSLOT &lt;slot&gt; IMPORTING &lt;source_id&gt; 命令：将目标节点 clusterState.importing_slots_from[slot] 的值设置为 source_id 所代表节点的 clusterNode 结构 CLUSTER SETSLOT &lt;slot&gt; MIGRATING &lt;target_id&gt; 命令：将源节点 clusterState.migrating_slots_to[slot] 的值设置为target_id 所代表节点的 clusterNode 结构 ASK 错误重新分片期间，源节点向目标节点迁移一个槽的过程中，可能出现被迁移槽的一部分键值对保存在源节点，另一部分保存在目标节点 客户端向源节点发送命令请求，并且命令要处理的数据库键属于被迁移的槽： 源节点会先在数据库里面查找指定键，如果找到的话，就直接执行客户端发送的命令 未找到会检查 clusterState.migrating_slots_to[slot]，看键 key 所属的槽 slot 是否正在进行迁移 槽 slot 正在迁移则源节点将向客户端返回一个 ASK 错误，指引客户端转向正在导入槽的目标节点 1ASK &lt;slot&gt; &lt;ip:port&gt; 接到 ASK 错误的客户端，会根据错误提供的 IP 地址和端口号转向目标节点，首先向目标节点发送一个 ASKING 命令，再重新发送原本想要执行的命令 和 MOVED 错误情况类似，集群模式的 redis-cli 在接到 ASK 错误时不会打印错误进行自动转向；单机模式的 redis-cli 会打印错误 对比 MOVED 错误： MOVED 错误代表槽的负责权已经从一个节点转移到了另一个节点，转向是一种持久性的转向 ASK 错误只是两个节点在迁移槽的过程中使用的一种临时措施，ASK 的转向不会对客户端今后发送关于槽 slot 的命令请求产生任何影响，客户端仍然会将槽 slot 的命令请求发送至目前负责处理槽 slot 的节点，除非 ASK 错误再次出现 ASKING客户端不发送 ASKING 命令，而是直接发送执行的命令，那么客户端发送的命令将被节点拒绝执行，并返回 MOVED 错误 ASKING 命令作用是打开发送该命令的客户端的 REDIS_ASKING 标识，该命令的伪代码实现： 12345def ASKING (): // 打开标识 client.flags |= REDIS_ASKING // 向客户端返回OK回复 reply(&quot;OK&quot;) 当前节点正在导入槽 slot，并且发送命令的客户端带有 REDIS_ASKING 标识，那么节点将破例执行这个关于槽 slot 的命令一次 客户端的 REDIS_ASKING 标识是一次性标识，当节点执行了一个带有 REDIS_ASKING 标识的客户端发送的命令之后，该客户端的 REDIS_ASKING 标识就会被移除 高可用节点复制Redis 集群中的节点分为主节点（master）和从节点（slave），其中主节点用于处理槽，而从节点则用于复制主节点，并在被复制的主节点下线时，代替下线主节点继续处理命令请求 1CLUSTER REPLICATE &lt;node_id&gt; 向一个节点发送命令可以让接收命令的节点成为 node_id 所指定节点的从节点，并开始对主节点进行复制 接受命令的节点首先会在的 clusterState.nodes 字典中找到 node_id 所对应节点的 clusterNode 结构，并将自己的节点中的 clusterState.myself.slaveof 指针指向这个结构，记录这个节点正在复制的主节点 节点会修改 clusterState.myself.flags 中的属性，关闭 REDIS_NODE_MASTER 标识，打开 REDIS_NODE_SLAVE 标识 节点会调用复制代码，对主节点进行复制（节点的复制功能和单机 Redis 服务器的使用了相同的代码） 一个节点成为从节点，并开始复制某个主节点这一信息会通过消息发送给集群中的其他节点，最终集群中的所有节点都会知道某个从节点正在复制某个主节点 主节点的 clusterNode 结构的 slaves 属性和 numslaves 属性中记录正在复制这个主节点的从节点名单： 1234567struct clusterNode &#123; // 正在复制这个主节点的从节点数量 int numslaves; // 数组项指向一个正在复制这个主节点的从节点的clusterNode结构 struct clusterNode **slaves; &#125; 故障检测集群中的每个节点都会定期地向集群中的其他节点发送 PING 消息，来检测对方是否在线，如果接收 PING 的节点没有在规定的时间内返回 PONG 消息，那么发送消息节点就会将接收节点标记为疑似下线（probable fail） 集群中的节点会互相发送消息，来交换集群中各个节点的状态信息，当一个主节点 A 通过消息得知主节点 B 认为主节点 C 进入了疑似下线状态时，主节点 A 会在 clusterState.nodes 字典中找到主节点 C 所对应的节点，并将主节点 B 的下线报告（failure report）添加到 clusterNode.fail_reports 链表里面 12345678910111213struct clusterNode &#123; // 一个链表，记录了所有其他节点对该节点的下线报告 list *fail_reports;&#125;// 每个下线报告由一个 clusterNodeFailReport 结构表示struct clusterNodeFailReport &#123; // 报告目标节点巳经下线的节点 struct clusterNode *node; // 最后一次从node节点收到下线报告的时间 // 程序使用这个时间戳来检查下线报告是否过期，与当前时间相差太久的下线报告会被删除 mstime_t time; &#125;; 集群里半数以上负责处理槽的主节点都将某个主节点 X 报告为疑似下线，那么 X 将被标记为已下线（FAIL），将 X 标记为已下线的节点会向集群广播一条关于主节点 X 的 FAIL 消息，所有收到消息的节点都会将 X 标记为已下线 故障转移当一个从节点发现所属的主节点进入了已下线状态，从节点将开始对下线主节点进行故障转移，执行步骤： 下属的从节点通过选举产生一个节点 被选中的从节点会执行 SLAVEOF no one 命令，成为新的主节点 新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己 新的主节点向集群广播一条 PONG 消息，让集群中的其他节点知道当前节点变成了主节点，并且接管了下线节点负责处理的槽 新的主节点开始接收有关的命令请求，故障转移完成 选举算法集群选举新的主节点的规则： 集群的配置纪元是一个自增的计数器，初始值为 0 当集群里某个节点开始一次故障转移，集群的配置纪元就是增加一 每个配置纪元里，集群中每个主节点都有一次投票的机会，而第一个向主节点要求投票的从节点将获得该主节点的投票 具有投票权的主节点是必须具有正在处理的槽 集群里有 N 个具有投票权的主节点，那么当一个从节点收集到大于等于 N/2+1 张支持票时，从节点就会当选 每个配置纪元里，具有投票权的主节点只能投一次票，所以获得一半以上票的节点只会有一个 选举流程： 当某个从节点发现正在复制的主节点进入已下线状态时，会向集群广播一条 CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST 消息，要求所有收到这条消息、并且具有投票权的主节点向这个从节点投票 如果主节点尚未投票给其他从节点，将向要求投票的从节点返回一条 CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK 消息，表示这个主节点支持从节点成为新的主节点 如果从节点获取到了半数以上的选票，则会当选新的主节点 如果一个配置纪元里没有从节点能收集到足够多的支待票，那么集群进入一个新的配置纪元，并再次进行选举，直到选出新的主节点 选举新主节点的方法和选举领头 Sentinel 的方法非常相似，两者都是基于 Raft 算法的领头选举（leader election）方法实现的 消息机制消息结构集群中的各个节点通过发送和接收消息（message）来进行通信，将发送消息的节点称为发送者（sender），接收消息的节点称为接收者（receiver） 节点发送的消息主要有： MEET 消息：当发送者接到客户端发送的 CLUSTER MEET 命令时，会向接收者发送 MEET 消息，请求接收者加入到发送者当前所处的集群里 PING 消息：集群里的每个节点默认每隔一秒钟就会从已知节点列表中随机选出五个，然后对这五个节点中最长时间没有发送过 PING 消息的节点发送 PING，以此来随机检测被选中的节点是否在线 如果节点 A 最后一次收到节点 B 发送的 PONG 消息的时间，距离当前已经超过了节点 A 的 cluster-node­-timeout 设置时长的一半，那么 A 也会向 B 发送 PING 消息，防止 A 因为长时间没有随机选中 B 发送 PING，而导致对节点 B 的信息更新滞后 PONG 消息：当接收者收到 MEET 消息或者 PING 消息时，为了让发送者确认已经成功接收消息，会向发送者返回一条 PONG；节点也可以通过向集群广播 PONG 消息来让集群中的其他节点立即刷新关于这个节点的认识（从升级为主） FAIL 消息：当一个主节点 A 判断另一个主节点 B 已经进入 FAIL 状态时，节点 A 会向集群广播一条 B 节点的 FAIL 信息 PUBLISH 消息：当节点接收到一个 PUBLISH 命令时，节点会执行这个命令并向集群广播一条 PUBLISH 消息，接收到 PUBLISH 消息的节点都会执行相同的 PUBLISH 命令 消息头节点发送的所有消息都由一个消息头包裹，消息头除了包含消息正文之外，还记录了消息发送者自身的一些信息 消息头： 1234567891011121314151617181920212223242526272829303132typedef struct clusterMsg &#123; // 消息的长度（包括这个消息头的长度和消息正文的长度）\tuint32_t totlen;\t// 消息的类型\tuint16_t type; // 消息正文包含的节点信息数量，只在发送MEET、PING、PONG这三种Gossip协议消息时使用 uint16_t count; // 发送者所处的配置纪元 uint64_t currentEpoch; // 如果发送者是一个主节点，那么这里记录的是发送者的配置纪元 // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的配置纪元 uint64_t configEpoch; // 发送者的名字(ID)\tchar sender[REDIS CLUSTER NAMELEN];\t// 发送者目前的槽指派信息\tunsigned char myslots[REDIS_CLUSTER_SLOTS/8]; // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的名字 // 如果发送者是一个主节点，那么这里记录的是 REDIS_NODE_NULL_NAME，一个 40 宇节长值全为 0 的字节数组 char slaveof[REDIS_CLUSTER_NAMELEN]; // 发送者的端口号\tuint16_t port;\t// 发送者的标识值 uint16_t flags; //发送者所处集群的状态 unsigned char state;\t// 消息的正文（或者说， 内容） union clusterMsgData data;&#125; clusterMsg 结构的 currentEpoch、sender、myslots 等属性记录了发送者的节点信息，接收者会根据这些信息在 clusterState.nodes 字典里找到发送者对应的 clusterNode 结构，并对结构进行更新，比如传播节点的槽指派信息 消息正文： 12345678910111213141516171819union clusterMsgData &#123; // MEET、PING、PONG 消息的正文 struct &#123; // 每条 MEET、PING、PONG 消息都包含两个 clusterMsgDataGossip 结构 clusterMsgDataGossip gossip[1]; &#125; ping; // FAIL 消息的正文 struct &#123; clusterMsgDataFail about; &#125; fail; // PUBLISH 消息的正文 struct &#123; clusterMsgDataPublish msg; &#125; publish; // 其他消息正文...&#125; GossipRedis 集群中的各个节点通过 Gossip 协议来交换各自关于不同节点的状态信息，其中 Gossip 协议由 MEET、PING、PONG 消息实现，三种消息使用相同的消息正文，所以节点通过消息头的 type 属性来判断消息的具体类型 发送者发送这三种消息时，会从已知节点列表中随机选出两个节点（主从都可以），将两个被选中节点信息保存到两个 Gossip 结构 12345678910111213141516typedef struct clusterMsgDataGossip &#123; // 节点的名字\tchar nodename[REDIS CLUSTER NAMELEN]; // 最后一次向该节点发送PING消息的时间戳 uint32_t ping_sent;\t// 最后一次从该节点接收到PONG消息的时间戳 uint32_t pong_received; // 节点的IP地址\tchar ip[16]; // 节点的端口号 uint16_t port;\t// 节点的标识值 uint16_t flags;&#125; 当接收者收到消息时，会访问消息正文中的两个数据结构，来进行相关操作 如果被选中节点不存在于接收者的已知节点列表，接收者将根据结构中记录的 IP 地址和端口号，与节点进行握手 如果存在，根据 Gossip 结构记录的信息对节点所对应的 clusterNode 结构进行更新 FAIL在集群的节点数量比较大的情况下，使用 Gossip 协议来传播节点的已下线信息会带来一定延迟，因为 Gossip 协议消息通常需要一段时间才能传播至整个集群，所以通过发送 FAIL消息可以让集群里的所有节点立即知道某个主节点已下线，从而尽快进行其他操作 FAIL 消息的正文由 clusterMsgDataFail 结构表示，该结构只有一个属性，记录了已下线节点的名字 123typedef struct clusterMsgDataFail &#123;\tchar nodename[REDIS_CLUSTER_NAMELEN)];&#125;; 因为传播下线信息不需要其他属性，所以节省了传播的资源 PUBLISH当客户端向集群中的某个节点发送命令，接收到 PUBLISH 命令的节点不仅会向 channel 频道发送消息 message，还会向集群广播一条 PUBLISH 消息，所有接收到这条 PUBLISH 消息的节点都会向 channel 频道发送 message 消息，最终集群中所有节点都发了 1PUBLISH &lt;channel&gt; &lt;message&gt; PUBLISH 消息的正文由 clusterMsgDataPublish 结构表示： 1234567891011typedef struct clusterMsgDataPublish &#123; // channel参数的长度 uint32_t channel_len; // message参数的长度 uint32_t message_len; // 定义为8字节只是为了对齐其他消息结构，实际的长度由保存的内容决定 // bulk_data 的 0 至 channel_len-1 字节保存的是channel参数 // bulk_data的 channel_len 字节至 channel_len + message_len-1 字节保存的则是message参数 unsigned char bulk_data[8];&#125; 让集群的所有节点执行相同的 PUBLISH 命令，最简单的方法就是向所有节点广播相同的 PUBLISH 命令，这也是 Redis 复制 PUBLISH 命令时所使用的，但是这种做法并不符合 Redis 集群的各个节点通过发送和接收消息来进行通信的规则 脑裂问题脑裂指在主从集群中，同时有两个相同的主节点能接收写请求，导致客户端不知道应该往哪个主节点写入数据，最后 不同客户端往不同的主节点上写入数据 原主节点并没有真的发生故障，由于某些原因无法处理请求（CPU 利用率很高、自身阻塞），无法按时响应心跳请求，被哨兵&#x2F;集群主节点错误的判断为下线 在被判断下线之后，原主库又重新开始处理请求了，哨兵&#x2F;集群主节点还没有完成主从切换，客户端仍然可以和原主库通信，客户端发送的写操作就会在原主库上写入数据，造成脑裂问题 数据丢失问题：从库一旦升级为新主库，哨兵就会让原主库执行 slave of 命令，和新主库重新进行全量同步，原主库需要清空本地的数据，加载新主库发送的 RDB 文件，所以原主库在主从切换期间保存的新写数据就丢失了 预防脑裂：在主从集群部署时，合理地配置参数 min-slaves-to-write 和 min-slaves-max-lag 假设从库有 K 个，可以将 min-slaves-to-write 设置为 K&#x2F;2+1（如果 K 等于 1，就设为 1） 将 min-slaves-max-lag 设置为十几秒（例如 10～20s） 在假故障期间无法响应哨兵发出的心跳测试，无法和从库进行 ACK 确认，并且没有足够的从库，拒绝客户端的写入 结构搭建整体框架： 配置服务器（3 主 3 从） 建立通信（Meet） 分槽（Slot） 搭建主从（master-slave） 创建集群 conf 配置文件： redis-6501.conf 12345678port 6501dir &quot;/redis/data&quot;dbfilename &quot;dump-6501.rdb&quot;cluster-enabled yescluster-config-file &quot;cluster-6501.conf&quot;cluster-node-timeout 5000#其他配置文件参照上面的修改端口即可，内容完全一样 服务端启动： 1redis-server config_file_name 客户端启动： 1redis-cli -p 6504 -c cluster 配置： 是否启用 cluster，加入 cluster 节点 1cluster-enabled yes|no cluster 配置文件名，该文件属于自动生成，仅用于快速查找文件并查询文件内容 1cluster-config-file filename 节点服务响应超时时间，用于判定该节点是否下线或切换为从节点 1cluster-node-timeout milliseconds master 连接的 slave 最小数量 1cluster-migration-barrier min_slave_number 客户端启动命令： cluster 节点操作命令（客户端命令）： 查看集群节点信息 1cluster nodes 更改 slave 指向新的 master 1cluster replicate master-id 发现一个新节点，新增 master 1cluster meet ip:port 忽略一个没有 solt 的节点 1cluster forget server_id 手动故障转移 1cluster failover 集群操作命令（Linux）： 创建集群 1redis-cli –-cluster create masterhost1:masterport1 masterhost2:masterport2 masterhost3:masterport3 [masterhostn:masterportn …] slavehost1:slaveport1 slavehost2:slaveport2 slavehost3:slaveport3 -–cluster-replicas n 注意：master 与 slave 的数量要匹配，一个 master 对应 n 个 slave，由最后的参数 n 决定。master 与 slave 的匹配顺序为第一个 master 与前 n 个 slave 分为一组，形成主从结构 添加 master 到当前集群中，连接时可以指定任意现有节点地址与端口 1redis-cli --cluster add-node new-master-host:new-master-port now-host:now-port 添加 slave 1redis-cli --cluster add-node new-slave-host:new-slave-port master-host:master-port --cluster-slave --cluster-master-id masterid 删除节点，如果删除的节点是 master，必须保障其中没有槽 slot 1redis-cli --cluster del-node del-slave-host:del-slave-port del-slave-id 重新分槽，分槽是从具有槽的 master 中划分一部分给其他 master，过程中不创建新的槽 1redis-cli --cluster reshard new-master-host:new-master:port --cluster-from src- master-id1, src-master-id2, src-master-idn --cluster-to target-master-id -- cluster-slots slots 注意：将需要参与分槽的所有 masterid 不分先后顺序添加到参数中，使用 , 分隔，指定目标得到的槽的数量，所有的槽将平均从每个来源的 master 处获取 重新分配槽，从具有槽的 master 中分配指定数量的槽到另一个 master 中，常用于清空指定 master 中的槽 1redis-cli --cluster reshard src-master-host:src-master-port --cluster-from src- master-id --cluster-to target-master-id --cluster-slots slots --cluster-yes 其他操作发布订阅基本指令Redis 发布订阅（pub&#x2F;sub）是一种消息通信模式：发送者（pub）发送消息，订阅者（sub）接收消息 Redis 客户端可以订阅任意数量的频道，每当有客户端向被订阅的频道发送消息（message）时，频道的所有订阅者都会收到消息 操作过程： 打开一个客户端订阅 channel1：SUBSCRIBE channel1 打开另一个客户端，给 channel1 发布消息 hello：PUBLISH channel1 hello 第一个客户端可以看到发送的消息 客户端还可以通过 PSUBSCRIBE 命令订阅一个或多个模式，每当有其他客户端向某个频道发送消息时，消息不仅会被发送给这个频道的所有订阅者，还会被发送给所有与这个频道相匹配的模式的订阅者，比如 PSUBSCRIBE channel* 订阅模式，与 channel1 匹配 注意：发布的消息没有持久化，所以订阅的客户端只能收到订阅后发布的消息 频道操作Redis 将所有频道的订阅关系都保存在服务器状态的 pubsub_channels 字典里，键是某个被订阅的频道，值是一个记录所有订阅这个频道的客户端链表 1234struct redisServer &#123;\t// 保存所有频道的订阅关系，\tdict *pubsub_channels;&#125; 客户端执行 SUBSCRIBE 命令订阅某个或某些频道，服务器会将客户端与频道进行关联： 频道已经存在，直接将客户端添加到链表末尾 频道还未有任何订阅者，在字典中为频道创建一个键值对，再将客户端添加到链表 UNSUBSCRIBE 命令用来退订某个频道，服务器将从 pubsub_channels 中解除客户端与被退订频道之间的关联 模式操作Redis 服务器将所有模式的订阅关系都保存在服务器状态的 pubsub_patterns 属性里 1234567891011struct redisServer &#123;\t// 保存所有模式订阅关系，链表中每个节点是一个 pubsubPattern\tlist *pubsub_patterns;&#125;typedef struct pubsubPattern &#123; // 订阅的客户端 redisClient *client;\t// 被订阅的模式，比如 channel* robj *pattern; &#125; 客户端执行 PSUBSCRIBE 命令订阅某个模式，服务器会新建一个 pubsubPattern 结构并赋值，放入 pubsub_patterns 链表结尾 模式的退订命令 PUNSUBSCRIBE 是订阅命令的反操作，服务器在 pubsub_patterns 链表中查找并删除对应的结构 发送消息Redis 客户端执行 PUBLISH &lt;channel&gt; &lt;message&gt; 命令将消息 message发送给频道 channel，服务器会执行： 在 pubsub_channels 字典里找到频道 channel 的订阅者名单，将消息 message 发送给所有订阅者 遍历整个 pubsub_patterns 链表，查找与 channel 频道相匹配的模式，并将消息发送给所有订阅了这些模式的客户端 12345// 如果频道和模式相匹配if match(channel, pubsubPattern.pattern) &#123; // 将消息发送给订阅该模式的客户端 send_message(pubsubPattern.client, message);&#125; 查看信息PUBSUB 命令用来查看频道或者模式的相关信息 PUBSUB CHANNELS [pattern] 返回服务器当前被订阅的频道，其中 pattern 参数是可选的 如果不给定 pattern 参数，那么命令返回服务器当前被订阅的所有频道 如果给定 pattern 参数，那么命令返回服务器当前被订阅的频道中与 pattern 模式相匹配的频道 PUBSUB NUMSUB [channel-1 channel-2 ... channel-n] 命令接受任意多个频道作为输入参数，并返回这些频道的订阅者数量 PUBSUB NUMPAT 命令用于返回服务器当前被订阅模式的数量 ACL 指令Redis ACL 是 Access Control List（访问控制列表）的缩写，该功能允许根据可以执行的命令和可以访问的键来限制某些连接 acl cat：查看添加权限指令类别 acl whoami：查看当前用户 acl setuser username on &gt;password ~cached:* +get：设置有用户名、密码、ACL 权限（只能 get） 监视器MONITOR 命令，可以将客户端变为一个监视器，实时地接收并打印出服务器当前处理的命令请求的相关信息 123456789// 实现原理def MONITOR():\t// 打开客户端的监视器标志\tclient.flags |= REDIS_MONITOR // 将客户端添加到服务器状态的 redisServer.monitors链表的末尾 server.monitors.append(client) // 向客户端返回 ok\tsend_reply(&quot;OK&quot;) 服务器每次处理命令请求都会调用 replicationFeedMonitors 函数，函数将被处理的命令请求的相关信息发送给各个监视器 123456789redis&gt; MONITOR OK 1378822099.421623 [0 127.0.0.1:56604] &quot;PING&quot; 1378822105.089572 [0 127.0.0.1:56604] &quot;SET&quot; &quot;msg&quot; &quot;hello world&quot; 1378822109.036925 [0 127.0.0.1:56604] &quot;SET&quot; &quot;number&quot; &quot;123&quot; 1378822140.649496 (0 127.0.0.1:56604] &quot;SADD&quot; &quot;fruits&quot; &quot;Apple&quot; &quot;Banana&quot; &quot;Cherry&quot; 1378822154.117160 [0 127.0.0.1:56604] &quot;EXPIRE&quot; &quot;msg&quot; &quot;10086&quot; 1378822257.329412 [0 127.0.0.1:56604] &quot;KEYS&quot; &quot;*&quot; 1378822258.690131 [0 127.0.0.1:56604] &quot;DBSIZE&quot; 批处理Redis 的管道 Pipeline 机制可以一次处理多条指令 Pipeline 中的多条命令非原子性，因为在向管道内添加命令时，其他客户端的发送的命令仍然在执行 原生批命令（MSET 等）是服务端实现，而 Pipeline 需要服务端与客户端共同完成 使用 Pipeline 封装的命令数量不能太多，数据量过大会增加客户端的等待时间，造成网络阻塞，Jedis 中的 Pipeline 使用方式： 12345678910// 创建管道Pipeline pipeline = jedis.pipelined();for (int i = 1; i &lt;= 100000; i++) &#123; // 放入命令到管道 pipeline.set(&quot;key_&quot; + i, &quot;value_&quot; + i); if (i % 1000 == 0) &#123; // 每放入1000条命令，批量执行 pipeline.sync(); &#125;&#125; 集群下模式下，批处理命令的多个 key 必须落在一个插槽中，否则就会导致执行失败，N 条批处理命令的优化方式： 串行命令：for 循环遍历，依次执行每个命令 串行 slot：在客户端计算每个 key 的 slot，将 slot 一致的分为一组，每组都利用 Pipeline 批处理，串行执行各组命令 并行 slot：在客户端计算每个 key 的 slot，将 slot 一致的分为一组，每组都利用 Pipeline 批处理，并行执行各组命令 hash_tag：将所有 key 设置相同的 hash_tag，则所有 key 的 slot 一定相同 耗时 优点 缺点 串行命令 N 次网络耗时 + N 次命令耗时 实现简单 耗时久 串行 slot m 次网络耗时 + N 次命令耗时，m &#x3D; key 的 slot 个数 耗时较短 实现稍复杂 并行 slot 1 次网络耗时 + N 次命令耗时 耗时非常短 实现复杂 hash_tag 1 次网络耗时 + N 次命令耗时 耗时非常短、实现简单 容易出现数据倾斜 解决方案缓存方案缓存模式旁路缓存缓存本质：弥补 CPU 的高算力和 IO 的慢读写之间巨大的鸿沟 旁路缓存模式 Cache Aside Pattern 是平时使用比较多的一个缓存读写模式，比较适合读请求比较多的场景 Cache Aside Pattern 中服务端需要同时维系 DB 和 cache，并且是以 DB 的结果为准 写操作：先更新 DB，然后直接删除 cache 读操作：从 cache 中读取数据，读取到就直接返回；读取不到就从 DB 中读取数据返回，并放到 cache 时序导致的不一致问题： 在写数据的过程中，不能先删除 cache 再更新 DB，因为会造成缓存的不一致。比如请求 1 先写数据 A，请求 2 随后读数据 A，当请求 1 删除 cache 后，请求 2 直接读取了 DB，此时请求 1 还没写入 DB（延迟双删） 在写数据的过程中，先更新 DB 再删除 cache 也会出现问题，但是概率很小，因为缓存的写入速度非常快 旁路缓存的缺点： 首次请求数据一定不在 cache 的问题，一般采用缓存预热的方法，将热点数据可以提前放入 cache 中 写操作比较频繁的话导致 cache 中的数据会被频繁被删除，影响缓存命中率 删除缓存而不是更新缓存的原因：每次更新数据库都更新缓存，造成无效写操作较多（懒惰加载，需要的时候再放入缓存） 读写穿透读写穿透模式 Read&#x2F;Write Through Pattern：服务端把 cache 视为主要数据存储，从中读取数据并将数据写入其中，cache 负责将此数据同步写入 DB，从而减轻了应用程序的职责 写操作：先查 cache，cache 中不存在，直接更新 DB；cache 中存在则先更新 cache，然后 cache 服务更新 DB（同步更新 cache 和 DB） 读操作：从 cache 中读取数据，读取到就直接返回 ；读取不到先从 DB 加载，写入到 cache 后返回响应 Read-Through Pattern 实际只是在 Cache-Aside Pattern 之上进行了封装。在 Cache-Aside Pattern 下，发生读请求的时候，如果 cache 中不存在对应的数据，是由客户端负责把数据写入 cache，而 Read Through Pattern 则是 cache 服务自己来写入缓存的，对客户端是透明的 Read-Through Pattern 也存在首次不命中的问题，采用缓存预热解决 异步缓存异步缓存写入 Write Behind Pattern 由 cache 服务来负责 cache 和 DB 的读写，对比读写穿透不同的是 Write Behind Caching 是只更新缓存，不直接更新 DB，改为异步批量的方式来更新 DB，可以减小写的成本 缺点：这种模式对数据一致性没有高要求，可能出现 cache 还没异步更新 DB，服务就挂掉了 应用： DB 的写性能非常高，适合一些数据经常变化又对数据一致性要求不高的场景，比如浏览量、点赞量 MySQL 的 InnoDB Buffer Pool 机制用到了这种策略 缓存一致使用缓存代表不需要强一致性，只需要最终一致性 缓存不一致的方法： 数据库和缓存数据强一致场景： 同步双写：更新 DB 时同样更新 cache，保证在一个事务中，通过加锁来保证更新 cache 时不存在线程安全问题 延迟双删：先淘汰缓存再写数据库，休眠 1 秒再次淘汰缓存，可以将 1 秒内造成的缓存脏数据再次删除 异步通知： 基于 MQ 的异步通知：对数据的修改后，代码需要发送一条消息到 MQ 中，缓存服务监听 MQ 消息 Canal 订阅 MySQL binlog 的变更上报给 Kafka，系统监听 Kafka 消息触发缓存失效，或者直接将变更发送到处理服务，没有任何代码侵入 低耦合，可以同时通知多个缓存服务，但是时效性一般，可能存在中间不一致状态 低一致性场景： 更新 DB 的时候同样更新 cache，但是给缓存加一个比较短的过期时间，这样就可以保证即使数据不一致影响也比较小 使用 Redis 自带的内存淘汰机制 缓存问题缓存预热场景：宕机，服务器启动后迅速宕机 问题排查： 请求数量较高，大量的请求过来之后都需要去从缓存中获取数据，但是缓存中又没有，此时从数据库中查找数据然后将数据再存入缓存，造成了短期内对 redis 的高强度操作从而导致问题 主从之间数据吞吐量较大，数据同步操作频度较高 解决方案： 前置准备工作： 日常例行统计数据访问记录，统计访问频度较高的热点数据 利用 LRU 数据删除策略，构建数据留存队列例如：storm 与 kafka 配合 准备工作： 将统计结果中的数据分类，根据级别，redis 优先加载级别较高的热点数据 利用分布式多服务器同时进行数据读取，提速数据加载过程 热点数据主从同时预热 实施： 使用脚本程序固定触发数据预热过程 如果条件允许，使用了 CDN（内容分发网络），效果会更好 总的来说：缓存预热就是系统启动前，提前将相关的缓存数据直接加载到缓存系统。避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题，用户直接查询事先被预热的缓存数据 缓存雪崩场景：数据库服务器崩溃，一连串的问题会随之而来 问题排查：在一个较短的时间内，缓存中较多的 key 集中过期，此周期内请求访问过期的数据 Redis 未命中，Redis 向数据库获取数据，数据库同时收到大量的请求无法及时处理。 解决方案： 加锁，慎用 设置热点数据永远不过期，如果缓存数据库是分布式部署，将热点数据均匀分布在不同搞得缓存数据库中 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生 构建多级缓存架构，Nginx 缓存 + Redis 缓存 + ehcache 缓存 灾难预警机制，监控 Redis 服务器性能指标，CPU 使用率、内存容量、平均响应时间、线程数 限流、降级：短时间范围内牺牲一些客户体验，限制一部分请求访问，降低应用服务器压力，待业务低速运转后再逐步放开访问 总的来说：缓存雪崩就是瞬间过期数据量太大，导致对数据库服务器造成压力。如能够有效避免过期时间集中，可以有效解决雪崩现象的出现（约 40%），配合其他策略一起使用，并监控服务器的运行数据，根据运行记录做快速调整。 缓存击穿缓存击穿也叫热点 Key 问题 Redis 中某个 key 过期，该 key 访问量巨大 多个数据请求从服务器直接压到 Redis 后，均未命中 Redis 在短时间内发起了大量对数据库中同一数据的访问 解决方案： 预先设定：以电商为例，每个商家根据店铺等级，指定若干款主打商品，在购物节期间，加大此类信息 key 的过期时长 注意：购物节不仅仅指当天，以及后续若干天，访问峰值呈现逐渐降低的趋势 现场调整：监控访问量，对自然流量激增的数据延长过期时间或设置为永久性 key 后台刷新数据：启动定时任务，高峰期来临之前，刷新数据有效期，确保不丢失 二级缓存：设置不同的失效时间，保障不会被同时淘汰就行 加锁：分布式锁，防止被击穿，但是要注意也是性能瓶颈，慎重 总的来说：缓存击穿就是单个高热数据过期的瞬间，数据访问量较大，未命中 Redis 后，发起了大量对同一数据的数据库访问，导致对数据库服务器造成压力。应对策略应该在业务数据分析与预防方面进行，配合运行监控测试与即时调整策略，毕竟单个 key 的过期监控难度较高，配合雪崩处理策略即可 缓存穿透场景：系统平稳运行过程中，应用服务器流量随时间增量较大，Redis 服务器命中率随时间逐步降低，Redis 内存平稳，内存无压力，Redis 服务器 CPU 占用激增，数据库服务器压力激增，数据库崩溃 问题排查： Redis 中大面积出现未命中 出现非正常 URL 访问 问题分析： 访问了不存在的数据，跳过了 Redis 缓存，数据库页查询不到对应数据 Redis 获取到 null 数据未进行持久化，直接返回 出现黑客攻击服务器 解决方案： 缓存 null：对查询结果为 null 的数据进行缓存，设定短时限，例如 30-60 秒，最高 5 分钟 白名单策略：提前预热各种分类数据 id 对应的 bitmaps，id 作为 bitmaps 的 offset，相当于设置了数据白名单。当加载正常数据时放行，加载异常数据时直接拦截（效率偏低），也可以使用布隆过滤器（有关布隆过滤器的命中问题对当前状况可以忽略） 实时监控：实时监控 Redis 命中率（业务正常范围时，通常会有一个波动值）与 null 数据的占比 非活动时段波动：通常检测 3-5 倍，超过 5 倍纳入重点排查对象 活动时段波动：通常检测10-50 倍，超过 50 倍纳入重点排查对象 根据倍数不同，启动不同的排查流程。然后使用黑名单进行防控 key 加密：临时启动防灾业务 key，对 key 进行业务层传输加密服务，设定校验程序，过来的 key 校验；例如每天随机分配 60 个加密串，挑选 2 到 3 个，混淆到页面数据 id 中，发现访问 key 不满足规则，驳回数据访问 总的来说：缓存击穿是指访问了不存在的数据，跳过了合法数据的 Redis 数据缓存阶段，每次访问数据库，导致对数据库服务器造成压力。通常此类数据的出现量是一个较低的值，当出现此类情况以毒攻毒，并及时报警。无论是黑名单还是白名单，都是对整体系统的压力，警报解除后尽快移除 参考视频：https://www.bilibili.com/video/BV15y4y1r7X3 Key 设计大 Key：通常以 Key 的大小和 Key 中成员的数量来综合判定，引发的问题： 客户端执行命令的时长变慢 Redis 内存达到 maxmemory 定义的上限引发操作阻塞或重要的 Key 被逐出，甚至引发内存溢出（OOM） 集群架构下，某个数据分片的内存使用率远超其他数据分片，使数据分片的内存资源不均衡 对大 Key 执行读请求，会使 Redis 实例的带宽使用率被占满，导致自身服务变慢，同时易波及相关的服务 对大 Key 执行删除操作，会造成主库较长时间的阻塞，进而可能引发同步中断或主从切换 热 Key：通常以其接收到的 Key 被请求频率来判定，引发的问题： 占用大量的 CPU 资源，影响其他请求并导致整体性能降低 分布式集群架构下，产生访问倾斜，即某个数据分片被大量访问，而其他数据分片处于空闲状态，可能引起该数据分片的连接数被耗尽，新的连接建立请求被拒绝等问题 在抢购或秒杀场景下，可能因商品对应库存 Key 的请求量过大，超出 Redis 处理能力造成超卖 热 Key 的请求压力数量超出 Redis 的承受能力易造成缓存击穿，即大量请求将被直接指向后端的存储层，导致存储访问量激增甚至宕机，从而影响其他业务 热 Key 分类两种，治理方式如下： 一种是单一数据，比如秒杀场景，假设总量 10000 可以拆为多个 Key 进行访问，每次对请求进行路由到不同的 Key 访问，保证最终一致性，但是会出现访问不同 Key 产生的剩余量是不同的，这时可以通过前端进行 Mock 假数据 一种是多数据集合，比如进行 ID 过滤，这时可以添加本地 LRU 缓存，减少对热 Key 的访问 参考文档：https://help.aliyun.com/document_detail/353223.html 慢查询确认服务和 Redis 之间的链路是否正常，排除网络原因后进行 Redis 的排查： 使用复杂度过高的命令 操作大 key，分配内存和释放内存会比较耗时 key 集中过期，导致定时任务需要更长的时间去清理 实例内存达到上限，每次写入新的数据之前，Redis 必须先从实例中踢出一部分数据 参考文章：https://www.cnblogs.com/traditional/p/15633919.html（非常好） JavaJDBC概述JDBC（Java DataBase Connectivity，Java 数据库连接）是一种用于执行 SQL 语句的 Java API，可以为多种关系型数据库提供统一访问，是由一组用 Java 语言编写的类和接口组成的。 JDBC 是 Java 官方提供的一套规范（接口），用于帮助开发人员快速实现不同关系型数据库的连接 功能类DriverManagerDriverManager：驱动管理对象 注册驱动： 注册给定的驱动：public static void registerDriver(Driver driver) 代码实现语法：Class.forName(&quot;com.mysql.jdbc.Driver) com.mysql.jdbc.Driver 中存在静态代码块 1234567static &#123; try &#123; DriverManager.registerDriver(new Driver()); &#125; catch (SQLException var1) &#123; throw new RuntimeException(&quot;Can&#x27;t register driver!&quot;); &#125;&#125; 不需要通过 DriverManager 调用静态方法 registerDriver，因为 Driver 类被使用，则自动执行静态代码块完成注册驱动 jar 包中 META-INF 目录下存在一个 java.sql.Driver 配置文件，文件中指定了 com.mysql.jdbc.Driver 获取数据库连接并返回连接对象： 方法：public static Connection getConnection(String url, String user, String password) url：指定连接的路径，语法为 jdbc:mysql://ip地址(域名):端口号/数据库名称 user：用户名 password：密码 ConnectionConnection：数据库连接对象 获取执行者对象 获取普通执行者对象：Statement createStatement() 获取预编译执行者对象：PreparedStatement prepareStatement(String sql) 管理事务 开启事务：setAutoCommit(boolean autoCommit)，false 开启事务，true 自动提交模式（默认） 提交事务：void commit() 回滚事务：void rollback() 释放资源 释放此 Connection 对象的数据库和 JDBC 资源：void close() StatementStatement：执行 sql 语句的对象 执行 DML 语句：int executeUpdate(String sql) 返回值 int：返回影响的行数 参数 sql：可以执行 insert、update、delete 语句 执行 DQL 语句：ResultSet executeQuery(String sql) 返回值 ResultSet：封装查询的结果 参数 sql：可以执行 select 语句 释放资源 释放此 Statement 对象的数据库和 JDBC 资源：void close() ResultSetResultSet：结果集对象，ResultSet 对象维护了一个游标，指向当前的数据行，初始在第一行 判断结果集中是否有数据：boolean next() 有数据返回 true，并将索引向下移动一行 没有数据返回 false 获取结果集中当前行的数据：XXX getXxx(&quot;列名&quot;) XXX 代表数据类型（要获取某列数据，这一列的数据类型） 例如：String getString(“name”); int getInt(“age”); 释放资源 释放 ResultSet 对象的数据库和 JDBC 资源：void close() 代码实现数据准备 1234567891011121314151617-- 创建db14数据库CREATE DATABASE db14;-- 使用db14数据库USE db14;-- 创建student表CREATE TABLE student(\tsid INT PRIMARY KEY AUTO_INCREMENT,\t-- 学生id\tNAME VARCHAR(20), -- 学生姓名\tage INT, -- 学生年龄\tbirthday DATE, -- 学生生日);-- 添加数据INSERT INTO student VALUES (NULL,&#x27;张三&#x27;,23,&#x27;1999-09-23&#x27;),(NULL,&#x27;李四&#x27;,24,&#x27;1998-08-10&#x27;),(NULL,&#x27;王五&#x27;,25,&#x27;1996-06-06&#x27;),(NULL,&#x27;赵六&#x27;,26,&#x27;1994-10-20&#x27;); JDBC 连接代码： 12345678910111213141516171819202122232425262728public class JDBCDemo01 &#123; public static void main(String[] args) throws Exception&#123; //1.导入jar包 //2.注册驱动 Class.forName(&quot;com.mysql.jdbc.Driver&quot;); //3.获取连接 Connection con = DriverManager.getConnection(&quot;jdbc:mysql://192.168.2.184:3306/db2&quot;,&quot;root&quot;,&quot;123456&quot;); //4.获取执行者对象 Statement stat = con.createStatement(); //5.执行sql语句，并且接收结果 String sql = &quot;SELECT * FROM user&quot;; ResultSet rs = stat.executeQuery(sql); //6.处理结果 while(rs.next()) &#123; System.out.println(rs.getInt(&quot;id&quot;) + &quot;\\t&quot; + rs.getString(&quot;name&quot;)); &#125; //7.释放资源 con.close(); stat.close(); con.close(); &#125;&#125; 注入攻击攻击演示SQL 注入攻击演示 在登录界面，输入一个错误的用户名或密码，也可以登录成功 原理：我们在密码处输入的所有内容，都应该认为是密码的组成，但是 Statement 对象在执行 SQL 语句时，将一部分内容当做查询条件来执行 1SELECT * FROM user WHERE loginname=&#x27;aaa&#x27; AND password=&#x27;aaa&#x27; OR &#x27;1&#x27;=&#x27;1&#x27;; 攻击解决PreparedStatement：预编译 sql 语句的执行者对象，继承 PreparedStatement extends Statement 在执行 sql 语句之前，将 sql 语句进行提前编译，明确 sql 语句的格式，剩余的内容都会认为是参数 sql 语句中的参数使用 ? 作为占位符 为 ? 占位符赋值的方法：setXxx(int parameterIndex, xxx data) 参数1：? 的位置编号（编号从 1 开始） 参数2：? 的实际参数 1234String sql = &quot;SELECT * FROM user WHERE loginname=? AND password=?&quot;;pst = con.prepareStatement(sql);pst.setString(1,loginName);pst.setString(2,password); 执行 sql 语句的方法 执行 insert、update、delete 语句：int executeUpdate() 执行 select 语句：ResultSet executeQuery() 连接池概念数据库连接背景：数据库连接是一种关键的、有限的、昂贵的资源，这一点在多用户的网页应用程序中体现得尤为突出。对数据库连接的管理能显著影响到整个应用程序的伸缩性和健壮性，影响到程序的性能指标。 数据库连接池：数据库连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是再重新建立一个，这项技术能明显提高对数据库操作的性能。 数据库连接池原理 归还连接使用动态代理的方式来改进 自定义数据库连接池类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class MyDataSource implements DataSource &#123; //1.准备一个容器。用于保存多个数据库连接对象 private static List&lt;Connection&gt; pool = Collections.synchronizedList(new ArrayList&lt;&gt;()); //2.定义静态代码块,获取多个连接对象保存到容器中 static&#123; for(int i = 1; i &lt;= 10; i++) &#123; Connection con = JDBCUtils.getConnection(); pool.add(con); &#125; &#125; //3.提供一个获取连接池大小的方法 public int getSize() &#123; return pool.size(); &#125; //动态代理方式 @Override public Connection getConnection() throws SQLException &#123; if(pool.size() &gt; 0) &#123; Connection con = pool.remove(0); Connection proxyCon = (Connection) Proxy.newProxyInstance( con.getClass().getClassLoader(), new Class[]&#123;Connection.class&#125;, new InvocationHandler() &#123; /* 执行Connection实现类连接对象所有的方法都会经过invoke 如果是close方法，归还连接 如果不是，直接执行连接对象原有的功能即可 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if(method.getName().equals(&quot;close&quot;)) &#123; //归还连接 pool.add(con); return null; &#125;else &#123; return method.invoke(con,args); &#125; &#125; &#125;); return proxyCon; &#125;else &#123; throw new RuntimeException(&quot;连接数量已用尽&quot;); &#125; &#125;&#125; 开源项目C3P0使用 C3P0 连接池： 配置文件名称：c3p0-config.xml，必须放在 src 目录下 1234567891011121314151617181920212223&lt;c3p0-config&gt; &lt;!-- 使用默认的配置读取连接池对象 --&gt; &lt;default-config&gt; &lt;!-- 连接参数 --&gt; &lt;property name=&quot;driverClass&quot;&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot;&gt;jdbc:mysql://192.168.2.184:3306/db14&lt;/property&gt; &lt;property name=&quot;user&quot;&gt;root&lt;/property&gt; &lt;property name=&quot;password&quot;&gt;123456&lt;/property&gt; &lt;!-- 连接池参数 --&gt; &lt;!--初始化数量--&gt; &lt;property name=&quot;initialPoolSize&quot;&gt;5&lt;/property&gt; &lt;!--最大连接数量--&gt; &lt;property name=&quot;maxPoolSize&quot;&gt;10&lt;/property&gt; &lt;!--超时时间 3000ms--&gt; &lt;property name=&quot;checkoutTimeout&quot;&gt;3000&lt;/property&gt; &lt;/default-config&gt; &lt;named-config name=&quot;otherc3p0&quot;&gt; &lt;!-- 连接参数 --&gt; &lt;!-- 连接池参数 --&gt; &lt;/named-config&gt;&lt;/c3p0-config&gt; 代码演示 123456789101112131415161718192021222324public class C3P0Test1 &#123; public static void main(String[] args) throws Exception&#123; //1.创建c3p0的数据库连接池对象 DataSource dataSource = new ComboPooledDataSource(); //2.通过连接池对象获取数据库连接 Connection con = dataSource.getConnection(); //3.执行操作 String sql = &quot;SELECT * FROM student&quot;; PreparedStatement pst = con.prepareStatement(sql); //4.执行sql语句，接收结果集 ResultSet rs = pst.executeQuery(); //5.处理结果集 while(rs.next()) &#123; System.out.println(rs.getInt(&quot;sid&quot;) + &quot;\\t&quot; + rs.getString(&quot;name&quot;) + &quot;\\t&quot; + rs.getInt(&quot;age&quot;) + &quot;\\t&quot; + rs.getDate(&quot;birthday&quot;)); &#125; //6.释放资源 rs.close(); pst.close(); con.close(); &#125;&#125; DruidDruid 连接池： 配置文件：druid.properties，必须放在 src 目录下 1234567driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://192.168.2.184:3306/db14username=rootpassword=123456initialSize=5maxActive=10maxWait=3000 代码演示 123456789101112131415161718192021222324252627282930public class DruidTest1 &#123; public static void main(String[] args) throws Exception&#123; //获取配置文件的流对象 InputStream is = DruidTest1.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;); //1.通过Properties集合，加载配置文件 Properties prop = new Properties(); prop.load(is); //2.通过Druid连接池工厂类获取数据库连接池对象 DataSource dataSource = DruidDataSourceFactory.createDataSource(prop); //3.通过连接池对象获取数据库连接进行使用 Connection con = dataSource.getConnection(); //4.执行sql语句，接收结果集 String sql = &quot;SELECT * FROM student&quot;; PreparedStatement pst = con.prepareStatement(sql); ResultSet rs = pst.executeQuery(); //5.处理结果集 while(rs.next()) &#123; System.out.println(rs.getInt(&quot;sid&quot;) + &quot;\\t&quot; + rs.getString(&quot;name&quot;) + &quot;\\t&quot; + rs.getInt(&quot;age&quot;) + &quot;\\t&quot; + rs.getDate(&quot;birthday&quot;)); &#125; //6.释放资源 rs.close(); pst.close(); con.close(); &#125;&#125; Jedis基本使用Jedis 用于 Java 语言连接 Redis 服务，并提供对应的操作 API jar 包导入 下载地址：https://mvnrepository.com/artifact/redis.clients/jedis 基于 maven： 12345&lt;dependency&gt;\t&lt;groupId&gt;redis.clients&lt;/groupId&gt;\t&lt;artifactId&gt;jedis&lt;/artifactId&gt;\t&lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 客户端连接 Redis：API 文档 http://xetorthio.github.io/jedis/ 连接 redis：Jedis jedis = new Jedis(&quot;192.168.0.185&quot;, 6379) 操作 redis：jedis.set(&quot;name&quot;, &quot;seazean&quot;); jedis.get(&quot;name&quot;) 关闭 redis：jedis.close() 代码实现： 1234567891011121314151617181920public class JedisTest &#123; public static void main(String[] args) &#123; //1.获取连接对象 Jedis jedis = new Jedis(&quot;192.168.2.185&quot;,6379); //2.执行操作 jedis.set(&quot;age&quot;,&quot;39&quot;); String hello = jedis.get(&quot;hello&quot;); System.out.println(hello); jedis.lpush(&quot;list1&quot;,&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;); List&lt;String&gt; list1 = jedis.lrange(&quot;list1&quot;, 0, -1); for (String s:list1 ) &#123; System.out.println(s); &#125; jedis.sadd(&quot;set1&quot;,&quot;abc&quot;,&quot;abc&quot;,&quot;def&quot;,&quot;poi&quot;,&quot;cba&quot;); Long len = jedis.scard(&quot;set1&quot;); System.out.println(len); //3.关闭连接 jedis.close(); &#125;&#125; 工具类连接池对象： JedisPool：Jedis 提供的连接池技术 poolConfig：连接池配置对象 host：Redis 服务地址 port：Redis 服务端口号 JedisPool 的构造器如下： 123public JedisPool(GenericObjectPoolConfig poolConfig, String host, int port) &#123;\tthis(poolConfig, host, port, 2000, (String)null, 0, (String)null);&#125; 创建配置文件 redis.properties 1234redis.maxTotal=50redis.maxIdel=10redis.host=192.168.2.185redis.port=6379 工具类： 123456789101112131415161718192021222324252627282930public class JedisUtils &#123; private static int maxTotal; private static int maxIdel; private static String host; private static int port; private static JedisPoolConfig jpc; private static JedisPool jp; static &#123; ResourceBundle bundle = ResourceBundle.getBundle(&quot;redis&quot;); //最大连接数 maxTotal = Integer.parseInt(bundle.getString(&quot;redis.maxTotal&quot;)); //活动连接数 maxIdel = Integer.parseInt(bundle.getString(&quot;redis.maxIdel&quot;)); host = bundle.getString(&quot;redis.host&quot;); port = Integer.parseInt(bundle.getString(&quot;redis.port&quot;)); //Jedis连接配置 jpc = new JedisPoolConfig(); jpc.setMaxTotal(maxTotal); jpc.setMaxIdle(maxIdel); //连接池对象 jp = new JedisPool(jpc, host, port); &#125; //对外访问接口，提供jedis连接对象，连接从连接池获取 public static Jedis getJedis() &#123; return jp.getResource(); &#125;&#125;"},{"title":"并发编程","path":"/wiki/javaNode/Prog.html","content":"JUC进程概述进程：程序是静止的，进程实体的运行过程就是进程，是系统进行资源分配的基本单位 进程的特征：并发性、异步性、动态性、独立性、结构性 线程：线程是属于进程的，是一个基本的 CPU 执行单元，是程序执行流的最小单元。线程是进程中的一个实体，是系统独立调度的基本单位，线程本身不拥有系统资源，只拥有一点在运行中必不可少的资源，与同属一个进程的其他线程共享进程所拥有的全部资源 关系：一个进程可以包含多个线程，这就是多线程，比如看视频是进程，图画、声音、广告等就是多个线程 线程的作用：使多道程序更好的并发执行，提高资源利用率和系统吞吐量，增强操作系统的并发性能 并发并行： 并行：在同一时刻，有多个指令在多个 CPU 上同时执行 并发：在同一时刻，有多个指令在单个 CPU 上交替执行 同步异步： 需要等待结果返回，才能继续运行就是同步 不需要等待结果返回，就能继续运行就是异步 参考视频：https://www.bilibili.com/video/BV16J411h7Rd 笔记的整体结构依据视频编写，并随着学习的深入补充了很多知识 对比线程进程对比： 进程基本上相互独立的，而线程存在于进程内，是进程的一个子集 进程拥有共享的资源，如内存空间等，供其内部的线程共享 进程间通信较为复杂 同一台计算机的进程通信称为 IPC（Inter-process communication） 信号量：信号量是一个计数器，用于多进程对共享数据的访问，解决同步相关的问题并避免竞争条件 共享存储：多个进程可以访问同一块内存空间，需要使用信号量用来同步对共享存储的访问 管道通信：管道是用于连接一个读进程和一个写进程以实现它们之间通信的一个共享文件 pipe 文件，该文件同一时间只允许一个进程访问，所以只支持半双工通信 匿名管道（Pipes）：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信 命名管道（Names Pipes）：以磁盘文件的方式存在，可以实现本机任意两个进程通信，遵循 FIFO 消息队列：内核中存储消息的链表，由消息队列标识符标识，能在不同进程之间提供全双工通信，对比管道： 匿名管道存在于内存中的文件；命名管道存在于实际的磁盘介质或者文件系统；消息队列存放在内核中，只有在内核重启（操作系统重启）或者显示地删除一个消息队列时，该消息队列才被真正删除 读进程可以根据消息类型有选择地接收消息，而不像 FIFO 那样只能默认地接收 不同计算机之间的进程通信，需要通过网络，并遵守共同的协议，例如 HTTP 套接字：与其它通信机制不同的是，可用于不同机器间的互相通信 线程通信相对简单，因为线程之间共享进程内的内存，一个例子是多个线程可以访问同一个共享变量 Java 中的通信机制：volatile、等待&#x2F;通知机制、join 方式、InheritableThreadLocal、MappedByteBuffer 线程更轻量，线程上下文切换成本一般上要比进程上下文切换低 线程创建线程ThreadThread 创建线程方式：创建线程类，匿名内部类方式 start() 方法底层其实是给 CPU 注册当前线程，并且触发 run() 方法执行 线程的启动必须调用 start() 方法，如果线程直接调用 run() 方法，相当于变成了普通类的执行，此时主线程将只有执行该线程 建议线程先创建子线程，主线程的任务放在之后，否则主线程（main）永远是先执行完 Thread 构造器： public Thread() public Thread(String name) 123456789101112131415161718public class ThreadDemo &#123; public static void main(String[] args) &#123; Thread t = new MyThread(); t.start(); for(int i = 0 ; i &lt; 100 ; i++ )&#123; System.out.println(&quot;main线程&quot; + i) &#125; // main线程输出放在上面 就变成有先后顺序了，因为是 main 线程驱动的子线程运行 &#125;&#125;class MyThread extends Thread &#123; @Override public void run() &#123; for(int i = 0 ; i &lt; 100 ; i++ ) &#123; System.out.println(&quot;子线程输出：&quot;+i) &#125; &#125;&#125; 继承 Thread 类的优缺点： 优点：编码简单 缺点：线程类已经继承了 Thread 类无法继承其他类了，功能不能通过继承拓展（单继承的局限性） RunnableRunnable 创建线程方式：创建线程类，匿名内部类方式 Thread 的构造器： public Thread(Runnable target) public Thread(Runnable target, String name) 1234567891011121314151617public class ThreadDemo &#123; public static void main(String[] args) &#123; Runnable target = new MyRunnable(); Thread t1 = new Thread(target,&quot;1号线程&quot;); t1.start(); Thread t2 = new Thread(target);//Thread-0 &#125;&#125;public class MyRunnable implements Runnable&#123; @Override public void run() &#123; for(int i = 0 ; i &lt; 10 ; i++ )&#123; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i); &#125; &#125;&#125; Thread 类本身也是实现了 Runnable 接口，Thread 类中持有 Runnable 的属性，执行线程 run 方法底层是调用 Runnable#run： 12345678910public class Thread implements Runnable &#123; private Runnable target; public void run() &#123; if (target != null) &#123; // 底层调用的是 Runnable 的 run 方法 target.run(); &#125; &#125;&#125; Runnable 方式的优缺点： 缺点：代码复杂一点。 优点： 线程任务类只是实现了 Runnable 接口，可以继续继承其他类，避免了单继承的局限性 同一个线程任务对象可以被包装成多个线程对象 适合多个多个线程去共享同一个资源 实现解耦操作，线程任务代码可以被多个线程共享，线程任务代码和线程独立 线程池可以放入实现 Runnable 或 Callable 线程任务对象 ​ Callable实现 Callable 接口： 定义一个线程任务类实现 Callable 接口，申明线程执行的结果类型 重写线程任务类的 call 方法，这个方法可以直接返回执行的结果 创建一个 Callable 的线程任务对象 把 Callable 的线程任务对象包装成一个未来任务对象 把未来任务对象包装成线程对象 调用线程的 start() 方法启动线程 public FutureTask(Callable&lt;V&gt; callable)：未来任务对象，在线程执行完后得到线程的执行结果 FutureTask 就是 Runnable 对象，因为 Thread 类只能执行 Runnable 实例的任务对象，所以把 Callable 包装成未来任务对象 线程池部分详解了 FutureTask 的源码 public V get()：同步等待 task 执行完毕的结果，如果在线程中获取另一个线程执行结果，会阻塞等待，用于线程同步 get() 线程会阻塞等待任务执行完成 run() 执行完后会把结果设置到 FutureTask 的一个成员变量，get() 线程可以获取到该变量的值 优缺点： 优点：同 Runnable，并且能得到线程执行的结果 缺点：编码复杂 1234567891011121314151617181920public class ThreadDemo &#123; public static void main(String[] args) &#123; Callable call = new MyCallable(); FutureTask&lt;String&gt; task = new FutureTask&lt;&gt;(call); Thread t = new Thread(task); t.start(); try &#123; String s = task.get(); // 获取call方法返回的结果（正常/异常结果） System.out.println(s); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;public class MyCallable implements Callable&lt;String&gt; &#123; @Override//重写线程任务类方法 public String call() throws Exception &#123; return Thread.currentThread().getName() + &quot;-&gt;&quot; + &quot;Hello World&quot;; &#125;&#125; 线程方法APIThread 类 API： 方法 说明 public void start() 启动一个新线程，Java虚拟机调用此线程的 run 方法 public void run() 线程启动后调用该方法 public void setName(String name) 给当前线程取名字 public void getName() 获取当前线程的名字线程存在默认名称：子线程是 Thread-索引，主线程是 main public static Thread currentThread() 获取当前线程对象，代码在哪个线程中执行 public static void sleep(long time) 让当前线程休眠多少毫秒再继续执行Thread.sleep(0) : 让操作系统立刻重新进行一次 CPU 竞争 public static native void yield() 提示线程调度器让出当前线程对 CPU 的使用 public final int getPriority() 返回此线程的优先级 public final void setPriority(int priority) 更改此线程的优先级，常用 1 5 10 public void interrupt() 中断这个线程，异常处理机制 public static boolean interrupted() 判断当前线程是否被打断，清除打断标记 public boolean isInterrupted() 判断当前线程是否被打断，不清除打断标记 public final void join() 等待这个线程结束 public final void join(long millis) 等待这个线程死亡 millis 毫秒，0 意味着永远等待 public final native boolean isAlive() 线程是否存活（还没有运行完毕） public final void setDaemon(boolean on) 将此线程标记为守护线程或用户线程 run startrun：称为线程体，包含了要执行的这个线程的内容，方法运行结束，此线程随即终止。直接调用 run 是在主线程中执行了 run，没有启动新的线程，需要顺序执行 start：使用 start 是启动新的线程，此线程处于就绪（可运行）状态，通过新的线程间接执行 run 中的代码 说明：线程控制资源类 run() 方法中的异常不能抛出，只能 try&#x2F;catch 因为父类中没有抛出任何异常，子类不能比父类抛出更多的异常 异常不能跨线程传播回 main() 中，因此必须在本地进行处理 sleep yieldsleep： 调用 sleep 会让当前线程从 Running 进入 Timed Waiting 状态（阻塞） sleep() 方法的过程中，线程不会释放对象锁 其它线程可以使用 interrupt 方法打断正在睡眠的线程，这时 sleep 方法会抛出 InterruptedException 睡眠结束后的线程未必会立刻得到执行，需要抢占 CPU 建议用 TimeUnit 的 sleep 代替 Thread 的 sleep 来获得更好的可读性 yield： 调用 yield 会让提示线程调度器让出当前线程对 CPU 的使用 具体的实现依赖于操作系统的任务调度器 会放弃 CPU 资源，锁资源不会释放 joinpublic final void join()：等待这个线程结束 原理：调用者轮询检查线程 alive 状态，t1.join() 等价于： 123456public final synchronized void join(long millis) throws InterruptedException &#123; // 调用者线程进入 thread 的 waitSet 等待, 直到当前线程运行结束 while (isAlive()) &#123; wait(0); &#125;&#125; join 方法是被 synchronized 修饰的，本质上是一个对象锁，其内部的 wait 方法调用也是释放锁的，但是释放的是当前的线程对象锁，而不是外面的锁 当调用某个线程（t1）的 join 方法后，该线程（t1）抢占到 CPU 资源，就不再释放，直到线程执行完毕 线程同步： join 实现线程同步，因为会阻塞等待另一个线程的结束，才能继续向下运行 需要外部共享变量，不符合面向对象封装的思想 必须等待线程结束，不能配合线程池使用 Future 实现（同步）：get() 方法阻塞等待执行结果 main 线程接收结果 get 方法是让调用线程同步等待 12345678910111213141516171819public class Test &#123; static int r = 0; public static void main(String[] args) throws InterruptedException &#123; test1(); &#125; private static void test1() throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; r = 10; &#125;); t1.start(); t1.join();//不等待线程执行结束，输出的10 System.out.println(r); &#125;&#125; interrupt打断线程public void interrupt()：打断这个线程，异常处理机制 public static boolean interrupted()：判断当前线程是否被打断，打断返回 true，清除打断标记，连续调用两次一定返回 false public boolean isInterrupted()：判断当前线程是否被打断，不清除打断标记 打断的线程会发生上下文切换，操作系统会保存线程信息，抢占到 CPU 后会从中断的地方接着运行（打断不是停止） sleep、wait、join 方法都会让线程进入阻塞状态，打断线程会清空打断状态（false） 12345678910111213public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(()-&gt;&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;, &quot;t1&quot;); t1.start(); Thread.sleep(500); t1.interrupt(); System.out.println(&quot; 打断状态: &#123;&#125;&quot; + t1.isInterrupted());// 打断状态: &#123;&#125;false&#125; 打断正常运行的线程：不会清空打断状态（true） 123456789101112131415public static void main(String[] args) throws Exception &#123; Thread t2 = new Thread(()-&gt;&#123; while(true) &#123; Thread current = Thread.currentThread(); boolean interrupted = current.isInterrupted(); if(interrupted) &#123; System.out.println(&quot; 打断状态: &#123;&#125;&quot; + interrupted);//打断状态: &#123;&#125;true break; &#125; &#125; &#125;, &quot;t2&quot;); t2.start(); Thread.sleep(500); t2.interrupt();&#125; 打断 parkpark 作用类似 sleep，打断 park 线程，不会清空打断状态（true） 1234567891011public static void main(String[] args) throws Exception &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(&quot;park...&quot;); LockSupport.park(); System.out.println(&quot;unpark...&quot;); System.out.println(&quot;打断状态：&quot; + Thread.currentThread().isInterrupted());//打断状态：true &#125;, &quot;t1&quot;); t1.start(); Thread.sleep(2000); t1.interrupt();&#125; 如果打断标记已经是 true, 则 park 会失效 1234LockSupport.park();System.out.println(&quot;unpark...&quot;);LockSupport.park();//失效，不会阻塞System.out.println(&quot;unpark...&quot;);//和上一个unpark同时执行 可以修改获取打断状态方法，使用 Thread.interrupted()，清除打断标记 LockSupport 类在 同步 → park-un 详解 终止模式终止模式之两阶段终止模式：Two Phase Termination 目标：在一个线程 T1 中如何优雅终止线程 T2？优雅指的是给 T2 一个后置处理器 错误思想： 使用线程对象的 stop() 方法停止线程：stop 方法会真正杀死线程，如果这时线程锁住了共享资源，当它被杀死后就再也没有机会释放锁，其它线程将永远无法获取锁 使用 System.exit(int) 方法停止线程：目的仅是停止一个线程，但这种做法会让整个程序都停止 两阶段终止模式图示： 打断线程可能在任何时间，所以需要考虑在任何时刻被打断的处理方法： 123456789101112131415161718192021222324252627282930313233343536373839public class Test &#123; public static void main(String[] args) throws InterruptedException &#123; TwoPhaseTermination tpt = new TwoPhaseTermination(); tpt.start(); Thread.sleep(3500); tpt.stop(); &#125;&#125;class TwoPhaseTermination &#123; private Thread monitor; // 启动监控线程 public void start() &#123; monitor = new Thread(new Runnable() &#123; @Override public void run() &#123; while (true) &#123; Thread thread = Thread.currentThread(); if (thread.isInterrupted()) &#123; System.out.println(&quot;后置处理&quot;); break; &#125; try &#123; Thread.sleep(1000); // 睡眠 System.out.println(&quot;执行监控记录&quot;);\t// 在此被打断不会异常 &#125; catch (InterruptedException e) &#123; // 在睡眠期间被打断，进入异常处理的逻辑 e.printStackTrace(); // 重新设置打断标记，打断 sleep 会清除打断状态 thread.interrupt(); &#125; &#125; &#125; &#125;); monitor.start(); &#125; // 停止监控线程 public void stop() &#123; monitor.interrupt(); &#125;&#125; daemonpublic final void setDaemon(boolean on)：如果是 true ，将此线程标记为守护线程 线程启动前调用此方法： 123456789Thread t = new Thread() &#123; @Override public void run() &#123; System.out.println(&quot;running&quot;); &#125;&#125;;// 设置该线程为守护线程t.setDaemon(true);t.start(); 用户线程：平常创建的普通线程 守护线程：服务于用户线程，只要其它非守护线程运行结束了，即使守护线程代码没有执行完，也会强制结束。守护进程是脱离于终端并且在后台运行的进程，脱离终端是为了避免在执行的过程中的信息在终端上显示 说明：当运行的线程都是守护线程，Java 虚拟机将退出，因为普通线程执行完后，JVM 是守护线程，不会继续运行下去 常见的守护线程： 垃圾回收器线程就是一种守护线程 Tomcat 中的 Acceptor 和 Poller 线程都是守护线程，所以 Tomcat 接收到 shutdown 命令后，不会等待它们处理完当前请求 不推荐不推荐使用的方法，这些方法已过时，容易破坏同步代码块，造成线程死锁： public final void stop()：停止线程运行 废弃原因：方法粗暴，除非可能执行 finally 代码块以及释放 synchronized 外，线程将直接被终止，如果线程持有 JUC 的互斥锁可能导致锁来不及释放，造成其他线程永远等待的局面 public final void suspend()：挂起（暂停）线程运行 废弃原因：如果目标线程在暂停时对系统资源持有锁，则在目标线程恢复之前没有线程可以访问该资源，如果恢复目标线程的线程在调用 resume 之前会尝试访问此共享资源，则会导致死锁 public final void resume()：恢复线程运行 线程原理运行机制Java Virtual Machine Stacks（Java 虚拟机栈）：每个线程启动后，虚拟机就会为其分配一块栈内存 每个栈由多个栈帧（Frame）组成，对应着每次方法调用时所占用的内存 每个线程只能有一个活动栈帧，对应着当前正在执行的那个方法 线程上下文切换（Thread Context Switch）：一些原因导致 CPU 不再执行当前线程，转而执行另一个线程 线程的 CPU 时间片用完 垃圾回收 有更高优先级的线程需要运行 线程自己调用了 sleep、yield、wait、join、park 等方法 程序计数器（Program Counter Register）：记住下一条 JVM 指令的执行地址，是线程私有的 当 Context Switch 发生时，需要由操作系统保存当前线程的状态（PCB 中），并恢复另一个线程的状态，包括程序计数器、虚拟机栈中每个栈帧的信息，如局部变量、操作数栈、返回地址等 JVM 规范并没有限定线程模型，以 HotSopot 为例： Java 的线程是内核级线程（1:1 线程模型），每个 Java 线程都映射到一个操作系统原生线程，需要消耗一定的内核资源（堆栈） 线程的调度是在内核态运行的，而线程中的代码是在用户态运行，所以线程切换（状态改变）会导致用户与内核态转换进行系统调用，这是非常消耗性能 Java 中 main 方法启动的是一个进程也是一个主线程，main 方法里面的其他线程均为子线程，main 线程是这些线程的父线程 线程调度线程调度指系统为线程分配处理器使用权的过程，方式有两种：协同式线程调度、抢占式线程调度（Java 选择） 协同式线程调度：线程的执行时间由线程本身控制 优点：线程做完任务才通知系统切换到其他线程，相当于所有线程串行执行，不会出现线程同步问题 缺点：线程执行时间不可控，如果代码编写出现问题，可能导致程序一直阻塞，引起系统的奔溃 抢占式线程调度：线程的执行时间由系统分配 优点：线程执行时间可控，不会因为一个线程的问题而导致整体系统不可用 缺点：无法主动为某个线程多分配时间 Java 提供了线程优先级的机制，优先级会提示（hint）调度器优先调度该线程，但这仅仅是一个提示，调度器可以忽略它。在线程的就绪状态时，如果 CPU 比较忙，那么优先级高的线程会获得更多的时间片，但 CPU 闲时，优先级几乎没作用 说明：并不能通过优先级来判断线程执行的先后顺序 未来优化内核级线程调度的成本较大，所以引入了更轻量级的协程。用户线程的调度由用户自己实现（多对一的线程模型，多个用户线程映射到一个内核级线程），被设计为协同式调度，所以叫协程 有栈协程：协程会完整的做调用栈的保护、恢复工作，所以叫有栈协程 无栈协程：本质上是一种有限状态机，状态保存在闭包里，比有栈协程更轻量，但是功能有限 有栈协程中有一种特例叫纤程，在新并发模型中，一段纤程的代码被分为两部分，执行过程和调度器： 执行过程：用于维护执行现场，保护、恢复上下文状态 调度器：负责编排所有要执行的代码顺序 线程状态进程的状态参考操作系统：创建态、就绪态、运行态、阻塞态、终止态 线程由生到死的完整过程（生命周期）：当线程被创建并启动以后，既不是一启动就进入了执行状态，也不是一直处于执行状态，在 API 中 java.lang.Thread.State 这个枚举中给出了六种线程状态： 线程状态 导致状态发生条件 NEW（新建） 线程刚被创建，但是并未启动，还没调用 start 方法，只有线程对象，没有线程特征 Runnable（可运行） 线程可以在 Java 虚拟机中运行的状态，可能正在运行自己代码，也可能没有，这取决于操作系统处理器，调用了 t.start() 方法：就绪（经典叫法） Blocked（阻塞） 当一个线程试图获取一个对象锁，而该对象锁被其他的线程持有，则该线程进入 Blocked 状态；当该线程持有锁时，该线程将变成 Runnable 状态 Waiting（无限等待） 一个线程在等待另一个线程执行一个（唤醒）动作时，该线程进入 Waiting 状态，进入这个状态后不能自动唤醒，必须等待另一个线程调用 notify 或者 notifyAll 方法才能唤醒 Timed Waiting （限期等待） 有几个方法有超时参数，调用将进入 Timed Waiting 状态，这一状态将一直保持到超时期满或者接收到唤醒通知。带有超时参数的常用方法有 Thread.sleep 、Object.wait Teminated（结束） run 方法正常退出而死亡，或者因为没有捕获的异常终止了 run 方法而死亡 NEW → RUNNABLE：当调用 t.start() 方法时，由 NEW → RUNNABLE RUNNABLE &lt;–&gt; WAITING： 调用 obj.wait() 方法时 调用 obj.notify()、obj.notifyAll()、t.interrupt()： 竞争锁成功，t 线程从 WAITING → RUNNABLE 竞争锁失败，t 线程从 WAITING → BLOCKED 当前线程调用 t.join() 方法，注意是当前线程在 t 线程对象的监视器上等待 当前线程调用 LockSupport.park() 方法 RUNNABLE &lt;–&gt; TIMED_WAITING：调用 obj.wait(long n) 方法、当前线程调用 t.join(long n) 方法、当前线程调用 Thread.sleep(long n) RUNNABLE &lt;–&gt; BLOCKED：t 线程用 synchronized(obj) 获取了对象锁时竞争失败 查看线程Windows： 任务管理器可以查看进程和线程数，也可以用来杀死进程 tasklist 查看进程 taskkill 杀死进程 Linux： ps -ef 查看所有进程 ps -fT -p 查看某个进程（PID）的所有线程 kill 杀死进程 top 按大写 H 切换是否显示线程 top -H -p 查看某个进程（PID）的所有线程 Java： jps 命令查看所有 Java 进程 jstack 查看某个 Java 进程（PID）的所有线程状态 jconsole 来查看某个 Java 进程中线程的运行情况（图形界面） 同步临界区临界资源：一次仅允许一个进程使用的资源成为临界资源 临界区：访问临界资源的代码块 竞态条件：多个线程在临界区内执行，由于代码的执行序列不同而导致结果无法预测，称之为发生了竞态条件 一个程序运行多个线程是没有问题，多个线程读共享资源也没有问题，在多个线程对共享资源读写操作时发生指令交错，就会出现问题 为了避免临界区的竞态条件发生（解决线程安全问题）： 阻塞式的解决方案：synchronized，lock 非阻塞式的解决方案：原子变量 管程（monitor）：由局部于自己的若干公共变量和所有访问这些公共变量的过程所组成的软件模块，保证同一时刻只有一个进程在管程内活动，即管程内定义的操作在同一时刻只被一个进程调用（由编译器实现） synchronized：对象锁，保证了临界区内代码的原子性，采用互斥的方式让同一时刻至多只有一个线程能持有对象锁，其它线程获取这个对象锁时会阻塞，保证拥有锁的线程可以安全的执行临界区内的代码，不用担心线程上下文切换 互斥和同步都可以采用 synchronized 关键字来完成，区别： 互斥是保证临界区的竞态条件发生，同一时刻只能有一个线程执行临界区代码 同步是由于线程执行的先后、顺序不同、需要一个线程等待其它线程运行到某个点 性能： 线程安全，性能差 线程不安全性能好，假如开发中不会存在多线程安全问题，建议使用线程不安全的设计类 syn-ed使用锁同步块锁对象：理论上可以是任意的唯一对象 synchronized 是可重入、不公平的重量级锁 原则上： 锁对象建议使用共享资源 在实例方法中使用 this 作为锁对象，锁住的 this 正好是共享资源 在静态方法中使用类名 .class 字节码作为锁对象，因为静态成员属于类，被所有实例对象共享，所以需要锁住类 同步代码块格式： 123synchronized(锁对象)&#123;\t// 访问共享资源的核心代码&#125; 实例： 1234567891011121314151617181920212223242526public class demo &#123; static int counter = 0; //static修饰，则元素是属于类本身的，不属于对象 ，与类一起加载一次，只有一个 static final Object room = new Object(); public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 5000; i++) &#123; synchronized (room) &#123; counter++; &#125; &#125; &#125;, &quot;t1&quot;); Thread t2 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 5000; i++) &#123; synchronized (room) &#123; counter--; &#125; &#125; &#125;, &quot;t2&quot;); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(counter); &#125;&#125; 同步方法把出现线程安全问题的核心方法锁起来，每次只能一个线程进入访问 synchronized 修饰的方法的不具备继承性，所以子类是线程不安全的，如果子类的方法也被 synchronized 修饰，两个锁对象其实是一把锁，而且是子类对象作为锁 用法：直接给方法加上一个修饰符 synchronized 12345678//同步方法修饰符 synchronized 返回值类型 方法名(方法参数) &#123; 方法体；&#125;//同步静态方法修饰符 static synchronized 返回值类型 方法名(方法参数) &#123; 方法体；&#125; 同步方法底层也是有锁对象的： 如果方法是实例方法：同步方法默认用 this 作为的锁对象 1234public synchronized void test() &#123;&#125; //等价于public void test() &#123; synchronized(this) &#123;&#125;&#125; 如果方法是静态方法：同步方法默认用类名 .class 作为的锁对象 123456789class Test&#123;\tpublic synchronized static void test() &#123;&#125;&#125;//等价于class Test&#123; public void test() &#123; synchronized(Test.class) &#123;&#125;\t&#125;&#125; 线程八锁线程八锁就是考察 synchronized 锁住的是哪个对象，直接百度搜索相关的实例 说明：主要关注锁住的对象是不是同一个 锁住类对象，所有类的实例的方法都是安全的，类的所有实例都相当于同一把锁 锁住 this 对象，只有在当前实例对象的线程内是安全的，如果有多个实例就不安全 线程不安全：因为锁住的不是同一个对象，线程 1 调用 a 方法锁住的类对象，线程 2 调用 b 方法锁住的 n2 对象，不是同一个对象 123456789101112131415class Number&#123; public static synchronized void a()&#123; Thread.sleep(1000); System.out.println(&quot;1&quot;); &#125; public synchronized void b() &#123; System.out.println(&quot;2&quot;); &#125;&#125;public static void main(String[] args) &#123; Number n1 = new Number(); Number n2 = new Number(); new Thread(()-&gt;&#123; n1.a(); &#125;).start(); new Thread(()-&gt;&#123; n2.b(); &#125;).start();&#125; 线程安全：因为 n1 调用 a() 方法，锁住的是类对象，n2 调用 b() 方法，锁住的也是类对象，所以线程安全 123456789101112131415class Number&#123; public static synchronized void a()&#123; Thread.sleep(1000); System.out.println(&quot;1&quot;); &#125; public static synchronized void b() &#123; System.out.println(&quot;2&quot;); &#125;&#125;public static void main(String[] args) &#123; Number n1 = new Number(); Number n2 = new Number(); new Thread(()-&gt;&#123; n1.a(); &#125;).start(); new Thread(()-&gt;&#123; n2.b(); &#125;).start();&#125; 锁原理MonitorMonitor 被翻译为监视器或管程 每个 Java 对象都可以关联一个 Monitor 对象，Monitor 也是 class，其实例存储在堆中，如果使用 synchronized 给对象上锁（重量级）之后，该对象头的 Mark Word 中就被设置指向 Monitor 对象的指针，这就是重量级锁 Mark Word 结构：最后两位是锁标志位 64 位虚拟机 Mark Word： 工作流程： 开始时 Monitor 中 Owner 为 null 当 Thread-2 执行 synchronized(obj) 就会将 Monitor 的所有者 Owner 置为 Thread-2，Monitor 中只能有一个 Owner，obj 对象的 Mark Word 指向 Monitor，把对象原有的 MarkWord 存入线程栈中的锁记录中（轻量级锁部分详解） 在 Thread-2 上锁的过程，Thread-3、Thread-4、Thread-5 也执行 synchronized(obj)，就会进入 EntryList BLOCKED（双向链表） Thread-2 执行完同步代码块的内容，根据 obj 对象头中 Monitor 地址寻找，设置 Owner 为空，把线程栈的锁记录中的对象头的值设置回 MarkWord 唤醒 EntryList 中等待的线程来竞争锁，竞争是非公平的，如果这时有新的线程想要获取锁，可能直接就抢占到了，阻塞队列的线程就会继续阻塞 WaitSet 中的 Thread-0，是以前获得过锁，但条件不满足进入 WAITING 状态的线程（wait-notify 机制） 注意： synchronized 必须是进入同一个对象的 Monitor 才有上述的效果 不加 synchronized 的对象不会关联监视器，不遵从以上规则 字节码代码： 123456public static void main(String[] args) &#123; Object lock = new Object(); synchronized (lock) &#123; System.out.println(&quot;ok&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728290: new #2 // new Object3: dup4: invokespecial #1 // invokespecial &lt;init&gt;:()V，非虚方法7: astore_1 // lock引用 -&gt; lock8: aload_1 // lock （synchronized开始）9: dup // 一份用来初始化，一份用来引用10: astore_2 // lock引用 -&gt; slot 211: monitorenter // 【将 lock对象 MarkWord 置为 Monitor 指针】12: getstatic #3 // System.out15: ldc #4 // &quot;ok&quot;17: invokevirtual #5 // invokevirtual println:(Ljava/lang/String;)V20: aload_2 // slot 2(lock引用)21: monitorexit // 【将 lock对象 MarkWord 重置, 唤醒 EntryList】22: goto 3025: astore_3 // any -&gt; slot 326: aload_2 // slot 2(lock引用)27: monitorexit // 【将 lock对象 MarkWord 重置, 唤醒 EntryList】28: aload_329: athrow30: returnException table: from to target type 12 22 25 any 25 28 25 anyLineNumberTable: ...LocalVariableTable: Start Length Slot Name Signature 0 31 0 args [Ljava/lang/String; 8 23 1 lock Ljava/lang/Object; 说明： 通过异常 try-catch 机制，确保一定会被解锁 方法级别的 synchronized 不会在字节码指令中有所体现 锁升级升级过程synchronized 是可重入、不公平的重量级锁，所以可以对其进行优化 1无锁 -&gt; 偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁\t// 随着竞争的增加，只能锁升级，不能降级 偏向锁偏向锁的思想是偏向于让第一个获取锁对象的线程，这个线程之后重新获取该锁不再需要同步操作： 当锁对象第一次被线程获得的时候进入偏向状态，标记为 101，同时使用 CAS 操作将线程 ID 记录到 Mark Word。如果 CAS 操作成功，这个线程以后进入这个锁相关的同步块，查看这个线程 ID 是自己的就表示没有竞争，就不需要再进行任何同步操作 当有另外一个线程去尝试获取这个锁对象时，偏向状态就宣告结束，此时撤销偏向（Revoke Bias）后恢复到未锁定或轻量级锁状态 一个对象创建时： 如果开启了偏向锁（默认开启），那么对象创建后，MarkWord 值为 0x05 即最后 3 位为 101，thread、epoch、age 都为 0 偏向锁是默认是延迟的，不会在程序启动时立即生效，如果想避免延迟，可以加 VM 参数 -XX:BiasedLockingStartupDelay=0 来禁用延迟。JDK 8 延迟 4s 开启偏向锁原因：在刚开始执行代码时，会有好多线程来抢锁，如果开偏向锁效率反而降低 当一个对象已经计算过 hashCode，就再也无法进入偏向状态了 添加 VM 参数 -XX:-UseBiasedLocking 禁用偏向锁 撤销偏向锁的状态： 调用对象的 hashCode：偏向锁的对象 MarkWord 中存储的是线程 id，调用 hashCode 导致偏向锁被撤销 当有其它线程使用偏向锁对象时，会将偏向锁升级为轻量级锁 调用 wait&#x2F;notify，需要申请 Monitor，进入 WaitSet 批量撤销：如果对象被多个线程访问，但没有竞争，这时偏向了线程 T1 的对象仍有机会重新偏向 T2，重偏向会重置对象的 Thread ID 批量重偏向：当撤销偏向锁阈值超过 20 次后，JVM 会觉得是不是偏向错了，于是在给这些对象加锁时重新偏向至加锁线程 批量撤销：当撤销偏向锁阈值超过 40 次后，JVM 会觉得自己确实偏向错了，根本就不该偏向，于是整个类的所有对象都会变为不可偏向的，新建的对象也是不可偏向的 轻量级锁一个对象有多个线程要加锁，但加锁的时间是错开的（没有竞争），可以使用轻量级锁来优化，轻量级锁对使用者是透明的（不可见） 可重入锁：线程可以进入任何一个它已经拥有的锁所同步着的代码块，可重入锁最大的作用是避免死锁 轻量级锁在没有竞争时（锁重入时），每次重入仍然需要执行 CAS 操作，Java 6 才引入的偏向锁来优化 锁重入实例： 123456789101112static final Object obj = new Object();public static void method1() &#123; synchronized( obj ) &#123; // 同步块 A method2(); &#125;&#125;public static void method2() &#123; synchronized( obj ) &#123; // 同步块 B &#125;&#125; 创建锁记录（Lock Record）对象，每个线程的栈帧都会包含一个锁记录的结构，存储锁定对象的 Mark Word 让锁记录中 Object reference 指向锁住的对象，并尝试用 CAS 替换 Object 的 Mark Word，将 Mark Word 的值存入锁记录 如果 CAS 替换成功，对象头中存储了锁记录地址和状态 00（轻量级锁） ，表示由该线程给对象加锁 如果 CAS 失败，有两种情况： 如果是其它线程已经持有了该 Object 的轻量级锁，这时表明有竞争，进入锁膨胀过程 如果是线程自己执行了 synchronized 锁重入，就添加一条 Lock Record 作为重入的计数 当退出 synchronized 代码块（解锁时） 如果有取值为 null 的锁记录，表示有重入，这时重置锁记录，表示重入计数减 1 如果锁记录的值不为 null，这时使用 CAS 将 Mark Word 的值恢复给对象头 成功，则解锁成功 失败，说明轻量级锁进行了锁膨胀或已经升级为重量级锁，进入重量级锁解锁流程 锁膨胀在尝试加轻量级锁的过程中，CAS 操作无法成功，可能是其它线程为此对象加上了轻量级锁（有竞争），这时需要进行锁膨胀，将轻量级锁变为重量级锁 当 Thread-1 进行轻量级加锁时，Thread-0 已经对该对象加了轻量级锁 Thread-1 加轻量级锁失败，进入锁膨胀流程：为 Object 对象申请 Monitor 锁，通过 Object 对象头获取到持锁线程，将 Monitor 的 Owner 置为 Thread-0，将 Object 的对象头指向重量级锁地址，然后自己进入 Monitor 的 EntryList BLOCKED 当 Thread-0 退出同步块解锁时，使用 CAS 将 Mark Word 的值恢复给对象头失败，这时进入重量级解锁流程，即按照 Monitor 地址找到 Monitor 对象，设置 Owner 为 null，唤醒 EntryList 中 BLOCKED 线程 锁优化自旋锁重量级锁竞争时，尝试获取锁的线程不会立即阻塞，可以使用自旋（默认 10 次）来进行优化，采用循环的方式去尝试获取锁 注意： 自旋占用 CPU 时间，单核 CPU 自旋就是浪费时间，因为同一时刻只能运行一个线程，多核 CPU 自旋才能发挥优势 自旋失败的线程会进入阻塞状态 优点：不会进入阻塞状态，减少线程上下文切换的消耗 缺点：当自旋的线程越来越多时，会不断的消耗 CPU 资源 自旋锁情况： 自旋成功的情况： 自旋失败的情况： 自旋锁说明： 在 Java 6 之后自旋锁是自适应的，比如对象刚刚的一次自旋操作成功过，那么认为这次自旋成功的可能性会高，就多自旋几次；反之，就少自旋甚至不自旋，比较智能 Java 7 之后不能控制是否开启自旋功能，由 JVM 控制 123456789101112131415161718192021222324252627282930313233343536373839404142434445//手写自旋锁public class SpinLock &#123; // 泛型装的是Thread，原子引用线程 AtomicReference&lt;Thread&gt; atomicReference = new AtomicReference&lt;&gt;(); public void lock() &#123; Thread thread = Thread.currentThread(); System.out.println(thread.getName() + &quot; come in&quot;); //开始自旋，期望值为null，更新值是当前线程 while (!atomicReference.compareAndSet(null, thread)) &#123; Thread.sleep(1000); System.out.println(thread.getName() + &quot; 正在自旋&quot;); &#125; System.out.println(thread.getName() + &quot; 自旋成功&quot;); &#125; public void unlock() &#123; Thread thread = Thread.currentThread(); //线程使用完锁把引用变为null atomicReference.compareAndSet(thread, null); System.out.println(thread.getName() + &quot; invoke unlock&quot;); &#125; public static void main(String[] args) throws InterruptedException &#123; SpinLock lock = new SpinLock(); new Thread(() -&gt; &#123; //占有锁 lock.lock(); Thread.sleep(10000); //释放锁 lock.unlock(); &#125;,&quot;t1&quot;).start(); // 让main线程暂停1秒，使得t1线程，先执行 Thread.sleep(1000); new Thread(() -&gt; &#123; lock.lock(); lock.unlock(); &#125;,&quot;t2&quot;).start(); &#125;&#125; 锁消除锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除，这是 JVM 即时编译器的优化 锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除（同步消除：JVM 逃逸分析） 锁粗化对相同对象多次加锁，导致线程发生多次重入，频繁的加锁操作就会导致性能损耗，可以使用锁粗化方式优化 如果虚拟机探测到一串的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部 一些看起来没有加锁的代码，其实隐式的加了很多锁： 123public static String concatString(String s1, String s2, String s3) &#123; return s1 + s2 + s3;&#125; String 是一个不可变的类，编译器会对 String 的拼接自动优化。在 JDK 1.5 之前，转化为 StringBuffer 对象的连续 append() 操作，每个 append() 方法中都有一个同步块 1234567public static String concatString(String s1, String s2, String s3) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString();&#125; 扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，只需要加锁一次就可以 多把锁多把不相干的锁：一间大屋子有两个功能睡觉、学习，互不相干。现在一人要学习，一人要睡觉，如果只用一间屋子（一个对象锁）的话，那么并发度很低 将锁的粒度细分： 好处，是可以增强并发度 坏处，如果一个线程需要同时获得多把锁，就容易发生死锁 解决方法：准备多个对象锁 1234567891011121314151617181920212223public static void main(String[] args) &#123; BigRoom bigRoom = new BigRoom(); new Thread(() -&gt; &#123; bigRoom.study(); &#125;).start(); new Thread(() -&gt; &#123; bigRoom.sleep(); &#125;).start();&#125;class BigRoom &#123; private final Object studyRoom = new Object(); private final Object sleepRoom = new Object(); public void sleep() throws InterruptedException &#123; synchronized (sleepRoom) &#123; System.out.println(&quot;sleeping 2 小时&quot;); Thread.sleep(2000); &#125; &#125; public void study() throws InterruptedException &#123; synchronized (studyRoom) &#123; System.out.println(&quot;study 1 小时&quot;); Thread.sleep(1000); &#125; &#125;&#125; 活跃性死锁形成死锁：多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放，由于线程被无限期地阻塞，因此程序不可能正常终止 Java 死锁产生的四个必要条件： 互斥条件，即当资源被一个线程使用（占有）时，别的线程不能使用 不可剥夺条件，资源请求者不能强制从资源占有者手中夺取资源，资源只能由资源占有者主动释放 请求和保持条件，即当资源请求者在请求其他的资源的同时保持对原有资源的占有 循环等待条件，即存在一个等待循环队列：p1 要 p2 的资源，p2 要 p1 的资源，形成了一个等待环路 四个条件都成立的时候，便形成死锁。死锁情况下打破上述任何一个条件，便可让死锁消失 1234567891011121314151617181920212223242526public class Dead &#123; public static Object resources1 = new Object(); public static Object resources2 = new Object(); public static void main(String[] args) &#123; new Thread(() -&gt; &#123; // 线程1：占用资源1 ，请求资源2 synchronized(resources1)&#123; System.out.println(&quot;线程1已经占用了资源1，开始请求资源2&quot;); Thread.sleep(2000);//休息两秒，防止线程1直接运行完成。 //2秒内线程2肯定可以锁住资源2 synchronized (resources2)&#123; System.out.println(&quot;线程1已经占用了资源2&quot;); &#125; &#125;).start(); new Thread(() -&gt; &#123; // 线程2：占用资源2 ，请求资源1 synchronized(resources2)&#123; System.out.println(&quot;线程2已经占用了资源2，开始请求资源1&quot;); Thread.sleep(2000); synchronized (resources1)&#123; System.out.println(&quot;线程2已经占用了资源1&quot;); &#125; &#125;&#125; &#125;).start(); &#125;&#125; 定位定位死锁的方法： 使用 jps 定位进程 id，再用 jstack id 定位死锁，找到死锁的线程去查看源码，解决优化 1234567891011121314151617181920212223242526272829&quot;Thread-1&quot; #12 prio=5 os_prio=0 tid=0x000000001eb69000 nid=0xd40 waiting formonitor entry [0x000000001f54f000]\tjava.lang.Thread.State: BLOCKED (on object monitor)#省略 &quot;Thread-1&quot; #12 prio=5 os_prio=0 tid=0x000000001eb69000 nid=0xd40 waiting for monitor entry [0x000000001f54f000]\tjava.lang.Thread.State: BLOCKED (on object monitor)#省略Found one Java-level deadlock:===================================================&quot;Thread-1&quot;: waiting to lock monitor 0x000000000361d378 (object 0x000000076b5bf1c0, a java.lang.Object), which is held by &quot;Thread-0&quot;&quot;Thread-0&quot;: waiting to lock monitor 0x000000000361e768 (object 0x000000076b5bf1d0, a java.lang.Object), which is held by &quot;Thread-1&quot; Java stack information for the threads listed above:===================================================&quot;Thread-1&quot;: at thread.TestDeadLock.lambda$main$1(TestDeadLock.java:28) - waiting to lock &lt;0x000000076b5bf1c0&gt; (a java.lang.Object) - locked &lt;0x000000076b5bf1d0&gt; (a java.lang.Object) at thread.TestDeadLock$$Lambda$2/883049899.run(Unknown Source) at java.lang.Thread.run(Thread.java:745)&quot;Thread-0&quot;: at thread.TestDeadLock.lambda$main$0(TestDeadLock.java:15) - waiting to lock &lt;0x000000076b5bf1d0&gt; (a java.lang.Object) - locked &lt;0x000000076b5bf1c0&gt; (a java.lang.Object) at thread.TestDeadLock$$Lambda$1/495053715 Linux 下可以通过 top 先定位到 CPU 占用高的 Java 进程，再利用 top -Hp 进程id 来定位是哪个线程，最后再用 jstack 的输出来看各个线程栈 避免死锁：避免死锁要注意加锁顺序 可以使用 jconsole 工具，在 jdk\\bin 目录下 活锁活锁：指的是任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试—失败—尝试—失败的过程 两个线程互相改变对方的结束条件，最后谁也无法结束： 12345678910111213141516171819202122class TestLiveLock &#123; static volatile int count = 10; static final Object lock = new Object(); public static void main(String[] args) &#123; new Thread(() -&gt; &#123; // 期望减到 0 退出循环 while (count &gt; 0) &#123; Thread.sleep(200); count--; System.out.println(&quot;线程一count:&quot; + count); &#125; &#125;, &quot;t1&quot;).start(); new Thread(() -&gt; &#123; // 期望超过 20 退出循环 while (count &lt; 20) &#123; Thread.sleep(200); count++; System.out.println(&quot;线程二count:&quot;+ count); &#125; &#125;, &quot;t2&quot;).start(); &#125;&#125; 饥饿饥饿：一个线程由于优先级太低，始终得不到 CPU 调度执行，也不能够结束 wait-ify基本使用需要获取对象锁后才可以调用 锁对象.wait()，notify 随机唤醒一个线程，notifyAll 唤醒所有线程去竞争 CPU Object 类 API： 1234public final void notify():唤醒正在等待对象监视器的单个线程。public final void notifyAll():唤醒正在等待对象监视器的所有线程。public final void wait():导致当前线程等待，直到另一个线程调用该对象的 notify() 方法或 notifyAll()方法。public final native void wait(long timeout):有时限的等待, 到n毫秒后结束等待，或是被唤醒 说明：wait 是挂起线程，需要唤醒的都是挂起操作，阻塞线程可以自己去争抢锁，挂起的线程需要唤醒后去争抢锁 对比 sleep()： 原理不同：sleep() 方法是属于 Thread 类，是线程用来控制自身流程的，使此线程暂停执行一段时间而把执行机会让给其他线程；wait() 方法属于 Object 类，用于线程间通信 对锁的处理机制不同：调用 sleep() 方法的过程中，线程不会释放对象锁，当调用 wait() 方法的时候，线程会放弃对象锁，进入等待此对象的等待锁定池（不释放锁其他线程怎么抢占到锁执行唤醒操作），但是都会释放 CPU 使用区域不同：wait() 方法必须放在同步控制方法和同步代码块（先获取锁）中使用，sleep() 方法则可以放在任何地方使用 底层原理： Owner 线程发现条件不满足，调用 wait 方法，即可进入 WaitSet 变为 WAITING 状态 BLOCKED 和 WAITING 的线程都处于阻塞状态，不占用 CPU 时间片 BLOCKED 线程会在 Owner 线程释放锁时唤醒 WAITING 线程会在 Owner 线程调用 notify 或 notifyAll 时唤醒，唤醒后并不意味者立刻获得锁，需要进入 EntryList 重新竞争 代码优化虚假唤醒：notify 只能随机唤醒一个 WaitSet 中的线程，这时如果有其它线程也在等待，那么就可能唤醒不了正确的线程 解决方法：采用 notifyAll notifyAll 仅解决某个线程的唤醒问题，使用 if + wait 判断仅有一次机会，一旦条件不成立，无法重新判断 解决方法：用 while + wait，当条件不成立，再次 wait 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Slf4j(topic = &quot;c.demo&quot;)public class demo &#123; static final Object room = new Object(); static boolean hasCigarette = false; //有没有烟 static boolean hasTakeout = false; public static void main(String[] args) throws InterruptedException &#123; new Thread(() -&gt; &#123; synchronized (room) &#123; log.debug(&quot;有烟没？[&#123;&#125;]&quot;, hasCigarette); while (!hasCigarette) &#123;//while防止虚假唤醒 log.debug(&quot;没烟，先歇会！&quot;); try &#123; room.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;有烟没？[&#123;&#125;]&quot;, hasCigarette); if (hasCigarette) &#123; log.debug(&quot;可以开始干活了&quot;); &#125; else &#123; log.debug(&quot;没干成活...&quot;); &#125; &#125; &#125;, &quot;小南&quot;).start(); new Thread(() -&gt; &#123; synchronized (room) &#123; Thread thread = Thread.currentThread(); log.debug(&quot;外卖送到没？[&#123;&#125;]&quot;, hasTakeout); if (!hasTakeout) &#123; log.debug(&quot;没外卖，先歇会！&quot;); try &#123; room.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;外卖送到没？[&#123;&#125;]&quot;, hasTakeout); if (hasTakeout) &#123; log.debug(&quot;可以开始干活了&quot;); &#125; else &#123; log.debug(&quot;没干成活...&quot;); &#125; &#125; &#125;, &quot;小女&quot;).start(); Thread.sleep(1000); new Thread(() -&gt; &#123; // 这里能不能加 synchronized (room)？ synchronized (room) &#123; hasTakeout = true; //log.debug(&quot;烟到了噢！&quot;); log.debug(&quot;外卖到了噢！&quot;); room.notifyAll(); &#125; &#125;, &quot;送外卖的&quot;).start(); &#125;&#125; park-unLockSupport 是用来创建锁和其他同步类的线程原语 LockSupport 类方法： LockSupport.park()：暂停当前线程，挂起原语 LockSupport.unpark(暂停的线程对象)：恢复某个线程的运行 1234567891011121314public static void main(String[] args) &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(&quot;start...&quot;);\t//1 Thread.sleep(1000);// Thread.sleep(3000) // 先 park 再 unpark 和先 unpark 再 park 效果一样，都会直接恢复线程的运行 System.out.println(&quot;park...&quot;);\t//2 LockSupport.park(); System.out.println(&quot;resume...&quot;);//4 &#125;,&quot;t1&quot;); t1.start(); Thread.sleep(2000); System.out.println(&quot;unpark...&quot;);\t//3 LockSupport.unpark(t1);&#125; LockSupport 出现就是为了增强 wait &amp; notify 的功能： wait，notify 和 notifyAll 必须配合 Object Monitor 一起使用，而 park、unpark 不需要 park &amp; unpark 以线程为单位来阻塞和唤醒线程，而 notify 只能随机唤醒一个等待线程，notifyAll 是唤醒所有等待线程 park &amp; unpark 可以先 unpark，而 wait &amp; notify 不能先 notify。类比生产消费，先消费发现有产品就消费，没有就等待；先生产就直接产生商品，然后线程直接消费 wait 会释放锁资源进入等待队列，park 不会释放锁资源，只负责阻塞当前线程，会释放 CPU 原理：类似生产者消费者 先 park： 当前线程调用 Unsafe.park() 方法 检查 _counter ，本情况为 0，这时获得 _mutex 互斥锁 线程进入 _cond 条件变量挂起 调用 Unsafe.unpark(Thread_0) 方法，设置 _counter 为 1 唤醒 _cond 条件变量中的 Thread_0，Thread_0 恢复运行，设置 _counter 为 0 先 unpark： 调用 Unsafe.unpark(Thread_0) 方法，设置 _counter 为 1 当前线程调用 Unsafe.park() 方法 检查 _counter ，本情况为 1，这时线程无需挂起，继续运行，设置 _counter 为 0 安全分析成员变量和静态变量： 如果它们没有共享，则线程安全 如果它们被共享了，根据它们的状态是否能够改变，分两种情况： 如果只有读操作，则线程安全 如果有读写操作，则这段代码是临界区，需要考虑线程安全问题 局部变量： 局部变量是线程安全的 局部变量引用的对象不一定线程安全（逃逸分析）： 如果该对象没有逃离方法的作用访问，它是线程安全的（每一个方法有一个栈帧） 如果该对象逃离方法的作用范围，需要考虑线程安全问题（暴露引用） 常见线程安全类：String、Integer、StringBuffer、Random、Vector、Hashtable、java.util.concurrent 包 线程安全的是指，多个线程调用它们同一个实例的某个方法时，是线程安全的 每个方法是原子的，但多个方法的组合不是原子的，只能保证调用的方法内部安全： 12345Hashtable table = new Hashtable();// 线程1，线程2if(table.get(&quot;key&quot;) == null) &#123;\ttable.put(&quot;key&quot;, value);&#125; 无状态类线程安全，就是没有成员变量的类 不可变类线程安全：String、Integer 等都是不可变类，内部的状态不可以改变，所以方法是线程安全 replace 等方法底层是新建一个对象，复制过去 12345Map&lt;String,Object&gt; map = new HashMap&lt;&gt;();\t// 线程不安全String S1 = &quot;...&quot;; // 线程安全final String S2 = &quot;...&quot;; // 线程安全Date D1 = new Date(); // 线程不安全final Date D2 = new Date(); // 线程不安全，final让D2引用的对象不能变，但对象的内容可以变 抽象方法如果有参数，被重写后行为不确定可能造成线程不安全，被称之为外星方法：public abstract foo(Student s); 同步模式保护性暂停单任务版Guarded Suspension，用在一个线程等待另一个线程的执行结果 有一个结果需要从一个线程传递到另一个线程，让它们关联同一个 GuardedObject 如果有结果不断从一个线程到另一个线程那么可以使用消息队列（见生产者&#x2F;消费者） JDK 中，join 的实现、Future 的实现，采用的就是此模式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public static void main(String[] args) &#123; GuardedObject object = new GuardedObjectV2(); new Thread(() -&gt; &#123; sleep(1); object.complete(Arrays.asList(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)); &#125;).start(); Object response = object.get(2500); if (response != null) &#123; log.debug(&quot;get response: [&#123;&#125;] lines&quot;, ((List&lt;String&gt;) response).size()); &#125; else &#123; log.debug(&quot;can&#x27;t get response&quot;); &#125;&#125;class GuardedObject &#123; private Object response; private final Object lock = new Object(); //获取结果 //timeout :最大等待时间 public Object get(long millis) &#123; synchronized (lock) &#123; // 1) 记录最初时间 long begin = System.currentTimeMillis(); // 2) 已经经历的时间 long timePassed = 0; while (response == null) &#123; // 4) 假设 millis 是 1000，结果在 400 时唤醒了，那么还有 600 要等 long waitTime = millis - timePassed; log.debug(&quot;waitTime: &#123;&#125;&quot;, waitTime); //经历时间超过最大等待时间退出循环 if (waitTime &lt;= 0) &#123; log.debug(&quot;break...&quot;); break; &#125; try &#123; lock.wait(waitTime); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 3) 如果提前被唤醒，这时已经经历的时间假设为 400 timePassed = System.currentTimeMillis() - begin; log.debug(&quot;timePassed: &#123;&#125;, object is null &#123;&#125;&quot;, timePassed, response == null); &#125; return response; &#125; &#125; //产生结果 public void complete(Object response) &#123; synchronized (lock) &#123; // 条件满足，通知等待线程 this.response = response; log.debug(&quot;notify...&quot;); lock.notifyAll(); &#125; &#125;&#125; 多任务版多任务版保护性暂停： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 3; i++) &#123; new People().start(); &#125; Thread.sleep(1000); for (Integer id : Mailboxes.getIds()) &#123; new Postman(id, id + &quot;号快递到了&quot;).start(); &#125;&#125;@Slf4j(topic = &quot;c.People&quot;)class People extends Thread&#123; @Override public void run() &#123; // 收信 GuardedObject guardedObject = Mailboxes.createGuardedObject(); log.debug(&quot;开始收信i d:&#123;&#125;&quot;, guardedObject.getId()); Object mail = guardedObject.get(5000); log.debug(&quot;收到信id:&#123;&#125;，内容:&#123;&#125;&quot;, guardedObject.getId(),mail); &#125;&#125;class Postman extends Thread&#123; private int id; private String mail; //构造方法 @Override public void run() &#123; GuardedObject guardedObject = Mailboxes.getGuardedObject(id); log.debug(&quot;开始送信i d:&#123;&#125;，内容:&#123;&#125;&quot;, guardedObject.getId(),mail); guardedObject.complete(mail); &#125;&#125;class Mailboxes &#123; private static Map&lt;Integer, GuardedObject&gt; boxes = new Hashtable&lt;&gt;(); private static int id = 1; //产生唯一的id private static synchronized int generateId() &#123; return id++; &#125; public static GuardedObject getGuardedObject(int id) &#123; return boxes.remove(id); &#125; public static GuardedObject createGuardedObject() &#123; GuardedObject go = new GuardedObject(generateId()); boxes.put(go.getId(), go); return go; &#125; public static Set&lt;Integer&gt; getIds() &#123; return boxes.keySet(); &#125;&#125;class GuardedObject &#123; //标识，Guarded Object private int id;//添加get set方法&#125; 顺序输出顺序输出 2 1 1234567891011121314151617181920public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; while (true) &#123; //try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; // 当没有许可时，当前线程暂停运行；有许可时，用掉这个许可，当前线程恢复运行 LockSupport.park(); System.out.println(&quot;1&quot;); &#125; &#125;); Thread t2 = new Thread(() -&gt; &#123; while (true) &#123; System.out.println(&quot;2&quot;); // 给线程 t1 发放『许可』（多次连续调用 unpark 只会发放一个『许可』） LockSupport.unpark(t1); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125;); t1.start(); t2.start();&#125; 交替输出连续输出 5 次 abc 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class day2_14 &#123; public static void main(String[] args) throws InterruptedException &#123; AwaitSignal awaitSignal = new AwaitSignal(5); Condition a = awaitSignal.newCondition(); Condition b = awaitSignal.newCondition(); Condition c = awaitSignal.newCondition(); new Thread(() -&gt; &#123; awaitSignal.print(&quot;a&quot;, a, b); &#125;).start(); new Thread(() -&gt; &#123; awaitSignal.print(&quot;b&quot;, b, c); &#125;).start(); new Thread(() -&gt; &#123; awaitSignal.print(&quot;c&quot;, c, a); &#125;).start(); Thread.sleep(1000); awaitSignal.lock(); try &#123; a.signal(); &#125; finally &#123; awaitSignal.unlock(); &#125; &#125;&#125;class AwaitSignal extends ReentrantLock &#123; private int loopNumber; public AwaitSignal(int loopNumber) &#123; this.loopNumber = loopNumber; &#125; //参数1：打印内容 参数二：条件变量 参数二：唤醒下一个 public void print(String str, Condition condition, Condition next) &#123; for (int i = 0; i &lt; loopNumber; i++) &#123; lock(); try &#123; condition.await(); System.out.print(str); next.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; unlock(); &#125; &#125; &#125;&#125; 异步模式传统版异步模式之生产者&#x2F;消费者： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class ShareData &#123; private int number = 0; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void increment() throws Exception&#123; // 同步代码块，加锁 lock.lock(); try &#123; // 判断 防止虚假唤醒 while(number != 0) &#123; // 等待不能生产 condition.await(); &#125; // 干活 number++; System.out.println(Thread.currentThread().getName() + &quot;\\t &quot; + number); // 通知 唤醒 condition.signalAll(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void decrement() throws Exception&#123; // 同步代码块，加锁 lock.lock(); try &#123; // 判断 防止虚假唤醒 while(number == 0) &#123; // 等待不能消费 condition.await(); &#125; // 干活 number--; System.out.println(Thread.currentThread().getName() + &quot;\\t &quot; + number); // 通知 唤醒 condition.signalAll(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;public class TraditionalProducerConsumer &#123;\tpublic static void main(String[] args) &#123; ShareData shareData = new ShareData(); // t1线程，生产 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 5; i++) &#123; shareData.increment(); &#125; &#125;, &quot;t1&quot;).start(); // t2线程，消费 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 5; i++) &#123; shareData.decrement(); &#125; &#125;, &quot;t2&quot;).start(); &#125;&#125; 改进版异步模式之生产者&#x2F;消费者： 消费队列可以用来平衡生产和消费的线程资源，不需要产生结果和消费结果的线程一一对应 生产者仅负责产生结果数据，不关心数据该如何处理，而消费者专心处理结果数据 消息队列是有容量限制的，满时不会再加入数据，空时不会再消耗数据 JDK 中各种阻塞队列，采用的就是这种模式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class demo &#123; public static void main(String[] args) &#123; MessageQueue queue = new MessageQueue(2); for (int i = 0; i &lt; 3; i++) &#123; int id = i; new Thread(() -&gt; &#123; queue.put(new Message(id,&quot;值&quot;+id)); &#125;, &quot;生产者&quot; + i).start(); &#125; new Thread(() -&gt; &#123; while (true) &#123; try &#123; Thread.sleep(1000); Message message = queue.take(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,&quot;消费者&quot;).start(); &#125;&#125;//消息队列类，Java间线程之间通信class MessageQueue &#123; private LinkedList&lt;Message&gt; list = new LinkedList&lt;&gt;();//消息的队列集合 private int capacity;//队列容量 public MessageQueue(int capacity) &#123; this.capacity = capacity; &#125; //获取消息 public Message take() &#123; //检查队列是否为空 synchronized (list) &#123; while (list.isEmpty()) &#123; try &#123; sout(Thread.currentThread().getName() + &quot;:队列为空，消费者线程等待&quot;); list.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //从队列的头部获取消息返回 Message message = list.removeFirst(); sout(Thread.currentThread().getName() + &quot;：已消费消息--&quot; + message); list.notifyAll(); return message; &#125; &#125; //存入消息 public void put(Message message) &#123; synchronized (list) &#123; //检查队列是否满 while (list.size() == capacity) &#123; try &#123; sout(Thread.currentThread().getName()+&quot;:队列为已满，生产者线程等待&quot;); list.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //将消息加入队列尾部 list.addLast(message); sout(Thread.currentThread().getName() + &quot;:已生产消息--&quot; + message); list.notifyAll(); &#125; &#125;&#125;final class Message &#123; private int id; private Object value;\t//get set&#125; 阻塞队列1234567891011121314151617181920212223public static void main(String[] args) &#123; ExecutorService consumer = Executors.newFixedThreadPool(1); ExecutorService producer = Executors.newFixedThreadPool(1); BlockingQueue&lt;Integer&gt; queue = new SynchronousQueue&lt;&gt;(); producer.submit(() -&gt; &#123; try &#123; System.out.println(&quot;生产...&quot;); Thread.sleep(1000); queue.put(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); consumer.submit(() -&gt; &#123; try &#123; System.out.println(&quot;等待消费...&quot;); Integer result = queue.take(); System.out.println(&quot;结果为:&quot; + result); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;);&#125; 内存JMM内存模型Java 内存模型是 Java Memory Model（JMM），本身是一种抽象的概念，实际上并不存在，描述的是一组规则或规范，通过这组规范定义了程序中各个变量（包括实例字段，静态字段和构成数组对象的元素）的访问方式 JMM 作用： 屏蔽各种硬件和操作系统的内存访问差异，实现让 Java 程序在各种平台下都能达到一致的内存访问效果 规定了线程和内存之间的一些关系 根据 JMM 的设计，系统存在一个主内存（Main Memory），Java 中所有变量都存储在主存中，对于所有线程都是共享的；每条线程都有自己的工作内存（Working Memory），工作内存中保存的是主存中某些变量的拷贝，线程对所有变量的操作都是先对变量进行拷贝，然后在工作内存中进行，不能直接操作主内存中的变量；线程之间无法相互直接访问，线程间的通信（传递）必须通过主内存来完成 主内存和工作内存： 主内存：计算机的内存，也就是经常提到的 8G 内存，16G 内存，存储所有共享变量的值 工作内存：存储该线程使用到的共享变量在主内存的的值的副本拷贝 JVM 和 JMM 之间的关系：JMM 中的主内存、工作内存与 JVM 中的 Java 堆、栈、方法区等并不是同一个层次的内存划分，这两者基本上是没有关系的，如果两者一定要勉强对应起来： 主内存主要对应于 Java 堆中的对象实例数据部分，而工作内存则对应于虚拟机栈中的部分区域 从更低层次上说，主内存直接对应于物理硬件的内存，工作内存对应寄存器和高速缓存 内存交互Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作，每个操作都是原子的 非原子协定：没有被 volatile 修饰的 long、double 外，默认按照两次 32 位的操作 lock：作用于主内存，将一个变量标识为被一个线程独占状态（对应 monitorenter） unclock：作用于主内存，将一个变量从独占状态释放出来，释放后的变量才可以被其他线程锁定（对应 monitorexit） read：作用于主内存，把一个变量的值从主内存传输到工作内存中 load：作用于工作内存，在 read 之后执行，把 read 得到的值放入工作内存的变量副本中 use：作用于工作内存，把工作内存中一个变量的值传递给执行引擎，每当遇到一个使用到变量的操作时都要使用该指令 assign：作用于工作内存，把从执行引擎接收到的一个值赋给工作内存的变量 store：作用于工作内存，把工作内存的一个变量的值传送到主内存中 write：作用于主内存，在 store 之后执行，把 store 得到的值放入主内存的变量中 参考文章：https://github.com/CyC2018/CS-Notes/blob/master/notes/Java%20%E5%B9%B6%E5%8F%91.md 三大特性可见性可见性：是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值 存在不可见问题的根本原因是由于缓存的存在，线程持有的是共享变量的副本，无法感知其他线程对于共享变量的更改，导致读取的值不是最新的。但是 final 修饰的变量是不可变的，就算有缓存，也不会存在不可见的问题 main 线程对 run 变量的修改对于 t 线程不可见，导致了 t 线程无法停止： 1234567891011static boolean run = true;\t//添加volatilepublic static void main(String[] args) throws InterruptedException &#123; Thread t = new Thread(()-&gt;&#123; while(run)&#123; // .... &#125;\t&#125;); t.start(); sleep(1); run = false; // 线程t不会如预想的停下来&#125; 原因： 初始状态， t 线程刚开始从主内存读取了 run 的值到工作内存 因为 t 线程要频繁从主内存中读取 run 的值，JIT 编译器会将 run 的值缓存至自己工作内存中的高速缓存中，减少对主存中 run 的访问，提高效率 1 秒之后，main 线程修改了 run 的值，并同步至主存，而 t 是从自己工作内存中的高速缓存中读取这个变量的值，结果永远是旧值 原子性原子性：不可分割，完整性，也就是说某个线程正在做某个具体业务时，中间不可以被分割，需要具体完成，要么同时成功，要么同时失败，保证指令不会受到线程上下文切换的影响 定义原子操作的使用规则： 不允许 read 和 load、store 和 write 操作之一单独出现，必须顺序执行，但是不要求连续 不允许一个线程丢弃 assign 操作，必须同步回主存 不允许一个线程无原因地（没有发生过任何 assign 操作）把数据从工作内存同步会主内存中 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（assign 或者 load）的变量，即对一个变量实施 use 和 store 操作之前，必须先自行 assign 和 load 操作 一个变量在同一时刻只允许一条线程对其进行 lock 操作，但 lock 操作可以被同一线程重复执行多次，多次执行 lock 后，只有执行相同次数的 unlock 操作，变量才会被解锁，lock 和 unlock 必须成对出现 如果对一个变量执行 lock 操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量之前需要重新从主存加载 如果一个变量事先没有被 lock 操作锁定，则不允许执行 unlock 操作，也不允许去 unlock 一个被其他线程锁定的变量 对一个变量执行 unlock 操作之前，必须先把此变量同步到主内存中（执行 store 和 write 操作） 有序性有序性：在本线程内观察，所有操作都是有序的；在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序 CPU 的基本工作是执行存储的指令序列，即程序，程序的执行过程实际上是不断地取出指令、分析指令、执行指令的过程，为了提高性能，编译器和处理器会对指令重排，一般分为以下三种： 1源代码 -&gt; 编译器优化的重排 -&gt; 指令并行的重排 -&gt; 内存系统的重排 -&gt; 最终执行指令 现代 CPU 支持多级指令流水线，几乎所有的冯•诺伊曼型计算机的 CPU，其工作都可以分为 5 个阶段：取指令、指令译码、执行指令、访存取数和结果写回，可以称之为五级指令流水线。CPU 可以在一个时钟周期内，同时运行五条指令的不同阶段（每个线程不同的阶段），本质上流水线技术并不能缩短单条指令的执行时间，但变相地提高了指令地吞吐率 处理器在进行重排序时，必须要考虑指令之间的数据依赖性 单线程环境也存在指令重排，由于存在依赖性，最终执行结果和代码顺序的结果一致 多线程环境中线程交替执行，由于编译器优化重排，会获取其他线程处在不同阶段的指令同时执行 补充知识： 指令周期是取出一条指令并执行这条指令的时间，一般由若干个机器周期组成 机器周期也称为 CPU 周期，一条指令的执行过程划分为若干个阶段（如取指、译码、执行等），每一阶段完成一个基本操作，完成一个基本操作所需要的时间称为机器周期 振荡周期指周期性信号作周期性重复变化的时间间隔 cache缓存机制缓存结构在计算机系统中，CPU 高速缓存（CPU Cache，简称缓存）是用于减少处理器访问内存所需平均时间的部件；在存储体系中位于自顶向下的第二层，仅次于 CPU 寄存器；其容量远小于内存，但速度却可以接近处理器的频率 CPU 处理器速度远远大于在主内存中的，为了解决速度差异，在它们之间架设了多级缓存，如 L1、L2、L3 级别的缓存，这些缓存离 CPU 越近就越快，将频繁操作的数据缓存到这里，加快访问速度 从 CPU 到 大约需要的时钟周期 寄存器 1 cycle (4GHz 的 CPU 约为 0.25ns) L1 3~4 cycle L2 10~20 cycle L3 40~45 cycle 内存 120~240 cycle 缓存使用当处理器发出内存访问请求时，会先查看缓存内是否有请求数据，如果存在（命中），则不用访问内存直接返回该数据；如果不存在（失效），则要先把内存中的相应数据载入缓存，再将其返回处理器 缓存之所以有效，主要因为程序运行时对内存的访问呈现局部性（Locality）特征。既包括空间局部性（Spatial Locality），也包括时间局部性（Temporal Locality），有效利用这种局部性，缓存可以达到极高的命中率 伪共享缓存以缓存行 cache line 为单位，每个缓存行对应着一块内存，一般是 64 byte（8 个 long），在 CPU 从主存获取数据时，以 cache line 为单位加载，于是相邻的数据会一并加载到缓存中 缓存会造成数据副本的产生，即同一份数据会缓存在不同核心的缓存行中，CPU 要保证数据的一致性，需要做到某个 CPU 核心更改了数据，其它 CPU 核心对应的整个缓存行必须失效，这就是伪共享 解决方法： padding：通过填充，让数据落在不同的 cache line 中 @Contended：原理参考 无锁 → Adder → 优化机制 → 伪共享 Linux 查看 CPU 缓存行： 命令：cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size64 内存地址格式：[高位组标记] [低位索引] [偏移量] 缓存一致缓存一致性：当多个处理器运算任务都涉及到同一块主内存区域的时候，将可能导致各自的缓存数据不一样 MESI（Modified Exclusive Shared Or Invalid）是一种广泛使用的支持写回策略的缓存一致性协议，CPU 中每个缓存行（caceh line）使用 4 种状态进行标记（使用额外的两位 bit 表示)： M：被修改（Modified） 该缓存行只被缓存在该 CPU 的缓存中，并且是被修改过的，与主存中的数据不一致 (dirty)，该缓存行中的内存需要写回 (write back) 主存。该状态的数据再次被修改不会发送广播，因为其他核心的数据已经在第一次修改时失效一次 当被写回主存之后，该缓存行的状态会变成独享 (exclusive) 状态 E：独享的（Exclusive） 该缓存行只被缓存在该 CPU 的缓存中，是未被修改过的 (clear)，与主存中数据一致，修改数据不需要通知其他 CPU 核心，该状态可以在任何时刻有其它 CPU 读取该内存时变成共享状态 (shared) 当 CPU 修改该缓存行中内容时，该状态可以变成 Modified 状态 S：共享的（Shared） 该状态意味着该缓存行可能被多个 CPU 缓存，并且各个缓存中的数据与主存数据一致，当 CPU 修改该缓存行中，会向其它 CPU 核心广播一个请求，使该缓存行变成无效状态 (Invalid)，然后再更新当前 Cache 里的数据 I：无效的（Invalid） 该缓存是无效的，可能有其它 CPU 修改了该缓存行 解决方法：各个处理器访问缓存时都遵循一些协议，在读写时要根据协议进行操作，协议主要有 MSI、MESI 等 处理机制单核 CPU 处理器会自动保证基本内存操作的原子性 多核 CPU 处理器，每个 CPU 处理器内维护了一块内存，每个内核内部维护着一块缓存，当多线程并发读写时，就会出现缓存数据不一致的情况。处理器提供： 总线锁定：当处理器要操作共享变量时，在 BUS 总线上发出一个 LOCK 信号，其他处理器就无法操作这个共享变量，该操作会导致大量阻塞，从而增加系统的性能开销（平台级别的加锁） 缓存锁定：当处理器对缓存中的共享变量进行了操作，其他处理器有嗅探机制，将各自缓存中的该共享变量的失效，读取时会重新从主内存中读取最新的数据，基于 MESI 缓存一致性协议来实现 有如下两种情况处理器不会使用缓存锁定： 当操作的数据跨多个缓存行，或没被缓存在处理器内部，则处理器会使用总线锁定 有些处理器不支持缓存锁定，比如：Intel 486 和 Pentium 处理器也会调用总线锁定 总线机制： 总线嗅探：每个处理器通过嗅探在总线上传播的数据来检查自己缓存值是否过期了，当处理器发现自己的缓存对应的内存地址的数据被修改，就将当前处理器的缓存行设置为无效状态，当处理器对这个数据进行操作时，会重新从内存中把数据读取到处理器缓存中 总线风暴：当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心（写传播），CPU 需要每时每刻监听总线上的一切活动，但是不管别的核心的 Cache 是否缓存相同的数据，都需要发出一个广播事件，不断的从主内存嗅探和 CAS 循环，无效的交互会导致总线带宽达到峰值；因此不要大量使用 volatile 关键字，使用 volatile、syschonized 都需要根据实际场景 volatile同步机制volatile 是 Java 虚拟机提供的轻量级的同步机制（三大特性） 保证可见性 不保证原子性 保证有序性（禁止指令重排） 性能：volatile 修饰的变量进行读操作与普通变量几乎没什么差别，但是写操作相对慢一些，因为需要在本地代码中插入很多内存屏障来保证指令不会发生乱序执行，但是开销比锁要小 synchronized 无法禁止指令重排和处理器优化，为什么可以保证有序性可见性 加了锁之后，只能有一个线程获得到了锁，获得不到锁的线程就要阻塞，所以同一时间只有一个线程执行，相当于单线程，由于数据依赖性的存在，单线程的指令重排是没有问题的 线程加锁前，将清空工作内存中共享变量的值，使用共享变量时需要从主内存中重新读取最新的值；线程解锁前，必须把共享变量的最新值刷新到主内存中（JMM 内存交互章节有讲） 指令重排volatile 修饰的变量，可以禁用指令重排 指令重排实例： example 1： 123456public void mySort() &#123;\tint x = 11;\t//语句1\tint y = 12;\t//语句2 谁先执行效果一样\tx = x + 5;\t//语句3\ty = x * x;\t//语句4&#125; 执行顺序是：1 2 3 4、2 1 3 4、1 3 2 4 指令重排也有限制不会出现：4321，语句 4 需要依赖于 y 以及 x 的申明，因为存在数据依赖，无法首先执行 example 2： 123456789101112131415int num = 0;boolean ready = false;// 线程1 执行此方法public void actor1(I_Result r) &#123; if(ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125;&#125;// 线程2 执行此方法public void actor2(I_Result r) &#123;\tnum = 2;\tready = true;&#125; 情况一：线程 1 先执行，ready &#x3D; false，结果为 r.r1 &#x3D; 1 情况二：线程 2 先执行 num &#x3D; 2，但还没执行 ready &#x3D; true，线程 1 执行，结果为 r.r1 &#x3D; 1 情况三：线程 2 先执行 ready &#x3D; true，线程 1 执行，进入 if 分支结果为 r.r1 &#x3D; 4 情况四：线程 2 执行 ready &#x3D; true，切换到线程 1，进入 if 分支为 r.r1 &#x3D; 0，再切回线程 2 执行 num &#x3D; 2，发生指令重排 底层原理缓存一致使用 volatile 修饰的共享变量，底层通过汇编 lock 前缀指令进行缓存锁定，在线程修改完共享变量后写回主存，其他的 CPU 核心上运行的线程通过 CPU 总线嗅探机制会修改其共享变量为失效状态，读取时会重新从主内存中读取最新的数据 lock 前缀指令就相当于内存屏障，Memory Barrier（Memory Fence） 对 volatile 变量的写指令后会加入写屏障 对 volatile 变量的读指令前会加入读屏障 内存屏障有三个作用： 确保对内存的读-改-写操作原子执行 阻止屏障两侧的指令重排序 强制把缓存中的脏数据写回主内存，让缓存行中相应的数据失效 内存屏障保证可见性： 写屏障（sfence，Store Barrier）保证在该屏障之前的，对共享变量的改动，都同步到主存当中 12345public void actor2(I_Result r) &#123; num = 2; ready = true; // ready 是 volatile 赋值带写屏障 // 写屏障&#125; 读屏障（lfence，Load Barrier）保证在该屏障之后的，对共享变量的读取，从主存刷新变量值，加载的是主存中最新数据 123456789public void actor1(I_Result r) &#123; // 读屏障 // ready 是 volatile 读取值带读屏障 if(ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125;&#125; 全能屏障：mfence（modify&#x2F;mix Barrier），兼具 sfence 和 lfence 的功能 保证有序性： 写屏障会确保指令重排序时，不会将写屏障之前的代码排在写屏障之后 读屏障会确保指令重排序时，不会将读屏障之后的代码排在读屏障之前 不能解决指令交错： 写屏障仅仅是保证之后的读能够读到最新的结果，但不能保证其他线程的读跑到写屏障之前 有序性的保证也只是保证了本线程内相关代码不被重排序 123volatile i = 0;new Thread(() -&gt; &#123;i++&#125;);new Thread(() -&gt; &#123;i--&#125;); i++ 反编译后的指令： 1230: iconst_1 // 当int取值 -1~5 时，JVM采用iconst指令将常量压入栈中1: istore_1 // 将操作数栈顶数据弹出，存入局部变量表的 slot 12: iinc 1, 1 交互规则对于 volatile 修饰的变量： 线程对变量的 use 与 load、read 操作是相关联的，所以变量使用前必须先从主存加载 线程对变量的 assign 与 store、write 操作是相关联的，所以变量使用后必须同步至主存 线程 1 和线程 2 谁先对变量执行 read 操作，就会先进行 write 操作，防止指令重排 双端检锁检锁机制Double-Checked Locking：双端检锁机制 DCL（双端检锁）机制不一定是线程安全的，原因是有指令重排的存在，加入 volatile 可以禁止指令重排 1234567891011121314151617public final class Singleton &#123; private Singleton() &#123; &#125; private static Singleton INSTANCE = null; public static Singleton getInstance() &#123; if(INSTANCE == null) &#123; // t2，这里的判断不是线程安全的 // 首次访问会同步，而之后的使用没有 synchronized synchronized(Singleton.class) &#123; // 这里是线程安全的判断，防止其他线程在当前线程等待锁的期间完成了初始化 if (INSTANCE == null) &#123; INSTANCE = new Singleton(); &#125; &#125; &#125; return INSTANCE; &#125;&#125; 不锁 INSTANCE 的原因： INSTANCE 要重新赋值 INSTANCE 是 null，线程加锁之前需要获取对象的引用，设置对象头，null 没有引用 实现特点： 懒惰初始化 首次使用 getInstance() 才使用 synchronized 加锁，后续使用时无需加锁 第一个 if 使用了 INSTANCE 变量，是在同步块之外，但在多线程环境下会产生问题 DCL问题getInstance 方法对应的字节码为： 123456789101112131415161718192021220: getstatic #2 // Field INSTANCE:Ltest/Singleton;3: ifnonnull 376: ldc #3 // class test/Singleton8: dup9: astore_010: monitorenter11: getstatic #2 // Field INSTANCE:Ltest/Singleton;14: ifnonnull 2717: new #3 // class test/Singleton20: dup21: invokespecial #4 // Method &quot;&lt;init&gt;&quot;:()V24: putstatic #2 // Field INSTANCE:Ltest/Singleton;27: aload_028: monitorexit29: goto 3732: astore_133: aload_034: monitorexit35: aload_136: athrow37: getstatic #2 // Field INSTANCE:Ltest/Singleton;40: areturn 17 表示创建对象，将对象引用入栈 20 表示复制一份对象引用，引用地址 21 表示利用一个对象引用，调用构造方法初始化对象 24 表示利用一个对象引用，赋值给 static INSTANCE 步骤 21 和 24 之间不存在数据依赖关系，而且无论重排前后，程序的执行结果在单线程中并没有改变，因此这种重排优化是允许的 关键在于 0:getstatic 这行代码在 monitor 控制之外，可以越过 monitor 读取 INSTANCE 变量的值 当其他线程访问 INSTANCE 不为 null 时，由于 INSTANCE 实例未必已初始化，那么 t2 拿到的是将是一个未初始化完毕的单例返回，这就造成了线程安全的问题 解决方法指令重排只会保证串行语义的执行一致性（单线程），但并不会关系多线程间的语义一致性 引入 volatile，来保证出现指令重排的问题，从而保证单例模式的线程安全性： 1private static volatile SingletonDemo INSTANCE = null; ha-behappens-before 先行发生 Java 内存模型具备一些先天的“有序性”，即不需要通过任何同步手段（volatile、synchronized 等）就能够得到保证的安全，这个通常也称为 happens-before 原则，它是可见性与有序性的一套规则总结 不符合 happens-before 规则，JMM 并不能保证一个线程的可见性和有序性 程序次序规则 (Program Order Rule)：一个线程内，逻辑上书写在前面的操作先行发生于书写在后面的操作 ，因为多个操作之间有先后依赖关系，则不允许对这些操作进行重排序 锁定规则 (Monitor Lock Rule)：一个 unlock 操作先行发生于后面（时间的先后）对同一个锁的 lock 操作，所以线程解锁 m 之前对变量的写（解锁前会刷新到主内存中），对于接下来对 m 加锁的其它线程对该变量的读可见 volatile 变量规则 (Volatile Variable Rule)：对 volatile 变量的写操作先行发生于后面对这个变量的读 传递规则 (Transitivity)：具有传递性，如果操作 A 先行发生于操作 B，而操作 B 又先行发生于操作 C，则可以得出操作 A 先行发生于操作 C 线程启动规则 (Thread Start Rule)：Thread 对象的 start()方 法先行发生于此线程中的每一个操作 12static int x = 10;//线程 start 前对变量的写，对该线程开始后对该变量的读可见new Thread(()-&gt;&#123;\tSystem.out.println(x);\t&#125;,&quot;t1&quot;).start(); 线程中断规则 (Thread Interruption Rule)：对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生 线程终止规则 (Thread Termination Rule)：线程中所有的操作都先行发生于线程的终止检测，可以通过 Thread.join() 方法结束、Thread.isAlive() 的返回值手段检测到线程已经终止执行 对象终结规则（Finaizer Rule）：一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始 设计模式终止模式终止模式之两阶段终止模式：停止标记用 volatile 是为了保证该变量在多个线程之间的可见性 12345678910111213141516171819202122232425262728293031323334353637383940class TwoPhaseTermination &#123; // 监控线程 private Thread monitor; // 停止标记 private volatile boolean stop = false;; // 启动监控线程 public void start() &#123; monitor = new Thread(() -&gt; &#123; while (true) &#123; Thread thread = Thread.currentThread(); if (stop) &#123; System.out.println(&quot;后置处理&quot;); break; &#125; try &#123; Thread.sleep(1000);// 睡眠 System.out.println(thread.getName() + &quot;执行监控记录&quot;); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;被打断，退出睡眠&quot;); &#125; &#125; &#125;); monitor.start(); &#125; // 停止监控线程 public void stop() &#123; stop = true; monitor.interrupt();// 让线程尽快退出Timed Waiting &#125;&#125;// 测试public static void main(String[] args) throws InterruptedException &#123; TwoPhaseTermination tpt = new TwoPhaseTermination(); tpt.start(); Thread.sleep(3500); System.out.println(&quot;停止监控&quot;); tpt.stop();&#125; BalkingBalking （犹豫）模式用在一个线程发现另一个线程或本线程已经做了某一件相同的事，那么本线程就无需再做了，直接结束返回 1234567891011121314public class MonitorService &#123; // 用来表示是否已经有线程已经在执行启动了 private volatile boolean starting = false; public void start() &#123; System.out.println(&quot;尝试启动监控线程...&quot;); synchronized (this) &#123; if (starting) &#123; return; &#125; starting = true; &#125; // 真正启动监控线程... &#125;&#125; 对比保护性暂停模式：保护性暂停模式用在一个线程等待另一个线程的执行结果，当条件不满足时线程等待 例子：希望 doInit() 方法仅被调用一次，下面的实现出现的问题： 当 t1 线程进入 init() 准备 doInit()，t2 线程进来，initialized 还为f alse，则 t2 就又初始化一次 volatile 适合一个线程写，其他线程读的情况，这个代码需要加锁 12345678910111213public class TestVolatile &#123; volatile boolean initialized = false; void init() &#123; if (initialized) &#123; return; &#125; doInit(); initialized = true; &#125; private void doInit() &#123; &#125;&#125; 无锁CAS原理无锁编程：Lock Free CAS 的全称是 Compare-And-Swap，是 CPU 并发原语 CAS 并发原语体现在 Java 语言中就是 sun.misc.Unsafe 类的各个方法，调用 UnSafe 类中的 CAS 方法，JVM 会实现出 CAS 汇编指令，这是一种完全依赖于硬件的功能，实现了原子操作 CAS 是一种系统原语，原语属于操作系统范畴，是由若干条指令组成 ，用于完成某个功能的一个过程，并且原语的执行必须是连续的，执行过程中不允许被中断，所以 CAS 是一条 CPU 的原子指令，不会造成数据不一致的问题，是线程安全的 底层原理：CAS 的底层是 lock cmpxchg 指令（X86 架构），在单核和多核 CPU 下都能够保证比较交换的原子性 程序是在单核处理器上运行，会省略 lock 前缀，单处理器自身会维护处理器内的顺序一致性，不需要 lock 前缀的内存屏障效果 程序是在多核处理器上运行，会为 cmpxchg 指令加上 lock 前缀。当某个核执行到带 lock 的指令时，CPU 会执行总线锁定或缓存锁定，将修改的变量写入到主存，这个过程不会被线程的调度机制所打断，保证了多个线程对内存操作的原子性 作用：比较当前工作内存中的值和主物理内存中的值，如果相同则执行规定操作，否则继续比较直到主内存和工作内存的值一致为止 CAS 特点： CAS 体现的是无锁并发、无阻塞并发，线程不会陷入阻塞，线程不需要频繁切换状态（上下文切换，系统调用） CAS 是基于乐观锁的思想 CAS 缺点： 执行的是循环操作，如果比较不成功一直在循环，最差的情况某个线程一直取到的值和预期值都不一样，就会无限循环导致饥饿，使用 CAS 线程数不要超过 CPU 的核心数，采用分段 CAS 和自动迁移机制 只能保证一个共享变量的原子操作 对于一个共享变量执行操作时，可以通过循环 CAS 的方式来保证原子操作 对于多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候只能用锁来保证原子性 引出来 ABA 问题 乐观锁CAS 与 synchronized 总结： synchronized 是从悲观的角度出发：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程），因此 synchronized 也称之为悲观锁，ReentrantLock 也是一种悲观锁，性能较差 CAS 是从乐观的角度出发：总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据。如果别人修改过，则获取现在最新的值，如果别人没修改过，直接修改共享数据的值，CAS 这种机制也称之为乐观锁，综合性能较好 Atomic常用API常见原子类：AtomicInteger、AtomicBoolean、AtomicLong 构造方法： public AtomicInteger()：初始化一个默认值为 0 的原子型 Integer public AtomicInteger(int initialValue)：初始化一个指定值的原子型 Integer 常用API： 方法 作用 public final int get() 获取 AtomicInteger 的值 public final int getAndIncrement() 以原子方式将当前值加 1，返回的是自增前的值 public final int incrementAndGet() 以原子方式将当前值加 1，返回的是自增后的值 public final int getAndSet(int value) 以原子方式设置为 newValue 的值，返回旧值 public final int addAndGet(int data) 以原子方式将输入的数值与实例中的值相加并返回实例：AtomicInteger 里的 value 原理分析AtomicInteger 原理：自旋锁 + CAS 算法 CAS 算法：有 3 个操作数（内存值 V， 旧的预期值 A，要修改的值 B） 当旧的预期值 A &#x3D;&#x3D; 内存值 V 此时可以修改，将 V 改为 B 当旧的预期值 A !&#x3D; 内存值 V 此时不能修改，并重新获取现在的最新值，重新获取的动作就是自旋 分析 getAndSet 方法： AtomicInteger： 1234567public final int getAndSet(int newValue) &#123; /** * this: 当前对象 * valueOffset:\t内存偏移量，内存地址 */ return unsafe.getAndSetInt(this, valueOffset, newValue);&#125; valueOffset：偏移量表示该变量值相对于当前对象地址的偏移，Unsafe 就是根据内存偏移地址获取数据 1234valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;));//调用本地方法 --&gt;public native long objectFieldOffset(Field var1); unsafe 类： 12345678910// val1: AtomicInteger对象本身，var2: 该对象值得引用地址，var4: 需要变动的数public final int getAndSetInt(Object var1, long var2, int var4) &#123; int var5; do &#123; // var5: 用 var1 和 var2 找到的内存中的真实值 var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var4)); return var5;&#125; var5：从主内存中拷贝到工作内存中的值（每次都要从主内存拿到最新的值到本地内存），然后执行 compareAndSwapInt() 再和主内存的值进行比较，假设方法返回 false，那么就一直执行 while 方法，直到期望的值和真实值一样，修改数据 变量 value 用 volatile 修饰，保证了多线程之间的内存可见性，避免线程从工作缓存中获取失效的变量 1private volatile int value CAS 必须借助 volatile 才能读取到共享变量的最新值来实现比较并交换的效果 分析 getAndUpdate 方法： getAndUpdate： 12345678public final int getAndUpdate(IntUnaryOperator updateFunction) &#123; int prev, next; do &#123; prev = get();\t//当前值，cas的期望值 next = updateFunction.applyAsInt(prev);//期望值更新到该值 &#125; while (!compareAndSet(prev, next));//自旋 return prev;&#125; 函数式接口：可以自定义操作逻辑 12AtomicInteger a = new AtomicInteger();a.getAndUpdate(i -&gt; i + 10); compareAndSet： 123456789public final boolean compareAndSet(int expect, int update) &#123; /** * this: 当前对象 * valueOffset:\t内存偏移量，内存地址 * expect: 期望的值 * update: 更新的值 */ return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 原子引用原子引用：对 Object 进行原子操作，提供一种读和写都是原子性的对象引用变量 原子引用类：AtomicReference、AtomicStampedReference、AtomicMarkableReference AtomicReference 类： 构造方法：AtomicReference&lt;T&gt; atomicReference = new AtomicReference&lt;T&gt;() 常用 API： public final boolean compareAndSet(V expectedValue, V newValue)：CAS 操作 public final void set(V newValue)：将值设置为 newValue public final V get()：返回当前值 12345678910111213141516171819202122232425public class AtomicReferenceDemo &#123; public static void main(String[] args) &#123; Student s1 = new Student(33, &quot;z3&quot;); // 创建原子引用包装类 AtomicReference&lt;Student&gt; atomicReference = new AtomicReference&lt;&gt;(); // 设置主内存共享变量为s1 atomicReference.set(s1); // 比较并交换，如果现在主物理内存的值为 z3，那么交换成 l4 while (true) &#123; Student s2 = new Student(44, &quot;l4&quot;); if (atomicReference.compareAndSet(s1, s2)) &#123; break; &#125; &#125; System.out.println(atomicReference.get()); &#125;&#125;class Student &#123; private int id; private String name; //。。。。&#125; 原子数组原子数组类：AtomicIntegerArray、AtomicLongArray、AtomicReferenceArray AtomicIntegerArray 类方法： 12345678/*** i the index* expect the expected value* update the new value*/public final boolean compareAndSet(int i, int expect, int update) &#123; return compareAndSetRaw(checkedByteOffset(i), expect, update);&#125; 原子更新器原子更新器类：AtomicReferenceFieldUpdater、AtomicIntegerFieldUpdater、AtomicLongFieldUpdater 利用字段更新器，可以针对对象的某个域（Field）进行原子操作，只能配合 volatile 修饰的字段使用，否则会出现异常 IllegalArgumentException: Must be volatile type 常用 API： static &lt;U&gt; AtomicIntegerFieldUpdater&lt;U&gt; newUpdater(Class&lt;U&gt; c, String fieldName)：构造方法 abstract boolean compareAndSet(T obj, int expect, int update)：CAS 1234567891011public class UpdateDemo &#123; private volatile int field; public static void main(String[] args) &#123; AtomicIntegerFieldUpdater fieldUpdater = AtomicIntegerFieldUpdater .newUpdater(UpdateDemo.class, &quot;field&quot;); UpdateDemo updateDemo = new UpdateDemo(); fieldUpdater.compareAndSet(updateDemo, 0, 10); System.out.println(updateDemo.field);//10 &#125;&#125; 原子累加器原子累加器类：LongAdder、DoubleAdder、LongAccumulator、DoubleAccumulator LongAdder 和 LongAccumulator 区别： 相同点： LongAddr 与 LongAccumulator 类都是使用非阻塞算法 CAS 实现的 LongAddr 类是 LongAccumulator 类的一个特例，只是 LongAccumulator 提供了更强大的功能，可以自定义累加规则，当accumulatorFunction 为 null 时就等价于 LongAddr 不同点： 调用 casBase 时，LongAccumulator 使用 function.applyAsLong(b &#x3D; base, x) 来计算，LongAddr 使用 casBase(b &#x3D; base, b + x) LongAccumulator 类功能更加强大，构造方法参数中 accumulatorFunction 是一个双目运算器接口，可以指定累加规则，比如累加或者相乘，其根据输入的两个参数返回一个计算值，LongAdder 内置累加规则 identity 则是 LongAccumulator 累加器的初始值，LongAccumulator 可以为累加器提供非0的初始值，而 LongAdder 只能提供默认的 0 Adder优化机制LongAdder 是 Java8 提供的类，跟 AtomicLong 有相同的效果，但对 CAS 机制进行了优化，尝试使用分段 CAS 以及自动分段迁移的方式来大幅度提升多线程高并发执行 CAS 操作的性能 CAS 底层实现是在一个循环中不断地尝试修改目标值，直到修改成功。如果竞争不激烈修改成功率很高，否则失败率很高，失败后这些重复的原子性操作会耗费性能（导致大量线程空循环，自旋转） 优化核心思想：数据分离，将 AtomicLong 的单点的更新压力分担到各个节点，空间换时间，在低并发的时候直接更新，可以保障和 AtomicLong 的性能基本一致，而在高并发的时候通过分散减少竞争，提高了性能 分段 CAS 机制： 在发生竞争时，创建 Cell 数组用于将不同线程的操作离散（通过 hash 等算法映射）到不同的节点上 设置多个累加单元（会根据需要扩容，最大为 CPU 核数），Therad-0 累加 Cell[0]，而 Thread-1 累加 Cell[1] 等，最后将结果汇总 在累加时操作的不同的 Cell 变量，因此减少了 CAS 重试失败，从而提高性能 自动分段迁移机制：某个 Cell 的 value 执行 CAS 失败，就会自动寻找另一个 Cell 分段内的 value 值进行 CAS 操作 伪共享Cell 为累加单元：数组访问索引是通过 Thread 里的 threadLocalRandomProbe 域取模实现的，这个域是 ThreadLocalRandom 更新的 12345678910// Striped64.Cell@sun.misc.Contended static final class Cell &#123; volatile long value; Cell(long x) &#123; value = x; &#125; // 用 cas 方式进行累加, prev 表示旧值, next 表示新值 final boolean cas(long prev, long next) &#123; return UNSAFE.compareAndSwapLong(this, valueOffset, prev, next); &#125; // 省略不重要代码&#125; Cell 是数组形式，在内存中是连续存储的，64 位系统中，一个 Cell 为 24 字节（16 字节的对象头和 8 字节的 value），每一个 cache line 为 64 字节，因此缓存行可以存下 2 个的 Cell 对象，当 Core-0 要修改 Cell[0]、Core-1 要修改 Cell[1]，无论谁修改成功都会导致当前缓存行失效，从而导致对方的数据失效，需要重新去主存获取，影响效率 @sun.misc.Contended：防止缓存行伪共享，在使用此注解的对象或字段的前后各增加 128 字节大小的 padding，使用 2 倍于大多数硬件缓存行让 CPU 将对象预读至缓存时占用不同的缓存行，这样就不会造成对方缓存行的失效 源码解析Striped64 类成员属性： 12345678// 表示当前计算机CPU数量static final int NCPU = Runtime.getRuntime().availableProcessors()// 累加单元数组, 懒惰初始化transient volatile Cell[] cells;// 基础值, 如果没有竞争, 则用 cas 累加这个域，当 cells 扩容时，也会将数据写到 base 中transient volatile long base;// 在 cells 初始化或扩容时只能有一个线程执行, 通过 CAS 更新 cellsBusy 置为 1 来实现一个锁transient volatile int cellsBusy; 工作流程： cells 占用内存是相对比较大的，是惰性加载的，在无竞争或者其他线程正在初始化 cells 数组的情况下，直接更新 base 域 在第一次发生竞争时（casBase 失败）会创建一个大小为 2 的 cells 数组，将当前累加的值包装为 Cell 对象，放入映射的槽位上 分段累加的过程中，如果当前线程对应的 cells 槽位为空，就会新建 Cell 填充，如果出现竞争，就会重新计算线程对应的槽位，继续自旋尝试修改 分段迁移后还出现竞争就会扩容 cells 数组长度为原来的两倍，然后 rehash，数组长度总是 2 的 n 次幂，默认最大为 CPU 核数，但是可以超过，如果核数是 6 核，数组最长是 8 方法分析： LongAdder#add：累加方法 123456789101112131415161718192021222324public void add(long x) &#123; // as 为累加单元数组的引用，b 为基础值，v 表示期望值 // m 表示 cells 数组的长度 - 1，a 表示当前线程命中的 cell 单元格 Cell[] as; long b, v; int m; Cell a; // cells 不为空说明 cells 已经被初始化，线程发生了竞争，去更新对应的 cell 槽位 // 进入 || 后的逻辑去更新 base 域，更新失败表示发生竞争进入条件 if ((as = cells) != null || !casBase(b = base, b + x)) &#123; // uncontended 为 true 表示 cell 没有竞争 boolean uncontended = true; // 条件一: true 说明 cells 未初始化，多线程写 base 发生竞争需要进行初始化 cells 数组 // fasle 说明 cells 已经初始化，进行下一个条件寻找自己的 cell 去累加 // 条件二: getProbe() 获取 hash 值，&amp; m 的逻辑和 HashMap 的逻辑相同，保证散列的均匀性 // true 说明当前线程对应下标的 cell 为空，需要创建 cell // false 说明当前线程对应的 cell 不为空，进行下一个条件【将 x 值累加到对应的 cell 中】 // 条件三: 有取反符号，false 说明 cas 成功，直接返回，true 说明失败，当前线程对应的 cell 有竞争 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[getProbe() &amp; m]) == null || !(uncontended = a.cas(v = a.value, v + x))) longAccumulate(x, null, uncontended); // 【uncontended 在对应的 cell 上累加失败的时候才为 false，其余情况均为 true】 &#125;&#125; Striped64#longAccumulate：cell 数组创建 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113 // x null false | truefinal void longAccumulate(long x, LongBinaryOperator fn, boolean wasUncontended) &#123; int h; // 当前线程还没有对应的 cell, 需要随机生成一个 hash 值用来将当前线程绑定到 cell if ((h = getProbe()) == 0) &#123; // 初始化 probe，获取 hash 值 ThreadLocalRandom.current(); h = getProbe(); // 默认情况下 当前线程肯定是写入到了 cells[0] 位置，不把它当做一次真正的竞争 wasUncontended = true; &#125; // 表示【扩容意向】，false 一定不会扩容，true 可能会扩容 boolean collide = false; //自旋 for (;;) &#123; // as 表示cells引用，a 表示当前线程命中的 cell，n 表示 cells 数组长度，v 表示 期望值 Cell[] as; Cell a; int n; long v; // 【CASE1】: 表示 cells 已经初始化了，当前线程应该将数据写入到对应的 cell 中 if ((as = cells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; // CASE1.1: true 表示当前线程对应的索引下标的 Cell 为 null，需要创建 new Cell if ((a = as[(n - 1) &amp; h]) == null) &#123; // 判断 cellsBusy 是否被锁 if (cellsBusy == 0) &#123; // 创建 cell, 初始累加值为 x Cell r = new Cell(x); // 加锁 if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; // 创建成功标记，进入【创建 cell 逻辑】 boolean created = false; try &#123; Cell[] rs; int m, j; // 把当前 cells 数组赋值给 rs，并且不为 null if ((rs = cells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; // 再次判断防止其它线程初始化过该位置，当前线程再次初始化该位置会造成数据丢失 // 因为这里是线程安全的判断，进行的逻辑不会被其他线程影响 rs[j = (m - 1) &amp; h] == null) &#123; // 把新创建的 cell 填充至当前位置 rs[j] = r; created = true;\t// 表示创建完成 &#125; &#125; finally &#123; cellsBusy = 0; // 解锁 &#125; if (created) // true 表示创建完成，可以推出循环了 break; continue; &#125; &#125; collide = false; &#125; // CASE1.2: 条件成立说明线程对应的 cell 有竞争, 改变线程对应的 cell 来重试 cas else if (!wasUncontended) wasUncontended = true; // CASE 1.3: 当前线程 rehash 过，如果新命中的 cell 不为空，就尝试累加，false 说明新命中也有竞争 else if (a.cas(v = a.value, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // CASE 1.4: cells 长度已经超过了最大长度 CPU 内核的数量或者已经扩容 else if (n &gt;= NCPU || cells != as) collide = false; // 扩容意向改为false，【表示不能扩容了】 // CASE 1.5: 更改扩容意向，如果 n &gt;= NCPU，这里就永远不会执行到，case1.4 永远先于 1.5 执行 else if (!collide) collide = true; // CASE 1.6: 【扩容逻辑】，进行加锁 else if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; try &#123; // 线程安全的检查，防止期间被其他线程扩容了 if (cells == as) &#123; // 扩容为以前的 2 倍 Cell[] rs = new Cell[n &lt;&lt; 1]; // 遍历移动值 for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; // 把扩容后的引用给 cells cells = rs; &#125; &#125; finally &#123; cellsBusy = 0;\t// 解锁 &#125; collide = false;\t// 扩容意向改为 false，表示不扩容了 continue; &#125; // 重置当前线程 Hash 值，这就是【分段迁移机制】 h = advanceProbe(h); &#125; // 【CASE2】: 运行到这说明 cells 还未初始化，as 为null // 判断是否没有加锁，没有加锁就用 CAS 加锁 // 条件二判断是否其它线程在当前线程给 as 赋值之后修改了 cells，这里不是线程安全的判断 else if (cellsBusy == 0 &amp;&amp; cells == as &amp;&amp; casCellsBusy()) &#123; // 初始化标志，开始 【初始化 cells 数组】 boolean init = false; try &#123; // 再次判断 cells == as 防止其它线程已经提前初始化了，当前线程再次初始化导致丢失数据 // 因为这里是【线程安全的，重新检查，经典 DCL】 if (cells == as) &#123; Cell[] rs = new Cell[2];\t// 初始化数组大小为2 rs[h &amp; 1] = new Cell(x);\t// 填充线程对应的cell cells = rs; init = true; // 初始化成功，标记置为 true &#125; &#125; finally &#123; cellsBusy = 0; // 解锁啊 &#125; if (init) break; // 初始化成功直接跳出自旋 &#125; // 【CASE3】: 运行到这说明其他线程在初始化 cells，当前线程将值累加到 base，累加成功直接结束自旋 else if (casBase(v = base, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; &#125;&#125; sum：获取最终结果通过 sum 整合，保证最终一致性，不保证强一致性 123456789101112public long sum() &#123; Cell[] as = cells; Cell a; long sum = base; if (as != null) &#123; // 遍历 累加 for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125; ABAABA 问题：当进行获取主内存值时，该内存值在写入主内存时已经被修改了 N 次，但是最终又改成原来的值 其他线程先把 A 改成 B 又改回 A，主线程仅能判断出共享变量的值与最初值 A 是否相同，不能感知到这种从 A 改为 B 又 改回 A 的情况，这时 CAS 虽然成功，但是过程存在问题 构造方法： public AtomicStampedReference(V initialRef, int initialStamp)：初始值和初始版本号 常用API： public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp)：期望引用和期望版本号都一致才进行 CAS 修改数据 public void set(V newReference, int newStamp)：设置值和版本号 public V getReference()：返回引用的值 public int getStamp()：返回当前版本号 12345678910111213141516171819202122public static void main(String[] args) &#123; AtomicStampedReference&lt;Integer&gt; atomicReference = new AtomicStampedReference&lt;&gt;(100,1); int startStamp = atomicReference.getStamp(); new Thread(() -&gt;&#123; int stamp = atomicReference.getStamp(); atomicReference.compareAndSet(100, 101, stamp, stamp + 1); stamp = atomicReference.getStamp(); atomicReference.compareAndSet(101, 100, stamp, stamp + 1); &#125;,&quot;t1&quot;).start(); new Thread(() -&gt;&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (!atomicReference.compareAndSet(100, 200, startStamp, startStamp + 1)) &#123; System.out.println(atomicReference.getReference());//100 System.out.println(Thread.currentThread().getName() + &quot;线程修改失败&quot;); &#125; &#125;,&quot;t2&quot;).start();&#125; UnsafeUnsafe 是 CAS 的核心类，由于 Java 无法直接访问底层系统，需要通过本地（Native）方法来访问 Unsafe 类存在 sun.misc 包，其中所有方法都是 native 修饰的，都是直接调用操作系统底层资源执行相应的任务，基于该类可以直接操作特定的内存数据，其内部方法操作类似 C 的指针 模拟实现原子整数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public static void main(String[] args) &#123; MyAtomicInteger atomicInteger = new MyAtomicInteger(10); if (atomicInteger.compareAndSwap(20)) &#123; System.out.println(atomicInteger.getValue()); &#125;&#125;class MyAtomicInteger &#123; private static final Unsafe UNSAFE; private static final long VALUE_OFFSET; private volatile int value; static &#123; try &#123; //Unsafe unsafe = Unsafe.getUnsafe()这样会报错，需要反射获取 Field theUnsafe = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); theUnsafe.setAccessible(true); UNSAFE = (Unsafe) theUnsafe.get(null); // 获取 value 属性的内存地址，value 属性指向该地址，直接设置该地址的值可以修改 value 的值 VALUE_OFFSET = UNSAFE.objectFieldOffset( MyAtomicInteger.class.getDeclaredField(&quot;value&quot;)); &#125; catch (NoSuchFieldException | IllegalAccessException e) &#123; e.printStackTrace(); throw new RuntimeException(); &#125; &#125; public MyAtomicInteger(int value) &#123; this.value = value; &#125; public int getValue() &#123; return value; &#125; public boolean compareAndSwap(int update) &#123; while (true) &#123; int prev = this.value; int next = update; // 当前对象 内存偏移量 期望值 更新值 if (UNSAFE.compareAndSwapInt(this, VALUE_OFFSET, prev, update)) &#123; System.out.println(&quot;CAS成功&quot;); return true; &#125; &#125; &#125;&#125; final原理123public class TestFinal &#123;\tfinal int a = 20;&#125; 字节码： 12345670: aload_01: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V4: aload_05: bipush 20 // 将值直接放入栈中7: putfield #2 // Field a:I&lt;-- 写屏障10: return final 变量的赋值通过 putfield 指令来完成，在这条指令之后也会加入写屏障，保证在其它线程读到它的值时不会出现为 0 的情况 其他线程访问 final 修饰的变量 复制一份放入栈中直接访问，效率高 大于 short 最大值会将其复制到类的常量池，访问时从常量池获取 不可变不可变：如果一个对象不能够修改其内部状态（属性），那么就是不可变对象 不可变对象线程安全的，不存在并发修改和可见性问题，是另一种避免竞争的方式 String 类也是不可变的，该类和类中所有属性都是 final 的 类用 final 修饰保证了该类中的方法不能被覆盖，防止子类无意间破坏不可变性 无写入方法（set）确保外部不能对内部属性进行修改 属性用 final 修饰保证了该属性是只读的，不能修改 123456public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; //....&#125; 更改 String 类数据时，会构造新字符串对象，生成新的 char[] value，通过创建副本对象来避免共享的方式称之为保护性拷贝 State无状态：成员变量保存的数据也可以称为状态信息，无状态就是没有成员变量 Servlet 为了保证其线程安全，一般不为 Servlet 设置成员变量，这种没有任何成员变量的类是线程安全的 Local基本介绍ThreadLocal 类用来提供线程内部的局部变量，这种变量在多线程环境下访问（通过 get 和 set 方法访问）时能保证各个线程的变量相对独立于其他线程内的变量，分配在堆内的 TLAB 中 ThreadLocal 实例通常来说都是 private static 类型的，属于一个线程的本地变量，用于关联线程和线程上下文。每个线程都会在 ThreadLocal 中保存一份该线程独有的数据，所以是线程安全的 ThreadLocal 作用： 线程并发：应用在多线程并发的场景下 传递数据：通过 ThreadLocal 实现在同一线程不同函数或组件中传递公共变量，减少传递复杂度 线程隔离：每个线程的变量都是独立的，不会互相影响 对比 synchronized： synchronized ThreadLocal 原理 同步机制采用以时间换空间的方式，只提供了一份变量，让不同的线程排队访问 ThreadLocal 采用以空间换时间的方式，为每个线程都提供了一份变量的副本，从而实现同时访问而相不干扰 侧重点 多个线程之间访问资源的同步 多线程中让每个线程之间的数据相互隔离 基本使用常用方法 方法 描述 ThreadLocal&lt;&gt;() 创建 ThreadLocal 对象 protected T initialValue() 返回当前线程局部变量的初始值 public void set( T value) 设置当前线程绑定的局部变量 public T get() 获取当前线程绑定的局部变量 public void remove() 移除当前线程绑定的局部变量 123456789101112131415161718192021222324252627282930313233public class MyDemo &#123; private static ThreadLocal&lt;String&gt; tl = new ThreadLocal&lt;&gt;(); private String content; private String getContent() &#123; // 获取当前线程绑定的变量 return tl.get(); &#125; private void setContent(String content) &#123; // 变量content绑定到当前线程 tl.set(content); &#125; public static void main(String[] args) &#123; MyDemo demo = new MyDemo(); for (int i = 0; i &lt; 5; i++) &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; // 设置数据 demo.setContent(Thread.currentThread().getName() + &quot;的数据&quot;); System.out.println(&quot;-----------------------&quot;); System.out.println(Thread.currentThread().getName() + &quot;---&gt;&quot; + demo.getContent()); &#125; &#125;); thread.setName(&quot;线程&quot; + i); thread.start(); &#125; &#125;&#125; 应用场景ThreadLocal 适用于下面两种场景： 每个线程需要有自己单独的实例 实例需要在多个方法中共享，但不希望被多线程共享 ThreadLocal 方案有两个突出的优势： 传递数据：保存每个线程绑定的数据，在需要的地方可以直接获取，避免参数直接传递带来的代码耦合问题 线程隔离：各线程之间的数据相互隔离却又具备并发性，避免同步方式带来的性能损失 ThreadLocal 用于数据连接的事务管理： 12345678910111213141516171819public class JdbcUtils &#123; // ThreadLocal对象，将connection绑定在当前线程中 private static final ThreadLocal&lt;Connection&gt; tl = new ThreadLocal(); // c3p0 数据库连接池对象属性 private static final ComboPooledDataSource ds = new ComboPooledDataSource(); // 获取连接 public static Connection getConnection() throws SQLException &#123; //取出当前线程绑定的connection对象 Connection conn = tl.get(); if (conn == null) &#123; //如果没有，则从连接池中取出 conn = ds.getConnection(); //再将connection对象绑定到当前线程中，非常重要的操作 tl.set(conn); &#125; return conn; &#125;\t// ...&#125; 用 ThreadLocal 使 SimpleDateFormat 从独享变量变成单个线程变量： 12345678910111213141516public class ThreadLocalDateUtil &#123; private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#125; &#125;; public static Date parse(String dateStr) throws ParseException &#123; return threadLocal.get().parse(dateStr); &#125; public static String format(Date date) &#123; return threadLocal.get().format(date); &#125;&#125; 实现原理底层结构JDK8 以前：每个 ThreadLocal 都创建一个 Map，然后用线程作为 Map 的 key，要存储的局部变量作为 Map 的 value，达到各个线程的局部变量隔离的效果。这种结构会造成 Map 结构过大和内存泄露，因为 Thread 停止后无法通过 key 删除对应的数据 JDK8 以后：每个 Thread 维护一个 ThreadLocalMap，这个 Map 的 key 是 ThreadLocal 实例本身，value 是真正要存储的值 每个 Thread 线程内部都有一个 Map (ThreadLocalMap) Map 里面存储 ThreadLocal 对象（key）和线程的私有变量（value） Thread 内部的 Map 是由 ThreadLocal 维护的，由 ThreadLocal 负责向 map 获取和设置线程的变量值 对于不同的线程，每次获取副本值时，别的线程并不能获取到当前线程的副本值，形成副本的隔离，互不干扰 JDK8 前后对比： 每个 Map 存储的 Entry 数量会变少，因为之前的存储数量由 Thread 的数量决定，现在由 ThreadLocal 的数量决定，在实际编程当中，往往 ThreadLocal 的数量要少于 Thread 的数量 当 Thread 销毁之后，对应的 ThreadLocalMap 也会随之销毁，能减少内存的使用，防止内存泄露 成员变量 Thread 类的相关属性：每一个线程持有一个 ThreadLocalMap 对象，存放由 ThreadLocal 和数据组成的 Entry 键值对 1ThreadLocal.ThreadLocalMap threadLocals = null 计算 ThreadLocal 对象的哈希值： 1private final int threadLocalHashCode = nextHashCode() 使用 threadLocalHashCode &amp; (table.length - 1) 计算当前 entry 需要存放的位置 每创建一个 ThreadLocal 对象就会使用 nextHashCode 分配一个 hash 值给这个对象： 1private static AtomicInteger nextHashCode = new AtomicInteger() 斐波那契数也叫黄金分割数，hash 的增量就是这个数字，带来的好处是 hash 分布非常均匀： 1private static final int HASH_INCREMENT = 0x61c88647 成员方法方法都是线程安全的，因为 ThreadLocal 属于一个线程的，ThreadLocal 中的方法，逻辑都是获取当前线程维护的 ThreadLocalMap 对象，然后进行数据的增删改查，没有指定初始值的 threadlcoal 对象默认赋值为 null initialValue()：返回该线程局部变量的初始值 延迟调用的方法，在执行 get 方法时才执行 该方法缺省（默认）实现直接返回一个 null 如果想要一个初始值，可以重写此方法， 该方法是一个 protected 的方法，为了让子类覆盖而设计的 123protected T initialValue() &#123; return null;&#125; nextHashCode()：计算哈希值，ThreadLocal 的散列方式称之为斐波那契散列，每次获取哈希值都会加上 HASH_INCREMENT，这样做可以尽量避免 hash 冲突，让哈希值能均匀的分布在 2 的 n 次方的数组中 1234private static int nextHashCode() &#123; // 哈希值自增一个 HASH_INCREMENT 数值 return nextHashCode.getAndAdd(HASH_INCREMENT);&#125; set()：修改当前线程与当前 threadlocal 对象相关联的线程局部变量 12345678910111213public void set(T value) &#123; // 获取当前线程对象 Thread t = Thread.currentThread(); // 获取此线程对象中维护的 ThreadLocalMap 对象 ThreadLocalMap map = getMap(t); // 判断 map 是否存在 if (map != null) // 调用 threadLocalMap.set 方法进行重写或者添加 map.set(this, value); else // map 为空，调用 createMap 进行 ThreadLocalMap 对象的初始化。参数1是当前线程，参数2是局部变量 createMap(t, value);&#125; 123456789// 获取当前线程 Thread 对应维护的 ThreadLocalMap ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;// 创建当前线程Thread对应维护的ThreadLocalMap void createMap(Thread t, T firstValue) &#123; // 【这里的 this 是调用此方法的 threadLocal】，创建一个新的 Map 并设置第一个数据 t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; get()：获取当前线程与当前 ThreadLocal 对象相关联的线程局部变量 1234567891011121314151617181920public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); // 如果此map存在 if (map != null) &#123; // 以当前的 ThreadLocal 为 key，调用 getEntry 获取对应的存储实体 e ThreadLocalMap.Entry e = map.getEntry(this); // 对 e 进行判空 if (e != null) &#123; // 获取存储实体 e 对应的 value值 T result = (T)e.value; return result; &#125; &#125; /*有两种情况有执行当前代码 第一种情况: map 不存在，表示此线程没有维护的 ThreadLocalMap 对象 第二种情况: map 存在, 但是【没有与当前 ThreadLocal 关联的 entry】，就会设置为默认值 */ // 初始化当前线程与当前 threadLocal 对象相关联的 value return setInitialValue();&#125; 123456789101112131415private T setInitialValue() &#123; // 调用initialValue获取初始化的值，此方法可以被子类重写, 如果不重写默认返回 null T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); // 判断 map 是否初始化过 if (map != null) // 存在则调用 map.set 设置此实体 entry，value 是默认的值 map.set(this, value); else // 调用 createMap 进行 ThreadLocalMap 对象的初始化中 createMap(t, value); // 返回线程与当前 threadLocal 关联的局部变量 return value;&#125; remove()：移除当前线程与当前 threadLocal 对象相关联的线程局部变量 1234567public void remove() &#123; // 获取当前线程对象中维护的 ThreadLocalMap 对象 ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) // map 存在则调用 map.remove，this时当前ThreadLocal，以this为key删除对应的实体 m.remove(this);&#125; LocalMap成员属性ThreadLocalMap 是 ThreadLocal 的内部类，没有实现 Map 接口，用独立的方式实现了 Map 的功能，其内部 Entry 也是独立实现 1234567891011// 初始化当前 map 内部散列表数组的初始长度 16private static final int INITIAL_CAPACITY = 16;// 存放数据的table，数组长度必须是2的整次幂。private Entry[] table;// 数组里面 entrys 的个数，可以用于判断 table 当前使用量是否超过阈值private int size = 0;// 进行扩容的阈值，表使用量大于它的时候进行扩容。private int threshold; 存储结构 Entry： Entry 继承 WeakReference，key 是弱引用，目的是将 ThreadLocal 对象的生命周期和线程生命周期解绑 Entry 限制只能用 ThreadLocal 作为 key，key 为 null (entry.get() &#x3D;&#x3D; null) 意味着 key 不再被引用，entry 也可以从 table 中清除 12345678static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; // this.referent = referent = key; super(k); value = v; &#125;&#125; 构造方法：延迟初始化的，线程第一次存储 threadLocal - value 时才会创建 threadLocalMap 对象 123456789101112ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; // 初始化table，创建一个长度为16的Entry数组 table = new Entry[INITIAL_CAPACITY]; // 【寻址算法】计算索引 int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); // 创建 entry 对象，存放到指定位置的 slot 中 table[i] = new Entry(firstKey, firstValue); // 数据总量是 1 size = 1; // 将阈值设置为 （当前数组长度 * 2）/ 3。 setThreshold(INITIAL_CAPACITY);&#125; 成员方法 set()：添加数据，ThreadLocalMap 使用线性探测法来解决哈希冲突 该方法会一直探测下一个地址，直到有空的地址后插入，若插入后 Map 数量超过阈值，数组会扩容为原来的 2 倍 假设当前 table 长度为16，计算出来 key 的 hash 值为 14，如果 table[14] 上已经有值，并且其 key 与当前 key 不一致，那么就发生了 hash 冲突，这个时候将 14 加 1 得到 15，取 table[15] 进行判断，如果还是冲突会回到 0，取 table[0]，以此类推，直到可以插入，可以把 Entry[] table 看成一个环形数组 线性探测法会出现堆积问题，可以采取平方探测法解决 在探测过程中 ThreadLocal 会复用 key 为 null 的脏 Entry 对象，并进行垃圾清理，防止出现内存泄漏 1234567891011121314151617181920212223242526272829303132333435private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // 获取散列表 ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; // 哈希寻址 int i = key.threadLocalHashCode &amp; (len-1); // 使用线性探测法向后查找元素，碰到 entry 为空时停止探测 for (ThreadLocal.ThreadLocalMap.Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; // 获取当前元素 key ThreadLocal&lt;?&gt; k = e.get(); // ThreadLocal 对应的 key 存在，【直接覆盖之前的值】 if (k == key) &#123; e.value = value; return; &#125; // 【这两个条件谁先成立不一定，所以 replaceStaleEntry 中还需要判断 k == key 的情况】 // key 为 null，但是值不为 null，说明之前的 ThreadLocal 对象已经被回收了，当前是【过期数据】 if (k == null) &#123; // 【碰到一个过期的 slot，当前数据复用该槽位，替换过期数据】 // 这个方法还进行了垃圾清理动作，防止内存泄漏 replaceStaleEntry(key, value, i); return; &#125; &#125;\t// 逻辑到这说明碰到 slot == null 的位置，则在空元素的位置创建一个新的 Entry tab[i] = new Entry(key, value); // 数量 + 1 int sz = ++size; // 【做一次启发式清理】，如果没有清除任何 entry 并且【当前使用量达到了负载因子所定义，那么进行 rehash if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) // 扩容 rehash();&#125; 12345// 获取【环形数组】的下一个索引private static int nextIndex(int i, int len) &#123; // 索引越界后从 0 开始继续获取 return ((i + 1 &lt; len) ? i + 1 : 0);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 在指定位置插入指定的数据private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; // 获取散列表 Entry[] tab = table; int len = tab.length; Entry e;\t// 探测式清理的开始下标，默认从当前 staleSlot 开始 int slotToExpunge = staleSlot; // 以当前 staleSlot 开始【向前迭代查找】，找到索引靠前过期数据，找到以后替换 slotToExpunge 值 // 【保证在一个区间段内，从最前面的过期数据开始清理】 for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i;\t// 以 staleSlot 【向后去查找】，直到碰到 null 为止，还是线性探测 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; // 获取当前节点的 key ThreadLocal&lt;?&gt; k = e.get(); // 条件成立说明是【替换逻辑】 if (k == key) &#123; e.value = value; // 因为本来要在 staleSlot 索引处插入该数据，现在找到了i索引处的key与数据一致 // 但是 i 位置距离正确的位置更远，因为是向后查找，所以还是要在 staleSlot 位置插入当前 entry // 然后将 table[staleSlot] 这个过期数据放到当前循环到的 table[i] 这个位置， tab[i] = tab[staleSlot]; tab[staleSlot] = e; // 条件成立说明向前查找过期数据并未找到过期的 entry，但 staleSlot 位置已经不是过期数据了，i 位置才是 if (slotToExpunge == staleSlot) slotToExpunge = i; // 【清理过期数据，expungeStaleEntry 探测式清理，cleanSomeSlots 启发式清理】 cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; // 条件成立说明当前遍历的 entry 是一个过期数据，并且该位置前面也没有过期数据 if (k == null &amp;&amp; slotToExpunge == staleSlot) // 探测式清理过期数据的开始下标修改为当前循环的 index，因为 staleSlot 会放入要添加的数据 slotToExpunge = i; &#125;\t// 向后查找过程中并未发现 k == key 的 entry，说明当前是一个【取代过期数据逻辑】 // 删除原有的数据引用，防止内存泄露 tab[staleSlot].value = null; // staleSlot 位置添加数据，【上面的所有逻辑都不会更改 staleSlot 的值】 tab[staleSlot] = new Entry(key, value); // 条件成立说明除了 staleSlot 以外，还发现其它的过期 slot，所以要【开启清理数据的逻辑】 if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);&#125; 1234private static int prevIndex(int i, int len) &#123; // 形成一个环绕式的访问，头索引越界后置为尾索引 return ((i - 1 &gt;= 0) ? i - 1 : len - 1);&#125; getEntry()：ThreadLocal 的 get 方法以当前的 ThreadLocal 为 key，调用 getEntry 获取对应的存储实体 e 1234567891011121314151617181920212223242526272829303132333435363738private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; // 哈希寻址 int i = key.threadLocalHashCode &amp; (table.length - 1); // 访问散列表中指定指定位置的 slot Entry e = table[i]; // 条件成立，说明 slot 有值并且 key 就是要寻找的 key，直接返回 if (e != null &amp;&amp; e.get() == key) return e; else // 进行线性探测 return getEntryAfterMiss(key, i, e);&#125;// 线性探测寻址private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; // 获取散列表 Entry[] tab = table; int len = tab.length; // 开始遍历，碰到 slot == null 的情况，搜索结束 while (e != null) &#123; // 获取当前 slot 中 entry 对象的 key ThreadLocal&lt;?&gt; k = e.get(); // 条件成立说明找到了，直接返回 if (k == key) return e; if (k == null) // 过期数据，【探测式过期数据回收】 expungeStaleEntry(i); else // 更新 index 继续向后走 i = nextIndex(i, len); // 获取下一个槽位中的 entry e = tab[i]; &#125; // 说明当前区段没有找到相应数据 // 【因为存放数据是线性的向后寻找槽位，都是紧挨着的，不可能越过一个 空槽位 在后面放】，可以减少遍历的次数 return null;&#125; rehash()：触发一次全量清理，如果数组长度大于等于长度的 2/3 * 3/4 = 1/2，则进行 resize 12345678private void rehash() &#123; // 清楚当前散列表内的【所有】过期的数据 expungeStaleEntries(); // threshold = len * 2 / 3，就是 2/3 * (1 - 1/4) if (size &gt;= threshold - threshold / 4) resize();&#125; 12345678910private void expungeStaleEntries() &#123; Entry[] tab = table; int len = tab.length; // 【遍历所有的槽位，清理过期数据】 for (int j = 0; j &lt; len; j++) &#123; Entry e = tab[j]; if (e != null &amp;&amp; e.get() == null) expungeStaleEntry(j); &#125;&#125; Entry 数组为扩容为原来的 2 倍 ，重新计算 key 的散列值，如果遇到 key 为 null 的情况，会将其 value 也置为 null，帮助 GC 123456789101112131415161718192021222324252627282930313233343536private void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; // 新数组的长度是老数组的二倍 int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; // 统计新table中的entry数量 int count = 0;\t// 遍历老表，进行【数据迁移】 for (int j = 0; j &lt; oldLen; ++j) &#123; // 访问老表的指定位置的 entry Entry e = oldTab[j]; // 条件成立说明老表中该位置有数据，可能是过期数据也可能不是 if (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); // 过期数据 if (k == null) &#123; e.value = null; // Help the GC &#125; else &#123; // 非过期数据，在新表中进行哈希寻址 int h = k.threadLocalHashCode &amp; (newLen - 1); // 【线程探测】 while (newTab[h] != null) h = nextIndex(h, newLen); // 将数据存放到新表合适的 slot 中 newTab[h] = e; count++; &#125; &#125; &#125;\t// 设置下一次触发扩容的指标：threshold = len * 2 / 3; setThreshold(newLen); size = count; // 将扩容后的新表赋值给 threadLocalMap 内部散列表数组引用 table = newTab;&#125; remove()：删除 Entry 12345678910111213141516private void remove(ThreadLocal&lt;?&gt; key) &#123; Entry[] tab = table; int len = tab.length; // 哈希寻址 int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; // 找到了对应的 key if (e.get() == key) &#123; // 设置 key 为 null e.clear(); // 探测式清理 expungeStaleEntry(i); return; &#125; &#125;&#125; 清理方法 探测式清理：沿着开始位置向后探测清理过期数据，沿途中碰到未过期数据则将此数据 rehash 在 table 数组中的定位，重定位后的元素理论上更接近 i = entry.key &amp; (table.length - 1)，让数据的排列更紧凑，会优化整个散列表查询性能 123456789101112131415161718192021222324252627282930313233343536373839404142// table[staleSlot] 是一个过期数据，以这个位置开始继续向后查找过期数据private int expungeStaleEntry(int staleSlot) &#123; // 获取散列表和数组长度 Entry[] tab = table; int len = tab.length; // help gc，先把当前过期的 entry 置空，在取消对 entry 的引用 tab[staleSlot].value = null; tab[staleSlot] = null; // 数量-1 size--; Entry e; int i; // 从 staleSlot 开始向后遍历，直到碰到 slot == null 结束，【区间内清理过期数据】 for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); // 当前 entry 是过期数据 if (k == null) &#123; // help gc e.value = null; tab[i] = null; size--; &#125; else &#123; // 当前 entry 不是过期数据的逻辑，【rehash】 // 重新计算当前 entry 对应的 index int h = k.threadLocalHashCode &amp; (len - 1); // 条件成立说明当前 entry 存储时发生过 hash 冲突，向后偏移过了 if (h != i) &#123; // 当前位置置空 tab[i] = null; // 以正确位置 h 开始，向后查找第一个可以存放 entry 的位置 while (tab[h] != null) h = nextIndex(h, len); // 将当前元素放入到【距离正确位置更近的位置，有可能就是正确位置】 tab[h] = e; &#125; &#125; &#125; // 返回 slot = null 的槽位索引，图例是 7，这个索引代表【索引前面的区间已经清理完成垃圾了】 return i;&#125; 启发式清理：向后循环扫描过期数据，发现过期数据调用探测式清理方法，如果连续几次的循环都没有发现过期数据，就停止扫描 12345678910111213141516171819202122232425262728// i 表示启发式清理工作开始位置，一般是空 slot，n 一般传递的是 table.length private boolean cleanSomeSlots(int i, int n) &#123; // 表示启发式清理工作是否清除了过期数据 boolean removed = false; // 获取当前 map 的散列表引用 Entry[] tab = table; int len = tab.length; do &#123; // 获取下一个索引，因为探测式返回的 slot 为 null i = nextIndex(i, len); Entry e = tab[i]; // 条件成立说明是过期的数据，key 被 gc 了 if (e != null &amp;&amp; e.get() == null) &#123; // 【发现过期数据重置 n 为数组的长度】 n = len; // 表示清理过过期数据 removed = true; // 以当前过期的 slot 为开始节点 做一次探测式清理工作 i = expungeStaleEntry(i); &#125; // 假设 table 长度为 16 // 16 &gt;&gt;&gt; 1 ==&gt; 8，8 &gt;&gt;&gt; 1 ==&gt; 4，4 &gt;&gt;&gt; 1 ==&gt; 2，2 &gt;&gt;&gt; 1 ==&gt; 1，1 &gt;&gt;&gt; 1 ==&gt; 0 // 连续经过这么多次循环【没有扫描到过期数据】，就停止循环，扫描到空 slot 不算，因为不是过期数据 &#125; while ((n &gt;&gt;&gt;= 1) != 0); // 返回清除标记 return removed;&#125; 参考视频：https://space.bilibili.com/457326371/ 内存泄漏Memory leak：内存泄漏是指程序中动态分配的堆内存由于某种原因未释放或无法释放，造成系统内存的浪费，导致程序运行速度减慢甚至系统崩溃等严重后果，内存泄漏的堆积终将导致内存溢出 如果 key 使用强引用：使用完 ThreadLocal ，threadLocal Ref 被回收，但是 threadLocalMap 的 Entry 强引用了 threadLocal，造成 threadLocal 无法被回收，无法完全避免内存泄漏 如果 key 使用弱引用：使用完 ThreadLocal ，threadLocal Ref 被回收，ThreadLocalMap 只持有 ThreadLocal 的弱引用，所以threadlocal 也可以被回收，此时 Entry 中的 key &#x3D; null。但没有手动删除这个 Entry 或者 CurrentThread 依然运行，依然存在强引用链，value 不会被回收，而这块 value 永远不会被访问到，也会导致 value 内存泄漏 两个主要原因： 没有手动删除这个 Entry CurrentThread 依然运行 根本原因：ThreadLocalMap 是 Thread的一个属性，生命周期跟 Thread 一样长，如果没有手动删除对应 Entry 就会导致内存泄漏 解决方法：使用完 ThreadLocal 中存储的内容后将它 remove 掉就可以 ThreadLocal 内部解决方法：在 ThreadLocalMap 中的 set&#x2F;getEntry 方法中，通过线性探测法对 key 进行判断，如果 key 为 null（ThreadLocal 为 null）会对 Entry 进行垃圾回收。所以使用弱引用比强引用多一层保障，就算不调用 remove，也有机会进行 GC 变量传递基本使用父子线程：创建子线程的线程是父线程，比如实例中的 main 线程就是父线程 ThreadLocal 中存储的是线程的局部变量，如果想实现线程间局部变量传递可以使用 InheritableThreadLocal 类 1234567public static void main(String[] args) &#123; ThreadLocal&lt;String&gt; threadLocal = new InheritableThreadLocal&lt;&gt;(); threadLocal.set(&quot;父线程设置的值&quot;); new Thread(() -&gt; System.out.println(&quot;子线程输出：&quot; + threadLocal.get())).start();&#125;// 子线程输出：父线程设置的值 实现原理InheritableThreadLocal 源码： 1234567891011public class InheritableThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; &#123; protected T childValue(T parentValue) &#123; return parentValue; &#125; ThreadLocalMap getMap(Thread t) &#123; return t.inheritableThreadLocals; &#125; void createMap(Thread t, T firstValue) &#123; t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue); &#125;&#125; 实现父子线程间的局部变量共享需要追溯到 Thread 对象的构造方法： 1234567891011121314151617private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, // 该参数默认是 true boolean inheritThreadLocals) &#123; // ... Thread parent = currentThread(); // 判断父线程（创建子线程的线程）的 inheritableThreadLocals 属性不为 null if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) &#123; // 复制父线程的 inheritableThreadLocals 属性，实现父子线程局部变量共享 this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); &#125; // ..&#125;// 【本质上还是创建 ThreadLocalMap，只是把父类中的可继承数据设置进去了】static ThreadLocalMap createInheritedMap(ThreadLocalMap parentMap) &#123; return new ThreadLocalMap(parentMap);&#125; 12345678910111213141516171819202122232425private ThreadLocalMap(ThreadLocalMap parentMap) &#123; // 获取父线程的哈希表 Entry[] parentTable = parentMap.table; int len = parentTable.length; setThreshold(len); table = new Entry[len];\t// 【逐个复制父线程 ThreadLocalMap 中的数据】 for (int j = 0; j &lt; len; j++) &#123; Entry e = parentTable[j]; if (e != null) &#123; ThreadLocal&lt;Object&gt; key = (ThreadLocal&lt;Object&gt;) e.get(); if (key != null) &#123; // 调用的是 InheritableThreadLocal#childValue(T parentValue) Object value = key.childValue(e.value); Entry c = new Entry(key, value); int h = key.threadLocalHashCode &amp; (len - 1); // 线性探测 while (table[h] != null) h = nextIndex(h, len); table[h] = c; size++; &#125; &#125; &#125;&#125; 参考文章：https://blog.csdn.net/feichitianxia/article/details/110495764 线程池基本概述线程池：一个容纳多个线程的容器，容器中的线程可以重复使用，省去了频繁创建和销毁线程对象的操作 线程池作用： 降低资源消耗，减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务 提高响应速度，当任务到达时，如果有线程可以直接用，不会出现系统僵死 提高线程的可管理性，如果无限制的创建线程，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控 线程池的核心思想：线程复用，同一个线程可以被重复使用，来处理多个任务 池化技术 (Pool) ：一种编程技巧，核心思想是资源复用，在请求量大时能优化应用性能，降低系统频繁建连的资源开销 阻塞队列基本介绍有界队列和无界队列： 有界队列：有固定大小的队列，比如设定了固定大小的 LinkedBlockingQueue，又或者大小为 0 无界队列：没有设置固定大小的队列，这些队列可以直接入队，直到溢出（超过 Integer.MAX_VALUE），所以相当于无界 java.util.concurrent.BlockingQueue 接口有以下阻塞队列的实现：FIFO 队列 ArrayBlockQueue：由数组结构组成的有界阻塞队列 LinkedBlockingQueue：由链表结构组成的无界（默认大小 Integer.MAX_VALUE）的阻塞队列 PriorityBlockQueue：支持优先级排序的无界阻塞队列 DelayedWorkQueue：使用优先级队列实现的延迟无界阻塞队列 SynchronousQueue：不存储元素的阻塞队列，每一个生产线程会阻塞到有一个 put 的线程放入元素为止 LinkedTransferQueue：由链表结构组成的无界阻塞队列 LinkedBlockingDeque：由链表结构组成的双向阻塞队列 与普通队列（LinkedList、ArrayList等）的不同点在于阻塞队列中阻塞添加和阻塞删除方法，以及线程安全： 阻塞添加 put()：当阻塞队列元素已满时，添加队列元素的线程会被阻塞，直到队列元素不满时才重新唤醒线程执行 阻塞删除 take()：在队列元素为空时，删除队列元素的线程将被阻塞，直到队列不为空再执行删除操作（一般会返回被删除的元素) 核心方法 方法类型 抛出异常 特殊值 阻塞 超时 插入（尾） add(e) offer(e) put(e) offer(e,time,unit) 移除（头） remove() poll() take() poll(time,unit) 检查（队首元素） element() peek() 不可用 不可用 抛出异常组： 当阻塞队列满时：在往队列中 add 插入元素会抛出 IIIegalStateException: Queue full 当阻塞队列空时：再往队列中 remove 移除元素，会抛出 NoSuchException 特殊值组： 插入方法：成功 true，失败 false 移除方法：成功返回出队列元素，队列没有就返回 null 阻塞组： 当阻塞队列满时，生产者继续往队列里 put 元素，队列会一直阻塞生产线程直到队列有空间 put 数据或响应中断退出 当阻塞队列空时，消费者线程试图从队列里 take 元素，队列会一直阻塞消费者线程直到队列中有可用元素 超时退出：当阻塞队列满时，队里会阻塞生产者线程一定时间，超过限时后生产者线程会退出 链表队列入队出队LinkedBlockingQueue 源码： 123456789101112131415public class LinkedBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123;\tstatic class Node&lt;E&gt; &#123; E item; /** * 下列三种情况之一 * - 真正的后继节点 * - 自己, 发生在出队时 * - null, 表示是没有后继节点, 是尾节点了 */ Node&lt;E&gt; next; Node(E x) &#123; item = x; &#125; &#125;&#125; 入队：尾插法 初始化链表 last = head = new Node&lt;E&gt;(null)，Dummy 节点用来占位，item 为 null 123456public LinkedBlockingQueue(int capacity) &#123; // 默认是 Integer.MAX_VALUE if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; 当一个节点入队： 1234private void enqueue(Node&lt;E&gt; node) &#123; // 从右向左计算 last = last.next = node;&#125; 再来一个节点入队 last = last.next = node 出队：出队头节点，FIFO 出队源码： 12345678910111213private E dequeue() &#123; Node&lt;E&gt; h = head; // 获取临头节点 Node&lt;E&gt; first = h.next; // 自己指向自己，help GC h.next = h; head = first; // 出队的元素 E x = first.item; // 【当前节点置为 Dummy 节点】 first.item = null; return x;&#125; h = head → first = h.next h.next = h → head = first first.item = null：当前节点置为 Dummy 节点 加锁分析用了两把锁和 dummy 节点： 用一把锁，同一时刻，最多只允许有一个线程（生产者或消费者，二选一）执行 用两把锁，同一时刻，可以允许两个线程同时（一个生产者与一个消费者）执行 消费者与消费者线程仍然串行 生产者与生产者线程仍然串行 线程安全分析： 当节点总数大于 2 时（包括 dummy 节点），putLock 保证的是 last 节点的线程安全，takeLock 保证的是 head 节点的线程安全，两把锁保证了入队和出队没有竞争 当节点总数等于 2 时（即一个 dummy 节点，一个正常节点）这时候，仍然是两把锁锁两个对象，不会竞争 当节点总数等于 1 时（就一个 dummy 节点）这时 take 线程会被 notEmpty 条件阻塞，有竞争，会阻塞 1234567// 用于 put(阻塞) offer(非阻塞)private final ReentrantLock putLock = new ReentrantLock();private final Condition notFull = putLock.newCondition();\t// 阻塞等待不满，说明已经满了// 用于 take(阻塞) poll(非阻塞)private final ReentrantLock takeLock = new ReentrantLock();private final Condition notEmpty = takeLock.newCondition();\t// 阻塞等待不空，说明已经是空的 入队出队： put 操作： 123456789101112131415161718192021222324252627282930313233public void put(E e) throws InterruptedException &#123; // 空指针异常 if (e == null) throw new NullPointerException(); int c = -1; // 把待添加的元素封装为 node 节点 Node&lt;E&gt; node = new Node&lt;E&gt;(e); // 获取全局生产锁 final ReentrantLock putLock = this.putLock; // count 用来维护元素计数 final AtomicInteger count = this.count; // 获取可打断锁，会抛出异常 putLock.lockInterruptibly(); try &#123; // 队列满了等待 while (count.get() == capacity) &#123; // 【等待队列不满时，就可以生产数据】，线程处于 Waiting notFull.await(); &#125; // 有空位, 入队且计数加一，尾插法 enqueue(node); // 返回自增前的数字 c = count.getAndIncrement(); // put 完队列还有空位, 唤醒其他生产 put 线程，唤醒一个减少竞争 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; // 解锁 putLock.unlock(); &#125; // c自增前是0，说明生产了一个元素，唤醒一个 take 线程 if (c == 0) signalNotEmpty();&#125; 12345678910private void signalNotEmpty() &#123; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; // 调用 notEmpty.signal()，而不是 notEmpty.signalAll() 是为了减少竞争，因为只剩下一个元素 notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125;&#125; take 操作： 1234567891011121314151617181920212223242526272829303132public E take() throws InterruptedException &#123; E x; int c = -1; // 元素个数 final AtomicInteger count = this.count; // 获取全局消费锁 final ReentrantLock takeLock = this.takeLock; // 可打断锁 takeLock.lockInterruptibly(); try &#123; // 没有元素可以出队 while (count.get() == 0) &#123; // 【阻塞等待队列不空，就可以消费数据】，线程处于 Waiting notEmpty.await(); &#125; // 出队，计数减一，FIFO，出队头节点 x = dequeue(); // 返回自减前的数字 c = count.getAndDecrement(); // 队列还有元素 if (c &gt; 1) // 唤醒一个消费take线程 notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; // c 是消费前的数据，消费前满了，消费一个后还剩一个空位，唤醒生产线程 if (c == capacity) // 调用的是 notFull.signal() 而不是 notFull.signalAll() 是为了减少竞争 signalNotFull(); return x;&#125; 性能比较主要列举 LinkedBlockingQueue 与 ArrayBlockingQueue 的性能比较： Linked 支持有界，Array 强制有界 Linked 实现是链表，Array 实现是数组 Linked 是懒惰的，而 Array 需要提前初始化 Node 数组 Linked 每次入队会生成新 Node，而 Array 的 Node 是提前创建好的 Linked 两把锁，Array 一把锁 同步队列成员属性SynchronousQueue 是一个不存储元素的 BlockingQueue，每一个生产者必须阻塞匹配到一个消费者 成员变量： 运行当前程序的平台拥有 CPU 的数量： 1static final int NCPUS = Runtime.getRuntime().availableProcessors() 指定超时时间后，当前线程最大自旋次数： 12// 只有一个 CPU 时自旋次数为 0，所有程序都是串行执行，多核 CPU 时自旋 32 次是一个经验值static final int maxTimedSpins = (NCPUS &lt; 2) ? 0 : 32; 自旋的原因：线程挂起唤醒需要进行上下文切换，涉及到用户态和内核态的转变，是非常消耗资源的。自旋期间线程会一直检查自己的状态是否被匹配到，如果自旋期间被匹配到，那么直接就返回了，如果自旋次数达到某个指标后，还是会将当前线程挂起 未指定超时时间，当前线程最大自旋次数： 1static final int maxUntimedSpins = maxTimedSpins * 16;\t// maxTimedSpins 的 16 倍 指定超时限制的阈值，小于该值的线程不会被挂起： 1static final long spinForTimeoutThreshold = 1000L;\t// 纳秒 超时时间设置的小于该值，就会被禁止挂起，阻塞再唤醒的成本太高，不如选择自旋空转 转换器： 123456789101112private transient volatile Transferer&lt;E&gt; transferer;abstract static class Transferer&lt;E&gt; &#123; /** * 参数一：可以为 null，null 时表示这个请求是一个 REQUEST 类型的请求，反之是一个 DATA 类型的请求 * 参数二：如果为 true 表示指定了超时时间，如果为 false 表示不支持超时，会一直阻塞到匹配或者被打断 * 参数三：超时时间限制，单位是纳秒 * 返回值：返回值如果不为 null 表示匹配成功，DATA 类型的请求返回当前线程 put 的数据 * 如果返回 null，表示请求超时或被中断 */ abstract E transfer(E e, boolean timed, long nanos);&#125; 构造方法： 12345public SynchronousQueue(boolean fair) &#123; // fair 默认 false // 非公平模式实现的数据结构是栈，公平模式的数据结构是队列 transferer = fair ? new TransferQueue&lt;E&gt;() : new TransferStack&lt;E&gt;();&#125; 成员方法： 1234567public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); return transferer.transfer(e, true, 0) != null;&#125;public E poll() &#123; return transferer.transfer(null, true, 0);&#125; 非公实现TransferStack 是非公平的同步队列，因为所有的请求都被压入栈中，栈顶的元素会最先得到匹配，造成栈底的等待线程饥饿 TransferStack 类成员变量： 请求类型： 12345678// 表示 Node 类型为请求类型static final int REQUEST = 0;// 表示 Node类 型为数据类型static final int DATA = 1;// 表示 Node 类型为匹配中类型// 假设栈顶元素为 REQUEST-NODE，当前请求类型为 DATA，入栈会修改类型为 FULFILLING 【栈顶 &amp; 栈顶之下的一个node】// 假设栈顶元素为 DATA-NODE，当前请求类型为 REQUEST，入栈会修改类型为 FULFILLING 【栈顶 &amp; 栈顶之下的一个node】static final int FULFILLING = 2; 栈顶元素： 1volatile SNode head; 内部类 SNode： 成员变量： 1234567891011121314static final class SNode &#123; // 指向下一个栈帧 volatile SNode next; // 与当前 node 匹配的节点 volatile SNode match; // 假设当前node对应的线程自旋期间未被匹配成功，那么node对应的线程需要挂起， // 挂起前 waiter 保存对应的线程引用，方便匹配成功后，被唤醒。 volatile Thread waiter; // 数据域，不为空表示当前 Node 对应的请求类型为 DATA 类型，反之则表示 Node 为 REQUEST 类型 Object item; // 表示当前Node的模式 【DATA/REQUEST/FULFILLING】 int mode;&#125; 构造方法： 123SNode(Object item) &#123; this.item = item;&#125; 设置方法：设置 Node 对象的 next 字段，此处对 CAS 进行了优化，提升了 CAS 的效率 1234boolean casNext(SNode cmp, SNode val) &#123; //【优化：cmp == next】，可以提升一部分性能。 cmp == next 不相等，就没必要走 cas指令。 return cmp == next &amp;&amp; UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val);&#125; 匹配方法： 1234567891011121314151617boolean tryMatch(SNode s) &#123; // 当前 node 尚未与任何节点发生过匹配，CAS 设置 match 字段为 s 节点，表示当前 node 已经被匹配 if (match == null &amp;&amp; UNSAFE.compareAndSwapObject(this, matchOffset, null, s)) &#123; // 当前 node 如果自旋结束，会 park 阻塞，阻塞前将 node 对应的 Thread 保留到 waiter 字段 // 获取当前 node 对应的阻塞线程 Thread w = waiter; // 条件成立说明 node 对应的 Thread 正在阻塞 if (w != null) &#123; waiter = null; // 使用 unpark 方式唤醒线程 LockSupport.unpark(w); &#125; return true; &#125; // 匹配成功返回 true return match == s;&#125; 取消方法： 123456789// 取消节点的方法void tryCancel() &#123; // match 字段指向自己，表示这个 node 是取消状态，取消状态的 node，最终会被强制移除出栈 UNSAFE.compareAndSwapObject(this, matchOffset, null, this);&#125;boolean isCancelled() &#123; return match == this;&#125; TransferStack 类成员方法： snode()：填充节点方法 12345678static SNode snode(SNode s, Object e, SNode next, int mode) &#123; // 引用指向空时，snode 方法会创建一个 SNode 对象 if (s == null) s = new SNode(e); // 填充数据 s.mode = mode; s.next = next; return s;&#125; transfer()：核心方法，请求匹配出栈，不匹配阻塞 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485E transfer(E e, boolean timed, long nanos) &#123;\t// 包装当前线程的 node SNode s = null; // 根据元素判断当前的请求类型 int mode = (e == null) ? REQUEST : DATA;\t// 自旋 for (;;) &#123; // 获取栈顶指针 SNode h = head; // 【CASE1】：当前栈为空或者栈顶 node 模式与当前请求模式一致无法匹配，做入栈操作 if (h == null || h.mode == mode) &#123; // 当前请求是支持超时的，但是 nanos &lt;= 0 说明这个请求不支持 “阻塞等待” if (timed &amp;&amp; nanos &lt;= 0) &#123; // 栈顶元素是取消状态 if (h != null &amp;&amp; h.isCancelled()) // 栈顶出栈，设置新的栈顶 casHead(h, h.next); else // 表示【匹配失败】 return null; // 入栈 &#125; else if (casHead(h, s = snode(s, e, h, mode))) &#123; // 等待被匹配的逻辑，正常情况返回匹配的节点；取消情况返回当前节点，就是 s SNode m = awaitFulfill(s, timed, nanos); // 说明当前 node 是【取消状态】 if (m == s) &#123; // 将取消节点出栈 clean(s); return null; &#125; // 执行到这说明【匹配成功】了 // 栈顶有节点并且 匹配节点还未出栈，需要协助出栈 if ((h = head) != null &amp;&amp; h.next == s) casHead(h, s.next); // 当前 node 模式为 REQUEST 类型，返回匹配节点的 m.item 数据域 // 当前 node 模式为 DATA 类型：返回 node.item 数据域，当前请求提交的数据 e return (E) ((mode == REQUEST) ? m.item : s.item); &#125; // 【CASE2】：逻辑到这说明请求模式不一致，如果栈顶不是 FULFILLING 说明没被其他节点匹配，【当前可以匹配】 &#125; else if (!isFulfilling(h.mode)) &#123; // 头节点是取消节点，match 指向自己，协助出栈 if (h.isCancelled()) casHead(h, h.next); // 入栈当前请求的节点 else if (casHead(h, s=snode(s, e, h, FULFILLING|mode))) &#123; for (;;) &#123; // m 是 s 的匹配的节点 SNode m = s.next; // m 节点在 awaitFulfill 方法中被中断，clean 了自己 if (m == null) &#123; // 清空栈 casHead(s, null); s = null; // 返回到外层自旋中 break; &#125; // 获取匹配节点的下一个节点 SNode mn = m.next; // 尝试匹配，【匹配成功】，则将 fulfilling 和 m 一起出栈，并且唤醒被匹配的节点的线程 if (m.tryMatch(s)) &#123; casHead(s, mn); return (E) ((mode == REQUEST) ? m.item : s.item); &#125; else // 匹配失败，出栈 m s.casNext(m, mn); &#125; &#125; // 【CASE3】：栈顶模式为 FULFILLING 模式，表示【栈顶和栈顶下面的节点正在发生匹配】，当前请求需要做协助工作 &#125; else &#123; // h 表示的是 fulfilling 节点，m 表示 fulfilling 匹配的节点 SNode m = h.next; if (m == null) // 清空栈 casHead(h, null); else &#123; SNode mn = m.next; // m 和 h 匹配，唤醒 m 中的线程 if (m.tryMatch(h)) casHead(h, mn); else h.casNext(m, mn); &#125; &#125; &#125;&#125; awaitFulfill()：阻塞当前线程等待被匹配，返回匹配的节点，或者被取消的节点 123456789101112131415161718192021222324252627282930313233343536373839404142SNode awaitFulfill(SNode s, boolean timed, long nanos) &#123; // 等待的截止时间 final long deadline = timed ? System.nanoTime() + nanos : 0L; // 当前线程 Thread w = Thread.currentThread(); // 表示当前请求线程在下面的 for(;;) 自旋检查的次数 int spins = (shouldSpin(s) ? (timed ? maxTimedSpins : maxUntimedSpins) : 0); // 自旋检查逻辑：是否匹配、是否超时、是否被中断 for (;;) &#123; // 当前线程收到中断信号，需要设置 node 状态为取消状态 if (w.isInterrupted()) s.tryCancel(); // 获取与当前 s 匹配的节点 SNode m = s.match; if (m != null) // 可能是正常的匹配的，也可能是取消的 return m; // 执行了超时限制就判断是否超时 if (timed) &#123; nanos = deadline - System.nanoTime(); // 【超时了，取消节点】 if (nanos &lt;= 0L) &#123; s.tryCancel(); continue; &#125; &#125; // 说明当前线程还可以进行自旋检查 if (spins &gt; 0) // 自旋一次 递减 1 spins = shouldSpin(s) ? (spins - 1) : 0; // 说明没有自旋次数了 else if (s.waiter == null) //【把当前 node 对应的 Thread 保存到 node.waiter 字段中，要阻塞了】 s.waiter = w; // 没有超时限制直接阻塞 else if (!timed) LockSupport.park(this); // nanos &gt; 1000 纳秒的情况下，才允许挂起当前线程 else if (nanos &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanos); &#125;&#125; 12345678boolean shouldSpin(SNode s) &#123; // 获取栈顶 SNode h = head; // 条件一成立说明当前 s 就是栈顶，允许自旋检查 // 条件二成立说明当前 s 节点自旋检查期间，又来了一个与当前 s 节点匹配的请求，双双出栈后条件会成立 // 条件三成立前提当前 s 不是栈顶元素，并且当前栈顶正在匹配中，这种状态栈顶下面的元素，都允许自旋检查 return (h == s || h == null || isFulfilling(h.mode));&#125; clear()：指定节点出栈 12345678910111213141516171819202122232425void clean(SNode s) &#123; // 清空数据域和关联线程 s.item = null; s.waiter = null; // 获取取消节点的下一个节点 SNode past = s.next; // 判断后继节点是不是取消节点，是就更新 past if (past != null &amp;&amp; past.isCancelled()) past = past.next; SNode p; // 从栈顶开始向下检查，【将栈顶开始向下的 取消状态 的节点全部清理出去】，直到碰到 past 或者不是取消状态为止 while ((p = head) != null &amp;&amp; p != past &amp;&amp; p.isCancelled()) // 修改的是内存地址对应的值，p 指向该内存地址所以数据一直在变化 casHead(p, p.next);\t// 说明中间遇到了不是取消状态的节点，继续迭代下去 while (p != null &amp;&amp; p != past) &#123; SNode n = p.next; if (n != null &amp;&amp; n.isCancelled()) p.casNext(n, n.next); else p = n; &#125;&#125; 公平实现TransferQueue 是公平的同步队列，采用 FIFO 的队列实现，请求节点与队尾模式不同，需要与队头发生匹配 TransferQueue 类成员变量： 指向队列的 dummy 节点： 1transient volatile QNode head; 指向队列的尾节点： 1transient volatile QNode tail; 被清理节点的前驱节点： 1transient volatile QNode cleanMe; 入队操作是两步完成的，第一步是 t.next &#x3D; newNode，第二步是 tail &#x3D; newNode，所以队尾节点出队，是一种非常特殊的情况 TransferQueue 内部类： QNode： 1234567891011121314151617181920212223242526272829303132static final class QNode &#123; // 指向当前节点的下一个节点 volatile QNode next; // 数据域，Node 代表的是 DATA 类型 item 表示数据，否则 Node 代表的 REQUEST 类型，item == null volatile Object item; // 假设当前 node 对应的线程自旋期间未被匹配成功，那么 node 对应的线程需要挂起， // 挂起前 waiter 保存对应的线程引用，方便匹配成功后被唤醒。 volatile Thread waiter; // true 当前 Node 是一个 DATA 类型，false 表示当前 Node 是一个 REQUEST 类型 final boolean isData;\t// 构建方法 QNode(Object item, boolean isData) &#123; this.item = item; this.isData = isData; &#125; // 尝试取消当前 node，取消状态的 node 的 item 域指向自己 void tryCancel(Object cmp) &#123; UNSAFE.compareAndSwapObject(this, itemOffset, cmp, this); &#125; // 判断当前 node 是否为取消状态 boolean isCancelled() &#123; return item == this; &#125; // 判断当前节点是否 “不在” 队列内，当 next 指向自己时，说明节点已经出队。 boolean isOffList() &#123; return next == this; &#125;&#125; TransferQueue 类成员方法： 设置头尾节点： 1234567891011void advanceHead(QNode h, QNode nh) &#123; // 设置头指针指向新的节点， if (h == head &amp;&amp; UNSAFE.compareAndSwapObject(this, headOffset, h, nh)) // 老的头节点出队 h.next = h;&#125;void advanceTail(QNode t, QNode nt) &#123; if (tail == t) // 更新队尾节点为新的队尾 UNSAFE.compareAndSwapObject(this, tailOffset, t, nt);&#125; transfer()：核心方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778E transfer(E e, boolean timed, long nanos) &#123; // s 指向当前请求对应的 node QNode s = null; // 是否是 DATA 类型的请求 boolean isData = (e != null);\t// 自旋 for (;;) &#123; QNode t = tail; QNode h = head; if (t == null || h == null) continue; // head 和 tail 同时指向 dummy 节点，说明是空队列 // 队尾节点与当前请求类型是一致的情况，说明阻塞队列中都无法匹配， if (h == t || t.isData == isData) &#123; // 获取队尾 t 的 next 节点 QNode tn = t.next; // 多线程环境中其他线程可能修改尾节点 if (t != tail) continue; // 已经有线程入队了，更新 tail if (tn != null) &#123; advanceTail(t, tn); continue; &#125; // 允许超时，超时时间小于 0，这种方法不支持阻塞等待 if (timed &amp;&amp; nanos &lt;= 0) return null; // 创建 node 的逻辑 if (s == null) s = new QNode(e, isData); // 将 node 添加到队尾 if (!t.casNext(null, s)) continue; // 更新队尾指针 advanceTail(t, s); // 当前节点 等待匹配.... Object x = awaitFulfill(s, e, timed, nanos); // 说明【当前 node 状态为 取消状态】，需要做出队逻辑 if (x == s) &#123; clean(t, s); return null; &#125; // 说明当前 node 仍然在队列内，匹配成功，需要做出队逻辑 if (!s.isOffList()) &#123; // t 是当前 s 节点的前驱节点，判断 t 是不是头节点，是就更新 dummy 节点为 s 节点 advanceHead(t, s); // s 节点已经出队，所以需要把它的 item 域设置为它自己，表示它是个取消状态 if (x != null) s.item = s; s.waiter = null; &#125; return (x != null) ? (E)x : e; // 队尾节点与当前请求节点【互补匹配】 &#125; else &#123; // h.next 节点，【请求节点与队尾模式不同，需要与队头发生匹配】，TransferQueue 是一个【公平模式】 QNode m = h.next; // 并发导致其他线程修改了队尾节点，或者已经把 head.next 匹配走了 if (t != tail || m == null || h != head) continue; // 获取匹配节点的数据域保存到 x Object x = m.item; // 判断是否匹配成功 if (isData == (x != null) || x == m || !m.casItem(x, e)) &#123; advanceHead(h, m); continue; &#125; // 【匹配完成】，将头节点出队，让这个新的头结点成为 dummy 节点 advanceHead(h, m); // 唤醒该匹配节点的线程 LockSupport.unpark(m.waiter); return (x != null) ? (E)x : e; &#125; &#125;&#125; awaitFulfill()：阻塞当前线程等待被匹配 12345678910111213141516171819202122232425262728293031323334353637383940414243Object awaitFulfill(QNode s, E e, boolean timed, long nanos) &#123; // 表示等待截止时间 final long deadline = timed ? System.nanoTime() + nanos : 0L; Thread w = Thread.currentThread(); // 自选检查的次数 int spins = ((head.next == s) ? (timed ? maxTimedSpins : maxUntimedSpins) : 0); for (;;) &#123; // 被打断就取消节点 if (w.isInterrupted()) s.tryCancel(e); // 获取当前 Node 数据域 Object x = s.item; // 当前请求为 DATA 模式时：e 请求带来的数据 // s.item 修改为 this，说明当前 QNode 对应的线程 取消状态 // s.item 修改为 null 表示已经有匹配节点了，并且匹配节点拿走了 item 数据 // 当前请求为 REQUEST 模式时：e == null // s.item 修改为 this，说明当前 QNode 对应的线程 取消状态 // s.item != null 且 item != this 表示当前 REQUEST 类型的 Node 已经匹配到 DATA 了 if (x != e) return x; // 超时检查 if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; s.tryCancel(e); continue; &#125; &#125; // 自旋次数减一 if (spins &gt; 0) --spins; // 没有自旋次数了，把当前线程封装进去 waiter else if (s.waiter == null) s.waiter = w; // 阻塞 else if (!timed) LockSupport.park(this); else if (nanos &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanos); &#125;&#125; 操作Pool创建方式Executor存放线程的容器： 1private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); 构造方法： 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 参数介绍： corePoolSize：核心线程数，定义了最小可以同时运行的线程数量 maximumPoolSize：最大线程数，当队列中存放的任务达到队列容量时，当前可以同时运行的数量变为最大线程数，创建线程并立即执行最新的任务，与核心线程数之间的差值又叫救急线程数 keepAliveTime：救急线程最大存活时间，当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等到 keepAliveTime 时间超过销毁 unit：keepAliveTime 参数的时间单位 workQueue：阻塞队列，存放被提交但尚未被执行的任务 threadFactory：线程工厂，创建新线程时用到，可以为线程创建时起名字 handler：拒绝策略，线程到达最大线程数仍有新任务时会执行拒绝策略 RejectedExecutionHandler 下有 4 个实现类： AbortPolicy：让调用者抛出 RejectedExecutionException 异常，默认策略 CallerRunsPolicy：让调用者运行的调节机制，将某些任务回退到调用者，从而降低新任务的流量 DiscardPolicy：直接丢弃任务，不予任何处理也不抛出异常 DiscardOldestPolicy：放弃队列中最早的任务，把当前任务加入队列中尝试再次提交当前任务 补充：其他框架拒绝策略 Dubbo：在抛出 RejectedExecutionException 异常前记录日志，并 dump 线程栈信息，方便定位问题 Netty：创建一个新线程来执行任务 ActiveMQ：带超时等待（60s）尝试放入队列 PinPoint：它使用了一个拒绝策略链，会逐一尝试策略链中每种拒绝策略 工作原理： 创建线程池，这时没有创建线程（懒惰），等待提交过来的任务请求，调用 execute 方法才会创建线程 当调用 execute() 方法添加一个请求任务时，线程池会做如下判断： 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列 如果这时队列满了且正在运行的线程数量还小于 maximumPoolSize，那么会创建非核心线程立刻运行这个任务，对于阻塞队列中的任务不公平。这是因为创建每个 Worker（线程）对象会绑定一个初始任务，启动 Worker 时会优先执行 如果队列满了且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会启动饱和拒绝策略来执行 当一个线程完成任务时，会从队列中取下一个任务来执行 当一个线程空闲超过一定的时间（keepAliveTime）时，线程池会判断：如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉，所以线程池的所有任务完成后最终会收缩到 corePoolSize 大小 图片来源：https://space.bilibili.com/457326371/ ExecutorsExecutors 提供了四种线程池的创建：newCachedThreadPool、newFixedThreadPool、newSingleThreadExecutor、newScheduledThreadPool newFixedThreadPool：创建一个拥有 n 个线程的线程池 1234public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 核心线程数 &#x3D;&#x3D; 最大线程数（没有救急线程被创建），因此也无需超时时间 LinkedBlockingQueue 是一个单向链表实现的阻塞队列，默认大小为 Integer.MAX_VALUE，也就是无界队列，可以放任意数量的任务，在任务比较多的时候会导致 OOM（内存溢出） 适用于任务量已知，相对耗时的长期任务 newCachedThreadPool：创建一个可扩容的线程池 1234public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 核心线程数是 0， 最大线程数是 29 个 1，全部都是救急线程（60s 后可以回收），可能会创建大量线程，从而导致 OOM SynchronousQueue 作为阻塞队列，没有容量，对于每一个 take 的线程会阻塞直到有一个 put 的线程放入元素为止（类似一手交钱、一手交货） 适合任务数比较密集，但每个任务执行时间较短的情况 newSingleThreadExecutor：创建一个只有 1 个线程的单线程池 12345public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1,0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 保证所有任务按照指定顺序执行，线程数固定为 1，任务数多于 1 时会放入无界队列排队，任务执行完毕，这唯一的线程也不会被释放 对比： 创建一个单线程串行执行任务，如果任务执行失败而终止那么没有任何补救措施，线程池会新建一个线程，保证池的正常工作 Executors.newSingleThreadExecutor() 线程个数始终为 1，不能修改。FinalizableDelegatedExecutorService 应用的是装饰器模式，只对外暴露了 ExecutorService 接口，因此不能调用 ThreadPoolExecutor 中特有的方法 原因：父类不能直接调用子类中的方法，需要反射或者创建对象的方式，可以调用子类静态方法 Executors.newFixedThreadPool(1) 初始时为 1，可以修改。对外暴露的是 ThreadPoolExecutor 对象，可以强转后调用 setCorePoolSize 等方法进行修改 开发要求阿里巴巴 Java 开发手册要求： 线程资源必须通过线程池提供，不允许在应用中自行显式创建线程 使用线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源的开销，解决资源不足的问题 如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者过度切换的问题 线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式更加明确线程池的运行规则，规避资源耗尽的风险 Executors 返回的线程池对象弊端如下： FixedThreadPool 和 SingleThreadPool：请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM CacheThreadPool 和 ScheduledThreadPool：允许创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，导致 OOM 创建多大容量的线程池合适？ 一般来说池中总线程数是核心池线程数量两倍，确保当核心池有线程停止时，核心池外有线程进入核心池 过小会导致程序不能充分地利用系统资源、容易导致饥饿 过大会导致更多的线程上下文切换，占用更多内存 上下文切换：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态，任务从保存到再加载的过程就是一次上下文切换 核心线程数常用公式： CPU 密集型任务 (N+1)： 这种任务消耗的是 CPU 资源，可以将核心线程数设置为 N (CPU 核心数) + 1，比 CPU 核心数多出来的一个线程是为了防止线程发生缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 某个核心就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间 CPU 密集型简单理解就是利用 CPU 计算能力的任务比如在内存中对大量数据进行分析 I&#x2F;O 密集型任务： 这种系统 CPU 处于阻塞状态，用大部分的时间来处理 I&#x2F;O 交互，而线程在处理 I&#x2F;O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用，因此在 I&#x2F;O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N 或 CPU 核数&#x2F; (1-阻塞系数)，阻塞系数在 0.8~0.9 之间 IO 密集型就是涉及到网络读取，文件读取此类任务 ，特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上 提交方法ExecutorService 类 API： 方法 说明 void execute(Runnable command) 执行任务（Executor 类 API） Future&lt;?&gt; submit(Runnable task) 提交任务 task() Future submit(Callable task) 提交任务 task，用返回值 Future 获得任务执行结果 List&lt;Future&gt; invokeAll(Collection&lt;? extends Callable&gt; tasks) 提交 tasks 中所有任务 List&lt;Future&gt; invokeAll(Collection&lt;? extends Callable&gt; tasks, long timeout, TimeUnit unit) 提交 tasks 中所有任务，超时时间针对所有task，超时会取消没有执行完的任务，并抛出超时异常 T invokeAny(Collection&lt;? extends Callable&gt; tasks) 提交 tasks 中所有任务，哪个任务先成功执行完毕，返回此任务执行结果，其它任务取消 execute 和 submit 都属于线程池的方法，对比： execute 只能执行 Runnable 类型的任务，没有返回值； submit 既能提交 Runnable 类型任务也能提交 Callable 类型任务，底层是封装成 FutureTask，然后调用 execute 执行 execute 会直接抛出任务执行时的异常，submit 会吞掉异常，可通过 Future 的 get 方法将任务执行时的异常重新抛出 关闭方法ExecutorService 类 API： 方法 说明 void shutdown() 线程池状态变为 SHUTDOWN，等待任务执行完后关闭线程池，不会接收新任务，但已提交任务会执行完，而且也可以添加线程（不绑定任务） List shutdownNow() 线程池状态变为 STOP，用 interrupt 中断正在执行的任务，直接关闭线程池，不会接收新任务，会将队列中的任务返回 boolean isShutdown() 不在 RUNNING 状态的线程池，此执行者已被关闭，方法返回 true boolean isTerminated() 线程池状态是否是 TERMINATED，如果所有任务在关闭后完成，返回 true boolean awaitTermination(long timeout, TimeUnit unit) 调用 shutdown 后，由于调用线程不会等待所有任务运行结束，如果它想在线程池 TERMINATED 后做些事情，可以利用此方法等待 处理异常execute 会直接抛出任务执行时的异常，submit 会吞掉异常，有两种处理方法 方法 1：主动捉异常 123456789ExecutorService executorService = Executors.newFixedThreadPool(1);pool.submit(() -&gt; &#123; try &#123; System.out.println(&quot;task1&quot;); int i = 1 / 0; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125;); 方法 2：使用 Future 对象 1234567ExecutorService executorService = Executors.newFixedThreadPool(1);Future&lt;?&gt; future = pool.submit(() -&gt; &#123; System.out.println(&quot;task1&quot;); int i = 1 / 0; return true;&#125;);System.out.println(future.get()); 工作原理状态信息ThreadPoolExecutor 使用 int 的高 3 位来表示线程池状态，低 29 位表示线程数量。这些信息存储在一个原子变量 ctl 中，目的是将线程池状态与线程个数合二为一，这样就可以用一次 CAS 原子操作进行赋值 状态表示： 123456// 高3位：表示当前线程池运行状态，除去高3位之后的低位：表示当前线程池中所拥有的线程数量private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));// 表示在 ctl 中，低 COUNT_BITS 位，是用于存放当前线程数量的位private static final int COUNT_BITS = Integer.SIZE - 3;// 低 COUNT_BITS 位所能表达的最大数值，000 11111111111111111111 =&gt; 5亿多private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; 四种状态： 12345678910// 111 000000000000000000，转换成整数后其实就是一个【负数】private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;// 000 000000000000000000private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;// 001 000000000000000000private static final int STOP = 1 &lt;&lt; COUNT_BITS;// 010 000000000000000000private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;// 011 000000000000000000private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; 状态 高3位 接收新任务 处理阻塞任务队列 说明 RUNNING 111 Y Y SHUTDOWN 000 N Y 不接收新任务，但处理阻塞队列剩余任务 STOP 001 N N 中断正在执行的任务，并抛弃阻塞队列任务 TIDYING 010 - - 任务全执行完毕，活动线程为 0 即将进入终结 TERMINATED 011 - - 终止状态 获取当前线程池运行状态： 123456// ~CAPACITY = ~000 11111111111111111111 = 111 000000000000000000000（取反）// c == ctl = 111 000000000000000000111// 111 000000000000000000111// 111 000000000000000000000// 111 000000000000000000000\t获取到了运行状态private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125; 获取当前线程池线程数量： 1234// c = 111 000000000000000000111// CAPACITY = 000 111111111111111111111// 000 000000000000000000111 =&gt; 7private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125; 重置当前线程池状态 ctl： 12// rs 表示线程池状态，wc 表示当前线程池中 worker（线程）数量，相与以后就是合并后的状态private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; 比较当前线程池 ctl 所表示的状态： 1234567// 比较当前线程池 ctl 所表示的状态，是否小于某个状态 s// 状态对比：RUNNING &lt; SHUTDOWN &lt; STOP &lt; TIDYING &lt; TERMINATEDprivate static boolean runStateLessThan(int c, int s) &#123; return c &lt; s; &#125;// 比较当前线程池 ctl 所表示的状态，是否大于等于某个状态sprivate static boolean runStateAtLeast(int c, int s) &#123; return c &gt;= s; &#125;// 小于 SHUTDOWN 的一定是 RUNNING，SHUTDOWN == 0private static boolean isRunning(int c) &#123; return c &lt; SHUTDOWN; &#125; 设置线程池 ctl： 123456789101112// 使用 CAS 方式 让 ctl 值 +1 ，成功返回 true, 失败返回 falseprivate boolean compareAndIncrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect + 1);&#125;// 使用 CAS 方式 让 ctl 值 -1 ，成功返回 true, 失败返回 falseprivate boolean compareAndDecrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect - 1);&#125;// 将 ctl 值减一，do while 循环会一直重试，直到成功为止private void decrementWorkerCount() &#123; do &#123;&#125; while (!compareAndDecrementWorkerCount(ctl.get()));&#125; 成员属性成员变量 线程池中存放 Worker 的容器：线程池没有初始化，直接往池中加线程即可 1private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); 线程全局锁： 12// 增加减少 worker 或者时修改线程池运行状态需要持有 mainLockprivate final ReentrantLock mainLock = new ReentrantLock(); 可重入锁的条件变量： 12// 当外部线程调用 awaitTermination() 方法时，会等待当前线程池状态为 Termination 为止private final Condition termination = mainLock.newCondition() 线程池相关参数： 12345private volatile int corePoolSize; // 核心线程数量private volatile int maximumPoolSize; // 线程池最大线程数量private volatile long keepAliveTime; // 空闲线程存活时间private volatile ThreadFactory threadFactory;\t// 创建线程时使用的线程工厂，默认是 DefaultThreadFactoryprivate final BlockingQueue&lt;Runnable&gt; workQueue;// 【超过核心线程提交任务就放入 阻塞队列】 12private volatile RejectedExecutionHandler handler;\t// 拒绝策略，juc包提供了4中方式private static final RejectedExecutionHandler defaultHandler = new AbortPolicy();// 默认策略 记录线程池相关属性的数值： 12private int largestPoolSize; // 记录线程池生命周期内线程数最大值private long completedTaskCount;\t// 记录线程池所完成任务总数，当某个 worker 退出时将完成的任务累加到该属性 控制核心线程数量内的线程是否可以被回收： 123// false（默认）代表不可以，为 true 时核心线程空闲超过 keepAliveTime 也会被回收// allowCoreThreadTimeOut(boolean value) 方法可以设置该值private volatile boolean allowCoreThreadTimeOut; 内部类： Worker 类：每个 Worker 对象会绑定一个初始任务，启动 Worker 时优先执行，这也是造成线程池不公平的原因。Worker 继承自 AQS，本身具有锁的特性，采用独占锁模式，state &#x3D; 0 表示未被占用，&gt; 0 表示被占用，&lt; 0 表示初始状态不能被抢锁 1234567891011121314151617181920212223private final class Worker extends AbstractQueuedSynchronizer implements Runnable &#123;\tfinal Thread thread; // worker 内部封装的工作线程 Runnable firstTask; // worker 第一个执行的任务，普通的 Runnable 实现类或者是 FutureTask volatile long completedTasks;\t// 记录当前 worker 所完成任务数量 // 构造方法 Worker(Runnable firstTask) &#123; // 设置AQS独占模式为初始化中状态，这个状态不能被抢占锁 setState(-1); // firstTask不为空时，当worker启动后，内部线程会优先执行firstTask，执行完后会到queue中去获取下个任务 this.firstTask = firstTask; // 使用线程工厂创建一个线程，并且【将当前worker指定为Runnable】，所以thread启动时会调用 worker.run() this.thread = getThreadFactory().newThread(this); &#125; // 【不可重入锁】 protected boolean tryAcquire(int unused) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125;&#125; 123456789public Thread newThread(Runnable r) &#123; // 将当前 worker 指定为 thread 的执行方法，线程调用 start 会调用 r.run() Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t;&#125; 拒绝策略相关的内部类 成员方法提交方法 AbstractExecutorService#submit()：提交任务，把 Runnable 或 Callable 任务封装成 FutureTask 执行，可以通过方法返回的任务对象，调用 get 阻塞获取任务执行的结果或者异常，源码分析在笔记的 Future 部分 123456789101112131415161718public Future&lt;?&gt; submit(Runnable task) &#123; // 空指针异常 if (task == null) throw new NullPointerException(); // 把 Runnable 封装成未来任务对象，执行结果就是 null，也可以通过参数指定 FutureTask#get 返回数据 RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); // 执行方法 execute(ftask); return ftask;&#125;public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); // 把 Callable 封装成未来任务对象 RunnableFuture&lt;T&gt; ftask = newTaskFor(task); // 执行方法 execute(ftask); // 返回未来任务对象，用来获取返回值 return ftask;&#125; 12345678protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; // Runnable 封装成 FutureTask，【指定返回值】 return new FutureTask&lt;T&gt;(runnable, value);&#125;protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; // Callable 直接封装成 FutureTask return new FutureTask&lt;T&gt;(callable);&#125; execute()：执行任务，但是没有返回值，没办法获取任务执行结果，出现异常会直接抛出任务执行时的异常。根据线程池中的线程数，选择添加任务时的处理方式 12345678910111213141516171819202122232425262728293031323334353637// command 可以是普通的 Runnable 实现类，也可以是 FutureTask，不能是 Callablepublic void execute(Runnable command) &#123; // 非空判断 if (command == null) throw new NullPointerException(); // 获取 ctl 最新值赋值给 c，ctl 高 3 位表示线程池状态，低位表示当前线程池线程数量。 int c = ctl.get(); // 【1】当前线程数量小于核心线程数，此次提交任务直接创建一个新的 worker，线程池中多了一个新的线程 if (workerCountOf(c) &lt; corePoolSize) &#123; // addWorker 为创建线程的过程，会创建 worker 对象并且将 command 作为 firstTask，优先执行 if (addWorker(command, true)) return; // 执行到这条语句，说明 addWorker 一定是失败的，存在并发现象或者线程池状态被改变，重新获取状态 // SHUTDOWN 状态下也有可能创建成功，前提 firstTask == null 而且当前 queue 不为空（特殊情况） c = ctl.get(); &#125; // 【2】执行到这说明当前线程数量已经达到核心线程数量 或者 addWorker 失败 // 判断当前线程池是否处于running状态，成立就尝试将 task 放入到 workQueue 中 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // 条件一成立说明线程池状态被外部线程给修改了，可能是执行了 shutdown() 方法，该状态不能接收新提交的任务 // 所以要把刚提交的任务删除，删除成功说明提交之后线程池中的线程还未消费（处理）该任务 if (!isRunning(recheck) &amp;&amp; remove(command)) // 任务出队成功，走拒绝策略 reject(command); // 执行到这说明线程池是 running 状态，获取线程池中的线程数量，判断是否是 0 // 【担保机制】，保证线程池在 running 状态下，最起码得有一个线程在工作 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; // 【3】offer失败说明queue满了 // 如果线程数量尚未达到 maximumPoolSize，会创建非核心 worker 线程直接执行 command，【这也是不公平的原因】 // 如果当前线程数量达到 maximumPoolSiz，这里 addWorker 也会失败，走拒绝策略 else if (!addWorker(command, false)) reject(command);&#125; 添加线程 prestartAllCoreThreads()：提前预热，创建所有的核心线程 123456public int prestartAllCoreThreads() &#123; int n = 0; while (addWorker(null, true)) ++n; return n;&#125; addWorker()：添加线程到线程池，返回 true 表示创建 Worker 成功，且线程启动。首先判断线程池是否允许添加线程，允许就让线程数量 + 1，然后去创建 Worker 加入线程池 注意：SHUTDOWN 状态也能添加线程，但是要求新加的 Woker 没有 firstTask，而且当前 queue 不为空，所以创建一个线程来帮助线程池执行队列中的任务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// core == true 表示采用核心线程数量限制，false 表示采用 maximumPoolSizeprivate boolean addWorker(Runnable firstTask, boolean core) &#123; // 自旋【判断当前线程池状态是否允许创建线程】，允许就设置线程数量 + 1 retry: for (;;) &#123; // 获取 ctl 的值 int c = ctl.get(); // 获取当前线程池运行状态 int rs = runStateOf(c); // 判断当前线程池状态【是否允许添加线程】 // 当前线程池是 SHUTDOWN 状态，但是队列里面还有任务尚未处理完，需要处理完 queue 中的任务 // 【不允许再提交新的 task，所以 firstTask 为空，但是可以继续添加 worker】 if (rs &gt;= SHUTDOWN &amp;&amp; !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; !workQueue.isEmpty())) return false; for (;;) &#123; // 获取线程池中线程数量 int wc = workerCountOf(c); // 条件一一般不成立，CAPACITY是5亿多，根据 core 判断使用哪个大小限制线程数量，超过了返回 false if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // 记录线程数量已经加 1，类比于申请到了一块令牌，条件失败说明其他线程修改了数量 if (compareAndIncrementWorkerCount(c)) // 申请成功，跳出了 retry 这个 for 自旋 break retry; // CAS 失败，没有成功的申请到令牌 c = ctl.get(); // 判断当前线程池状态是否发生过变化，被其他线程修改了，可能其他线程调用了 shutdown() 方法 if (runStateOf(c) != rs) // 返回外层循环检查是否能创建线程，在 if 语句中返回 false continue retry; &#125; &#125; //【令牌申请成功，开始创建线程】 // 运行标记，表示创建的 worker 是否已经启动，false未启动 true启动 boolean workerStarted = false; // 添加标记，表示创建的 worker 是否添加到池子中了，默认false未添加，true是添加。 boolean workerAdded = false; Worker w = null; try &#123; // 【创建 Worker，底层通过线程工厂 newThread 方法创建执行线程，指定了首先执行的任务】 w = new Worker(firstTask); // 将新创建的 worker 节点中的线程赋值给 t final Thread t = w.thread; // 这里的判断为了防止 程序员自定义的 ThreadFactory 实现类有 bug，创造不出线程 if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; // 加互斥锁，要添加 worker 了 mainLock.lock(); try &#123; // 获取最新线程池运行状态保存到 rs int rs = runStateOf(ctl.get()); // 判断线程池是否为RUNNING状态，不是再【判断当前是否为SHUTDOWN状态且firstTask为空，特殊情况】 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; // 当线程start后，线程isAlive会返回true，这里还没开始启动线程，如果被启动了就需要报错 if (t.isAlive()) throw new IllegalThreadStateException(); //【将新建的 Worker 添加到线程池中】 workers.add(w); int s = workers.size(); // 当前池中的线程数量是一个新高，更新 largestPoolSize if (s &gt; largestPoolSize) largestPoolSize = s; // 添加标记置为 true workerAdded = true; &#125; &#125; finally &#123; // 解锁啊 mainLock.unlock(); &#125; // 添加成功就【启动线程执行任务】 if (workerAdded) &#123; // Thread 类中持有 Runnable 任务对象，调用的是 Runnable 的 run ，也就是 FutureTask t.start(); // 运行标记置为 true workerStarted = true; &#125; &#125; &#125; finally &#123; // 如果启动线程失败，做清理工作 if (! workerStarted) addWorkerFailed(w); &#125; // 返回新创建的线程是否启动 return workerStarted;&#125; addWorkerFailed()：清理任务 1234567891011121314151617private void addWorkerFailed(Worker w) &#123; final ReentrantLock mainLock = this.mainLock; // 持有线程池全局锁，因为操作的是线程池相关的东西 mainLock.lock(); try &#123; //条件成立需要将 worker 在 workers 中清理出去。 if (w != null) workers.remove(w); // 将线程池计数 -1，相当于归还令牌。 decrementWorkerCount(); // 尝试停止线程池 tryTerminate(); &#125; finally &#123; //释放线程池全局锁。 mainLock.unlock(); &#125;&#125; 运行方法 Worker#run：Worker 实现了 Runnable 接口，当线程启动时，会调用 Worker 的 run() 方法 1234public void run() &#123; // ThreadPoolExecutor#runWorker() runWorker(this);&#125; runWorker()：线程启动就要执行任务，会一直 while 循环获取任务并执行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); // 获取 worker 的 firstTask Runnable task = w.firstTask; // 引用置空，【防止复用该线程时重复执行该任务】 w.firstTask = null; // 初始化 worker 时设置 state = -1，表示不允许抢占锁 // 这里需要设置 state = 0 和 exclusiveOwnerThread = null，开始独占模式抢锁 w.unlock(); // true 表示发生异常退出，false 表示正常退出。 boolean completedAbruptly = true; try &#123; // firstTask 不是 null 就直接运行，否则去 queue 中获取任务 // 【getTask 如果是阻塞获取任务，会一直阻塞在take方法，直到获取任务，不会走返回null的逻辑】 while (task != null || (task = getTask()) != null) &#123; // worker 加锁，shutdown 时会判断当前 worker 状态，【根据独占锁状态判断是否空闲】 w.lock(); // 说明线程池状态大于 STOP，目前处于 STOP/TIDYING/TERMINATION，此时给线程一个中断信号 if ((runStateAtLeast(ctl.get(), STOP) || // 说明线程处于 RUNNING 或者 SHUTDOWN 状态，清除打断标记 (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) // 中断线程，设置线程的中断标志位为 true wt.interrupt(); try &#123; // 钩子方法，【任务执行的前置处理】 beforeExecute(wt, task); Throwable thrown = null; try &#123; // 【执行任务】 task.run(); &#125; catch (Exception x) &#123; //..... &#125; finally &#123; // 钩子方法，【任务执行的后置处理】 afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; // 将局部变量task置为null，代表任务执行完成 w.completedTasks++;\t// 更新worker完成任务数量 w.unlock(); // 解锁 &#125; &#125; // getTask()方法返回null时会走到这里，表示queue为空并且线程空闲超过保活时间，【当前线程执行退出逻辑】 completedAbruptly = false; &#125; finally &#123; // 正常退出 completedAbruptly = false // 异常退出 completedAbruptly = true，【从 task.run() 内部抛出异常】时，跳到这一行 processWorkerExit(w, completedAbruptly); &#125;&#125; unlock()：重置锁 1234567public void unlock() &#123; release(1); &#125;// 外部不会直接调用这个方法 这个方法是 AQS 内调用的，外部调用 unlock 时触发此方法protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); // 设置持有者为 null setState(0); // 设置 state = 0 return true;&#125; getTask()：获取任务，线程空闲时间超过 keepAliveTime 就会被回收，判断的依据是当前线程阻塞获取任务超过保活时间，方法返回 null 就代表当前线程要被回收了，返回到 runWorker 执行线程退出逻辑。线程池具有担保机制，对于 RUNNING 状态下的超时回收，要保证线程池中最少有一个线程运行，或者任务阻塞队列已经是空 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private Runnable getTask() &#123; // 超时标记，表示当前线程获取任务是否超时，true 表示已超时 boolean timedOut = false; for (;;) &#123; int c = ctl.get(); // 获取线程池当前运行状态 int rs = runStateOf(c); // 【tryTerminate】打断线程后执行到这，此时线程池状态为STOP或者线程池状态为SHUTDOWN并且队列已经是空 // 所以下面的 if 条件一定是成立的，可以直接返回 null，线程就应该退出了 if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; // 使用 CAS 自旋的方式让 ctl 值 -1 decrementWorkerCount(); return null; &#125; // 获取线程池中的线程数量 int wc = workerCountOf(c); // 线程没有明确的区分谁是核心或者非核心线程，是根据当前池中的线程数量判断 // timed = false 表示当前这个线程 获取task时不支持超时机制的，当前线程会使用 queue.take() 阻塞获取 // timed = true 表示当前这个线程 获取task时支持超时机制，使用 queue.poll(xxx,xxx) 超时获取 // 条件一代表允许回收核心线程，那就无所谓了，全部线程都执行超时回收 // 条件二成立说明线程数量大于核心线程数，当前线程认为是非核心线程，有保活时间，去超时获取任务 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; // 如果线程数量是否超过最大线程数，直接回收 // 如果当前线程【允许超时回收并且已经超时了】，就应该被回收了，由于【担保机制】还要做判断： // wc &gt; 1 说明线程池还用其他线程，当前线程可以直接回收 // workQueue.isEmpty() 前置条件是 wc = 1，【如果当前任务队列也是空了，最后一个线程就可以退出】 if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; // 使用 CAS 机制将 ctl 值 -1 ,减 1 成功的线程，返回 null，代表可以退出 if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; // 根据当前线程是否需要超时回收，【选择从队列获取任务的方法】是超时获取或者阻塞获取 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); // 获取到任务返回任务，【阻塞获取会阻塞到获取任务为止】，不会返回 null if (r != null) return r; // 获取任务为 null 说明超时了，将超时标记设置为 true，下次自旋时返 null timedOut = true; &#125; catch (InterruptedException retry) &#123; // 阻塞线程被打断后超时标记置为 false，【说明被打断不算超时】，要继续获取，直到超时或者获取到任务 // 如果线程池 SHUTDOWN 状态下的打断，会在循环获取任务前判断，返回 null timedOut = false; &#125; &#125;&#125; processWorkerExit()：线程退出线程池，也有担保机制，保证队列中的任务被执行 12345678910111213141516171819202122232425262728293031323334353637383940// 正常退出 completedAbruptly = false，异常退出为 trueprivate void processWorkerExit(Worker w, boolean completedAbruptly) &#123; // 条件成立代表当前 worker 是发生异常退出的，task 任务执行过程中向上抛出异常了 if (completedAbruptly) // 从异常时到这里 ctl 一直没有 -1，需要在这里 -1 decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; // 加锁 mainLock.lock(); try &#123; // 将当前 worker 完成的 task 数量，汇总到线程池的 completedTaskCount completedTaskCount += w.completedTasks; // 将 worker 从线程池中移除 workers.remove(w); &#125; finally &#123; mainLock.unlock();\t// 解锁 &#125;\t// 尝试停止线程池，唤醒下一个线程 tryTerminate(); int c = ctl.get(); // 线程池不是停止状态就应该有线程运行【担保机制】 if (runStateLessThan(c, STOP)) &#123; // 正常退出的逻辑，是对空闲线程回收，不是执行出错 if (!completedAbruptly) &#123; // 根据是否回收核心线程确定【线程池中的线程数量最小值】 int min = allowCoreThreadTimeOut ? 0 : corePoolSize; // 最小值为 0，但是线程队列不为空，需要一个线程来完成任务担保机制 if (min == 0 &amp;&amp; !workQueue.isEmpty()) min = 1; // 线程池中的线程数量大于最小值可以直接返回 if (workerCountOf(c) &gt;= min) return; &#125; // 执行 task 时发生异常，有个线程因为异常终止了，需要添加 // 或者线程池中的数量小于最小值，这里要创建一个新 worker 加进线程池 addWorker(null, false); &#125;&#125; 停止方法 shutdown()：停止线程池 123456789101112131415161718public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; // 获取线程池全局锁 mainLock.lock(); try &#123; checkShutdownAccess(); // 设置线程池状态为 SHUTDOWN，如果线程池状态大于 SHUTDOWN，就不会设置直接返回 advanceRunState(SHUTDOWN); // 中断空闲线程 interruptIdleWorkers(); // 空方法，子类可以扩展 onShutdown(); &#125; finally &#123; // 释放线程池全局锁 mainLock.unlock(); &#125; tryTerminate();&#125; interruptIdleWorkers()：shutdown 方法会中断所有空闲线程，根据是否可以获取 AQS 独占锁判断是否处于工作状态。线程之所以空闲是因为阻塞队列没有任务，不会中断正在运行的线程，所以 shutdown 方法会让所有的任务执行完毕 123456789101112131415161718192021222324252627282930313233// onlyOne == true 说明只中断一个线程 ，false 则中断所有线程private void interruptIdleWorkers(boolean onlyOne) &#123; final ReentrantLock mainLock = this.mainLock; / /持有全局锁 mainLock.lock(); try &#123; // 遍历所有 worker for (Worker w : workers) &#123; // 获取当前 worker 的线程 Thread t = w.thread; // 条件一成立：说明当前迭代的这个线程尚未中断 // 条件二成立：说明【当前worker处于空闲状态】，阻塞在poll或者take，因为worker执行task时是要加锁的 // 每个worker有一个独占锁，w.tryLock()尝试加锁，加锁成功返回 true if (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123; try &#123; // 中断线程，处于 queue 阻塞的线程会被唤醒，进入下一次自旋，返回 null，执行退出相逻辑 t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; finally &#123; // 释放worker的独占锁 w.unlock(); &#125; &#125; // false，代表中断所有的线程 if (onlyOne) break; &#125; &#125; finally &#123; // 释放全局锁 mainLock.unlock(); &#125;&#125; shutdownNow()：直接关闭线程池，不会等待任务执行完成 12345678910111213141516171819202122public List&lt;Runnable&gt; shutdownNow() &#123; // 返回值引用 List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; // 获取线程池全局锁 mainLock.lock(); try &#123; checkShutdownAccess(); // 设置线程池状态为STOP advanceRunState(STOP); // 中断线程池中【所有线程】 interruptWorkers(); // 从阻塞队列中导出未处理的task tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); // 返回当前任务队列中 未处理的任务。 return tasks;&#125; tryTerminate()：设置为 TERMINATED 状态 if either (SHUTDOWN and pool and queue empty) or (STOP and pool empty) 12345678910111213141516171819202122232425262728293031323334353637383940414243final void tryTerminate() &#123; for (;;) &#123; // 获取 ctl 的值 int c = ctl.get(); // 线程池正常，或者有其他线程执行了状态转换的方法，当前线程直接返回 if (isRunning(c) || runStateAtLeast(c, TIDYING) || // 线程池是 SHUTDOWN 并且任务队列不是空，需要去处理队列中的任务 (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty())) return; // 执行到这里说明线程池状态为 STOP 或者线程池状态为 SHUTDOWN 并且队列已经是空 // 判断线程池中线程的数量 if (workerCountOf(c) != 0) &#123; // 【中断一个空闲线程】，在 queue.take() | queue.poll() 阻塞空闲 // 唤醒后的线程会在getTask()方法返回null， // 执行 processWorkerExit 退出逻辑时会再次调用 tryTerminate() 唤醒下一个空闲线程 interruptIdleWorkers(ONLY_ONE); return; &#125; // 池中的线程数量为 0 来到这里 final ReentrantLock mainLock = this.mainLock; // 加全局锁 mainLock.lock(); try &#123; // 设置线程池状态为 TIDYING 状态，线程数量为 0 if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; // 结束线程池 terminated(); &#125; finally &#123; // 设置线程池状态为TERMINATED状态。 ctl.set(ctlOf(TERMINATED, 0)); // 【唤醒所有调用 awaitTermination() 方法的线程】 termination.signalAll(); &#125; return; &#125; &#125; finally &#123; // 释放线程池全局锁 mainLock.unlock(); &#125; &#125;&#125; Future线程使用FutureTask 未来任务对象，继承 Runnable、Future 接口，用于包装 Callable 对象，实现任务的提交 1234567891011public static void main(String[] args) throws ExecutionException, InterruptedException &#123; FutureTask&lt;String&gt; task = new FutureTask&lt;&gt;(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; return &quot;Hello World&quot;; &#125; &#125;); new Thread(task).start();\t//启动线程 String msg = task.get();\t//获取返回任务数据 System.out.println(msg);&#125; 构造方法： 1234public FutureTask(Callable&lt;V&gt; callable)&#123;\tthis.callable = callable;\t// 属性注入 this.state = NEW; // 任务状态设置为 new&#125; 1234567891011121314151617181920212223242526public FutureTask(Runnable runnable, V result) &#123; // 适配器模式 this.callable = Executors.callable(runnable, result); this.state = NEW; &#125;public static &lt;T&gt; Callable&lt;T&gt; callable(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); // 使用装饰者模式将 runnable 转换成 callable 接口，外部线程通过 get 获取 // 当前任务执行结果时，结果可能为 null 也可能为传进来的值，【传进来什么返回什么】 return new RunnableAdapter&lt;T&gt;(task, result);&#125;static final class RunnableAdapter&lt;T&gt; implements Callable&lt;T&gt; &#123; final Runnable task; final T result; // 构造方法 RunnableAdapter(Runnable task, T result) &#123; this.task = task; this.result = result; &#125; public T call() &#123; // 实则调用 Runnable#run 方法 task.run(); // 返回值为构造 FutureTask 对象时传入的返回值或者是 null return result; &#125;&#125; 成员属性FutureTask 类的成员属性： 任务状态： 12345678910111213141516// 表示当前task状态private volatile int state;// 当前任务尚未执行private static final int NEW = 0;// 当前任务正在结束，尚未完全结束，一种临界状态private static final int COMPLETING = 1;// 当前任务正常结束private static final int NORMAL = 2;// 当前任务执行过程中发生了异常，内部封装的 callable.run() 向上抛出异常了private static final int EXCEPTIONAL = 3;// 当前任务被取消private static final int CANCELLED = 4;// 当前任务中断中private static final int INTERRUPTING = 5;// 当前任务已中断private static final int INTERRUPTED = 6; 任务对象： 1private Callable&lt;V&gt; callable;\t// Runnable 使用装饰者模式伪装成 Callable 存储任务执行的结果，这是 run 方法返回值是 void 也可以获取到执行结果的原因： 123// 正常情况下：任务正常执行结束，outcome 保存执行结果，callable 返回值// 非正常情况：callable 向上抛出异常，outcome 保存异常private Object outcome; 执行当前任务的线程对象： 1private volatile Thread runner;\t// 当前任务被线程执行期间，保存当前执行任务的线程对象引用 线程阻塞队列的头节点： 12// 会有很多线程去 get 当前任务的结果，这里使用了一种数据结构头插头取（类似栈）的一个队列来保存所有的 get 线程private volatile WaitNode waiters; 内部类： 123456static final class WaitNode &#123; // 单向链表 volatile Thread thread; volatile WaitNode next; WaitNode() &#123; thread = Thread.currentThread(); &#125;&#125; 成员方法FutureTask 类的成员方法： FutureTask#run：任务执行入口 12345678910111213141516171819202122232425262728293031323334353637383940414243public void run() &#123; //条件一：成立说明当前 task 已经被执行过了或者被 cancel 了，非 NEW 状态的任务，线程就不需要处理了 //条件二：线程是 NEW 状态，尝试设置当前任务对象的线程是当前线程，设置失败说明其他线程抢占了该任务，直接返回 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; // 执行到这里，当前 task 一定是 NEW 状态，而且【当前线程也抢占 task 成功】 Callable&lt;V&gt; c = callable; // 判断任务是否为空，防止空指针异常；判断 state 状态，防止外部线程在此期间 cancel 掉当前任务 // 【因为 task 的执行者已经设置为当前线程，所以这里是线程安全的】 if (c != null &amp;&amp; state == NEW) &#123; V result; // true 表示 callable.run 代码块执行成功 未抛出异常 // false 表示 callable.run 代码块执行失败 抛出异常 boolean ran; try &#123; // 【调用自定义的方法，执行结果赋值给 result】 result = c.call(); // 没有出现异常 ran = true; &#125; catch (Throwable ex) &#123; // 出现异常，返回值置空，ran 置为 false result = null; ran = false; // 设置返回的异常 setException(ex); &#125; // 代码块执行正常 if (ran) // 设置返回的结果 set(result); &#125; &#125; finally &#123; // 任务执行完成，取消线程的引用，help GC runner = null; int s = state; // 判断任务是不是被中断 if (s &gt;= INTERRUPTING) // 执行中断处理方法 handlePossibleCancellationInterrupt(s); &#125;&#125; FutureTask#set：设置正常返回值，首先将任务状态设置为 COMPLETING 状态代表完成中，逻辑执行完设置为 NORMAL 状态代表任务正常执行完成，最后唤醒 get() 阻塞线程 12345678910protected void set(V v) &#123; // CAS 方式设置当前任务状态为完成中，设置失败说明其他线程取消了该任务 if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; // 【将结果赋值给 outcome】 outcome = v; // 将当前任务状态修改为 NORMAL 正常结束状态。 UNSAFE.putOrderedInt(this, stateOffset, NORMAL); finishCompletion(); &#125;&#125; FutureTask#setException：设置异常返回值 123456789protected void setException(Throwable t) &#123; if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; // 赋值给返回结果，用来向上层抛出来的异常 outcome = t; // 将当前任务的状态 修改为 EXCEPTIONAL UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); finishCompletion(); &#125;&#125; FutureTask#finishCompletion：唤醒 get() 阻塞线程 1234567891011121314151617181920212223242526272829private void finishCompletion() &#123; // 遍历所有的等待的节点，q 指向头节点 for (WaitNode q; (q = waiters) != null;) &#123; // 使用cas设置 waiters 为 null，防止外部线程使用cancel取消当前任务，触发finishCompletion方法重复执行 if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) &#123; // 自旋 for (;;) &#123; // 获取当前 WaitNode 节点封装的 thread Thread t = q.thread; // 当前线程不为 null，唤醒当前 get() 等待获取数据的线程 if (t != null) &#123; q.thread = null; LockSupport.unpark(t); &#125; // 获取当前节点的下一个节点 WaitNode next = q.next; // 当前节点是最后一个节点了 if (next == null) break; // 断开链表 q.next = null; // help gc q = next; &#125; break; &#125; &#125; done(); callable = null;\t// help GC&#125; FutureTask#handlePossibleCancellationInterrupt：任务中断处理 1234567private void handlePossibleCancellationInterrupt(int s) &#123; if (s == INTERRUPTING) // 中断状态中 while (state == INTERRUPTING) // 等待中断完成 Thread.yield();&#125; FutureTask#get：获取任务执行的返回值，执行 run 和 get 的不是同一个线程，一般有多个线程 get，只有一个线程 run 123456789public V get() throws InterruptedException, ExecutionException &#123; // 获取当前任务状态 int s = state; // 条件成立说明任务还没执行完成 if (s &lt;= COMPLETING) // 返回 task 当前状态，可能当前线程在里面已经睡了一会 s = awaitDone(false, 0L); return report(s);&#125; FutureTask#awaitDone：get 线程封装成 WaitNode 对象进入阻塞队列阻塞等待 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; // 0 不带超时 final long deadline = timed ? System.nanoTime() + nanos : 0L; // 引用当前线程，封装成 WaitNode 对象 WaitNode q = null; // 表示当前线程 waitNode 对象，是否进入阻塞队列 boolean queued = false; // 【三次自旋开始休眠】 for (;;) &#123; // 判断当前 get() 线程是否被打断，打断返回 true，清除打断标记 if (Thread.interrupted()) &#123; // 当前线程对应的等待 node 出队， removeWaiter(q); throw new InterruptedException(); &#125; // 获取任务状态 int s = state; // 条件成立说明当前任务执行完成已经有结果了 if (s &gt; COMPLETING) &#123; // 条件成立说明已经为当前线程创建了 WaitNode，置空 help GC if (q != null) q.thread = null; // 返回当前的状态 return s; &#125; // 条件成立说明当前任务接近完成状态，这里让当前线程释放一下 cpu ，等待进行下一次抢占 cpu else if (s == COMPLETING) Thread.yield(); // 【第一次自旋】，当前线程还未创建 WaitNode 对象，此时为当前线程创建 WaitNode对象 else if (q == null) q = new WaitNode(); // 【第二次自旋】，当前线程已经创建 WaitNode 对象了，但是node对象还未入队 else if (!queued) // waiters 指向队首，让当前 WaitNode 成为新的队首，【头插法】，失败说明其他线程修改了新的队首 queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); // 【第三次自旋】，会到这里，或者 else 内 else if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; // 阻塞指定的时间 LockSupport.parkNanos(this, nanos); &#125; // 条件成立：说明需要阻塞 else // 【当前 get 操作的线程被 park 阻塞】，除非有其它线程将唤醒或者将当前线程中断 LockSupport.park(this); &#125;&#125; FutureTask#report：封装运行结果，可以获取 run() 方法中设置的成员变量 outcome，这是 run 方法的返回值是 void 也可以获取到任务执行的结果的原因 123456789101112private V report(int s) throws ExecutionException &#123; // 获取执行结果，是在一个 futuretask 对象中的属性，可以直接获取 Object x = outcome; // 当前任务状态正常结束 if (s == NORMAL) return (V)x;\t// 直接返回 callable 的逻辑结果 // 当前任务被取消或者中断 if (s &gt;= CANCELLED) throw new CancellationException(); // 抛出异常 // 执行到这里说明自定义的 callable 中的方法有异常，使用 outcome 上层抛出异常 throw new ExecutionException((Throwable)x);\t&#125; FutureTask#cancel：任务取消，打断正在执行该任务的线程 123456789101112131415161718192021222324252627public boolean cancel(boolean mayInterruptIfRunning) &#123; // 条件一：表示当前任务处于运行中或者处于线程池任务队列中 // 条件二：表示修改状态，成功可以去执行下面逻辑，否则返回 false 表示 cancel 失败 if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try &#123; // 如果任务已经被执行，是否允许打断 if (mayInterruptIfRunning) &#123; try &#123; // 获取执行当前 FutureTask 的线程 Thread t = runner; if (t != null) // 打断执行的线程 t.interrupt(); &#125; finally &#123; // 设置任务状态为【中断完成】 UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); &#125; &#125; &#125; finally &#123; // 唤醒所有 get() 阻塞的线程 finishCompletion(); &#125; return true;&#125; 任务调度TimerTimer 实现定时功能，Timer 的优点在于简单易用，但由于所有任务都是由同一个线程来调度，因此所有任务都是串行执行的，同一时间只能有一个任务在执行，前一个任务的延迟或异常都将会影响到之后的任务 123456789101112131415161718192021private static void method1() &#123; Timer timer = new Timer(); TimerTask task1 = new TimerTask() &#123; @Override public void run() &#123; System.out.println(&quot;task 1&quot;); //int i = 1 / 0;//任务一的出错会导致任务二无法执行 Thread.sleep(2000); &#125; &#125;; TimerTask task2 = new TimerTask() &#123; @Override public void run() &#123; System.out.println(&quot;task 2&quot;); &#125; &#125;; // 使用 timer 添加两个任务，希望它们都在 1s 后执行\t// 但由于 timer 内只有一个线程来顺序执行队列中的任务，因此任务1的延时，影响了任务2的执行 timer.schedule(task1, 1000);//17:45:56 c.ThreadPool [Timer-0] - task 1 timer.schedule(task2, 1000);//17:45:58 c.ThreadPool [Timer-0] - task 2&#125; Scheduled任务调度线程池 ScheduledThreadPoolExecutor 继承 ThreadPoolExecutor： 使用内部类 ScheduledFutureTask 封装任务 使用内部类 DelayedWorkQueue 作为线程池队列 重写 onShutdown 方法去处理 shutdown 后的任务 提供 decorateTask 方法作为 ScheduledFutureTask 的修饰方法，以便开发者进行扩展 构造方法：Executors.newScheduledThreadPool(int corePoolSize) 123456public ScheduledThreadPoolExecutor(int corePoolSize) &#123; // 最大线程数固定为 Integer.MAX_VALUE，保活时间 keepAliveTime 固定为 0 super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, // 阻塞队列是 DelayedWorkQueue new DelayedWorkQueue());&#125; 常用 API： ScheduledFuture&lt;?&gt; schedule(Runnable/Callable&lt;V&gt;, long delay, TimeUnit u)：延迟执行任务 ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable/Callable&lt;V&gt;, long initialDelay, long period, TimeUnit unit)：定时执行周期任务，不考虑执行的耗时，参数为初始延迟时间、间隔时间、单位 ScheduledFuture&lt;?&gt; scheduleWithFixedDelay(Runnable/Callable&lt;V&gt;, long initialDelay, long delay, TimeUnit unit)：定时执行周期任务，考虑执行的耗时，参数为初始延迟时间、间隔时间、单位 基本使用： 延迟任务，但是出现异常并不会在控制台打印，也不会影响其他线程的执行 1234567891011121314public static void main(String[] args)&#123; // 线程池大小为1时也是串行执行 ScheduledExecutorService executor = Executors.newScheduledThreadPool(2); // 添加两个任务，都在 1s 后同时执行 executor.schedule(() -&gt; &#123; System.out.println(&quot;任务1，执行时间：&quot; + new Date()); //int i = 1 / 0; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; &#125; &#125;, 1000, TimeUnit.MILLISECONDS); executor.schedule(() -&gt; &#123; System.out.println(&quot;任务2，执行时间：&quot; + new Date()); &#125;, 1000, TimeUnit.MILLISECONDS);&#125; 定时任务 scheduleAtFixedRate：一次任务的启动到下一次任务的启动之间只要大于等于间隔时间，抢占到 CPU 就会立即执行 1234567891011121314public static void main(String[] args) &#123; ScheduledExecutorService pool = Executors.newScheduledThreadPool(1); System.out.println(&quot;start...&quot; + new Date()); pool.scheduleAtFixedRate(() -&gt; &#123; System.out.println(&quot;running...&quot; + new Date()); Thread.sleep(2000); &#125;, 1, 1, TimeUnit.SECONDS);&#125;/*start...Sat Apr 24 18:08:12 CST 2021running...Sat Apr 24 18:08:13 CST 2021running...Sat Apr 24 18:08:15 CST 2021running...Sat Apr 24 18:08:17 CST 2021 定时任务 scheduleWithFixedDelay：一次任务的结束到下一次任务的启动之间等于间隔时间，抢占到 CPU 就会立即执行，这个方法才是真正的设置两个任务之间的间隔 12345678910111213public static void main(String[] args)&#123; ScheduledExecutorService pool = Executors.newScheduledThreadPool(3); System.out.println(&quot;start...&quot; + new Date()); pool.scheduleWithFixedDelay(() -&gt; &#123; System.out.println(&quot;running...&quot; + new Date()); Thread.sleep(2000); &#125;, 1, 1, TimeUnit.SECONDS);&#125;/*start...Sat Apr 24 18:11:41 CST 2021running...Sat Apr 24 18:11:42 CST 2021running...Sat Apr 24 18:11:45 CST 2021running...Sat Apr 24 18:11:48 CST 2021 成员属性成员变量 shutdown 后是否继续执行周期任务： 1private volatile boolean continueExistingPeriodicTasksAfterShutdown; shutdown 后是否继续执行延迟任务： 1private volatile boolean executeExistingDelayedTasksAfterShutdown = true; 取消方法是否将该任务从队列中移除： 12// 默认 false，不移除，等到线程拿到任务之后抛弃private volatile boolean removeOnCancel = false; 任务的序列号，可以用来比较优先级： 1private static final AtomicLong sequencer = new AtomicLong(); 延迟任务ScheduledFutureTask 继承 FutureTask，实现 RunnableScheduledFuture 接口，具有延迟执行的特点，覆盖 FutureTask 的 run 方法来实现对延时执行、周期执行的支持。对于延时任务调用 FutureTask#run，而对于周期性任务则调用 FutureTask#runAndReset 并且在成功之后根据 fixed-delay&#x2F;fixed-rate 模式来设置下次执行时间并重新将任务塞到工作队列 在调度线程池中无论是 runnable 还是 callable，无论是否需要延迟和定时，所有的任务都会被封装成 ScheduledFutureTask 成员变量： 任务序列号： 1private final long sequenceNumber; 执行时间： 12private long time; // 任务可以被执行的时间，交付时间，以纳秒表示private final long period;\t// 0 表示非周期任务，正数表示 fixed-rate 模式的周期，负数表示 fixed-delay 模式 fixed-rate：两次开始启动的间隔，fixed-delay：一次执行结束到下一次开始启动 实际的任务对象： 1RunnableScheduledFuture&lt;V&gt; outerTask = this; 任务在队列数组中的索引下标： 12// DelayedWorkQueue 底层使用的数据结构是最小堆，记录当前任务在堆中的索引，-1 代表删除int heapIndex; 成员方法： 构造方法： 123456789ScheduledFutureTask(Runnable r, V result, long ns, long period) &#123; super(r, result); // 任务的触发时间 this.time = ns; // 任务的周期，多长时间执行一次 this.period = period; // 任务的序号 this.sequenceNumber = sequencer.getAndIncrement();&#125; compareTo()：ScheduledFutureTask 根据执行时间 time 正序排列，如果执行时间相同，在按照序列号 sequenceNumber 正序排列，任务需要放入 DelayedWorkQueue，延迟队列中使用该方法按照从小到大进行排序 123456789101112131415161718192021222324public int compareTo(Delayed other) &#123; if (other == this) // compare zero if same object return 0; if (other instanceof ScheduledFutureTask) &#123; // 类型强转 ScheduledFutureTask&lt;?&gt; x = (ScheduledFutureTask&lt;?&gt;)other; // 比较者 - 被比较者的执行时间 long diff = time - x.time; // 比较者先执行 if (diff &lt; 0) return -1; // 被比较者先执行 else if (diff &gt; 0) return 1; // 比较者的序列号小 else if (sequenceNumber &lt; x.sequenceNumber) return -1; else return 1; &#125; // 不是 ScheduledFutureTask 类型时，根据延迟时间排序 long diff = getDelay(NANOSECONDS) - other.getDelay(NANOSECONDS); return (diff &lt; 0) ? -1 : (diff &gt; 0) ? 1 : 0;&#125; run()：执行任务，非周期任务直接完成直接结束，周期任务执行完后会设置下一次的执行时间，重新放入线程池的阻塞队列，如果线程池中的线程数量少于核心线程，就会添加 Worker 开启新线程 1234567891011121314151617public void run() &#123; // 是否周期性，就是判断 period 是否为 0 boolean periodic = isPeriodic(); // 根据是否是周期任务检查当前状态能否执行任务，不能执行就取消任务 if (!canRunInCurrentRunState(periodic)) cancel(false); // 非周期任务，直接调用 FutureTask#run 执行 else if (!periodic) ScheduledFutureTask.super.run(); // 周期任务的执行，返回 true 表示执行成功 else if (ScheduledFutureTask.super.runAndReset()) &#123; // 设置周期任务的下一次执行时间 setNextRunTime(); // 任务的下一次执行安排，如果当前线程池状态可以执行周期任务，加入队列，并开启新线程 reExecutePeriodic(outerTask); &#125;&#125; 周期任务正常完成后任务的状态不会变化，依旧是 NEW，不会设置 outcome 属性。但是如果本次任务执行出现异常，会进入 setException 方法将任务状态置为异常，把异常保存在 outcome 中，方法返回 false，后续的该任务将不会再周期的执行 123456789101112131415161718192021222324252627282930protected boolean runAndReset() &#123; // 任务不是新建的状态了，或者被别的线程执行了，直接返回 false if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return false; boolean ran = false; int s = state; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; s == NEW) &#123; try &#123; // 执行方法，没有返回值 c.call(); ran = true; &#125; catch (Throwable ex) &#123; // 出现异常，把任务设置为异常状态，唤醒所有的 get 阻塞线程 setException(ex); &#125; &#125; &#125; finally &#123; // 执行完成把执行线程引用置为 null runner = null; s = state; // 如果线程被中断进行中断处理 if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; // 如果正常执行，返回 true，并且任务状态没有被取消 return ran &amp;&amp; s == NEW;&#125; 12345678910// 任务下一次的触发时间private void setNextRunTime() &#123; long p = period; if (p &gt; 0) // fixed-rate 模式，【时间设置为上一次执行任务的时间 + p】，两次任务执行的时间差 time += p; else // fixed-delay 模式，下一次执行时间是【当前这次任务结束的时间（就是现在） + delay 值】 time = triggerTime(-p);&#125; reExecutePeriodic()：准备任务的下一次执行，重新放入阻塞任务队列 1234567891011121314// ScheduledThreadPoolExecutor#reExecutePeriodicvoid reExecutePeriodic(RunnableScheduledFuture&lt;?&gt; task) &#123; if (canRunInCurrentRunState(true)) &#123; // 【放入任务队列】 super.getQueue().add(task); // 如果提交完任务之后，线程池状态变为了 shutdown 状态，需要再次检查是否可以执行， // 如果不能执行且任务还在队列中未被取走，则取消任务 if (!canRunInCurrentRunState(true) &amp;&amp; remove(task)) task.cancel(false); else // 当前线程池状态可以执行周期任务，加入队列，并【根据线程数量是否大于核心线程数确定是否开启新线程】 ensurePrestart(); &#125;&#125; cancel()：取消任务 123456789public boolean cancel(boolean mayInterruptIfRunning) &#123; // 调用父类 FutureTask#cancel 来取消任务 boolean cancelled = super.cancel(mayInterruptIfRunning); // removeOnCancel 用于控制任务取消后是否应该从阻塞队列中移除 if (cancelled &amp;&amp; removeOnCancel &amp;&amp; heapIndex &gt;= 0) // 从等待队列中删除该任务，并调用 tryTerminate() 判断是否需要停止线程池 remove(this); return cancelled;&#125; 延迟队列DelayedWorkQueue 是支持延时获取元素的阻塞队列，内部采用优先队列 PriorityQueue（小根堆、满二叉树）存储元素 其他阻塞队列存储节点的数据结构大都是链表，延迟队列是数组，所以延迟队列出队头元素后需要让其他元素（尾）替换到头节点，防止空指针异常 成员变量： 容量： 1234private static final int INITIAL_CAPACITY = 16; // 初始容量private int size = 0; // 节点数量private RunnableScheduledFuture&lt;?&gt;[] queue = new RunnableScheduledFuture&lt;?&gt;[INITIAL_CAPACITY];\t// 存放节点 锁： 12private final ReentrantLock lock = new ReentrantLock();\t// 控制并发private final Condition available = lock.newCondition();// 条件队列 阻塞等待头节点的线程：线程池内的某个线程去 take() 获取任务时，如果延迟队列顶层节点不为 null（队列内有任务），但是节点任务还不到触发时间，线程就去检查队列的 leader字段是否被占用 如果未被占用，则当前线程占用该字段，然后当前线程到 available 条件队列指定超时时间 堆顶任务.time - now() 挂起 如果被占用，当前线程直接到 available 条件队列不指定超时时间的挂起 12// leader 在 available 条件队列内是首元素，它超时之后会醒过来，然后再次将堆顶元素获取走，获取走之后，take()结束之前，会调用是 available.signal() 唤醒下一个条件队列内的等待者，然后释放 lock，下一个等待者被唤醒后去到 AQS 队列，做 acquireQueue(node) 逻辑private Thread leader = null; 成员方法 offer()：插入节点 12345678910111213141516171819202122232425262728293031323334353637383940public boolean offer(Runnable x) &#123; // 判空 if (x == null) throw new NullPointerException(); RunnableScheduledFuture&lt;?&gt; e = (RunnableScheduledFuture&lt;?&gt;)x; // 队列锁，增加删除数据时都要加锁 final ReentrantLock lock = this.lock; lock.lock(); try &#123; int i = size; // 队列数量大于存放节点的数组长度，需要扩容 if (i &gt;= queue.length) // 扩容为原来长度的 1.5 倍 grow(); size = i + 1; // 当前是第一个要插入的节点 if (i == 0) &#123; queue[0] = e; // 修改 ScheduledFutureTask 的 heapIndex 属性，表示该对象在队列里的下标 setIndex(e, 0); &#125; else &#123; // 向上调整元素的位置，并更新 heapIndex siftUp(i, e); &#125; // 情况1：当前任务是第一个加入到 queue 内的任务，所以在当前任务加入到 queue 之前，take() 线程会直接 // 到 available 队列不设置超时的挂起，并不会去占用 leader 字段，这时需会唤醒一个线程 让它去消费 // 情况2：当前任务【优先级最高】，原堆顶任务可能还未到触发时间，leader 线程设置超时的在 available 挂起 // 原先的 leader 等待的是原先的头节点，所以 leader 已经无效，需要将 leader 线程唤醒， // 唤醒之后它会检查堆顶，如果堆顶任务可以被消费，则直接获取走，否则继续成为 leader 等待新堆顶任务 if (queue[0] == e) &#123; // 将 leader 设置为 null leader = null; // 直接随便唤醒等待头结点的阻塞线程 available.signal(); &#125; &#125; finally &#123; lock.unlock(); &#125; return true;&#125; 12345678910111213141516// 插入新节点后对堆进行调整，进行节点上移，保持其特性【节点的值小于子节点的值】，小顶堆private void siftUp(int k, RunnableScheduledFuture&lt;?&gt; key) &#123; while (k &gt; 0) &#123; // 父节点，就是堆排序 int parent = (k - 1) &gt;&gt;&gt; 1; RunnableScheduledFuture&lt;?&gt; e = queue[parent]; // key 和父节点比，如果大于父节点可以直接返回，否则就继续上浮 if (key.compareTo(e) &gt;= 0) break; queue[k] = e; setIndex(e, k); k = parent; &#125; queue[k] = key; setIndex(key, k);&#125; poll()：非阻塞获取头结点，获取执行时间最近并且可以执行的 1234567891011121314151617// 非阻塞获取public RunnableScheduledFuture&lt;?&gt; poll() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; // 获取队头节点，因为是小顶堆 RunnableScheduledFuture&lt;?&gt; first = queue[0]; // 头结点为空或者的延迟时间没到返回 null if (first == null || first.getDelay(NANOSECONDS) &gt; 0) return null; else // 头结点达到延迟时间，【尾节点成为替代节点下移调整堆结构】，返回头结点 return finishPoll(first); &#125; finally &#123; lock.unlock(); &#125;&#125; 123456789101112131415private RunnableScheduledFuture&lt;?&gt; finishPoll(RunnableScheduledFuture&lt;?&gt; f) &#123; // 获取尾索引 int s = --size; // 获取尾节点 RunnableScheduledFuture&lt;?&gt; x = queue[s]; // 将堆结构最后一个节点占用的 slot 设置为 null，因为该节点要尝试升级成堆顶，会根据特性下调 queue[s] = null; // s == 0 说明 当前堆结构只有堆顶一个节点，此时不需要做任何的事情 if (s != 0) // 从索引处 0 开始向下调整 siftDown(0, x); // 出队的元素索引设置为 -1 setIndex(f, -1); return f;&#125; take()：阻塞获取头节点，读取当前堆中最小的也就是触发时间最近的任务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public RunnableScheduledFuture&lt;?&gt; take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; // 保证线程安全 lock.lockInterruptibly(); try &#123; for (;;) &#123; // 头节点 RunnableScheduledFuture&lt;?&gt; first = queue[0]; if (first == null) // 等待队列不空，直至有任务通过 offer 入队并唤醒 available.await(); else &#123; // 获取头节点的延迟时间是否到时 long delay = first.getDelay(NANOSECONDS); if (delay &lt;= 0) // 到达触发时间，获取头节点并调整堆，重新选择延迟时间最小的节点放入头部 return finishPoll(first); // 逻辑到这说明头节点的延迟时间还没到 first = null; // 说明有 leader 线程在等待获取头节点，当前线程直接去阻塞等待 if (leader != null) available.await(); else &#123; // 没有 leader 线程，【当前线程作为leader线程，并设置头结点的延迟时间作为阻塞时间】 Thread thisThread = Thread.currentThread(); leader = thisThread; try &#123; // 在条件队列 available 使用带超时的挂起（堆顶任务.time - now() 纳秒值..） available.awaitNanos(delay); // 到达阻塞时间时，当前线程会从这里醒来来 &#125; finally &#123; // t堆顶更新，leader 置为 null，offer 方法释放锁后， // 有其它线程通过 take/poll 拿到锁,读到 leader == null，然后将自身更新为leader。 if (leader == thisThread) // leader 置为 null 用以接下来判断是否需要唤醒后继线程 leader = null; &#125; &#125; &#125; &#125; &#125; finally &#123; // 没有 leader 线程，头结点不为 null，唤醒阻塞获取头节点的线程， // 【如果没有这一步，就会出现有了需要执行的任务，但是没有线程去执行】 if (leader == null &amp;&amp; queue[0] != null) available.signal(); lock.unlock(); &#125;&#125; remove()：删除节点，堆移除一个元素的时间复杂度是 O(log n)，延迟任务维护了 heapIndex，直接访问的时间复杂度是 O(1)，从而可以更快的移除元素，任务在队列中被取消后会进入该逻辑 123456789101112131415161718192021222324252627282930public boolean remove(Object x) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; // 查找对象在队列数组中的下标 int i = indexOf(x); // 节点不存在，返回 false if (i &lt; 0) return false; // 修改元素的 heapIndex，-1 代表删除 setIndex(queue[i], -1); // 尾索引是长度-1 int s = --size; // 尾节点作为替代节点 RunnableScheduledFuture&lt;?&gt; replacement = queue[s]; queue[s] = null; // s == i 说明头节点就是尾节点，队列空了 if (s != i) &#123; // 向下调整 siftDown(i, replacement); // 说明没发生调整 if (queue[i] == replacement) // 上移和下移不可能同时发生，替代节点大于子节点时下移，否则上移 siftUp(i, replacement); &#125; return true; &#125; finally &#123; lock.unlock(); &#125;&#125; 成员方法提交任务 schedule()：延迟执行方法，并指定执行的时间，默认是当前时间 1234public void execute(Runnable command) &#123; // 以零延时任务的形式实现 schedule(command, 0, NANOSECONDS);&#125; 12345678910public ScheduledFuture&lt;?&gt; schedule(Runnable command, long delay, TimeUnit unit) &#123; // 判空 if (command == null || unit == null) throw new NullPointerException(); // 没有做任何操作，直接将 task 返回，该方法主要目的是用于子类扩展，并且【根据延迟时间设置任务触发的时间点】 RunnableScheduledFuture&lt;?&gt; t = decorateTask(command, new ScheduledFutureTask&lt;Void&gt;( command, null, triggerTime(delay, unit))); // 延迟执行 delayedExecute(t); return t;&#125; 12345678910// 返回【当前时间 + 延迟时间】，就是触发当前任务执行的时间private long triggerTime(long delay, TimeUnit unit) &#123; // 设置触发的时间 return triggerTime(unit.toNanos((delay &lt; 0) ? 0 : delay));&#125;long triggerTime(long delay) &#123; // 如果 delay &lt; Long.Max_VALUE/2，则下次执行时间为当前时间 +delay // 否则为了避免队列中出现由于溢出导致的排序紊乱,需要调用overflowFree来修正一下delay return now() + ((delay &lt; (Long.MAX_VALUE &gt;&gt; 1)) ? delay : overflowFree(delay));&#125; overflowFree 的原因：如果某个任务的 delay 为负数，说明当前可以执行（其实早该执行了）。阻塞队列中维护任务顺序是基于 compareTo 比较的，比较两个任务的顺序会用 time 相减。那么可能出现一个 delay 为正数减去另一个为负数的 delay，结果上溢为负数，则会导致 compareTo 产生错误的结果 123456789101112private long overflowFree(long delay) &#123; Delayed head = (Delayed) super.getQueue().peek(); if (head != null) &#123; long headDelay = head.getDelay(NANOSECONDS); // 判断一下队首的delay是不是负数，如果是正数就不用管，怎么减都不会溢出 // 否则拿当前 delay 减去队首的 delay 来比较看，如果不出现上溢，排序不会乱 // 不然就把当前 delay 值给调整为 Long.MAX_VALUE + 队首 delay if (headDelay &lt; 0 &amp;&amp; (delay - headDelay &lt; 0)) delay = Long.MAX_VALUE + headDelay; &#125; return delay;&#125; scheduleAtFixedRate()：定时执行，一次任务的启动到下一次任务的启动的间隔 12345678910111213141516public ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) &#123; if (command == null || unit == null) throw new NullPointerException(); if (period &lt;= 0) throw new IllegalArgumentException(); // 任务封装，【指定初始的延迟时间和周期时间】 ScheduledFutureTask&lt;Void&gt; sft =new ScheduledFutureTask&lt;Void&gt;(command, null, triggerTime(initialDelay, unit), unit.toNanos(period)); // 默认返回本身 RunnableScheduledFuture&lt;Void&gt; t = decorateTask(command, sft); sft.outerTask = t; // 开始执行这个任务 delayedExecute(t); return t;&#125; scheduleWithFixedDelay()：定时执行，一次任务的结束到下一次任务的启动的间隔 1234567891011121314public ScheduledFuture&lt;?&gt; scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) &#123; if (command == null || unit == null) throw new NullPointerException(); if (delay &lt;= 0) throw new IllegalArgumentException(); // 任务封装，【指定初始的延迟时间和周期时间】，周期时间为 - 表示是 fixed-delay 模式 ScheduledFutureTask&lt;Void&gt; sft = new ScheduledFutureTask&lt;Void&gt;(command, null, triggerTime(initialDelay, unit), unit.toNanos(-delay)); RunnableScheduledFuture&lt;Void&gt; t = decorateTask(command, sft); sft.outerTask = t; delayedExecute(t); return t;&#125; 运行任务 delayedExecute()：校验线程池状态，延迟或周期性任务的主要执行方法 123456789101112131415private void delayedExecute(RunnableScheduledFuture&lt;?&gt; task) &#123; // 线程池是 SHUTDOWN 状态，需要执行拒绝策略 if (isShutdown()) reject(task); else &#123; // 把当前任务放入阻塞队列，因为需要【获取执行时间最近的】，当前任务需要比较 super.getQueue().add(task); // 线程池状态为 SHUTDOWN 并且不允许执行任务了，就从队列删除该任务，并设置任务的状态为取消状态 if (isShutdown() &amp;&amp; !canRunInCurrentRunState(task.isPeriodic()) &amp;&amp; remove(task)) task.cancel(false); else // 可以执行 ensurePrestart(); &#125;&#125; ensurePrestart()：开启线程执行任务 1234567891011// ThreadPoolExecutor#ensurePrestartvoid ensurePrestart() &#123; int wc = workerCountOf(ctl.get()); // worker数目小于corePoolSize，则添加一个worker。 if (wc &lt; corePoolSize) // 第二个参数 true 表示采用核心线程数量限制，false 表示采用 maximumPoolSize addWorker(null, true); // corePoolSize = 0的情况，至少开启一个线程，【担保机制】 else if (wc == 0) addWorker(null, false);&#125; canRunInCurrentRunState()：任务运行时都会被调用以校验当前状态是否可以运行任务 12345boolean canRunInCurrentRunState(boolean periodic) &#123; // 根据是否是周期任务判断，在线程池 shutdown 后是否继续执行该任务，默认非周期任务是继续执行的 return isRunningOrShutdown(periodic ? continueExistingPeriodicTasksAfterShutdown : executeExistingDelayedTasksAfterShutdown);&#125; onShutdown()：删除并取消工作队列中的不需要再执行的任务 1234567891011121314151617181920212223242526272829void onShutdown() &#123; BlockingQueue&lt;Runnable&gt; q = super.getQueue(); // shutdown 后是否仍然执行延时任务 boolean keepDelayed = getExecuteExistingDelayedTasksAfterShutdownPolicy(); // shutdown 后是否仍然执行周期任务 boolean keepPeriodic = getContinueExistingPeriodicTasksAfterShutdownPolicy(); // 如果两者皆不可，则对队列中【所有任务】调用 cancel 取消并清空队列 if (!keepDelayed &amp;&amp; !keepPeriodic) &#123; for (Object e : q.toArray()) if (e instanceof RunnableScheduledFuture&lt;?&gt;) ((RunnableScheduledFuture&lt;?&gt;) e).cancel(false); q.clear(); &#125; else &#123; for (Object e : q.toArray()) &#123; if (e instanceof RunnableScheduledFuture) &#123; RunnableScheduledFuture&lt;?&gt; t = (RunnableScheduledFuture&lt;?&gt;)e; // 不需要执行的任务删除并取消，已经取消的任务也需要从队列中删除 if ((t.isPeriodic() ? !keepPeriodic : !keepDelayed) || t.isCancelled()) &#123; if (q.remove(t)) t.cancel(false); &#125; &#125; &#125; &#125; // 因为任务被从队列中清理掉，所以需要调用 tryTerminate 尝试【改变线程池的状态】 tryTerminate();&#125; ForkJoinFork&#x2F;Join：线程池的实现，体现是分治思想，适用于能够进行任务拆分的 CPU 密集型运算，用于并行计算 任务拆分：将一个大任务拆分为算法上相同的小任务，直至不能拆分可以直接求解。跟递归相关的一些计算，如归并排序、斐波那契数列都可以用分治思想进行求解 Fork&#x2F;Join 在分治的基础上加入了多线程，把每个任务的分解和合并交给不同的线程来完成，提升了运算效率 ForkJoin 使用 ForkJoinPool 来启动，是一个特殊的线程池，默认会创建与 CPU 核心数大小相同的线程池 任务有返回值继承 RecursiveTask，没有返回值继承 RecursiveAction 123456789101112131415161718192021222324252627282930313233public static void main(String[] args) &#123; ForkJoinPool pool = new ForkJoinPool(4); System.out.println(pool.invoke(new MyTask(5))); //拆分 5 + MyTask(4) --&gt; 4 + MyTask(3) --&gt;&#125;// 1~ n 之间整数的和class MyTask extends RecursiveTask&lt;Integer&gt; &#123; private int n; public MyTask(int n) &#123; this.n = n; &#125; @Override public String toString() &#123; return &quot;MyTask&#123;&quot; + &quot;n=&quot; + n + &#x27;&#125;&#x27;; &#125; @Override protected Integer compute() &#123; // 如果 n 已经为 1，可以求得结果了 if (n == 1) &#123; return n; &#125; // 将任务进行拆分(fork) MyTask t1 = new MyTask(n - 1); t1.fork(); // 合并(join)结果 int result = n + t1.join(); return result; &#125;&#125; 继续拆分优化： 123456789101112131415161718192021222324252627282930313233class AddTask extends RecursiveTask&lt;Integer&gt; &#123; int begin; int end; public AddTask(int begin, int end) &#123; this.begin = begin; this.end = end; &#125; @Override public String toString() &#123; return &quot;&#123;&quot; + begin + &quot;,&quot; + end + &#x27;&#125;&#x27;; &#125; @Override protected Integer compute() &#123; // 5, 5 if (begin == end) &#123; return begin; &#125; // 4, 5 防止多余的拆分 提高效率 if (end - begin == 1) &#123; return end + begin; &#125; // 1 5 int mid = (end + begin) / 2; // 3 AddTask t1 = new AddTask(begin, mid); // 1,3 t1.fork(); AddTask t2 = new AddTask(mid + 1, end); // 4,5 t2.fork(); int result = t1.join() + t2.join(); return result; &#125;&#125; ForkJoinPool 实现了工作窃取算法来提高 CPU 的利用率： 每个线程都维护了一个双端队列，用来存储需要执行的任务 工作窃取算法允许空闲的线程从其它线程的双端队列中窃取一个任务来执行 窃取的必须是最晚的任务，避免和队列所属线程发生竞争，但是队列中只有一个任务时还是会发生竞争 享元模式享元模式（Flyweight pattern）： 用于减少创建对象的数量，以减少内存占用和提高性能，这种类型的设计模式属于结构型模式，它提供了减少对象数量从而改善应用所需的对象结构的方式 异步模式：让有限的工作线程（Worker Thread）来轮流异步处理无限多的任务，也可将其归类为分工模式，典型实现就是线程池 工作机制：享元模式尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象 自定义连接池： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public static void main(String[] args) &#123; Pool pool = new Pool(2); for (int i = 0; i &lt; 5; i++) &#123; new Thread(() -&gt; &#123; Connection con = pool.borrow(); try &#123; Thread.sleep(new Random().nextInt(1000)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; pool.free(con); &#125;).start(); &#125;&#125;class Pool &#123; //连接池的大小 private final int poolSize; //连接对象的数组 private Connection[] connections; //连接状态数组 0表示空闲 1表示繁忙 private AtomicIntegerArray states; //int[] -&gt; AtomicIntegerArray //构造方法 public Pool(int poolSize) &#123; this.poolSize = poolSize; this.connections = new Connection[poolSize]; this.states = new AtomicIntegerArray(new int[poolSize]); for (int i = 0; i &lt; poolSize; i++) &#123; connections[i] = new MockConnection(&quot;连接&quot; + (i + 1)); &#125; &#125; //使用连接 public Connection borrow() &#123; while (true) &#123; for (int i = 0; i &lt; poolSize; i++) &#123; if (states.get(i) == 0) &#123; if (states.compareAndSet(i, 0, 1)) &#123; System.out.println(Thread.currentThread().getName() + &quot; borrow &quot; + connections[i]); return connections[i]; &#125; &#125; &#125; //如果没有空闲连接，当前线程等待 synchronized (this) &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot; wait...&quot;); this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //归还连接 public void free(Connection con) &#123; for (int i = 0; i &lt; poolSize; i++) &#123; if (connections[i] == con) &#123;//判断是否是同一个对象 states.set(i, 0);//不用cas的原因是只会有一个线程使用该连接 synchronized (this) &#123; System.out.println(Thread.currentThread().getName() + &quot; free &quot; + con); this.notifyAll(); &#125; break; &#125; &#125; &#125;&#125;class MockConnection implements Connection &#123; private String name; //.....&#125; 同步器AQS核心思想AQS：AbstractQueuedSynchronizer，是阻塞式锁和相关的同步器工具的框架，许多同步类实现都依赖于该同步器 AQS 用状态属性来表示资源的状态（分独占模式和共享模式），子类需要定义如何维护这个状态，控制如何获取锁和释放锁 独占模式是只有一个线程能够访问资源，如 ReentrantLock 共享模式允许多个线程访问资源，如 Semaphore，ReentrantReadWriteLock 是组合式 AQS 核心思想： 如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并将共享资源设置锁定状态 请求的共享资源被占用，AQS 用队列实现线程阻塞等待以及被唤醒时锁分配的机制，将暂时获取不到锁的线程加入到队列中 CLH 是一种基于单向链表的高性能、公平的自旋锁，AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配 设计原理设计原理： 获取锁： 123456while(state 状态不允许获取) &#123;\t// tryAcquire(arg) if(队列中还没有此线程) &#123; 入队并阻塞 park &#125;&#125;当前线程出队 释放锁： 123if(state 状态允许了) &#123;\t// tryRelease(arg)\t恢复阻塞的线程(s) unpark&#125; AbstractQueuedSynchronizer 中 state 设计： state 使用了 32bit int 来维护同步状态，独占模式 0 表示未加锁状态，大于 0 表示已经加锁状态 1private volatile int state; state 使用 volatile 修饰配合 cas 保证其修改时的原子性 state 表示线程重入的次数（独占模式）或者剩余许可数（共享模式） state API： protected final int getState()：获取 state 状态 protected final void setState(int newState)：设置 state 状态 protected final boolean compareAndSetState(int expect,int update)：CAS 安全设置 state 封装线程的 Node 节点中 waitstate 设计： 使用 volatile 修饰配合 CAS 保证其修改时的原子性 表示 Node 节点的状态，有以下几种状态： 12345678910// 默认为 0volatile int waitStatus;// 由于超时或中断，此节点被取消，不会再改变状态static final int CANCELLED = 1;// 此节点后面的节点已（或即将）被阻止（通过park），【当前节点在释放或取消时必须唤醒后面的节点】static final int SIGNAL = -1;// 此节点当前在条件队列中static final int CONDITION = -2;// 将releaseShared传播到其他节点static final int PROPAGATE = -3; 阻塞恢复设计： 使用 park &amp; unpark 来实现线程的暂停和恢复，因为命令的先后顺序不影响结果 park &amp; unpark 是针对线程的，而不是针对同步器的，因此控制粒度更为精细 park 线程可以通过 interrupt 打断 队列设计： 使用了 FIFO 先入先出队列，并不支持优先级队列，同步队列是双向链表，便于出队入队 12345678910111213141516171819// 头结点，指向哑元节点private transient volatile Node head;// 阻塞队列的尾节点，阻塞队列不包含头结点，从 head.next → tail 认为是阻塞队列private transient volatile Node tail;static final class Node &#123; // 枚举：共享模式 static final Node SHARED = new Node(); // 枚举：独占模式 static final Node EXCLUSIVE = null; // node 需要构建成 FIFO 队列，prev 指向前继节点 volatile Node prev; // next 指向后继节点 volatile Node next; // 当前 node 封装的线程 volatile Thread thread; // 条件队列是单向链表，只有后继指针，条件队列使用该属性 Node nextWaiter;&#125; 条件变量来实现等待、唤醒机制，支持多个条件变量，类似于 Monitor 的 WaitSet，条件队列是单向链表 123456public class ConditionObject implements Condition, java.io.Serializable &#123; // 指向条件队列的第一个 node 节点 private transient Node firstWaiter; // 指向条件队列的最后一个 node 节点 private transient Node lastWaiter;&#125; 模板对象同步器的设计是基于模板方法模式，该模式是基于继承的，主要是为了在不改变模板结构的前提下在子类中重新定义模板中的内容以实现复用代码 使用者继承 AbstractQueuedSynchronizer 并重写指定的方法 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，这些模板方法会调用使用者重写的方法 AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法： 12345isHeldExclusively() //该线程是否正在独占资源。只有用到condition才需要去实现它tryAcquire(int) //独占方式。尝试获取资源，成功则返回true，失败则返回falsetryRelease(int) //独占方式。尝试释放资源，成功则返回true，失败则返回falsetryAcquireShared(int)\t//共享方式。尝试获取资源。负数表示失败；0表示成功但没有剩余可用资源；正数表示成功且有剩余资源tryReleaseShared(int)\t//共享方式。尝试释放资源，成功则返回true，失败则返回false 默认情况下，每个方法都抛出 UnsupportedOperationException 这些方法的实现必须是内部线程安全的 AQS 类中的其他方法都是 final ，所以无法被其他类使用，只有这几个方法可以被其他类使用 自定义自定义一个不可重入锁： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class MyLock implements Lock &#123; //独占锁 不可重入 class MySync extends AbstractQueuedSynchronizer &#123; @Override protected boolean tryAcquire(int arg) &#123; if (compareAndSetState(0, 1)) &#123; // 加上锁 设置 owner 为当前线程 setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; @Override //解锁 protected boolean tryRelease(int arg) &#123; setExclusiveOwnerThread(null); setState(0);//volatile 修饰的变量放在后面，防止指令重排 return true; &#125; @Override //是否持有独占锁 protected boolean isHeldExclusively() &#123; return getState() == 1; &#125; public Condition newCondition() &#123; return new ConditionObject(); &#125; &#125; private MySync sync = new MySync(); @Override //加锁（不成功进入等待队列等待） public void lock() &#123; sync.acquire(1); &#125; @Override //加锁 可打断 public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1); &#125; @Override //尝试加锁，尝试一次 public boolean tryLock() &#123; return sync.tryAcquire(1); &#125; @Override //尝试加锁，带超时 public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireNanos(1, unit.toNanos(time)); &#125; @Override //解锁 public void unlock() &#123; sync.release(1); &#125; @Override //条件变量 public Condition newCondition() &#123; return sync.newCondition(); &#125;&#125; Re-Lock锁对比ReentrantLock 相对于 synchronized 具备如下特点： 锁的实现：synchronized 是 JVM 实现的，而 ReentrantLock 是 JDK 实现的 性能：新版本 Java 对 synchronized 进行了很多优化，synchronized 与 ReentrantLock 大致相同 使用：ReentrantLock 需要手动解锁，synchronized 执行完代码块自动解锁 可中断：ReentrantLock 可中断，而 synchronized 不行 公平锁：公平锁是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁 ReentrantLock 可以设置公平锁，synchronized 中的锁是非公平的 不公平锁的含义是阻塞队列内公平，队列外非公平 锁超时：尝试获取锁，超时获取不到直接放弃，不进入阻塞队列 ReentrantLock 可以设置超时时间，synchronized 会一直等待 锁绑定多个条件：一个 ReentrantLock 可以同时绑定多个 Condition 对象，更细粒度的唤醒线程 两者都是可重入锁 使用锁构造方法：ReentrantLock lock = new ReentrantLock(); ReentrantLock 类 API： public void lock()：获得锁 如果锁没有被另一个线程占用，则将锁定计数设置为 1 如果当前线程已经保持锁定，则保持计数增加 1 如果锁被另一个线程保持，则当前线程被禁用线程调度，并且在锁定已被获取之前处于休眠状态 public void unlock()：尝试释放锁 如果当前线程是该锁的持有者，则保持计数递减 如果保持计数现在为零，则锁定被释放 如果当前线程不是该锁的持有者，则抛出异常 基本语法： 12345678// 获取锁reentrantLock.lock();try &#123; // 临界区&#125; finally &#123;\t// 释放锁\treentrantLock.unlock();&#125; 公平锁基本使用构造方法：ReentrantLock lock = new ReentrantLock(true) 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; ReentrantLock 默认是不公平的： 123public ReentrantLock() &#123; sync = new NonfairSync();&#125; 说明：公平锁一般没有必要，会降低并发度 非公原理加锁NonfairSync 继承自 AQS 123public void lock() &#123; sync.lock();&#125; 没有竞争：ExclusiveOwnerThread 属于 Thread-0，state 设置为 1 123456789// ReentrantLock.NonfairSync#lockfinal void lock() &#123; // 用 cas 尝试（仅尝试一次）将 state 从 0 改为 1, 如果成功表示【获得了独占锁】 if (compareAndSetState(0, 1)) // 设置当前线程为独占线程 setExclusiveOwnerThread(Thread.currentThread()); else acquire(1);//失败进入&#125; 第一个竞争出现：Thread-1 执行，CAS 尝试将 state 由 0 改为 1，结果失败（第一次），进入 acquire 逻辑 12345678// AbstractQueuedSynchronizer#acquirepublic final void acquire(int arg) &#123; // tryAcquire 尝试获取锁失败时, 会调用 addWaiter 将当前线程封装成node入队，acquireQueued 阻塞当前线程， // acquireQueued 返回 true 表示挂起过程中线程被中断唤醒过，false 表示未被中断过 if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 如果线程被中断了逻辑来到这，完成一次真正的打断效果 selfInterrupt();&#125; 进入 tryAcquire 尝试获取锁逻辑，这时 state 已经是1，结果仍然失败（第二次），加锁成功有两种情况： 当前 AQS 处于无锁状态 加锁线程就是当前线程，说明发生了锁重入 1234567891011121314151617181920212223242526272829303132// ReentrantLock.NonfairSync#tryAcquireprotected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125;// 抢占成功返回 true，抢占失败返回 falsefinal boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); // state 值 int c = getState(); // 条件成立说明当前处于【无锁状态】 if (c == 0) &#123; //如果还没有获得锁，尝试用cas获得，这里体现非公平性: 不去检查 AQS 队列是否有阻塞线程直接获取锁 if (compareAndSetState(0, acquires)) &#123; // 获取锁成功设置当前线程为独占锁线程。 setExclusiveOwnerThread(current); return true; &#125; &#125; // 如果已经有线程获得了锁, 独占锁线程还是当前线程, 表示【发生了锁重入】\telse if (current == getExclusiveOwnerThread()) &#123; // 更新锁重入的值 int nextc = c + acquires; // 越界判断，当重入的深度很深时，会导致 nextc &lt; 0，int值达到最大之后再 + 1 变负数 if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); // 更新 state 的值，这里不使用 cas 是因为当前线程正在持有锁，所以这里的操作相当于在一个管程内 setState(nextc); return true; &#125; // 获取失败 return false;&#125; 接下来进入 addWaiter 逻辑，构造 Node 队列（不是阻塞队列），前置条件是当前线程获取锁失败，说明有线程占用了锁 图中黄色三角表示该 Node 的 waitStatus 状态，其中 0 为默认正常状态 Node 的创建是懒惰的，其中第一个 Node 称为 Dummy（哑元）或哨兵，用来占位，并不关联线程 12345678910111213141516171819// AbstractQueuedSynchronizer#addWaiter，返回当前线程的 node 节点private Node addWaiter(Node mode) &#123; // 将当前线程关联到一个 Node 对象上, 模式为独占模式 Node node = new Node(Thread.currentThread(), mode); Node pred = tail; // 快速入队，如果 tail 不为 null，说明存在队列 if (pred != null) &#123; // 将当前节点的前驱节点指向 尾节点 node.prev = pred; // 通过 cas 将 Node 对象加入 AQS 队列，成为尾节点，【尾插法】 if (compareAndSetTail(pred, node)) &#123; pred.next = node;// 双向链表 return node; &#125; &#125; // 初始时队列为空，或者 CAS 失败进入这里 enq(node); return node;&#125; 12345678910111213141516171819202122// AbstractQueuedSynchronizer#enqprivate Node enq(final Node node) &#123; // 自旋入队，必须入队成功才结束循环 for (;;) &#123; Node t = tail; // 说明当前锁被占用，且当前线程可能是【第一个获取锁失败】的线程，【还没有建立队列】 if (t == null) &#123; // 设置一个【哑元节点】，头尾指针都指向该节点 if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; // 自旋到这，普通入队方式，首先赋值尾节点的前驱节点【尾插法】 node.prev = t; // 【在设置完尾节点后，才更新的原始尾节点的后继节点，所以此时从前往后遍历会丢失尾节点】 if (compareAndSetTail(t, node)) &#123; //【此时 t.next = null，并且这里已经 CAS 结束，线程并不是安全的】 t.next = node; return t;\t// 返回当前 node 的前驱节点 &#125; &#125; &#125;&#125; 线程节点加入队列成功，进入 AbstractQueuedSynchronizer#acquireQueued 逻辑阻塞线程 acquireQueued 会在一个自旋中不断尝试获得锁，失败后进入 park 阻塞 如果当前线程是在 head 节点后，会再次 tryAcquire 尝试获取锁，state 仍为 1 则失败（第三次） 12345678910111213141516171819202122232425262728293031final boolean acquireQueued(final Node node, int arg) &#123; // true 表示当前线程抢占锁失败，false 表示成功 boolean failed = true; try &#123; // 中断标记，表示当前线程是否被中断 boolean interrupted = false; for (;;) &#123; // 获得当前线程节点的前驱节点 final Node p = node.predecessor(); // 前驱节点是 head, FIFO 队列的特性表示轮到当前线程可以去获取锁 if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 获取成功, 设置当前线程自己的 node 为 head setHead(node); p.next = null; // help GC // 表示抢占锁成功 failed = false; // 返回当前线程是否被中断 return interrupted; &#125; // 判断是否应当 park，返回 false 后需要新一轮的循环，返回 true 进入条件二阻塞线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 条件二返回结果是当前线程是否被打断，没有被打断返回 false 不进入这里的逻辑 // 【就算被打断了，也会继续循环，并不会返回】 interrupted = true; &#125; &#125; finally &#123; // 【可打断模式下才会进入该逻辑】 if (failed) cancelAcquire(node); &#125;&#125; 进入 shouldParkAfterFailedAcquire 逻辑，将前驱 node 的 waitStatus 改为 -1，返回 false；waitStatus 为 -1 的节点用来唤醒下一个节点 1234567891011121314151617181920private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; // 表示前置节点是个可以唤醒当前节点的节点，返回 true if (ws == Node.SIGNAL) return true; // 前置节点的状态处于取消状态，需要【删除前面所有取消的节点】, 返回到外层循环重试 if (ws &gt; 0) &#123; do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); // 获取到非取消的节点，连接上当前节点 pred.next = node; // 默认情况下 node 的 waitStatus 是 0，进入这里的逻辑 &#125; else &#123; // 【设置上一个节点状态为 Node.SIGNAL】，返回外层循环重试 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; // 返回不应该 park，再次尝试一次 return false;&#125; shouldParkAfterFailedAcquire 执行完毕回到 acquireQueued ，再次 tryAcquire 尝试获取锁，这时 state 仍为 1 获取失败（第四次） 当再次进入 shouldParkAfterFailedAcquire 时，这时其前驱 node 的 waitStatus 已经是 -1 了，返回 true 进入 parkAndCheckInterrupt， Thread-1 park（灰色表示） 123456private final boolean parkAndCheckInterrupt() &#123; // 阻塞当前线程，如果打断标记已经是 true, 则 park 会失效 LockSupport.park(this); // 判断当前线程是否被打断，清除打断标记 return Thread.interrupted();&#125; 再有多个线程经历竞争失败后： 解锁ReentrantLock#unlock：释放锁 123public void unlock() &#123; sync.release(1);&#125; Thread-0 释放锁，进入 release 流程 进入 tryRelease，设置 exclusiveOwnerThread 为 null，state &#x3D; 0 当前队列不为 null，并且 head 的 waitStatus &#x3D; -1，进入 unparkSuccessor 1234567891011121314// AbstractQueuedSynchronizer#releasepublic final boolean release(int arg) &#123; // 尝试释放锁，tryRelease 返回 true 表示当前线程已经【完全释放锁，重入的释放了】 if (tryRelease(arg)) &#123; // 队列头节点 Node h = head; // 头节点什么时候是空？没有发生锁竞争，没有竞争线程创建哑元节点 // 条件成立说明阻塞队列有等待线程，需要唤醒 head 节点后面的线程 if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 123456789101112131415161718// ReentrantLock.Sync#tryReleaseprotected final boolean tryRelease(int releases) &#123; // 减去释放的值，可能重入 int c = getState() - releases; // 如果当前线程不是持有锁的线程直接报错 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); // 是否已经完全释放锁 boolean free = false; // 支持锁重入, 只有 state 减为 0, 才完全释放锁成功 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; // 当前线程就是持有锁线程，所以可以直接更新锁，不需要使用 CAS setState(c); return free;&#125; 进入 AbstractQueuedSynchronizer#unparkSuccessor 方法，唤醒当前节点的后继节点 找到队列中距离 head 最近的一个没取消的 Node，unpark 恢复其运行，本例中即为 Thread-1 回到 Thread-1 的 acquireQueued 流程 12345678910111213141516171819202122private void unparkSuccessor(Node node) &#123; // 当前节点的状态 int ws = node.waitStatus; if (ws &lt; 0) // 【尝试重置状态为 0】，因为当前节点要完成对后续节点的唤醒任务了，不需要 -1 了 compareAndSetWaitStatus(node, ws, 0); // 找到需要 unpark 的节点，当前节点的下一个 Node s = node.next; // 已取消的节点不能唤醒，需要找到距离头节点最近的非取消的节点 if (s == null || s.waitStatus &gt; 0) &#123; s = null; // AQS 队列【从后至前】找需要 unpark 的节点，直到 t == 当前的 node 为止，找不到就不唤醒了 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) // 说明当前线程状态需要被唤醒 if (t.waitStatus &lt;= 0) // 置换引用 s = t; &#125; // 【找到合适的可以被唤醒的 node，则唤醒线程】 if (s != null) LockSupport.unpark(s.thread);&#125; 从后向前的唤醒的原因：enq 方法中，节点是尾插法，首先赋值的是尾节点的前驱节点，此时前驱节点的 next 并没有指向尾节点，从前遍历会丢失尾节点 唤醒的线程会从 park 位置开始执行，如果加锁成功（没有竞争），会设置 exclusiveOwnerThread 为 Thread-1，state &#x3D; 1 head 指向刚刚 Thread-1 所在的 Node，该 Node 会清空 Thread 原本的 head 因为从链表断开，而可被垃圾回收（图中有错误，原来的头节点的 waitStatus 被改为 0 了） 如果这时有其它线程来竞争（非公平），例如这时有 Thread-4 来了并抢占了锁 Thread-4 被设置为 exclusiveOwnerThread，state &#x3D; 1 Thread-1 再次进入 acquireQueued 流程，获取锁失败，重新进入 park 阻塞 公平原理与非公平锁主要区别在于 tryAcquire 方法：先检查 AQS 队列中是否有前驱节点，没有才去 CAS 竞争 123456789101112131415161718192021static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; final void lock() &#123; acquire(1); &#125; protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 先检查 AQS 队列中是否有前驱节点, 没有(false)才去竞争 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; // 锁重入 return false; &#125;&#125; 12345678910public final boolean hasQueuedPredecessors() &#123; Node t = tail; Node h = head; Node s; // 头尾指向一个节点，链表为空，返回false return h != t &amp;&amp; // 头尾之间有节点，判断头节点的下一个是不是空 // 不是空进入最后的判断，第二个节点的线程是否是本线程，不是返回 true，表示当前节点有前驱节点 ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 可重入可重入是指同一个线程如果首次获得了这把锁，那么它是这把锁的拥有者，因此有权利再次获取这把锁，如果不可重入锁，那么第二次获得锁时，自己也会被锁挡住，直接造成死锁 源码解析参考：nonfairTryAcquire(int acquires)) 和 tryRelease(int releases) 123456789101112131415161718192021static ReentrantLock lock = new ReentrantLock();public static void main(String[] args) &#123; method1();&#125;public static void method1() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + &quot; execute method1&quot;); method2(); &#125; finally &#123; lock.unlock(); &#125;&#125;public static void method2() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + &quot; execute method2&quot;); &#125; finally &#123; lock.unlock(); &#125;&#125; 在 Lock 方法加两把锁会是什么情况呢？ 加锁两次解锁两次：正常执行 加锁两次解锁一次：程序直接卡死，线程不能出来，也就说明申请几把锁，最后需要解除几把锁 加锁一次解锁两次：运行程序会直接报错 12345678910public void getLock() &#123; lock.lock(); lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t get Lock&quot;); &#125; finally &#123; lock.unlock(); //lock.unlock(); &#125;&#125; 可打断基本使用public void lockInterruptibly()：获得可打断的锁 如果没有竞争此方法就会获取 lock 对象锁 如果有竞争就进入阻塞队列，可以被其他线程用 interrupt 打断 注意：如果是不可中断模式，那么即使使用了 interrupt 也不会让等待状态中的线程中断 12345678910111213141516171819202122public static void main(String[] args) throws InterruptedException &#123; ReentrantLock lock = new ReentrantLock(); Thread t1 = new Thread(() -&gt; &#123; try &#123; System.out.println(&quot;尝试获取锁&quot;); lock.lockInterruptibly(); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;没有获取到锁，被打断，直接返回&quot;); return; &#125; try &#123; System.out.println(&quot;获取到锁&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125;, &quot;t1&quot;); lock.lock(); t1.start(); Thread.sleep(2000); System.out.println(&quot;主线程进行打断锁&quot;); t1.interrupt();&#125; 实现原理 不可打断模式：即使它被打断，仍会驻留在 AQS 阻塞队列中，一直要等到获得锁后才能得知自己被打断了 123456789public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg))//阻塞等待 // 如果acquireQueued返回true，打断状态 interrupted = true selfInterrupt();&#125;static void selfInterrupt() &#123; // 知道自己被打断了，需要重新产生一次中断完成中断效果 Thread.currentThread().interrupt();&#125; 12345678910111213141516171819202122232425262728final boolean acquireQueued(final Node node, int arg) &#123; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; // 还是需要获得锁后, 才能返回打断状态 return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt())&#123; // 条件二中判断当前线程是否被打断，被打断返回true，设置中断标记为 true，【获取锁后返回】 interrupted = true; &#125; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; private final boolean parkAndCheckInterrupt() &#123; // 阻塞当前线程，如果打断标记已经是 true, 则 park 会失效 LockSupport.park(this); // 判断当前线程是否被打断，清除打断标记，被打断返回true return Thread.interrupted(); &#125; 可打断模式：AbstractQueuedSynchronizer#acquireInterruptibly，被打断后会直接抛出异常 1234567891011public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1);&#125;public final void acquireInterruptibly(int arg) &#123; // 被其他线程打断了直接返回 false if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) // 没获取到锁，进入这里 doAcquireInterruptibly(arg);&#125; 123456789101112131415161718private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; // 返回封装当前线程的节点 final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; //... if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 【在 park 过程中如果被 interrupt 会抛出异常】, 而不会再次进入循环获取锁后才完成打断效果 throw new InterruptedException(); &#125; &#125; finally &#123; // 抛出异常前会进入这里 if (failed) // 取消当前线程的节点 cancelAcquire(node); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 取消节点出队的逻辑private void cancelAcquire(Node node) &#123; // 判空 if (node == null) return;\t// 把当前节点封装的 Thread 置为空 node.thread = null;\t// 获取当前取消的 node 的前驱节点 Node pred = node.prev; // 前驱节点也被取消了，循环找到前面最近的没被取消的节点 while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // 获取前驱节点的后继节点，可能是当前 node，也可能是 waitStatus &gt; 0 的节点 Node predNext = pred.next; // 把当前节点的状态设置为 【取消状态 1】 node.waitStatus = Node.CANCELLED; // 条件成立说明当前节点是尾节点，把当前节点的前驱节点设置为尾节点 if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; // 把前驱节点的后继节点置空，这里直接把所有的取消节点出队 compareAndSetNext(pred, predNext, null); &#125; else &#123; // 说明当前节点不是 tail 节点 int ws; // 条件一成立说明当前节点不是 head.next 节点 if (pred != head &amp;&amp; // 判断前驱节点的状态是不是 -1，不成立说明前驱状态可能是 0 或者刚被其他线程取消排队了 ((ws = pred.waitStatus) == Node.SIGNAL || // 如果状态不是 -1，设置前驱节点的状态为 -1 (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; // 前驱节点的线程不为null pred.thread != null) &#123; Node next = node.next; // 当前节点的后继节点是正常节点 if (next != null &amp;&amp; next.waitStatus &lt;= 0) // 把 前驱节点的后继节点 设置为 当前节点的后继节点，【从队列中删除了当前节点】 compareAndSetNext(pred, predNext, next); &#125; else &#123; // 当前节点是 head.next 节点，唤醒当前节点的后继节点 unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 锁超时基本使用public boolean tryLock()：尝试获取锁，获取到返回 true，获取不到直接放弃，不进入阻塞队列 public boolean tryLock(long timeout, TimeUnit unit)：在给定时间内获取锁，获取不到就退出 注意：tryLock 期间也可以被打断 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; ReentrantLock lock = new ReentrantLock(); Thread t1 = new Thread(() -&gt; &#123; try &#123; if (!lock.tryLock(2, TimeUnit.SECONDS)) &#123; System.out.println(&quot;获取不到锁&quot;); return; &#125; &#125; catch (InterruptedException e) &#123; System.out.println(&quot;被打断，获取不到锁&quot;); return; &#125; try &#123; log.debug(&quot;获取到锁&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125;, &quot;t1&quot;); lock.lock(); System.out.println(&quot;主线程获取到锁&quot;); t1.start(); Thread.sleep(1000); try &#123; System.out.println(&quot;主线程释放了锁&quot;); &#125; finally &#123; lock.unlock(); &#125;&#125; 实现原理 成员变量：指定超时限制的阈值，小于该值的线程不会被挂起 1static final long spinForTimeoutThreshold = 1000L; 超时时间设置的小于该值，就会被禁止挂起，因为阻塞在唤醒的成本太高，不如选择自旋空转 tryLock() 1234public boolean tryLock() &#123; // 只尝试一次 return sync.nonfairTryAcquire(1);&#125; tryLock(long timeout, TimeUnit unit) 123456789public final boolean tryAcquireNanos(int arg, long nanosTimeout) &#123; if (Thread.interrupted()) throw new InterruptedException(); // tryAcquire 尝试一次 return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout);&#125;protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125; 1234567891011121314151617181920212223private boolean doAcquireNanos(int arg, long nanosTimeout) &#123; if (nanosTimeout &lt;= 0L) return false; // 获取最后期限的时间戳 final long deadline = System.nanoTime() + nanosTimeout; //... try &#123; for (;;) &#123; //... // 计算还需等待的时间 nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout &lt;= 0L)\t//时间已到 return false; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // 如果 nanosTimeout 大于该值，才有阻塞的意义，否则直接自旋会好点 nanosTimeout &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); // 【被打断会报异常】 if (Thread.interrupted()) throw new InterruptedException(); &#125; &#125;&#125; 哲学家就餐12345678910111213141516171819202122232425262728293031323334353637383940414243public static void main(String[] args) &#123; Chopstick c1 = new Chopstick(&quot;1&quot;);//... Chopstick c5 = new Chopstick(&quot;5&quot;); new Philosopher(&quot;苏格拉底&quot;, c1, c2).start(); new Philosopher(&quot;柏拉图&quot;, c2, c3).start(); new Philosopher(&quot;亚里士多德&quot;, c3, c4).start(); new Philosopher(&quot;赫拉克利特&quot;, c4, c5).start(); new Philosopher(&quot;阿基米德&quot;, c5, c1).start();&#125;class Philosopher extends Thread &#123; Chopstick left; Chopstick right; public void run() &#123; while (true) &#123; // 尝试获得左手筷子 if (left.tryLock()) &#123; try &#123; // 尝试获得右手筷子 if (right.tryLock()) &#123; try &#123; System.out.println(&quot;eating...&quot;); Thread.sleep(1000); &#125; finally &#123; right.unlock(); &#125; &#125; &#125; finally &#123; left.unlock(); &#125; &#125; &#125; &#125;&#125;class Chopstick extends ReentrantLock &#123; String name; public Chopstick(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;筷子&#123;&quot; + name + &#x27;&#125;&#x27;; &#125;&#125; 条件变量基本使用synchronized 的条件变量，是当条件不满足时进入 WaitSet 等待；ReentrantLock 的条件变量比 synchronized 强大之处在于支持多个条件变量 ReentrantLock 类获取 Condition 对象：public Condition newCondition() Condition 类 API： void await()：当前线程从运行状态进入等待状态，释放锁 void signal()：唤醒一个等待在 Condition 上的线程，但是必须获得与该 Condition 相关的锁 使用流程： await &#x2F; signal 前需要获得锁 await 执行后，会释放锁进入 ConditionObject 等待 await 的线程被唤醒去重新竞争 lock 锁 线程在条件队列被打断会抛出中断异常 竞争 lock 锁成功后，从 await 后继续执行 123456789101112131415161718192021222324252627282930public static void main(String[] args) throws InterruptedException &#123; ReentrantLock lock = new ReentrantLock(); //创建一个新的条件变量 Condition condition1 = lock.newCondition(); Condition condition2 = lock.newCondition(); new Thread(() -&gt; &#123; try &#123; lock.lock(); System.out.println(&quot;进入等待&quot;); //进入休息室等待 condition1.await(); System.out.println(&quot;被唤醒了&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;).start(); Thread.sleep(1000); //叫醒 new Thread(() -&gt; &#123; try &#123; lock.lock(); //唤醒 condition2.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125;).start();&#125; 实现原理await总体流程是将 await 线程包装成 node 节点放入 ConditionObject 的条件队列，如果被唤醒就将 node 转移到 AQS 的执行阻塞队列，等待获取锁，每个 Condition 对象都包含一个等待队列 开始 Thread-0 持有锁，调用 await，线程进入 ConditionObject 等待，直到被唤醒或打断，调用 await 方法的线程都是持锁状态的，所以说逻辑里不存在并发 12345678910111213141516171819202122232425262728293031323334public final void await() throws InterruptedException &#123; // 判断当前线程是否是中断状态，是就直接给个中断异常 if (Thread.interrupted()) throw new InterruptedException(); // 将调用 await 的线程包装成 Node，添加到条件队列并返回 Node node = addConditionWaiter(); // 完全释放节点持有的锁，因为其他线程唤醒当前线程的前提是【持有锁】 int savedState = fullyRelease(node); // 设置打断模式为没有被打断，状态码为 0 int interruptMode = 0; // 如果该节点还没有转移至 AQS 阻塞队列, park 阻塞，等待进入阻塞队列 while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); // 如果被打断，退出等待队列，对应的 node 【也会被迁移到阻塞队列】尾部，状态设置为 0 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 逻辑到这说明当前线程退出等待队列，进入【阻塞队列】 // 尝试枪锁，释放了多少锁就【重新获取多少锁】，获取锁成功判断打断模式 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; // node 在条件队列时 如果被外部线程中断唤醒，会加入到阻塞队列，但是并未设 nextWaiter = null if (node.nextWaiter != null) // 清理条件队列内所有已取消的 Node unlinkCancelledWaiters(); // 条件成立说明挂起期间发生过中断 if (interruptMode != 0) // 应用打断模式 reportInterruptAfterWait(interruptMode);&#125; 1234// 打断模式 - 在退出等待时重新设置打断状态private static final int REINTERRUPT = 1;// 打断模式 - 在退出等待时抛出异常private static final int THROW_IE = -1; 创建新的 Node 状态为 -2（Node.CONDITION），关联 Thread-0，加入等待队列尾部 12345678910111213141516171819private Node addConditionWaiter() &#123; // 获取当前条件队列的尾节点的引用，保存到局部变量 t 中 Node t = lastWaiter; // 当前队列中不是空，并且节点的状态不是 CONDITION（-2），说明当前节点发生了中断 if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; // 清理条件队列内所有已取消的 Node unlinkCancelledWaiters(); // 清理完成重新获取 尾节点 的引用 t = lastWaiter; &#125; // 创建一个关联当前线程的新 node, 设置状态为 CONDITION(-2)，添加至队列尾部 Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; // 空队列直接放在队首【不用CAS因为执行线程是持锁线程，并发安全】 else t.nextWaiter = node;\t// 非空队列队尾追加 lastWaiter = node; // 更新队尾的引用 return node;&#125; 1234567891011121314151617181920212223242526272829303132// 清理条件队列内所有已取消（不是CONDITION）的 node，【链表删除的逻辑】private void unlinkCancelledWaiters() &#123; // 从头节点开始遍历【FIFO】 Node t = firstWaiter; // 指向正常的 CONDITION 节点 Node trail = null; // 等待队列不空 while (t != null) &#123; // 获取当前节点的后继节点 Node next = t.nextWaiter; // 判断 t 节点是不是 CONDITION 节点，条件队列内不是 CONDITION 就不是正常的 if (t.waitStatus != Node.CONDITION) &#123; // 不是正常节点，需要 t 与下一个节点断开 t.nextWaiter = null; // 条件成立说明遍历到的节点还未碰到过正常节点 if (trail == null) // 更新 firstWaiter 指针为下个节点 firstWaiter = next; else // 让上一个正常节点指向 当前取消节点的 下一个节点，【删除非正常的节点】 trail.nextWaiter = next; // t 是尾节点了，更新 lastWaiter 指向最后一个正常节点 if (next == null) lastWaiter = trail; &#125; else &#123; // trail 指向的是正常节点 trail = t; &#125; // 把 t.next 赋值给 t，循环遍历 t = next; &#125;&#125; 接下来 Thread-0 进入 AQS 的 fullyRelease 流程，释放同步器上的锁 1234567891011121314151617181920212223// 线程可能重入，需要将 state 全部释放final int fullyRelease(Node node) &#123; // 完全释放锁是否成功，false 代表成功 boolean failed = true; try &#123; // 获取当前线程所持有的 state 值总数 int savedState = getState(); // release -&gt; tryRelease 解锁重入锁 if (release(savedState)) &#123; // 释放成功 failed = false; // 返回解锁的深度 return savedState; &#125; else &#123; // 解锁失败抛出异常 throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; // 没有释放成功，将当前 node 设置为取消状态 if (failed) node.waitStatus = Node.CANCELLED; &#125;&#125; fullyRelease 中会 unpark AQS 队列中的下一个节点竞争锁，假设 Thread-1 竞争成功 Thread-0 进入 isOnSyncQueue 逻辑判断节点是否移动到阻塞队列，没有就 park 阻塞 Thread-0 1234567891011final boolean isOnSyncQueue(Node node) &#123; // node 的状态是 CONDITION，signal 方法是先修改状态再迁移，所以前驱节点为空证明还【没有完成迁移】 if (node.waitStatus == Node.CONDITION || node.prev == null) return false; // 说明当前节点已经成功入队到阻塞队列，且当前节点后面已经有其它 node，因为条件队列的 next 指针为 null if (node.next != null) return true;\t// 说明【可能在阻塞队列，但是是尾节点】 // 从阻塞队列的尾节点开始向前【遍历查找 node】，如果查找到返回 true，查找不到返回 false return findNodeFromTail(node);&#125; await 线程 park 后如果被 unpark 或者被打断，都会进入 checkInterruptWhileWaiting 判断线程是否被打断：在条件队列被打断的线程需要抛出异常 12345private int checkInterruptWhileWaiting(Node node) &#123; // Thread.interrupted() 返回当前线程中断标记位，并且重置当前标记位 为 false // 如果被中断了，根据是否在条件队列被中断的，设置中断状态码 return Thread.interrupted() ?(transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) : 0;&#125; 123456789101112131415161718192021// 这个方法只有在线程是被打断唤醒时才会调用final boolean transferAfterCancelledWait(Node node) &#123; // 条件成立说明当前node一定是在条件队列内，因为 signal 迁移节点到阻塞队列时，会将节点的状态修改为 0 if (compareAndSetWaitStatus(node, Node.CONDITION, 0)) &#123; // 把【中断唤醒的 node 加入到阻塞队列中】 enq(node); // 表示是在条件队列内被中断了，设置为 THROW_IE 为 -1 return true; &#125; //执行到这里的情况： //1.当前node已经被外部线程调用 signal 方法将其迁移到 阻塞队列 内了 //2.当前node正在被外部线程调用 signal 方法将其迁移至 阻塞队列 进行中状态 // 如果当前线程还没到阻塞队列，一直释放 CPU while (!isOnSyncQueue(node)) Thread.yield(); // 表示当前节点被中断唤醒时不在条件队列了，设置为 REINTERRUPT 为 1 return false;&#125; 最后开始处理中断状态： 12345678910private void reportInterruptAfterWait(int interruptMode) throws InterruptedException &#123; // 条件成立说明【在条件队列内发生过中断，此时 await 方法抛出中断异常】 if (interruptMode == THROW_IE) throw new InterruptedException(); // 条件成立说明【在条件队列外发生的中断，此时设置当前线程的中断标记位为 true】 else if (interruptMode == REINTERRUPT) // 进行一次自己打断，产生中断的效果 selfInterrupt();&#125; signal 假设 Thread-1 要来唤醒 Thread-0，进入 ConditionObject 的 doSignal 流程，取得等待队列中第一个 Node，即 Thread-0 所在 Node，必须持有锁才能唤醒, 因此 doSignal 内线程安全 12345678910public final void signal() &#123; // 判断调用 signal 方法的线程是否是独占锁持有线程 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); // 获取条件队列中第一个 Node Node first = firstWaiter; // 不为空就将第该节点【迁移到阻塞队列】 if (first != null) doSignal(first);&#125; 12345678910111213141516171819202122// 唤醒 - 【将没取消的第一个节点转移至 AQS 队列尾部】private void doSignal(Node first) &#123; do &#123; // 成立说明当前节点的下一个节点是 null，当前节点是尾节点了，队列中只有当前一个节点了 if ((firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; // 将等待队列中的 Node 转移至 AQS 队列，不成功且还有节点则继续循环 &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null);&#125;// signalAll() 会调用这个函数，唤醒所有的节点private void doSignalAll(Node first) &#123; lastWaiter = firstWaiter = null; do &#123; Node next = first.nextWaiter; first.nextWaiter = null; transferForSignal(first); first = next; // 唤醒所有的节点，都放到阻塞队列中 &#125; while (first != null);&#125; 执行 transferForSignal，先将节点的 waitStatus 改为 0，然后加入 AQS 阻塞队列尾部，将 Thread-3 的 waitStatus 改为 -1 12345678910111213141516171819// 如果节点状态是取消, 返回 false 表示转移失败, 否则转移成功final boolean transferForSignal(Node node) &#123; // CAS 修改当前节点的状态，修改为 0，因为当前节点马上要迁移到阻塞队列了 // 如果状态已经不是 CONDITION, 说明线程被取消（await 释放全部锁失败）或者被中断（可打断 cancelAcquire） if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) // 返回函数调用处继续寻找下一个节点 return false; // 【先改状态，再进行迁移】 // 将当前 node 入阻塞队列，p 是当前节点在阻塞队列的【前驱节点】 Node p = enq(node); int ws = p.waitStatus; // 如果前驱节点被取消或者不能设置状态为 Node.SIGNAL，就 unpark 取消当前节点线程的阻塞状态, // 让 thread-0 线程竞争锁，重新同步状态 if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true;&#125; Thread-1 释放锁，进入 unlock 流程 ReadWrite读写锁独占锁：指该锁一次只能被一个线程所持有，对 ReentrantLock 和 Synchronized 而言都是独占锁 共享锁：指该锁可以被多个线程锁持有 ReentrantReadWriteLock 其读锁是共享锁，写锁是独占锁 作用：多个线程同时读一个资源类没有任何问题，为了满足并发量，读取共享资源应该同时进行，但是如果一个线程想去写共享资源，就不应该再有其它线程可以对该资源进行读或写 使用规则： 加锁解锁格式： 123456r.lock();try &#123; // 临界区&#125; finally &#123;\tr.unlock();&#125; 读-读能共存、读-写不能共存、写-写不能共存 读锁不支持条件变量 重入时升级不支持：持有读锁的情况下去获取写锁会导致获取写锁永久等待，需要先释放读，再去获得写 重入时降级支持：持有写锁的情况下去获取读锁，造成只有当前线程会持有读锁，因为写锁会互斥其他的锁 1234567891011w.lock();try &#123; r.lock();// 降级为读锁, 释放写锁, 这样能够让其它线程读取缓存 try &#123; // ... &#125; finally&#123; w.unlock();// 要在写锁释放之前获取读锁 &#125;&#125; finally&#123;\tr.unlock();&#125; 构造方法： public ReentrantReadWriteLock()：默认构造方法，非公平锁 public ReentrantReadWriteLock(boolean fair)：true 为公平锁 常用API： public ReentrantReadWriteLock.ReadLock readLock()：返回读锁 public ReentrantReadWriteLock.WriteLock writeLock()：返回写锁 public void lock()：加锁 public void unlock()：解锁 public boolean tryLock()：尝试获取锁 读读并发： 123456789101112131415161718192021222324public static void main(String[] args) &#123; ReentrantReadWriteLock rw = new ReentrantReadWriteLock(); ReentrantReadWriteLock.ReadLock r = rw.readLock(); ReentrantReadWriteLock.WriteLock w = rw.writeLock(); new Thread(() -&gt; &#123; r.lock(); try &#123; Thread.sleep(2000); System.out.println(&quot;Thread 1 running &quot; + new Date()); &#125; finally &#123; r.unlock(); &#125; &#125;,&quot;t1&quot;).start(); new Thread(() -&gt; &#123; r.lock(); try &#123; Thread.sleep(2000); System.out.println(&quot;Thread 2 running &quot; + new Date()); &#125; finally &#123; r.unlock(); &#125; &#125;,&quot;t2&quot;).start();&#125; 缓存应用缓存更新时，是先清缓存还是先更新数据库 先清缓存：可能造成刚清理缓存还没有更新数据库，线程直接查询了数据库更新过期数据到缓存 先更新据库：可能造成刚更新数据库，还没清空缓存就有线程从缓存拿到了旧数据 补充情况：查询线程 A 查询数据时恰好缓存数据由于时间到期失效，或是第一次查询 可以使用读写锁进行操作 实现原理成员属性读写锁用的是同一个 Sycn 同步器，因此等待队列、state 等也是同一个，原理与 ReentrantLock 加锁相比没有特殊之处，不同是写锁状态占了 state 的低 16 位，而读锁使用的是 state 的高 16 位 读写锁： 12private final ReentrantReadWriteLock.ReadLock readerLock; private final ReentrantReadWriteLock.WriteLock writerLock; 构造方法：默认是非公平锁，可以指定参数创建公平锁 1234567public ReentrantReadWriteLock(boolean fair) &#123; // true 为公平锁 sync = fair ? new FairSync() : new NonfairSync(); // 这两个 lock 共享同一个 sync 实例，都是由 ReentrantReadWriteLock 的 sync 提供同步实现 readerLock = new ReadLock(this); writerLock = new WriteLock(this);&#125; Sync 类的属性： 统计变量： 12345678// 用来移位static final int SHARED_SHIFT = 16;// 高16位的1static final int SHARED_UNIT = (1 &lt;&lt; SHARED_SHIFT);// 65535，16个1，代表写锁的最大重入次数static final int MAX_COUNT = (1 &lt;&lt; SHARED_SHIFT) - 1;// 低16位掩码：0b 1111 1111 1111 1111，用来获取写锁重入的次数static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1; 获取读写锁的次数： 1234// 获取读写锁的读锁分配的总次数static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125;// 写锁（独占）锁的重入次数static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125; 内部类： 123456789101112// 记录读锁线程自己的持有读锁的数量（重入次数），因为 state 高16位记录的是全局范围内所有的读线程获取读锁的总量static final class HoldCounter &#123; int count = 0; // Use id, not reference, to avoid garbage retention final long tid = getThreadId(Thread.currentThread());&#125;// 线程安全的存放线程各自的 HoldCounter 对象static final class ThreadLocalHoldCounter extends ThreadLocal&lt;HoldCounter&gt; &#123; public HoldCounter initialValue() &#123; return new HoldCounter(); &#125;&#125; 内部类实例： 1234// 当前线程持有的可重入读锁的数量，计数为 0 时删除private transient ThreadLocalHoldCounter readHolds;// 记录最后一个获取【读锁】线程的 HoldCounter 对象private transient HoldCounter cachedHoldCounter; 首次获取锁： 1234// 第一个获取读锁的线程private transient Thread firstReader = null;// 记录该线程持有的读锁次数（读锁重入次数）private transient int firstReaderHoldCount; Sync 构造方法： 12345Sync() &#123; readHolds = new ThreadLocalHoldCounter(); // 确保其他线程的数据可见性，state 是 volatile 修饰的变量，重写该值会将线程本地缓存数据【同步至主存】 setState(getState()); &#125; 加锁原理 t1 线程：w.lock（写锁），成功上锁 state &#x3D; 0_1 123456789// lock() -&gt; sync.acquire(1);public void lock() &#123; sync.acquire(1);&#125;public final void acquire(int arg) &#123; // 尝试获得写锁，获得写锁失败，将当前线程关联到一个 Node 对象上, 模式为独占模式 if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 1234567891011121314151617181920212223242526272829303132333435protected final boolean tryAcquire(int acquires) &#123; Thread current = Thread.currentThread(); int c = getState(); // 获得低 16 位, 代表写锁的 state 计数 int w = exclusiveCount(c); // 说明有读锁或者写锁 if (c != 0) &#123; // c != 0 and w == 0 表示有读锁，【读锁不能升级】，直接返回 false // w != 0 说明有写锁，写锁的拥有者不是自己，获取失败 if (w == 0 || current != getExclusiveOwnerThread()) return false; // 执行到这里只有一种情况：【写锁重入】，所以下面几行代码不存在并发 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // 写锁重入, 获得锁成功，没有并发，所以不使用 CAS setState(c + acquires); return true; &#125; // c == 0，说明没有任何锁，判断写锁是否该阻塞，是 false 就尝试获取锁，失败返回 false if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; // 获得锁成功，设置锁的持有线程为当前线程 setExclusiveOwnerThread(current); return true;&#125;// 非公平锁 writerShouldBlock 总是返回 false, 无需阻塞final boolean writerShouldBlock() &#123; return false; &#125;// 公平锁会检查 AQS 队列中是否有前驱节点, 没有(false)才去竞争final boolean writerShouldBlock() &#123; return hasQueuedPredecessors();&#125; t2 r.lock（读锁），进入 tryAcquireShared 流程： 返回 -1 表示失败 如果返回 0 表示成功 返回正数表示还有多少后继节点支持共享模式，读写锁返回 1 12345678public void lock() &#123; sync.acquireShared(1);&#125;public final void acquireShared(int arg) &#123; // tryAcquireShared 返回负数, 表示获取读锁失败 if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 尝试以共享模式获取protected final int tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); int c = getState(); // exclusiveCount(c) 代表低 16 位, 写锁的 state，成立说明有线程持有写锁 // 写锁的持有者不是当前线程，则获取读锁失败，【写锁允许降级】 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; // 高 16 位，代表读锁的 state，共享锁分配出去的总次数 int r = sharedCount(c); // 读锁是否应该阻塞 if (!readerShouldBlock() &amp;&amp;\tr &lt; MAX_COUNT &amp;&amp; compareAndSetState(c, c + SHARED_UNIT)) &#123;\t// 尝试增加读锁计数 // 加锁成功 // 加锁之前读锁为 0，说明当前线程是第一个读锁线程 if (r == 0) &#123; firstReader = current; firstReaderHoldCount = 1; // 第一个读锁线程是自己就发生了读锁重入 &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; // cachedHoldCounter 设置为当前线程的 holdCounter 对象，即最后一个获取读锁的线程 HoldCounter rh = cachedHoldCounter; // 说明还没设置 rh if (rh == null || rh.tid != getThreadId(current)) // 获取当前线程的锁重入的对象，赋值给 cachedHoldCounter cachedHoldCounter = rh = readHolds.get(); // 还没重入 else if (rh.count == 0) readHolds.set(rh); // 重入 + 1 rh.count++; &#125; // 读锁加锁成功 return 1; &#125; // 逻辑到这 应该阻塞，或者 cas 加锁失败 // 会不断尝试 for (;;) 获取读锁, 执行过程中无阻塞 return fullTryAcquireShared(current);&#125;// 非公平锁 readerShouldBlock 偏向写锁一些，看 AQS 阻塞队列中第一个节点是否是写锁，是则阻塞，反之不阻塞// 防止一直有读锁线程，导致写锁线程饥饿// true 则该阻塞, false 则不阻塞final boolean readerShouldBlock() &#123; return apparentlyFirstQueuedIsExclusive();&#125;final boolean readerShouldBlock() &#123; return hasQueuedPredecessors();&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657final int fullTryAcquireShared(Thread current) &#123; // 当前读锁线程持有的读锁次数对象 HoldCounter rh = null; for (;;) &#123; int c = getState(); // 说明有线程持有写锁 if (exclusiveCount(c) != 0) &#123; // 写锁不是自己则获取锁失败 if (getExclusiveOwnerThread() != current) return -1; &#125; else if (readerShouldBlock()) &#123; // 条件成立说明当前线程是 firstReader，当前锁是读忙碌状态，而且当前线程也是读锁重入 if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; &#125; else &#123; if (rh == null) &#123; // 最后一个读锁的 HoldCounter rh = cachedHoldCounter; // 说明当前线程也不是最后一个读锁 if (rh == null || rh.tid != getThreadId(current)) &#123; // 获取当前线程的 HoldCounter rh = readHolds.get(); // 条件成立说明 HoldCounter 对象是上一步代码新建的 // 当前线程不是锁重入，在 readerShouldBlock() 返回 true 时需要去排队 if (rh.count == 0) // 防止内存泄漏 readHolds.remove(); &#125; &#125; if (rh.count == 0) return -1; &#125; &#125; // 越界判断 if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // 读锁加锁，条件内的逻辑与 tryAcquireShared 相同 if (compareAndSetState(c, c + SHARED_UNIT)) &#123; if (sharedCount(c) == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; cachedHoldCounter = rh; // cache for release &#125; return 1; &#125; &#125;&#125; 获取读锁失败，进入 sync.doAcquireShared(1) 流程开始阻塞，首先也是调用 addWaiter 添加节点，不同之处在于节点被设置为 Node.SHARED 模式而非 Node.EXCLUSIVE 模式，注意此时 t2 仍处于活跃状态 123456789101112131415161718192021222324252627282930313233private void doAcquireShared(int arg) &#123; // 将当前线程关联到一个 Node 对象上, 模式为共享模式 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; // 获取前驱节点 final Node p = node.predecessor(); // 如果前驱节点就头节点就去尝试获取锁 if (p == head) &#123; // 再一次尝试获取读锁 int r = tryAcquireShared(arg); // r &gt;= 0 表示获取成功 if (r &gt;= 0) &#123; //【这里会设置自己为头节点，唤醒相连的后序的共享节点】 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; // 是否在获取读锁失败时阻塞 park 当前线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 如果没有成功，在 doAcquireShared 内 for (;;) 循环一次，shouldParkAfterFailedAcquire 内把前驱节点的 waitStatus 改为 -1，再 for (;;) 循环一次尝试 tryAcquireShared，不成功在 parkAndCheckInterrupt() 处 park 这种状态下，假设又有 t3 r.lock，t4 w.lock，这期间 t1 仍然持有锁，就变成了下面的样子 解锁原理 t1 w.unlock， 写锁解锁 1234567891011121314151617181920212223242526public void unlock() &#123; // 释放锁 sync.release(1);&#125;public final boolean release(int arg) &#123; // 尝试释放锁 if (tryRelease(arg)) &#123; Node h = head; // 头节点不为空并且不是等待状态不是 0，唤醒后继的非取消节点 if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125;protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; // 因为可重入的原因, 写锁计数为 0, 才算释放成功 boolean free = exclusiveCount(nextc) == 0; if (free) setExclusiveOwnerThread(null); setState(nextc); return free;&#125; 唤醒流程 sync.unparkSuccessor，这时 t2 在 doAcquireShared 的 parkAndCheckInterrupt() 处恢复运行，继续循环，执行 tryAcquireShared 成功则让读锁计数加一 接下来 t2 调用 setHeadAndPropagate(node, 1)，它原本所在节点被置为头节点；还会检查下一个节点是否是 shared，如果是则调用 doReleaseShared() 将 head 的状态从 -1 改为 0 并唤醒下一个节点，这时 t3 在 doAcquireShared 内 parkAndCheckInterrupt() 处恢复运行，唤醒连续的所有的共享节点 123456789101112131415private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // 设置自己为 head 节点 setHead(node); // propagate 表示有共享资源（例如共享读锁或信号量），为 0 就没有资源 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; // 获取下一个节点 Node s = node.next; // 如果当前是最后一个节点，或者下一个节点是【等待共享读锁的节点】 if (s == null || s.isShared()) // 唤醒后继节点 doReleaseShared(); &#125;&#125; 1234567891011121314151617181920212223242526private void doReleaseShared() &#123; // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark\t// 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // SIGNAL 唤醒后继 if (ws == Node.SIGNAL) &#123; // 因为读锁共享，如果其它线程也在释放读锁，那么需要将 waitStatus 先改为 0 // 防止 unparkSuccessor 被多次执行 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // 唤醒后继节点 unparkSuccessor(h); &#125; // 如果已经是 0 了，改为 -3，用来解决传播性 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; &#125; // 条件不成立说明被唤醒的节点非常积极，直接将自己设置为了新的 head， // 此时唤醒它的节点（前驱）执行 h == head 不成立，所以不会跳出循环，会继续唤醒新的 head 节点的后继节点 if (h == head) break; &#125;&#125; 下一个节点不是 shared 了，因此不会继续唤醒 t4 所在节点 t2 读锁解锁，进入 sync.releaseShared(1) 中，调用 tryReleaseShared(1) 让计数减一，但计数还不为零，t3 同样让计数减一，计数为零，进入doReleaseShared() 将头节点从 -1 改为 0 并唤醒下一个节点 12345678910public void unlock() &#123; sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; 1234567891011protected final boolean tryReleaseShared(int unused) &#123; for (;;) &#123; int c = getState(); int nextc = c - SHARED_UNIT; // 读锁的计数不会影响其它获取读锁线程, 但会影响其它获取写锁线程，计数为 0 才是真正释放 if (compareAndSetState(c, nextc)) // 返回是否已经完全释放了 return nextc == 0; &#125;&#125; t4 在 acquireQueued 中 parkAndCheckInterrupt 处恢复运行，再次 for (;;) 这次自己是头节点的临节点，并且没有其他节点竞争，tryAcquire(1) 成功，修改头结点，流程结束 StampedStampedLock：读写锁，该类自 JDK 8 加入，是为了进一步优化读性能 特点： 在使用读锁、写锁时都必须配合戳使用 StampedLock 不支持条件变量 StampedLock 不支持重入 基本用法 加解读锁： 12long stamp = lock.readLock();lock.unlockRead(stamp); // 类似于 unpark，解指定的锁 加解写锁： 12long stamp = lock.writeLock();lock.unlockWrite(stamp); 乐观读，StampedLock 支持 tryOptimisticRead() 方法，读取完毕后做一次戳校验，如果校验通过，表示这期间没有其他线程的写操作，数据可以安全使用，如果校验没通过，需要重新获取读锁，保证数据一致性 12345long stamp = lock.tryOptimisticRead();// 验戳if(!lock.validate(stamp))&#123;\t// 锁升级&#125; 提供一个数据容器类内部分别使用读锁保护数据的 read() 方法，写锁保护数据的 write() 方法： 读-读可以优化 读-写优化读，补加读锁 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public static void main(String[] args) throws InterruptedException &#123; DataContainerStamped dataContainer = new DataContainerStamped(1); new Thread(() -&gt; &#123; dataContainer.read(1000); &#125;,&quot;t1&quot;).start(); Thread.sleep(500); new Thread(() -&gt; &#123; dataContainer.write(1000); &#125;,&quot;t2&quot;).start();&#125;class DataContainerStamped &#123; private int data; private final StampedLock lock = new StampedLock(); public int read(int readTime) throws InterruptedException &#123; long stamp = lock.tryOptimisticRead(); System.out.println(new Date() + &quot; optimistic read locking&quot; + stamp); Thread.sleep(readTime); // 戳有效，直接返回数据 if (lock.validate(stamp)) &#123; Sout(new Date() + &quot; optimistic read finish...&quot; + stamp); return data; &#125; // 说明其他线程更改了戳，需要锁升级了，从乐观读升级到读锁 System.out.println(new Date() + &quot; updating to read lock&quot; + stamp); try &#123; stamp = lock.readLock(); System.out.println(new Date() + &quot; read lock&quot; + stamp); Thread.sleep(readTime); System.out.println(new Date() + &quot; read finish...&quot; + stamp); return data; &#125; finally &#123; System.out.println(new Date() + &quot; read unlock &quot; + stamp); lock.unlockRead(stamp); &#125; &#125; public void write(int newData) &#123; long stamp = lock.writeLock(); System.out.println(new Date() + &quot; write lock &quot; + stamp); try &#123; Thread.sleep(2000); this.data = newData; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(new Date() + &quot; write unlock &quot; + stamp); lock.unlockWrite(stamp); &#125; &#125;&#125; CountDown基本使用CountDownLatch：计数器，用来进行线程同步协作，等待所有线程完成 构造器： public CountDownLatch(int count)：初始化唤醒需要的 down 几步 常用API： public void await() ：让当前线程等待，必须 down 完初始化的数字才可以被唤醒，否则进入无限等待 public void countDown()：计数器进行减 1（down 1） 应用：同步等待多个 Rest 远程调用结束 12345678910111213141516171819202122232425// LOL 10人进入游戏倒计时public static void main(String[] args) throws InterruptedException &#123; CountDownLatch latch = new CountDownLatch(10); ExecutorService service = Executors.newFixedThreadPool(10); String[] all = new String[10]; Random random = new Random(); for (int j = 0; j &lt; 10; j++) &#123; int finalJ = j;//常量 service.submit(() -&gt; &#123; for (int i = 0; i &lt;= 100; i++) &#123; Thread.sleep(random.nextInt(100));\t//随机休眠 all[finalJ] = i + &quot;%&quot;; System.out.print(&quot;\\r&quot; + Arrays.toString(all));\t// \\r代表覆盖 &#125; latch.countDown(); &#125;); &#125; latch.await(); System.out.println(&quot; 游戏开始&quot;); service.shutdown();&#125;/*[100%, 100%, 100%, 100%, 100%, 100%, 100%, 100%, 100%, 100%]游戏开始 实现原理阻塞等待： 线程调用 await() 等待其他线程完成任务：支持打断 1234567891011121314151617public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125;// AbstractQueuedSynchronizer#acquireSharedInterruptiblypublic final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; // 判断线程是否被打断，抛出打断异常 if (Thread.interrupted()) throw new InterruptedException(); // 尝试获取共享锁，条件成立说明 state &gt; 0，此时线程入队阻塞等待，等待其他线程获取共享资源 // 条件不成立说明 state = 0，此时不需要阻塞线程，直接结束函数调用 if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125;// CountDownLatch.Sync#tryAcquireSharedprotected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1;&#125; 线程进入 AbstractQueuedSynchronizer#doAcquireSharedInterruptibly 函数阻塞挂起，等待 latch 变为 0： 123456789101112131415161718192021222324252627282930private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; // 将调用latch.await()方法的线程 包装成 SHARED 类型的 node 加入到 AQS 的阻塞队列中 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; // 获取当前节点的前驱节点 final Node p = node.predecessor(); // 前驱节点时头节点就可以尝试获取锁 if (p == head) &#123; // 再次尝试获取锁，获取成功返回 1 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 获取锁成功，设置当前节点为 head 节点，并且向后传播 setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; // 阻塞在这里 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; // 阻塞线程被中断后抛出异常，进入取消节点的逻辑 if (failed) cancelAcquire(node); &#125;&#125; 获取共享锁成功，进入唤醒阻塞队列中与头节点相连的 SHARED 模式的节点： 1234567891011121314private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // 将当前节点设置为新的 head 节点，前驱节点和持有线程置为 null setHead(node);\t// propagate = 1，条件一成立 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; // 获取当前节点的后继节点 Node s = node.next; // 当前节点是尾节点时 next 为 null，或者后继节点是 SHARED 共享模式 if (s == null || s.isShared()) // 唤醒所有的等待共享锁的节点 doReleaseShared(); &#125;&#125; 计数减一： 线程进入 countDown() 完成计数器减一（释放锁）的操作 123456789101112public void countDown() &#123; sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123; // 尝试释放共享锁 if (tryReleaseShared(arg)) &#123; // 释放锁成功开始唤醒阻塞节点 doReleaseShared(); return true; &#125; return false;&#125; 更新 state 值，每调用一次，state 值减一，当 state -1 正好为 0 时，返回 true 12345678910111213protected boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int c = getState(); // 条件成立说明前面【已经有线程触发唤醒操作】了，这里返回 false if (c == 0) return false; // 计数器减一 int nextc = c-1; if (compareAndSetState(c, nextc)) // 计数器为 0 时返回 true return nextc == 0; &#125;&#125; state &#x3D; 0 时，当前线程需要执行唤醒阻塞节点的任务 123456789101112131415161718192021222324private void doReleaseShared() &#123; for (;;) &#123; Node h = head; // 判断队列是否是空队列 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // 头节点的状态为 signal，说明后继节点没有被唤醒过 if (ws == Node.SIGNAL) &#123; // cas 设置头节点的状态为 0，设置失败继续自旋 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // 唤醒后继节点 unparkSuccessor(h); &#125; // 如果有其他线程已经设置了头节点的状态，重新设置为 PROPAGATE 传播属性 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; &#125; // 条件不成立说明被唤醒的节点非常积极，直接将自己设置为了新的head， // 此时唤醒它的节点（前驱）执行 h == head 不成立，所以不会跳出循环，会继续唤醒新的 head 节点的后继节点 if (h == head) break; &#125;&#125; CyclicBarrier基本使用CyclicBarrier：循环屏障，用来进行线程协作，等待线程满足某个计数，才能触发自己执行 常用方法： public CyclicBarrier(int parties, Runnable barrierAction)：用于在线程到达屏障 parties 时，执行 barrierAction parties：代表多少个线程到达屏障开始触发线程任务 barrierAction：线程任务 public int await()：线程调用 await 方法通知 CyclicBarrier 本线程已经到达屏障 与 CountDownLatch 的区别：CyclicBarrier 是可以重用的 应用：可以实现多线程中，某个任务在等待其他线程执行完毕以后触发 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(2); CyclicBarrier barrier = new CyclicBarrier(2, () -&gt; &#123; System.out.println(&quot;task1 task2 finish...&quot;); &#125;); for (int i = 0; i &lt; 3; i++) &#123; // 循环重用 service.submit(() -&gt; &#123; System.out.println(&quot;task1 begin...&quot;); try &#123; Thread.sleep(1000); barrier.await(); // 2 - 1 = 1 &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;); service.submit(() -&gt; &#123; System.out.println(&quot;task2 begin...&quot;); try &#123; Thread.sleep(2000); barrier.await(); // 1 - 1 = 0 &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; service.shutdown();&#125; 实现原理成员属性 全局锁：利用可重入锁实现的工具类 1234// barrier 实现是依赖于Condition条件队列，condition 条件队列必须依赖lock才能使用private final ReentrantLock lock = new ReentrantLock();// 线程挂起实现使用的 condition 队列，当前代所有线程到位，这个条件队列内的线程才会被唤醒private final Condition trip = lock.newCondition(); 线程数量： 12private final int parties;\t// 代表多少个线程到达屏障开始触发线程任务private int count; // 表示当前“代”还有多少个线程未到位，初始值为 parties 当前代中最后一个线程到位后要执行的事件： 1private final Runnable barrierCommand; 代： 1234567// 表示 barrier 对象当前 代private Generation generation = new Generation();private static class Generation &#123; // 表示当前“代”是否被打破，如果被打破再来到这一代的线程 就会直接抛出 BrokenException 异常 // 且在这一代挂起的线程都会被唤醒，然后抛出 BrokerException 异常。 boolean broken = false;&#125; 构造方法： 123456789public CyclicBarrie(int parties, Runnable barrierAction) &#123; // 因为小于等于 0 的 barrier 没有任何意义 if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; // 可以为 null this.barrierCommand = barrierAction;&#125; 成员方法 await()：阻塞等待所有线程到位 1234567public int await() throws InterruptedException, BrokenBarrierException &#123; try &#123; return dowait(false, 0L); &#125; catch (TimeoutException toe) &#123; throw new Error(toe); // cannot happen &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// timed：表示当前调用await方法的线程是否指定了超时时长，如果 true 表示线程是响应超时的// nanos：线程等待超时时长，单位是纳秒private int dowait(boolean timed, long nanos) &#123; final ReentrantLock lock = this.lock; // 加锁 lock.lock(); try &#123; // 获取当前代 final Generation g = generation; // 【如果当前代是已经被打破状态，则当前调用await方法的线程，直接抛出Broken异常】 if (g.broken) throw new BrokenBarrierException(); // 如果当前线程被中断了，则打破当前代，然后当前线程抛出中断异常 if (Thread.interrupted()) &#123; // 设置当前代的状态为 broken 状态，唤醒在 trip 条件队列内的线程 breakBarrier(); throw new InterruptedException(); &#125; // 逻辑到这说明，当前线程中断状态是 false， 当前代的 broken 为 false（未打破状态） // 假设 parties 给的是 5，那么index对应的值为 4,3,2,1,0 int index = --count; // 条件成立说明当前线程是最后一个到达 barrier 的线程，【需要开启新代，唤醒阻塞线程】 if (index == 0) &#123; // 栅栏任务启动标记 boolean ranAction = false; try &#123; final Runnable command = barrierCommand; if (command != null) // 启动触发的任务 command.run(); // run()未抛出异常的话，启动标记设置为 true ranAction = true; // 开启新的一代，这里会【唤醒所有的阻塞队列】 nextGeneration(); // 返回 0 因为当前线程是此代最后一个到达的线程，index == 0 return 0; &#125; finally &#123; // 如果 command.run() 执行抛出异常的话，会进入到这里 if (!ranAction) breakBarrier(); &#125; &#125; // 自旋，一直到条件满足、当前代被打破、线程被中断，等待超时 for (;;) &#123; try &#123; // 根据是否需要超时等待选择阻塞方法 if (!timed) // 当前线程释放掉 lock，【进入到 trip 条件队列的尾部挂起自己】，等待被唤醒 trip.await(); else if (nanos &gt; 0L) nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; // 被中断后来到这里的逻辑 // 当前代没有变化并且没有被打破 if (g == generation &amp;&amp; !g.broken) &#123; // 打破屏障 breakBarrier(); // node 节点在【条件队列】内收到中断信号时 会抛出中断异常 throw ie; &#125; else &#123; // 等待过程中代变化了，完成一次自我打断 Thread.currentThread().interrupt(); &#125; &#125; // 唤醒后的线程，【判断当前代已经被打破，线程唤醒后依次抛出 BrokenBarrier 异常】 if (g.broken) throw new BrokenBarrierException(); // 当前线程挂起期间，最后一个线程到位了，然后触发了开启新的一代的逻辑 if (g != generation) return index; // 当前线程 trip 中等待超时，然后主动转移到阻塞队列 if (timed &amp;&amp; nanos &lt;= 0L) &#123; breakBarrier(); // 抛出超时异常 throw new TimeoutException(); &#125; &#125; &#125; finally &#123; // 解锁 lock.unlock(); &#125;&#125; breakBarrier()：打破 Barrier 屏障 12345678private void breakBarrier() &#123; // 将代中的 broken 设置为 true，表示这一代是被打破了，再来到这一代的线程，直接抛出异常 generation.broken = true; // 重置 count 为 parties count = parties; // 将在trip条件队列内挂起的线程全部唤醒，唤醒后的线程会检查当前是否是打破的，然后抛出异常 trip.signalAll();&#125; nextGeneration()：开启新的下一代 123456789private void nextGeneration() &#123; // 将在 trip 条件队列内挂起的线程全部唤醒 trip.signalAll(); // 重置 count 为 parties count = parties; // 开启新的一代，使用一个新的generation对象，表示新的一代，新的一代和上一代【没有任何关系】 generation = new Generation();&#125; 参考视频：https://space.bilibili.com/457326371/ Semaphore基本使用synchronized 可以起到锁的作用，但某个时间段内，只能有一个线程允许执行 Semaphore（信号量）用来限制能同时访问共享资源的线程上限，非重入锁 构造方法： public Semaphore(int permits)：permits 表示许可线程的数量（state） public Semaphore(int permits, boolean fair)：fair 表示公平性，如果设为 true，下次执行的线程会是等待最久的线程 常用API： public void acquire()：表示获取许可 public void release()：表示释放许可，acquire() 和 release() 方法之间的代码为同步代码 12345678910111213141516171819202122public static void main(String[] args) &#123; // 1.创建Semaphore对象 Semaphore semaphore = new Semaphore(3); // 2. 10个线程同时运行 for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; try &#123; // 3. 获取许可 semaphore.acquire(); sout(Thread.currentThread().getName() + &quot; running...&quot;); Thread.sleep(1000); sout(Thread.currentThread().getName() + &quot; end...&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4. 释放许可 semaphore.release(); &#125; &#125;).start(); &#125;&#125; 实现原理加锁流程： Semaphore 的 permits（state）为 3，这时 5 个线程来获取资源 123Sync(int permits) &#123; setState(permits);&#125; 假设其中 Thread-1，Thread-2，Thread-4 CAS 竞争成功，permits 变为 0，而 Thread-0 和 Thread-3 竞争失败，进入 AQS 队列park 阻塞 12345678910111213141516171819202122232425// acquire() -&gt; sync.acquireSharedInterruptibly(1)，可中断public final void acquireSharedInterruptibly(int arg) &#123; if (Thread.interrupted()) throw new InterruptedException(); // 尝试获取通行证，获取成功返回 &gt;= 0的值 if (tryAcquireShared(arg) &lt; 0) // 获取许可证失败，进入阻塞 doAcquireSharedInterruptibly(arg);&#125;// tryAcquireShared() -&gt; nonfairTryAcquireShared()// 非公平，公平锁会在循环内 hasQueuedPredecessors()方法判断阻塞队列是否有临头节点(第二个节点)final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; // 获取 state ，state 这里【表示通行证】 int available = getState(); // 计算当前线程获取通行证完成之后，通行证还剩余数量 int remaining = available - acquires; // 如果许可已经用完, 返回负数, 表示获取失败, if (remaining &lt; 0 || // 许可证足够分配的，如果 cas 重试成功, 返回正数, 表示获取成功 compareAndSetState(available, remaining)) return remaining; &#125;&#125; 12345678910111213141516171819202122232425262728293031private void doAcquireSharedInterruptibly(int arg) &#123; // 将调用 Semaphore.aquire 方法的线程，包装成 node 加入到 AQS 的阻塞队列中 final Node node = addWaiter(Node.SHARED); // 获取标记 boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); // 前驱节点是头节点可以再次获取许可 if (p == head) &#123; // 再次尝试获取许可，【返回剩余的许可证数量】 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 成功后本线程出队（AQS）, 所在 Node设置为 head // r 表示【可用资源数】, 为 0 则不会继续传播 setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; // 不成功, 设置上一个节点 waitStatus = Node.SIGNAL, 下轮进入 park 阻塞 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; // 被打断后进入该逻辑 if (failed) cancelAcquire(node); &#125;&#125; 1234567891011121314private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // 设置自己为 head 节点 setHead(node); // propagate 表示有【共享资源】（例如共享读锁或信号量） // head waitStatus == Node.SIGNAL 或 Node.PROPAGATE，doReleaseShared 函数中设置的 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; // 如果是最后一个节点或者是等待共享读锁的节点，做一次唤醒 if (s == null || s.isShared()) doReleaseShared(); &#125;&#125; 这时 Thread-4 释放了 permits，状态如下 123456789101112131415161718192021222324252627// release() -&gt; releaseShared()public final boolean releaseShared(int arg) &#123; // 尝试释放锁 if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125;protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; // 获取当前锁资源的可用许可证数量 int current = getState(); int next = current + releases; // 索引越界判断 if (next &lt; current) throw new Error(&quot;Maximum permit count exceeded&quot;); // 释放锁 if (compareAndSetState(current, next)) return true; &#125;&#125;private void doReleaseShared() &#123; // PROPAGATE 详解 // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark // 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE&#125; 接下来 Thread-0 竞争成功，permits 再次设置为 0，设置自己为 head 节点，并且 unpark 接下来的共享状态的 Thread-3 节点，但由于 permits 是 0，因此 Thread-3 在尝试不成功后再次进入 park 状态 PROPAGATE假设存在某次循环中队列里排队的结点情况为 head(-1) → t1(-1) → t2(0)，存在将要释放信号量的 T3 和 T4，释放顺序为先 T3 后 T4 1234567891011// 老版本代码private void setHeadAndPropagate(Node node, int propagate) &#123; setHead(node); // 有空闲资源 if (propagate &gt; 0 &amp;&amp; node.waitStatus != 0) &#123; Node s = node.next; // 下一个 if (s == null || s.isShared()) unparkSuccessor(node); &#125;&#125; 正常流程： T3 调用 releaseShared(1)，直接调用了 unparkSuccessor(head)，head.waitStatus 从 -1 变为 0 T1 由于 T3 释放信号量被唤醒，然后 T4 释放，唤醒 T2 BUG 流程： T3 调用 releaseShared(1)，直接调用了 unparkSuccessor(head)，head.waitStatus 从 -1 变为 0 T1 由于 T3 释放信号量被唤醒，调用 tryAcquireShared，返回值为 0（获取锁成功，但没有剩余资源量） T1 还没调用 setHeadAndPropagate 方法，T4 调用 releaseShared(1)，此时 head.waitStatus 为 0（此时读到的 head 和 1 中为同一个 head），不满足条件，因此不调用 unparkSuccessor(head) T1 获取信号量成功，调用 setHeadAndPropagate(t1.node, 0) 时，因为不满足 propagate &gt; 0（剩余资源量 &#x3D;&#x3D; 0），从而不会唤醒后继结点， T2 线程得不到唤醒 更新后流程： T3 调用 releaseShared(1)，直接调用了 unparkSuccessor(head)，head.waitStatus 从 -1 变为 0 T1 由于 T3 释放信号量被唤醒，调用 tryAcquireShared，返回值为 0（获取锁成功，但没有剩余资源量） T1 还没调用 setHeadAndPropagate 方法，T4 调用 releaseShared()，此时 head.waitStatus 为 0（此时读到的 head 和 1 中为同一个 head），调用 doReleaseShared() 将等待状态置为 PROPAGATE（-3） T1 获取信号量成功，调用 setHeadAndPropagate 时，读到 h.waitStatus &lt; 0，从而调用 doReleaseShared() 唤醒 T2 1234567891011121314private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // 设置自己为 head 节点 setHead(node); // propagate 表示有共享资源（例如共享读锁或信号量） // head waitStatus == Node.SIGNAL 或 Node.PROPAGATE if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; // 如果是最后一个节点或者是等待共享读锁的节点，做一次唤醒 if (s == null || s.isShared()) doReleaseShared(); &#125;&#125; 1234567891011121314151617181920212223// 唤醒private void doReleaseShared() &#123; // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark // 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; // 防止 unparkSuccessor 被多次执行 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // 唤醒后继节点 unparkSuccessor(h); &#125; // 如果已经是 0 了，改为 -3，用来解决传播性 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; &#125; if (h == head) break; &#125;&#125; ExchangerExchanger：交换器，是一个用于线程间协作的工具类，用于进行线程间的数据交换 工作流程：两个线程通过 exchange 方法交换数据，如果第一个线程先执行 exchange() 方法，它会一直等待第二个线程也执行 exchange 方法，当两个线程都到达同步点时，这两个线程就可以交换数据 常用方法： public Exchanger()：创建一个新的交换器 public V exchange(V x)：等待另一个线程到达此交换点 public V exchange(V x, long timeout, TimeUnit unit)：等待一定的时间 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class ExchangerDemo &#123; public static void main(String[] args) &#123; // 创建交换对象（信使） Exchanger&lt;String&gt; exchanger = new Exchanger&lt;&gt;(); new ThreadA(exchanger).start(); new ThreadB(exchanger).start(); &#125; &#125;class ThreadA extends Thread&#123; private Exchanger&lt;String&gt; exchanger(); public ThreadA(Exchanger&lt;String&gt; exchanger)&#123; this.exchanger = exchanger; &#125; @Override public void run() &#123; try&#123; sout(&quot;线程A，做好了礼物A，等待线程B送来的礼物B&quot;); //如果等待了5s还没有交换就死亡（抛出异常）！ String s = exchanger.exchange(&quot;礼物A&quot;,5,TimeUnit.SECONDS); sout(&quot;线程A收到线程B的礼物：&quot; + s); &#125; catch (Exception e) &#123; System.out.println(&quot;线程A等待了5s，没有收到礼物,最终就执行结束了!&quot;); &#125; &#125;&#125;class ThreadB extends Thread&#123; private Exchanger&lt;String&gt; exchanger; public ThreadB(Exchanger&lt;String&gt; exchanger) &#123; this.exchanger = exchanger; &#125; @Override public void run() &#123; try &#123; sout(&quot;线程B,做好了礼物B,等待线程A送来的礼物A.....&quot;); // 开始交换礼物。参数是送给其他线程的礼物! sout(&quot;线程B收到线程A的礼物：&quot; + exchanger.exchange(&quot;礼物B&quot;)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 并发包ConHashMap并发集合集合对比三种集合： HashMap 是线程不安全的，性能好 Hashtable 线程安全基于 synchronized，综合性能差，已经被淘汰 ConcurrentHashMap 保证了线程安全，综合性能较好，不止线程安全，而且效率高，性能好 集合对比： Hashtable 继承 Dictionary 类，HashMap、ConcurrentHashMap 继承 AbstractMap，均实现 Map 接口 Hashtable 底层是数组 + 链表，JDK8 以后 HashMap 和 ConcurrentHashMap 底层是数组 + 链表 + 红黑树 HashMap 线程非安全，Hashtable 线程安全，Hashtable 的方法都加了 synchronized 关来确保线程同步 ConcurrentHashMap、Hashtable 不允许 null 值，HashMap 允许 null 值 ConcurrentHashMap、HashMap 的初始容量为 16，Hashtable 初始容量为11，填充因子默认都是 0.75，两种 Map 扩容是当前容量翻倍：capacity * 2，Hashtable 扩容时是容量翻倍 + 1：capacity*2 + 1 工作步骤： 初始化，使用 cas 来保证并发安全，懒惰初始化 table 树化，当 table.length &lt; 64 时，先尝试扩容，超过 64 时，并且 bin.length &gt; 8 时，会将链表树化，树化过程会用 synchronized 锁住链表头 说明：锁住某个槽位的对象头，是一种很好的细粒度的加锁方式，类似 MySQL 中的行锁 put，如果该 bin 尚未创建，只需要使用 cas 创建 bin；如果已经有了，锁住链表头进行后续 put 操作，元素添加至 bin 的尾部 get，无锁操作仅需要保证可见性，扩容过程中 get 操作拿到的是 ForwardingNode 会让 get 操作在新 table 进行搜索 扩容，扩容时以 bin 为单位进行，需要对 bin 进行 synchronized，但这时其它竞争线程也不是无事可做，它们会帮助把其它 bin 进行扩容 size，元素个数保存在 baseCount 中，并发时的个数变动保存在 CounterCell[] 当中，最后统计数量时累加 123456789101112131415161718192021//需求：多个线程同时往HashMap容器中存入数据会出现安全问题public class ConcurrentHashMapDemo&#123; public static Map&lt;String,String&gt; map = new ConcurrentHashMap(); public static void main(String[] args)&#123; new AddMapDataThread().start(); new AddMapDataThread().start(); Thread.sleep(1000 * 5);//休息5秒，确保两个线程执行完毕 System.out.println(&quot;Map大小：&quot; + map.size());//20万 &#125;&#125;public class AddMapDataThread extends Thread&#123; @Override public void run() &#123; for(int i = 0 ; i &lt; 1000000 ; i++ )&#123; ConcurrentHashMapDemo.map.put(&quot;键：&quot;+i , &quot;值&quot;+i); &#125; &#125;&#125; 并发死链JDK1.7 的 HashMap 采用的头插法（拉链法）进行节点的添加，HashMap 的扩容长度为原来的 2 倍 resize() 中节点（Entry）转移的源代码： 12345678910111213141516171819202122void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length;//得到新数组的长度 // 遍历整个数组对应下标下的链表，e代表一个节点 for (Entry&lt;K,V&gt; e : table) &#123; // 当e == null时，则该链表遍历完了，继续遍历下一数组下标的链表 while(null != e) &#123; // 先把e节点的下一节点存起来 Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; //得到新的hash值 e.hash = null == e.key ? 0 : hash(e.key); &#125; // 在新数组下得到新的数组下标 int i = indexFor(e.hash, newCapacity); // 将e的next指针指向新数组下标的位置 e.next = newTable[i]; // 将该数组下标的节点变为e节点 newTable[i] = e; // 遍历链表的下一节点 e = next; &#125; &#125;&#125; JDK 8 虽然将扩容算法做了调整，改用了尾插法，但仍不意味着能够在多线程环境下能够安全扩容，还会出现其它问题（如扩容丢数据） B站视频解析：https://www.bilibili.com/video/BV1n541177Ea 成员属性变量 存储数组： 1transient volatile Node&lt;K,V&gt;[] table; 散列表的长度： 12private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;\t// 最大长度private static final int DEFAULT_CAPACITY = 16; // 默认长度 并发级别，JDK7 遗留下来，1.8 中不代表并发级别： 1private static final int DEFAULT_CONCURRENCY_LEVEL = 16; 负载因子，JDK1.8 的 ConcurrentHashMap 中是固定值： 1private static final float LOAD_FACTOR = 0.75f; 阈值： 123static final int TREEIFY_THRESHOLD = 8; // 链表树化的阈值static final int UNTREEIFY_THRESHOLD = 6;\t// 红黑树转化为链表的阈值static final int MIN_TREEIFY_CAPACITY = 64;\t// 当数组长度达到64且某个桶位中的链表长度超过8，才会真正树化 扩容相关： 1234private static final int MIN_TRANSFER_STRIDE = 16;\t// 线程迁移数据【最小步长】，控制线程迁移任务的最小区间private static int RESIZE_STAMP_BITS = 16; // 用来计算扩容时生成的【标识戳】private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;// 65535-1并发扩容最多线程数private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS; // 扩容时使用 节点哈希值： 1234static final int MOVED = -1; // 表示当前节点是 FWD 节点static final int TREEBIN = -2; // 表示当前节点已经树化，且当前节点为 TreeBin 对象static final int RESERVED = -3; // 表示节点时临时节点static final int HASH_BITS = 0x7fffffff; // 正常节点的哈希值的可用的位数 扩容过程：volatile 修饰保证多线程的可见性 1234// 扩容过程中，会将扩容中的新 table 赋值给 nextTable 保持引用，扩容结束之后，这里会被设置为 nullprivate transient volatile Node&lt;K,V&gt;[] nextTable;// 记录扩容进度，所有线程都要从 0 - transferIndex 中分配区间任务，简单说就是老表转移到哪了，索引从高到低转移private transient volatile int transferIndex; 累加统计： 123456// LongAdder 中的 baseCount 未发生竞争时或者当前LongAdder处于加锁状态时，增量累到到 baseCount 中private transient volatile long baseCount;// LongAdder 中的 cellsBuzy，0 表示当前 LongAdder 对象无锁状态，1 表示当前 LongAdder 对象加锁状态private transient volatile int cellsBusy;// LongAdder 中的 cells 数组，private transient volatile CounterCell[] counterCells; 控制变量： sizeCtl &lt; 0： -1 表示当前 table 正在初始化（有线程在创建 table 数组），当前线程需要自旋等待 其他负数表示当前 map 的 table 数组正在进行扩容，高 16 位表示扩容的标识戳；低 16 位表示 (1 + nThread) 当前参与并发扩容的线程数量 + 1 sizeCtl &#x3D; 0，表示创建 table 数组时使用 DEFAULT_CAPACITY 为数组大小 sizeCtl &gt; 0： 如果 table 未初始化，表示初始化大小 如果 table 已经初始化，表示下次扩容时的触发条件（阈值，元素个数，不是数组的长度） 1private transient volatile int sizeCtl; // volatile 保持可见性 内部类 Node 节点： 12345678static class Node&lt;K,V&gt; implements Entry&lt;K,V&gt; &#123; // 节点哈希值 final int hash; final K key; volatile V val; // 单向链表 volatile Node&lt;K,V&gt; next;&#125; TreeBin 节点： 12345678910111213141516static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; // 红黑树根节点 TreeNode&lt;K,V&gt; root; // 链表的头节点 volatile TreeNode&lt;K,V&gt; first; // 等待者线程 volatile Thread waiter; volatile int lockState; // 写锁状态 写锁是独占状态，以散列表来看，真正进入到 TreeBin 中的写线程同一时刻只有一个线程 static final int WRITER = 1; // 等待者状态（写线程在等待），当 TreeBin 中有读线程目前正在读取数据时，写线程无法修改数据 static final int WAITER = 2; // 读锁状态是共享，同一时刻可以有多个线程 同时进入到 TreeBi 对象中获取数据，每一个线程都给 lockState + 4 static final int READER = 4;&#125; TreeNode 节点： 1234567static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; //双向链表 boolean red;&#125; ForwardingNode 节点：转移节点 123456789static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; // 持有扩容后新的哈希表的引用 final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; // ForwardingNode 节点的 hash 值设为 -1 super(MOVED, null, null, null); this.nextTable = tab; &#125;&#125; 代码块 变量： 1234567891011121314// 表示sizeCtl属性在 ConcurrentHashMap 中内存偏移地址private static final long SIZECTL;// 表示transferIndex属性在 ConcurrentHashMap 中内存偏移地址private static final long TRANSFERINDEX;// 表示baseCount属性在 ConcurrentHashMap 中内存偏移地址private static final long BASECOUNT;// 表示cellsBusy属性在 ConcurrentHashMap 中内存偏移地址private static final long CELLSBUSY;// 表示cellValue属性在 CounterCell 中内存偏移地址private static final long CELLVALUE;// 表示数组第一个元素的偏移地址private static final long ABASE;// 用位移运算替代乘法private static final int ASHIFT; 赋值方法： 12345678910111213// 表示数组单元所占用空间大小，scale 表示 Node[] 数组中每一个单元所占用空间大小，int 是 4 字节int scale = U.arrayIndexScale(ak);// 判断一个数是不是 2 的 n 次幂，比如 8：1000 &amp; 0111 = 0000if ((scale &amp; (scale - 1)) != 0) throw new Error(&quot;data type scale not a power of two&quot;);// numberOfLeadingZeros(n)：返回当前数值转换为二进制后，从高位到低位开始统计，看有多少个0连续在一起// 8 → 1000 numberOfLeadingZeros(8) = 28// 4 → 100 numberOfLeadingZeros(4) = 29 int 值就是占4个字节ASHIFT = 31 - Integer.numberOfLeadingZeros(scale);// ASHIFT = 31 - 29 = 2 ，int 的大小就是 2 的 2 次方，获取次方数// ABASE + （5 &lt;&lt; ASHIFT） 用位移运算替代了乘法，获取 arr[5] 的值 构造方法 无参构造， 散列表结构延迟初始化，默认的数组大小是 16： 12public ConcurrentHashMap() &#123;&#125; 有参构造： 1234567891011public ConcurrentHashMap(int initialCapacity) &#123; // 指定容量初始化 if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : // 假如传入的参数是 16，16 + 8 + 1 ，最后得到 32 // 传入 12， 12 + 6 + 1 = 19，最后得到 32，尽可能的大，与 HashMap不一样 tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); // sizeCtl &gt; 0，当目前 table 未初始化时，sizeCtl 表示初始化容量 this.sizeCtl = cap;&#125; 123456789private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; HashMap 部分详解了该函数，核心思想就是把最高位是 1 的位以及右边的位全部置 1，结果加 1 后就是 2 的 n 次幂 多个参数构造方法： 1234567891011121314public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); // 初始容量小于并发级别 if (initialCapacity &lt; concurrencyLevel) // 把并发级别赋值给初始容量 initialCapacity = concurrencyLevel; // loadFactor 默认是 0.75 long size = (long)(1.0 + (long)initialCapacity / loadFactor); int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); // sizeCtl &gt; 0，当目前 table 未初始化时，sizeCtl 表示初始化容量 this.sizeCtl = cap;&#125; 集合构造方法： 12345678910public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY;\t// 默认16 putAll(m);&#125;public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; // 尝试触发扩容 tryPresize(m.size()); for (Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) putVal(e.getKey(), e.getValue(), false);&#125; 12345678910111213141516171819202122232425262728293031private final void tryPresize(int size) &#123; // 扩容为大于 2 倍的最小的 2 的 n 次幂 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; // 数组还未初始化，【一般是调用集合构造方法才会成立，put 后调用该方法都是不成立的】 if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2);// 扩容阈值：n - 1/4 n &#125; &#125; finally &#123; sizeCtl = sc;\t// 扩容阈值赋值给sizeCtl &#125; &#125; &#125; // 未达到扩容阈值或者数组长度已经大于最大长度 else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; // 与 addCount 逻辑相同 else if (tab == table) &#123; &#125; &#125;&#125; 成员方法数据访存 tabAt()：获取数组某个槽位的头节点，类似于数组中的直接寻址 arr[i] 12345// i 是数组索引static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; // (i &lt;&lt; ASHIFT) + ABASE == ABASE + i * 4 （一个 int 占 4 个字节），这就相当于寻址，替代了乘法 return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; casTabAt()：指定数组索引位置修改原值为指定的值 123static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; setTabAt()：指定数组索引位置设置值 123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 添加方法1234public V put(K key, V value) &#123; // 第三个参数 onlyIfAbsent 为 false 表示哈希表中存在相同的 key 时【用当前数据覆盖旧数据】 return putVal(key, value, false);&#125; putVal() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100final V putVal(K key, V value, boolean onlyIfAbsent) &#123; // 【ConcurrentHashMap 不能存放 null 值】 if (key == null || value == null) throw new NullPointerException(); // 扰动运算，高低位都参与寻址运算 int hash = spread(key.hashCode()); // 表示当前 k-v 封装成 node 后插入到指定桶位后，在桶位中的所属链表的下标位置 int binCount = 0; // tab 引用当前 map 的数组 table，开始自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; // f 表示桶位的头节点，n 表示哈希表数组的长度 // i 表示 key 通过寻址计算后得到的桶位下标，fh 表示桶位头结点的 hash 值 Node&lt;K,V&gt; f; int n, i, fh; // 【CASE1】：表示当前 map 中的 table 尚未初始化 if (tab == null || (n = tab.length) == 0) //【延迟初始化】 tab = initTable(); // 【CASE2】：i 表示 key 使用【寻址算法】得到 key 对应数组的下标位置，tabAt 获取指定桶位的头结点f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // 对应的数组为 null 说明没有哈希冲突，直接新建节点添加到表中 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; &#125; // 【CASE3】：逻辑说明数组已经被初始化，并且当前 key 对应的位置不为 null // 条件成立表示当前桶位的头结点为 FWD 结点，表示目前 map 正处于扩容过程中 else if ((fh = f.hash) == MOVED) // 当前线程【需要去帮助哈希表完成扩容】 tab = helpTransfer(tab, f); // 【CASE4】：哈希表没有在扩容，当前桶位可能是链表也可能是红黑树 else &#123; // 当插入 key 存在时，会将旧值赋值给 oldVal 返回 V oldVal = null; // 【锁住当前 key 寻址的桶位的头节点】 synchronized (f) &#123; // 这里重新获取一下桶的头节点有没有被修改，因为可能被其他线程修改过，这里是线程安全的获取 if (tabAt(tab, i) == f) &#123; // 【头节点的哈希值大于 0 说明当前桶位是普通的链表节点】 if (fh &gt;= 0) &#123; // 当前的插入操作没出现重复的 key，追加到链表的末尾，binCount表示链表长度 -1 // 插入的key与链表中的某个元素的 key 一致，变成替换操作，binCount 表示第几个节点冲突 binCount = 1; // 迭代循环当前桶位的链表，e 是每次循环处理节点，e 初始是头节点 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; // 当前循环节点 key K ek; // key 的哈希值与当前节点的哈希一致，并且 key 的值也相同 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; // 把当前节点的 value 赋值给 oldVal oldVal = e.val; // 允许覆盖 if (!onlyIfAbsent) // 新数据覆盖旧数据 e.val = value; // 跳出循环 break; &#125; Node&lt;K,V&gt; pred = e; // 如果下一个节点为空，把数据封装成节点插入链表尾部，【binCount 代表长度 - 1】 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 当前桶位头节点是红黑树 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // 条件成立说明当前是链表或者红黑树 if (binCount != 0) &#123; // 如果 binCount &gt;= 8 表示处理的桶位一定是链表，说明长度是 9 if (binCount &gt;= TREEIFY_THRESHOLD) // 树化 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 统计当前 table 一共有多少数据，判断是否达到扩容阈值标准，触发扩容 // binCount = 0 表示当前桶位为 null，node 可以直接放入，2 表示当前桶位已经是红黑树 addCount(1L, binCount); return null;&#125; spread()：扰动函数 将 hashCode 无符号右移 16 位，高 16bit 和低 16bit 做异或，最后与 HASH_BITS 相与变成正数，与树化节点和转移节点区分，把高低位都利用起来减少哈希冲突，保证散列的均匀性 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS; // 0111 1111 1111 1111 1111 1111 1111 1111&#125; initTable()：初始化数组，延迟初始化 12345678910111213141516171819202122232425262728293031private final Node&lt;K,V&gt;[] initTable() &#123; // tab 引用 map.table，sc 引用 sizeCtl Node&lt;K,V&gt;[] tab; int sc; // table 尚未初始化，开始自旋 while ((tab = table) == null || tab.length == 0) &#123; // sc &lt; 0 说明 table 正在初始化或者正在扩容，当前线程可以释放 CPU 资源 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // sizeCtl 设置为 -1，相当于加锁，【设置的是 SIZECTL 位置的数据】， // 因为是 sizeCtl 是基本类型，不是引用类型，所以 sc 保存的是数据的副本 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; // 线程安全的逻辑，再进行一次判断 if ((tab = table) == null || tab.length == 0) &#123; // sc &gt; 0 创建 table 时使用 sc 为指定大小，否则使用 16 默认值 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 创建哈希表数组 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; // 扩容阈值，n &gt;&gt;&gt; 2 =&gt; 等于 1/4 n ，n - (1/4)n = 3/4 n =&gt; 0.75 * n sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 解锁，把下一次扩容的阈值赋值给 sizeCtl sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; treeifyBin()：树化方法 1234567891011121314151617181920212223242526272829private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; // 条件成立：【说明当前 table 数组长度未达到 64，此时不进行树化操作，进行扩容操作】 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) // 当前容量的 2 倍 tryPresize(n &lt;&lt; 1); // 条件成立：说明当前桶位有数据，且是普通 node 数据。 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; // 【树化加锁】 synchronized (b) &#123; // 条件成立：表示加锁没问题。 if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val,null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; addCount()：添加计数，代表哈希表中的数据总量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566private final void addCount(long x, int check) &#123; // 【上面这部分的逻辑就是 LongAdder 的累加逻辑】 CounterCell[] as; long b, s; // 判断累加数组 cells 是否初始化，没有就去累加 base 域，累加失败进入条件内逻辑 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; // true 未竞争，false 发生竞争 boolean uncontended = true; // 判断 cells 是否被其他线程初始化 if (as == null || (m = as.length - 1) &lt; 0 || // 前面的条件为 fasle 说明 cells 被其他线程初始化，通过 hash 寻址对应的槽位 (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || // 尝试去对应的槽位累加，累加失败进入 fullAddCount 进行重试或者扩容 !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; // 与 Striped64#longAccumulate 方法相同 fullAddCount(x, uncontended); return; &#125; // 表示当前桶位是 null，或者一个链表节点 if (check &lt;= 1) return; // 【获取当前散列表元素个数】，这是一个期望值 s = sumCount(); &#125; // 表示一定 【是一个 put 操作调用的 addCount】 if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; // 条件一：true 说明当前 sizeCtl 可能为一个负数表示正在扩容中，或者 sizeCtl 是一个正数，表示扩容阈值 // false 表示哈希表的数据的数量没达到扩容条件 // 然后判断当前 table 数组是否初始化了，当前 table 长度是否小于最大值限制，就可以进行扩容 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; // 16 -&gt; 32 扩容 标识为：1000 0000 0001 1011，【负数，扩容批次唯一标识戳】 int rs = resizeStamp(n); // 表示当前 table，【正在扩容】，sc 高 16 位是扩容标识戳，低 16 位是线程数 + 1 if (sc &lt; 0) &#123; // 条件一：判断扩容标识戳是否一样，fasle 代表一样 // 勘误两个条件： // 条件二是：sc == (rs &lt;&lt; 16 ) + 1，true 代表扩容完成，因为低16位是1代表没有线程扩容了 // 条件三是：sc == (rs &lt;&lt; 16) + MAX_RESIZERS，判断是否已经超过最大允许的并发扩容线程数 // 条件四：判断新表的引用是否是 null，代表扩容完成 // 条件五：【扩容是从高位到低位转移】，transferIndex &lt; 0 说明没有区间需要扩容了 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 设置当前线程参与到扩容任务中，将 sc 低 16 位值加 1，表示多一个线程参与扩容 // 设置失败其他线程或者 transfer 内部修改了 sizeCtl 值 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //【协助扩容线程】，持有nextTable参数 transfer(tab, nt); &#125; // 逻辑到这说明当前线程是触发扩容的第一个线程，线程数量 + 2 // 1000 0000 0001 1011 0000 0000 0000 0000 +2 =&gt; 1000 0000 0001 1011 0000 0000 0000 0010 else if (U.compareAndSwapInt(this, SIZECTL, sc,(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) //【触发扩容条件的线程】，不持有 nextTable，初始线程会新建 nextTable transfer(tab, null); s = sumCount(); &#125; &#125;&#125; resizeStamp()：扩容标识符，每次扩容都会产生一个，不是每个线程都产生，16 扩容到 32 产生一个，32 扩容到 64 产生一个 123456789101112131415/** * 扩容的标识符 * 16 -&gt; 32 从16扩容到32 * numberOfLeadingZeros(16) =&gt; 1 0000 =&gt; 32 - 5 = 27 =&gt; 0000 0000 0001 1011 * (1 &lt;&lt; (RESIZE_STAMP_BITS - 1)) =&gt; 1000 0000 0000 0000 =&gt; 32768 * --------------------------------------------------------------- * 0000 0000 0001 1011 * 1000 0000 0000 0000 * 1000 0000 0001 1011 * 永远是负数 */static final int resizeStamp(int n) &#123; // 或运算 return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1)); // (16 -1 = 15)&#125; 扩容方法扩容机制： 当链表中元素个数超过 8 个，数组的大小还未超过 64 时，此时进行数组的扩容，如果超过则将链表转化成红黑树 put 数据后调用 addCount() 方法，判断当前哈希表的容量超过阈值 sizeCtl，超过进行扩容 增删改线程发现其他线程正在扩容，帮其扩容 常见方法： transfer()：数据转移到新表中，完成扩容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; // n 表示扩容之前 table 数组的长度 int n = tab.length, stride; // stride 表示分配给线程任务的步长，默认就是 16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // 如果当前线程为触发本次扩容的线程，需要做一些扩容准备工作，【协助线程不做这一步】 if (nextTab == null) &#123; try &#123; // 创建一个容量是之前【二倍的 table 数组】 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; sizeCtl = Integer.MAX_VALUE; return; &#125; // 把新表赋值给对象属性 nextTable，方便其他线程获取新表 nextTable = nextTab; // 记录迁移数据整体位置的一个标记，transferIndex 计数从1开始不是 0，所以这里是长度，不是长度-1 transferIndex = n; &#125; // 新数组的长度 int nextn = nextTab.length; // 当某个桶位数据处理完毕后，将此桶位设置为 fwd 节点，其它写线程或读线程看到后，可以从中获取到新表 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // 推进标记 boolean advance = true; // 完成标记 boolean finishing = false; // i 表示分配给当前线程任务，执行到的桶位 // bound 表示分配给当前线程任务的下界限制，因为是倒序迁移，16 迁移完 迁移 15，15完成去迁移14 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 给当前线程【分配任务区间】 while (advance) &#123; // 分配任务的开始下标，分配任务的结束下标 int nextIndex, nextBound; // --i 让当前线程处理下一个索引，true说明当前的迁移任务尚未完成，false说明线程已经完成或者还未分配 if (--i &gt;= bound || finishing) advance = false; // 迁移的开始下标，小于0说明没有区间需要迁移了，设置当前线程的 i 变量为 -1 跳出循环 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; // 逻辑到这说明还有区间需要分配，然后给当前线程分配任务， else if (U.compareAndSwapInt(this, TRANSFERINDEX, nextIndex, // 判断区间是否还够一个步长，不够就全部分配 nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; // 当前线程的结束下标 bound = nextBound; // 当前线程的开始下标，上一个线程结束的下标的下一个索引就是这个线程开始的下标 i = nextIndex - 1; // 任务分配结束，跳出循环执行迁移操作 advance = false; &#125; &#125; // 【分配完成，开始数据迁移操作】 // 【CASE1】：i &lt; 0 成立表示当前线程未分配到任务，或者任务执行完了 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; // 如果迁移完成 if (finishing) &#123; nextTable = null;\t// help GC table = nextTab;\t// 新表赋值给当前对象 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1);// 扩容阈值为 2n - n/2 = 3n/2 = 0.75*(2n) return; &#125; // 当前线程完成了分配的任务区间，可以退出，先把 sizeCtl 赋值给 sc 保留 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // 判断当前线程是不是最后一个线程，不是的话直接 return， if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 所以最后一个线程退出的时候，sizeCtl 的低 16 位为 1 finishing = advance = true; // 【这里表示最后一个线程需要重新检查一遍是否有漏掉的区间】 i = n; &#125; &#125; // 【CASE2】：当前桶位未存放数据，只需要将此处设置为 fwd 节点即可。 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 【CASE3】：说明当前桶位已经迁移过了，当前线程不用再处理了，直接处理下一个桶位即可 else if ((fh = f.hash) == MOVED) advance = true; // 【CASE4】：当前桶位有数据，而且 node 节点不是 fwd 节点，说明这些数据需要迁移 else &#123; // 【锁住头节点】 synchronized (f) &#123; // 二次检查，防止头节点已经被修改了，因为这里才是线程安全的访问 if (tabAt(tab, i) == f) &#123; // 【迁移数据的逻辑，和 HashMap 相似】 // ln 表示低位链表引用 // hn 表示高位链表引用 Node&lt;K,V&gt; ln, hn; // 哈希 &gt; 0 表示当前桶位是链表桶位 if (fh &gt;= 0) &#123; // 和 HashMap 的处理方式一致，与老数组长度相与，16 是 10000 // 判断对应的 1 的位置上是 0 或 1 分成高低位链表 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; // 遍历链表，寻找【逆序看】最长的对应位相同的链表，看下面的图更好的理解 for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; // 将当前节点的哈希 与 n int b = p.hash &amp; n; // 如果当前值与前面节点的值 对应位 不同，则修改 runBit，把 lastRun 指向当前节点 if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; // 判断筛选出的链表是低位的还是高位的 if (runBit == 0) &#123; ln = lastRun;\t// ln 指向该链表 hn = null; // hn 为 null &#125; // 说明 lastRun 引用的链表为高位链表，就让 hn 指向高位链表头节点 else &#123; hn = lastRun; ln = null; &#125; // 从头开始遍历所有的链表节点，迭代到 p == lastRun 节点跳出循环 for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) // 【头插法】，从右往左看，首先 ln 指向的是上一个节点， // 所以这次新建的节点的 next 指向上一个节点，然后更新 ln 的引用 ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // 高低位链设置到新表中的指定位置 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); // 老表中的该桶位设置为 fwd 节点 setTabAt(tab, i, fwd); advance = true; &#125; // 条件成立：表示当前桶位是 红黑树结点 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; // 迭代 TreeBin 中的双向链表，从头结点至尾节点 for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; // 迭代的当前元素的 hash int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); // 条件成立表示当前循环节点属于低位链节点 if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else //【尾插法】 loTail.next = p; // loTail 指向尾节点 loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; // 拆成的高位低位两个链，【判断是否需要需要转化为链表】，反之保持树化 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 链表处理的 LastRun 机制，可以减少节点的创建 helpTransfer()：帮助扩容机制 1234567891011121314151617181920212223final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; Node&lt;K,V&gt;[] nextTab; int sc; // 数组不为空，节点是转发节点，获取转发节点指向的新表开始协助主线程扩容 if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; // 扩容标识戳 int rs = resizeStamp(tab.length); // 判断数据迁移是否完成，迁移完成会把 新表赋值给 nextTable 属性 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; // 设置扩容线程数量 + 1 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; // 协助扩容 transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125; 获取方法ConcurrentHashMap 使用 get() 方法获取指定 key 的数据 get()：获取指定数据的方法 1234567891011121314151617181920212223242526public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // 扰动运算，获取 key 的哈希值 int h = spread(key.hashCode()); // 判断当前哈希表的数组是否初始化 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; // 如果 table 已经初始化，进行【哈希寻址】，映射到数组对应索引处，获取该索引处的头节点 (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 对比头结点 hash 与查询 key 的 hash 是否一致 if ((eh = e.hash) == h) &#123; // 进行值的判断，如果成功就说明当前节点就是要查询的节点，直接返回 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 当前槽位的【哈希值小于0】说明是红黑树节点或者是正在扩容的 fwd 节点 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; // 当前桶位是【链表】，循环遍历查找 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; ForwardingNode#find：转移节点的查找方法 12345678910111213141516171819202122232425262728293031323334353637Node&lt;K,V&gt; find(int h, Object k) &#123; // 获取新表的引用 outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) &#123; // e 表示在扩容而创建新表使用寻址算法得到的桶位头结点，n 表示为扩容而创建的新表的长度 Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || // 在新表中重新定位 hash 对应的头结点，表示在 oldTable 中对应的桶位在迁移之前就是 null (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; for (;;) &#123; int eh; K ek; // 【哈希相同值也相同】，表示新表当前命中桶位中的数据，即为查询想要数据 if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; // eh &lt; 0 说明当前新表中该索引的头节点是 TreeBin 类型，或者是 FWD 类型 if (eh &lt; 0) &#123; // 在并发很大的情况下新扩容的表还没完成可能【再次扩容】，在此方法处再次拿到 FWD 类型 if (e instanceof ForwardingNode) &#123; // 继续获取新的 fwd 指向的新数组的地址，递归了 tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; &#125; else // 说明此桶位为 TreeBin 节点，使用TreeBin.find 查找红黑树中相应节点。 return e.find(h, k); &#125; // 逻辑到这说明当前桶位是链表，将当前元素指向链表的下一个元素，判断当前元素的下一个位置是否为空 if ((e = e.next) == null) // 条件成立说明迭代到链表末尾，【未找到对应的数据，返回 null】 return null; &#125; &#125;&#125; 删除方法 remove()：删除指定元素 123public V remove(Object key) &#123; return replaceNode(key, null, null);&#125; replaceNode()：替代指定的元素，会协助扩容，增删改（写）都会协助扩容，查询（读）操作不会，因为读操作不涉及加锁 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990final V replaceNode(Object key, V value, Object cv) &#123; // 计算 key 扰动运算后的 hash int hash = spread(key.hashCode()); // 开始自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 【CASE1】：table 还未初始化或者哈希寻址的数组索引处为 null，直接结束自旋，返回 null if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; // 【CASE2】：条件成立说明当前 table 正在扩容，【当前是个写操作，所以当前线程需要协助 table 完成扩容】 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); // 【CASE3】：当前桶位可能是 链表 也可能是 红黑树 else &#123; // 保留替换之前数据引用 V oldVal = null; // 校验标记 boolean validated = false; // 【加锁当前桶位头结点】，加锁成功之后会进入代码块 synchronized (f) &#123; // 双重检查 if (tabAt(tab, i) == f) &#123; // 说明当前节点是链表节点 if (fh &gt;= 0) &#123; validated = true; //遍历所有的节点 for (Node&lt;K,V&gt; e = f, pred = null;;) &#123; K ek; // hash 和值都相同，定位到了具体的节点 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; // 当前节点的value V ev = e.val; if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; // 将当前节点的值 赋值给 oldVal 后续返回会用到 oldVal = ev; if (value != null) // 条件成立说明是替换操作 e.val = value; else if (pred != null)\t// 非头节点删除操作，断开链表 pred.next = e.next; else // 说明当前节点即为头结点，将桶位头节点设置为以前头节点的下一个节点 setTabAt(tab, i, e.next); &#125; break; &#125; pred = e; if ((e = e.next) == null) break; &#125; &#125; // 说明是红黑树节点 else if (f instanceof TreeBin) &#123; validated = true; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; r, p; if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; V pv = p.val; if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; oldVal = pv; // 条件成立说明替换操作 if (value != null) p.val = value; // 删除操作 else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; // 其他线程修改过桶位头结点时，当前线程 sync 头结点锁错对象，validated 为 false，会进入下次 for 自旋 if (validated) &#123; if (oldVal != null) &#123; // 替换的值为 null，【说明当前是一次删除操作，更新当前元素个数计数器】 if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null;&#125; 参考视频：https://space.bilibili.com/457326371/ JDK7原理ConcurrentHashMap 对锁粒度进行了优化，分段锁技术，将整张表分成了多个数组（Segment），每个数组又是一个类似 HashMap 数组的结构。允许多个修改操作并发进行，Segment 是一种可重入锁，继承 ReentrantLock，并发时锁住的是每个 Segment，其他 Segment 还是可以操作的，这样不同 Segment 之间就可以实现并发，大大提高效率。 底层结构： Segment 数组 + HashEntry 数组 + 链表（数组 + 链表是 HashMap 的结构） 优点：如果多个线程访问不同的 segment，实际是没有冲突的，这与 JDK8 中是类似的 缺点：Segments 数组默认大小为16，这个容量初始化指定后就不能改变了，并且不是懒惰初始化 ![](https://seazean.oss-cn-beijing.aliyuncs.com/img/Java/JUC-ConcurrentHashMap 1.7底层结构.png) CopyOnWrite原理分析CopyOnWriteArrayList 采用了写入时拷贝的思想，增删改操作会将底层数组拷贝一份，在新数组上执行操作，不影响其它线程的并发读，读写分离 CopyOnWriteArraySet 底层对 CopyOnWriteArrayList 进行了包装，装饰器模式 123public CopyOnWriteArraySet() &#123; al = new CopyOnWriteArrayList&lt;E&gt;();&#125; 存储结构： 1private transient volatile Object[] array;\t// volatile 保证了读写线程之间的可见性 全局锁：保证线程的执行安全 1final transient ReentrantLock lock = new ReentrantLock(); 新增数据：需要加锁，创建新的数组操作 12345678910111213141516171819public boolean add(E e) &#123; final ReentrantLock lock = this.lock; // 加锁，保证线程安全 lock.lock(); try &#123; // 获取旧的数组 Object[] elements = getArray(); int len = elements.length; // 【拷贝新的数组（这里是比较耗时的操作，但不影响其它读线程）】 Object[] newElements = Arrays.copyOf(elements, len + 1); // 添加新元素 newElements[len] = e; // 替换旧的数组，【这个操作以后，其他线程获取数组就是获取的新数组了】 setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; 读操作：不加锁，在原数组上操作 123456public E get(int index) &#123; return get(getArray(), index);&#125;private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 适合读多写少的应用场景 迭代器：CopyOnWriteArrayList 在返回迭代器时，创建一个内部数组当前的快照（引用），即使其他线程替换了原始数组，迭代器遍历的快照依然引用的是创建快照时的数组，所以这种实现方式也存在一定的数据延迟性，对其他线程并行添加的数据不可见 1234567891011121314151617181920public Iterator&lt;E&gt; iterator() &#123; // 获取到数组引用，整个遍历的过程该数组都不会变，一直引用的都是老数组， return new COWIterator&lt;E&gt;(getArray(), 0);&#125;// 迭代器会创建一个底层array的快照，故主类的修改不影响该快照static final class COWIterator&lt;E&gt; implements ListIterator&lt;E&gt; &#123; // 内部数组快照 private final Object[] snapshot; private COWIterator(Object[] elements, int initialCursor) &#123; cursor = initialCursor; // 数组的引用在迭代过程不会改变 snapshot = elements; &#125; // 【不支持写操作】，因为是在快照上操作，无法同步回去 public void remove() &#123; throw new UnsupportedOperationException(); &#125; &#125; 弱一致性数据一致性就是读到最新更新的数据： 强一致性：当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值 弱一致性：系统并不保证进程或者线程的访问都会返回最新的更新过的值，也不会承诺多久之后可以读到 时间点 操作 1 Thread-0 getArray() 2 Thread-1 getArray() 3 Thread-1 setArray(arrayCopy) 4 Thread-0 array[index] Thread-0 读到了脏数据 不一定弱一致性就不好 数据库的事务隔离级别就是弱一致性的表现 并发高和一致性是矛盾的，需要权衡 安全失败在 java.util 包的集合类就都是快速失败的，而 java.util.concurrent 包下的类都是安全失败 快速失败：在 A 线程使用迭代器对集合进行遍历的过程中，此时 B 线程对集合进行修改（增删改），或者 A 线程在遍历过程中对集合进行修改，都会导致 A 线程抛出 ConcurrentModificationException 异常 AbstractList 类中的成员变量 modCount，用来记录 List 结构发生变化的次数，结构发生变化是指添加或者删除至少一个元素的操作，或者是调整内部数组的大小，仅仅设置元素的值不算结构发生变化 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了抛出 CME 异常 安全失败：采用安全失败机制的集合容器，在迭代器遍历时直接在原集合数组内容上访问，但其他线程的增删改都会新建数组进行修改，就算修改了集合底层的数组容器，迭代器依然引用着以前的数组（快照思想），所以不会出现异常 ConcurrentHashMap 不会出现并发时的迭代异常，因为在迭代过程中 CHM 的迭代器并没有判断结构的变化，迭代器还可以根据迭代的节点状态去寻找并发扩容时的新表进行迭代 123ConcurrentHashMap map = new ConcurrentHashMap();// KeyIteratorIterator iterator = map.keySet().iterator(); 12345678Traverser(Node&lt;K,V&gt;[] tab, int size, int index, int limit) &#123; // 引用还是原来集合的 Node 数组，所以其他线程对数据的修改是可见的 this.tab = tab; this.baseSize = size; this.baseIndex = this.index = index; this.baseLimit = limit; this.next = null;&#125; 1234567891011public final boolean hasNext() &#123; return next != null; &#125;public final K next() &#123; Node&lt;K,V&gt; p; if ((p = next) == null) throw new NoSuchElementException(); K k = p.key; lastReturned = p; // 在方法中进行下一个节点的获取，会进行槽位头节点的状态判断 advance(); return k;&#125; CollectionsCollections类是用来操作集合的工具类，提供了集合转换成线程安全的方法： 123456 public static &lt;T&gt; Collection&lt;T&gt; synchronizedCollection(Collection&lt;T&gt; c) &#123; return new SynchronizedCollection&lt;&gt;(c); &#125;public static &lt;K,V&gt; Map&lt;K,V&gt; synchronizedMap(Map&lt;K,V&gt; m) &#123; return new SynchronizedMap&lt;&gt;(m);&#125; 源码：底层也是对方法进行加锁 123public boolean add(E e) &#123; synchronized (mutex) &#123;return c.add(e);&#125;&#125; SkipListMap底层结构跳表 SkipList 是一个有序的链表，默认升序，底层是链表加多级索引的结构。跳表可以对元素进行快速查询，类似于平衡树，是一种利用空间换时间的算法 对于单链表，即使链表是有序的，如果查找数据也只能从头到尾遍历链表，所以采用链表上建索引的方式提高效率，跳表的查询时间复杂度是 **O(logn)**，空间复杂度 O(n) ConcurrentSkipListMap 提供了一种线程安全的并发访问的排序映射表，内部是跳表结构实现，通过 CAS + volatile 保证线程安全 平衡树和跳表的区别： 对平衡树的插入和删除往往很可能导致平衡树进行一次全局的调整；而对跳表的插入和删除，只需要对整个结构的局部进行操作 在高并发的情况下，保证整个平衡树的线程安全需要一个全局锁；对于跳表则只需要部分锁，拥有更好的性能 BaseHeader 存储数据，headIndex 存储索引，纵向上所有索引都指向链表最下面的节点 成员变量 标识索引头节点位置 1private static final Object BASE_HEADER = new Object(); 跳表的顶层索引 1private transient volatile HeadIndex&lt;K,V&gt; head; 比较器，为 null 则使用自然排序 1final Comparator&lt;? super K&gt; comparator; Node 节点 12345static final class Node&lt;K, V&gt;&#123; final K key; // key 是 final 的, 说明节点一旦定下来, 除了删除, 一般不会改动 key volatile Object value; // 对应的 value volatile Node&lt;K, V&gt; next; // 下一个节点，单向链表&#125; 索引节点 Index，只有向下和向右的指针 123456789101112131415161718static class Index&lt;K, V&gt;&#123; final Node&lt;K, V&gt; node; // 索引指向的节点，每个都会指向数据节点 final Index&lt;K, V&gt; down; // 下边level层的Index，分层索引 volatile Index&lt;K, V&gt; right; // 右边的Index，单向 // 在 index 本身和 succ 之间插入一个新的节点 newSucc final boolean link(Index&lt;K, V&gt; succ, Index&lt;K, V&gt; newSucc)&#123; Node&lt;K, V&gt; n = node; newSucc.right = succ; // 把当前节点的右指针从 succ 改为 newSucc return n.value != null &amp;&amp; casRight(succ, newSucc); &#125; // 断开当前节点和 succ 节点，将当前的节点 index 设置其的 right 为 succ.right，就是把 succ 删除 final boolean unlink(Index&lt;K, V&gt; succ)&#123; return node.value != null &amp;&amp; casRight(succ, succ.right); &#125;&#125; 头索引节点 HeadIndex 1234567static final class HeadIndex&lt;K,V&gt; extends Index&lt;K,V&gt; &#123; final int level;\t// 表示索引层级，所有的 HeadIndex 都指向同一个 Base_header 节点 HeadIndex(Node&lt;K,V&gt; node, Index&lt;K,V&gt; down, Index&lt;K,V&gt; right, int level) &#123; super(node, down, right); this.level = level; &#125;&#125; 成员方法其他方法 构造方法： 1234public ConcurrentSkipListMap() &#123; this.comparator = null;\t// comparator 为 null，使用 key 的自然序，如字典序 initialize();&#125; 123456789private void initialize() &#123; keySet = null; entrySet = null; values = null; descendingMap = null; // 初始化索引头节点，Node 的 key 为 null，value 为 BASE_HEADER 对象，下一个节点为 null // head 的分层索引 down 为 null，链表的后续索引 right 为 null，层级 level 为第 1 层 head = new HeadIndex&lt;K,V&gt;(new Node&lt;K,V&gt;(null, BASE_HEADER, null), null, null, 1);&#125; cpr：排序 1234//　x 是比较者，y 是被比较者，比较者大于被比较者 返回正数，小于返回负数，相等返回 0static final int cpr(Comparator c, Object x, Object y) &#123; return (c != null) ? c.compare(x, y) : ((Comparable)x).compareTo(y);&#125; 添加方法 findPredecessor()：寻找前置节点 从最上层的头索引开始向右查找（链表的后续索引），如果后续索引的节点的 key 大于要查找的 key，则头索引移到下层链表，在下层链表查找，以此反复，一直查找到没有下层的分层索引为止，返回该索引的节点。如果后续索引的节点的 key 小于要查找的 key，则在该层链表中向后查找。由于查找的 key 可能永远大于索引节点的 key，所以只能找到目标的前置索引节点。如果遇到空值索引的存在，通过 CAS 来断开索引 123456789101112131415161718192021222324252627282930313233343536373839private Node&lt;K,V&gt; findPredecessor(Object key, Comparator&lt;? super K&gt; cmp) &#123; if (key == null) throw new NullPointerException(); // don&#x27;t postpone errors for (;;) &#123; // 1.初始数据 q 是 head，r 是最顶层 h 的右 Index 节点 for (Index&lt;K,V&gt; q = head, r = q.right, d;;) &#123; // 2.右索引节点不为空，则进行向下查找 if (r != null) &#123; Node&lt;K,V&gt; n = r.node; K k = n.key; // 3.n.value 为 null 说明节点 n 正在删除的过程中，此时【当前线程帮其删除索引】 if (n.value == null) &#123; // 在 index 层直接删除 r 索引节点 if (!q.unlink(r)) // 删除失败重新从 head 节点开始查找，break 一个 for 到步骤 1，又从初始值开始 break; // 删除节点 r 成功，获取新的 r 节点, r = q.right; // 回到步骤 2，还是从这层索引开始向右遍历 continue; &#125; // 4.若参数 key &gt; r.node.key，则继续向右遍历, continue 到步骤 2 处获取右节点 // 若参数 key &lt; r.node.key，说明需要进入下层索引，到步骤 5 if (cpr(cmp, key, k) &gt; 0) &#123; q = r; r = r.right; continue; &#125; &#125; // 5.先让 d 指向 q 的下一层，判断是否是 null，是则说明已经到了数据层，也就是第一层 if ((d = q.down) == null) return q.node; // 6.未到数据层, 进行重新赋值向下扫描 q = d; // q 指向 d r = d.right;// r 指向 q 的后续索引节点，此时(q.key &lt; key &lt; r.key) &#125; &#125;&#125; put()：添加数据 123456public V put(K key, V value) &#123; // 非空判断，value不能为空 if (value == null) throw new NullPointerException(); return doPut(key, value, false);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205private V doPut(K key, V value, boolean onlyIfAbsent) &#123; Node&lt;K,V&gt; z; // 非空判断，key 不能为空 if (key == null) throw new NullPointerException(); Comparator&lt;? super K&gt; cmp = comparator; // outer 循环，【把待插入数据插入到数据层的合适的位置，并在扫描过程中处理已删除(value = null)的数据】 outer: for (;;) &#123; //0.for (;;) //1.将 key 对应的前继节点找到, b 为前继节点，是数据层的, n 是前继节点的 next, // 若没发生条件竞争，最终 key 在 b 与 n 之间 (找到的 b 在 base_level 上) for (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123; // 2.n 不为 null 说明 b 不是链表的最后一个节点 if (n != null) &#123; Object v; int c; // 3.获取 n 的右节点 Node&lt;K,V&gt; f = n.next; // 4.条件竞争，并发下其他线程在 b 之后插入节点或直接删除节点 n, break 到步骤 0 if (n != b.next) break; // 若节点 n 已经删除, 则调用 helpDelete 进行【帮助删除节点】 if ((v = n.value) == null) &#123; n.helpDelete(b, f); break; &#125; // 5.节点 b 被删除中，则 break 到步骤 0, // 【调用findPredecessor帮助删除index层的数据, node层的数据会通过helpDelete方法进行删除】 if (b.value == null || v == n) break; // 6.若 key &gt; n.key，则进行向后扫描 // 若 key &lt; n.key，则证明 key 应该存储在 b 和 n 之间 if ((c = cpr(cmp, key, n.key)) &gt; 0) &#123; b = n; n = f; continue; &#125; // 7.key 的值和 n.key 相等，则可以直接覆盖赋值 if (c == 0) &#123; // onlyIfAbsent 默认 false， if (onlyIfAbsent || n.casValue(v, value)) &#123; @SuppressWarnings(&quot;unchecked&quot;) V vv = (V)v; // 返回被覆盖的值 return vv; &#125; // cas失败，break 一层循环，返回 0 重试 break; &#125; // else c &lt; 0; fall through &#125; // 8.此时的情况 b.key &lt; key &lt; n.key，对应流程图1中的7，创建z节点指向n z = new Node&lt;K,V&gt;(key, value, n); // 9.尝试把 b.next 从 n 设置成 z if (!b.casNext(n, z)) // cas失败，返回到步骤0，重试 break; // 10.break outer 后, 上面的 for 循环不会再执行, 而后执行下面的代码 break outer; &#125; &#125;\t// 【以上插入节点已经完成，剩下的任务要根据随机数的值来表示是否向上增加层数与上层索引】 // 随机数 int rnd = ThreadLocalRandom.nextSecondarySeed(); // 如果随机数的二进制与 10000000000000000000000000000001 进行与运算为 0 // 即随机数的二进制最高位与最末尾必须为 0，其他位无所谓，就进入该循环 // 如果随机数的二进制最高位与最末位不为 0，不增加新节点的层数 // 11.判断是否需要添加 level，32 位 if ((rnd &amp; 0x80000001) == 0) &#123; // 索引层 level，从 1 开始，就是最底层 int level = 1, max; // 12.判断最低位前面有几个 1，有几个leve就加几：0..0 0001 1110，这是4个，则1+4=5 // 【最大有30个就是 1 + 30 = 31 while (((rnd &gt;&gt;&gt;= 1) &amp; 1) != 0) ++level; // 最终会指向 z 节点，就是添加的节点 Index&lt;K,V&gt; idx = null; // 指向头索引节点 HeadIndex&lt;K,V&gt; h = head; // 13.判断level是否比当前最高索引小，图中 max 为 3 if (level &lt;= (max = h.level)) &#123; for (int i = 1; i &lt;= level; ++i) // 根据层数level不断创建新增节点的上层索引，索引的后继索引留空 // 第一次idx为null，也就是下层索引为空，第二次把上次的索引作为下层索引，【类似头插法】 idx = new Index&lt;K,V&gt;(z, idx, null); // 循环以后的索引结构 // index-3\t← idx // ↓ // index-2 // ↓ // index-1 // ↓ // z-node &#125; // 14.若 level &gt; max，则【只增加一层 index 索引层】，3 + 1 = 4 else &#123; level = max + 1; //创建一个 index 数组，长度是 level+1，假设 level 是 4，创建的数组长度为 5 Index&lt;K,V&gt;[] idxs = (Index&lt;K,V&gt;[])new Index&lt;?,?&gt;[level+1]; // index[0]的数组 slot 并没有使用，只使用 [1,level] 这些数组的 slot for (int i = 1; i &lt;= level; ++i) idxs[i] = idx = new Index&lt;K,V&gt;(z, idx, null); // index-4 ← idx // ↓ // ...... // ↓ // index-1 // ↓ // z-node for (;;) &#123; h = head; // 获取头索引的层数，3 int oldLevel = h.level; // 如果 level &lt;= oldLevel，说明其他线程进行了 index 层增加操作，退出循环 if (level &lt;= oldLevel) break; // 定义一个新的头索引节点 HeadIndex&lt;K,V&gt; newh = h; // 获取头索引的节点，就是 BASE_HEADER Node&lt;K,V&gt; oldbase = h.node; // 升级 baseHeader 索引，升高一级，并发下可能升高多级 for (int j = oldLevel + 1; j &lt;= level; ++j) // 参数1：底层node，参数二：down，为以前的头节点，参数三：right，新建 newh = new HeadIndex&lt;K,V&gt;(oldbase, newh, idxs[j], j); // 执行完for循环之后，baseHeader 索引长这个样子，这里只升高一级 // index-4 → index-4\t← idx // ↓ ↓ // index-3 index-3 // ↓ ↓ // index-2 index-2 // ↓ ↓ // index-1 index-1 // ↓ ↓ // baseHeader → .... → z-node // cas 成功后，head 字段指向最新的 headIndex，baseHeader 的 index-4 if (casHead(h, newh)) &#123; // h 指向最新的 index-4 节点 h = newh; // 让 idx 指向 z-node 的 index-3 节点， // 因为从 index-3 - index-1 的这些 z-node 索引节点 都没有插入到索引链表 idx = idxs[level = oldLevel]; break; &#125; &#125; &#125; // 15.【把新加的索引插入索引链表中】，有上述两种情况，一种索引高度不变，另一种是高度加 1 // 要插入的是第几层的索引 splice: for (int insertionLevel = level;;) &#123; // 获取头索引的层数，情况 1 是 3，情况 2 是 4 int j = h.level; // 【遍历 insertionLevel 层的索引，找到合适的插入位置】 for (Index&lt;K,V&gt; q = h, r = q.right, t = idx;;) &#123; // 如果头索引为 null 或者新增节点索引为 null，退出插入索引的总循环 if (q == null || t == null) // 此处表示有其他线程删除了头索引或者新增节点的索引 break splice; // 头索引的链表后续索引存在，如果是新层则为新节点索引，如果是老层则为原索引 if (r != null) &#123; // 获取r的节点 Node&lt;K,V&gt; n = r.node; // 插入的key和n.key的比较值 int c = cpr(cmp, key, n.key); // 【删除空值索引】 if (n.value == null) &#123; if (!q.unlink(r)) break; r = q.right; continue; &#125; // key &gt; r.node.key，向右扫描 if (c &gt; 0) &#123; q = r; r = r.right; continue; &#125; &#125; // 执行到这里，说明 key &lt; r.node.key，判断是否是第 j 层插入新增节点的前置索引 if (j == insertionLevel) &#123; // 【将新索引节点 t 插入 q r 之间】 if (!q.link(r, t)) break; // 如果新增节点的值为 null，表示该节点已经被其他线程删除 if (t.node.value == null) &#123; // 找到该节点 findNode(key); break splice; &#125; // 插入层逐层自减，当为最底层时退出循环 if (--insertionLevel == 0) break splice; &#125; // 其他节点随着插入节点的层数下移而下移 if (--j &gt;= insertionLevel &amp;&amp; j &lt; level) t = t.down; q = q.down; r = q.right; &#125; &#125; &#125; return null;&#125; findNode() 12345private Node&lt;K,V&gt; findNode(Object key) &#123; // 原理与doGet相同，无非是 findNode 返回节点，doGet 返回 value if ((c = cpr(cmp, key, n.key)) == 0) return n;&#125; 获取方法 get(key)：获取对应的数据 123public V get(Object key) &#123; return doGet(key);&#125; doGet()：扫描过程会对已 value &#x3D; null 的元素进行删除处理 1234567891011121314151617181920212223242526272829303132333435363738private V doGet(Object key) &#123; if (key == null) throw new NullPointerException(); Comparator&lt;? super K&gt; cmp = comparator; outer: for (;;) &#123; // 1.找到最底层节点的前置节点 for (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123; Object v; int c; // 2.【如果该前置节点的链表后续节点为 null，说明不存在该节点】 if (n == null) break outer; // b → n → f Node&lt;K,V&gt; f = n.next; // 3.如果n不为前置节点的后续节点，表示已经有其他线程删除了该节点 if (n != b.next) break; // 4.如果后续节点的值为null，【需要帮助删除该节点】 if ((v = n.value) == null) &#123; n.helpDelete(b, f); break; &#125; // 5.如果前置节点已被其他线程删除，重新循环 if (b.value == null || v == n) break; // 6.如果要获取的key与后续节点的key相等，返回节点的value if ((c = cpr(cmp, key, n.key)) == 0) &#123; @SuppressWarnings(&quot;unchecked&quot;) V vv = (V)v; return vv; &#125; // 7.key &lt; n.key，因位 key &gt; b.key，b 和 n 相连，说明不存在该节点或者被其他线程删除了 if (c &lt; 0) break outer; b = n; n = f; &#125; &#125; return null;&#125; 删除方法 remove() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V remove(Object key) &#123; return doRemove(key, null);&#125;final V doRemove(Object key, Object value) &#123; if (key == null) throw new NullPointerException(); Comparator&lt;? super K&gt; cmp = comparator; outer: for (;;) &#123; // 1.找到最底层目标节点的前置节点，b.key &lt; key for (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123; Object v; int c; // 2.如果该前置节点的链表后续节点为 null，退出循环，说明不存在这个元素 if (n == null) break outer; // b → n → f Node&lt;K,V&gt; f = n.next; if (n != b.next) // inconsistent read break; if ((v = n.value) == null) &#123; // n is deleted n.helpDelete(b, f); break; &#125; if (b.value == null || v == n) // b is deleted break; //3.key &lt; n.key，说明被其他线程删除了，或者不存在该节点 if ((c = cpr(cmp, key, n.key)) &lt; 0) break outer; //4.key &gt; n.key，继续向后扫描 if (c &gt; 0) &#123; b = n; n = f; continue; &#125; //5.到这里是 key = n.key，value 不为空的情况下判断 value 和 n.value 是否相等 if (value != null &amp;&amp; !value.equals(v)) break outer; //6.【把 n 节点的 value 置空】 if (!n.casValue(v, null)) break; //7.【给 n 添加一个删除标志 mark】，mark.next = f，然后把 b.next 设置为 f，成功后n出队 if (!n.appendMarker(f) || !b.casNext(n, f)) // 对 key 对应的 index 进行删除，调用了 findPredecessor 方法 findNode(key); else &#123; // 进行操作失败后通过 findPredecessor 中进行 index 的删除 findPredecessor(key, cmp); if (head.right == null) // 进行headIndex 对应的index 层的删除 tryReduceLevel(); &#125; @SuppressWarnings(&quot;unchecked&quot;) V vv = (V)v; return vv; &#125; &#125; return null;&#125; 经过 findPredecessor() 中的 unlink() 后索引已经被删除 appendMarker()：添加删除标记节点 1234boolean appendMarker(Node&lt;K,V&gt; f) &#123; // 通过 CAS 让 n.next 指向一个 key 为 null，value 为 this，next 为 f 的标记节点 return casNext(f, new Node&lt;K,V&gt;(f));&#125; helpDelete()：将添加了删除标记的节点清除，参数是该节点的前驱和后继节点 1234567891011void helpDelete(Node&lt;K,V&gt; b, Node&lt;K,V&gt; f) &#123; // this 节点的后续节点为 f，且本身为 b 的后续节点，一般都是正确的，除非被别的线程删除 if (f == next &amp;&amp; this == b.next) &#123; // 如果 n 还还没有被标记 if (f == null || f.value != f) casNext(f, new Node&lt;K,V&gt;(f)); else // 通过 CAS，将 b 的下一个节点 n 变成 f.next，即成为图中的样式 b.casNext(this, f.next); &#125;&#125; tryReduceLevel()：删除索引 1234567891011121314151617private void tryReduceLevel() &#123; HeadIndex&lt;K,V&gt; h = head; HeadIndex&lt;K,V&gt; d; HeadIndex&lt;K,V&gt; e; if (h.level &gt; 3 &amp;&amp; (d = (HeadIndex&lt;K,V&gt;)h.down) != null &amp;&amp; (e = (HeadIndex&lt;K,V&gt;)d.down) != null &amp;&amp; e.right == null &amp;&amp; d.right == null &amp;&amp; h.right == null &amp;&amp; // 设置头索引 casHead(h, d) &amp;&amp; // 重新检查 h.right != null) // 重新检查返回true，说明其他线程增加了索引层级，把索引头节点设置回来 casHead(d, h); &#125; 参考文章：https://my.oschina.net/u/3768341/blog/3135659 参考视频：https://www.bilibili.com/video/BV1Er4y1P7k1 NoBlocking非阻塞队列并发编程中，需要用到安全的队列，实现安全队列可以使用 2 种方式： 加锁，这种实现方式是阻塞队列 使用循环 CAS 算法实现，这种方式是非阻塞队列 ConcurrentLinkedQueue 是一个基于链接节点的无界线程安全队列，采用先进先出的规则对节点进行排序，当添加一个元素时，会添加到队列的尾部，当获取一个元素时，会返回队列头部的元素 补充：ConcurrentLinkedDeque 是双向链表结构的无界并发队列 ConcurrentLinkedQueue 使用约定： 不允许 null 入列 队列中所有未删除的节点的 item 都不能为 null 且都能从 head 节点遍历到 删除节点是将 item 设置为 null，队列迭代时跳过 item 为 null 节点 head 节点跟 tail 不一定指向头节点或尾节点，可能存在滞后性 ConcurrentLinkedQueue 由 head 节点和 tail 节点组成，每个节点由节点元素和指向下一个节点的引用组成，组成一张链表结构的队列 12345678private transient volatile Node&lt;E&gt; head;private transient volatile Node&lt;E&gt; tail;private static class Node&lt;E&gt; &#123; volatile E item; volatile Node&lt;E&gt; next; //.....&#125; 构造方法 无参构造方法： 1234public ConcurrentLinkedQueue() &#123; // 默认情况下 head 节点存储的元素为空，dummy 节点，tail 节点等于 head 节点 head = tail = new Node&lt;E&gt;(null);&#125; 有参构造方法 12345678910111213141516171819public ConcurrentLinkedQueue(Collection&lt;? extends E&gt; c) &#123; Node&lt;E&gt; h = null, t = null; // 遍历节点 for (E e : c) &#123; checkNotNull(e); Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); if (h == null) h = t = newNode; else &#123; // 单向链表 t.lazySetNext(newNode); t = newNode; &#125; &#125; if (h == null) h = t = new Node&lt;E&gt;(null); head = h; tail = t;&#125; 入队方法与传统的链表不同，单线程入队的工作流程： 将入队节点设置成当前队列尾节点的下一个节点 更新 tail 节点，如果 tail 节点的 next 节点不为空，则将入队节点设置成 tail 节点；如果 tail 节点的 next 节点为空，则将入队节点设置成 tail 的 next 节点，所以 tail 节点不总是尾节点，存在滞后性 123456789101112131415161718192021222324252627282930public boolean offer(E e) &#123; checkNotNull(e); // 创建入队节点 final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); // 循环 CAS 直到入队成功 for (Node&lt;E&gt; t = tail, p = t;;) &#123; // p 用来表示队列的尾节点，初始情况下等于 tail 节点，q 是 p 的 next 节点 Node&lt;E&gt; q = p.next; // 条件成立说明 p 是尾节点 if (q == null) &#123; // p 是尾节点，设置 p 节点的下一个节点为新节点 // 设置成功则 casNext 返回 true，否则返回 false，说明有其他线程更新过尾节点，继续寻找尾节点，继续 CAS if (p.casNext(null, newNode)) &#123; // 首次添加时，p 等于 t，不进行尾节点更新，所以尾节点存在滞后性 if (p != t) // 将 tail 设置成新入队的节点，设置失败表示其他线程更新了 tail 节点 casTail(t, newNode); return true; &#125; &#125; else if (p == q) // 当 tail 不指向最后节点时，如果执行出列操作，可能将 tail 也移除，tail 不在链表中 // 此时需要对 tail 节点进行复位，复位到 head 节点 p = (t != (t = tail)) ? t : head; else // 推动 tail 尾节点往队尾移动 p = (p != t &amp;&amp; t != (t = tail)) ? t : q; &#125;&#125; 图解入队： 当 tail 节点和尾节点的距离大于等于 1 时（每入队两次）更新 tail，可以减少 CAS 更新 tail 节点的次数，提高入队效率 线程安全问题： 线程 1 线程 2 同时入队，无论从哪个位置开始并发入队，都可以循环 CAS，直到入队成功，线程安全 线程 1 遍历，线程 2 入队，所以造成 ConcurrentLinkedQueue 的 size 是变化，需要加锁保证安全 线程 1 线程 2 同时出列，线程也是安全的 出队方法出队列的就是从队列里返回一个节点元素，并清空该节点对元素的引用，并不是每次出队都更新 head 节点 当 head 节点里有元素时，直接弹出 head 节点里的元素，而不会更新 head 节点 当 head 节点里没有元素时，出队操作才会更新 head 节点 批处理方式可以减少使用 CAS 更新 head 节点的消耗，从而提高出队效率 123456789101112131415161718192021222324252627282930313233public E poll() &#123; restartFromHead: for (;;) &#123; // p 节点表示首节点，即需要出队的节点，FIFO for (Node&lt;E&gt; h = head, p = h, q;;) &#123; E item = p.item; // 如果 p 节点的元素不为 null，则通过 CAS 来设置 p 节点引用元素为 null，成功返回 item if (item != null &amp;&amp; p.casItem(item, null)) &#123; if (p != h) // 对 head 进行移动 updateHead(h, ((q = p.next) != null) ? q : p); return item; &#125; // 逻辑到这说明头节点的元素为空或头节点发生了变化，头节点被另外一个线程修改了 // 那么获取 p 节点的下一个节点，如果 p 节点的下一节点也为 null，则表明队列已经空了 else if ((q = p.next) == null) &#123; updateHead(h, p); return null; &#125; // 第一轮操作失败，下一轮继续，调回到循环前 else if (p == q) continue restartFromHead; // 如果下一个元素不为空，则将头节点的下一个节点设置成头节点 else p = q; &#125; &#125;&#125;final void updateHead(Node&lt;E&gt; h, Node&lt;E&gt; p) &#123; if (h != p &amp;&amp; casHead(h, p)) // 将旧结点 h 的 next 域指向为 h，help gc h.lazySetNext(h);&#125; 在更新完 head 之后，会将旧的头结点 h 的 next 域指向为 h，图中所示的虚线也就表示这个节点的自引用，被移动的节点（item 为 null 的节点）会被 GC 回收 如果这时，有一个线程来添加元素，通过 tail 获取的 next 节点则仍然是它本身，这就出现了p &#x3D;&#x3D; q 的情况，出现该种情况之后，则会触发执行 head 的更新，将 p 节点重新指向为 head 参考文章：https://www.jianshu.com/p/231caf90f30b 成员方法 peek()：会改变 head 指向，执行 peek() 方法后 head 会指向第一个具有非空元素的节点 123456789101112131415161718// 获取链表的首部元素，只读取而不移除public E peek() &#123; restartFromHead: for (;;) &#123; for (Node&lt;E&gt; h = head, p = h, q;;) &#123; E item = p.item; if (item != null || (q = p.next) == null) &#123; // 更改h的位置为非空元素节点 updateHead(h, p); return item; &#125; else if (p == q) continue restartFromHead; else p = q; &#125; &#125;&#125; size()：用来获取当前队列的元素个数，因为整个过程都没有加锁，在并发环境中从调用 size 方法到返回结果期间有可能增删元素，导致统计的元素个数不精确 123456789101112public int size() &#123; int count = 0; // first() 获取第一个具有非空元素的节点，若不存在，返回 null // succ(p) 方法获取 p 的后继节点，若 p == p.next，则返回 head // 类似遍历链表 for (Node&lt;E&gt; p = first(); p != null; p = succ(p)) if (p.item != null) // 最大返回Integer.MAX_VALUE if (++count == Integer.MAX_VALUE) break; return count;&#125; remove()：移除元素 12345678910111213141516171819202122232425262728public boolean remove(Object o) &#123; // 删除的元素不能为null if (o != null) &#123; Node&lt;E&gt; next, pred = null; for (Node&lt;E&gt; p = first(); p != null; pred = p, p = next) &#123; boolean removed = false; E item = p.item; // 节点元素不为null if (item != null) &#123; // 若不匹配，则获取next节点继续匹配 if (!o.equals(item)) &#123; next = succ(p); continue; &#125; // 若匹配，则通过 CAS 操作将对应节点元素置为 null removed = p.casItem(item, null); &#125; // 获取删除节点的后继节点 next = succ(p); // 将被删除的节点移除队列 if (pred != null &amp;&amp; next != null) // unlink pred.casNext(p, next); if (removed) return true; &#125; &#125; return false;&#125; NETDES网络编程网络编程，就是在一定的协议下，实现两台计算机的通信的技术 通信一定是基于软件结构实现的: C&#x2F;S 结构 ：全称为 Client&#x2F;Server 结构，是指客户端和服务器结构，常见程序有 QQ、IDEA 等软件 B&#x2F;S 结构 ：全称为 Browser&#x2F;Server 结构，是指浏览器和服务器结构 两种架构各有优势，但是无论哪种架构，都离不开网络的支持 网络通信的三要素： 协议：计算机网络客户端与服务端通信必须约定和彼此遵守的通信规则，HTTP、FTP、TCP、UDP、SMTP IP 地址：互联网协议地址（Internet Protocol Address），用来给一个网络中的计算机设备做唯一的编号 IPv4：4 个字节，32 位组成，192.168.1.1 IPv6：可以实现为所有设备分配 IP，128 位 ipconfig：查看本机的 IP ping 检查本机与某个 IP 指定的机器是否联通，或者说是检测对方是否在线。 ping 空格 IP地址 ：ping 220.181.57.216，ping www.baidu.com 特殊的IP地址： 本机IP地址，127.0.0.1 &#x3D;&#x3D; localhost，回环测试 端口：端口号就可以唯一标识设备中的进程（应用程序）。端口号是用两个字节表示的整数，取值范围是 0-65535，0-1023 之间的端口号用于一些知名的网络服务和应用普通的应用程序需要使用 1024 以上的端口号。如果端口号被另外一个服务或应用所占用，会导致当前程序启动失败，报出端口被占用异常 利用协议+IP 地址+端口号三元组合，就可以标识网络中的进程了，那么进程间的通信就可以利用这个标识与其它进程进行交互 参考视频：https://www.bilibili.com/video/BV1kT4y1M7vt 通信协议网络通信协议：对计算机必须遵守的规则，只有遵守这些规则，计算机之间才能进行通信 通信是进程与进程之间的通信，不是主机与主机之间的通信 TCP&#x2F;IP协议：传输控制协议 (Transmission Control Protocol) 传输控制协议 TCP（Transmission Control Protocol）是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流，每一条 TCP 连接只能是点对点的（一对一） 在通信之前必须确定对方在线并且连接成功才可以通信 例如下载文件、浏览网页等（要求可靠传输） 用户数据报协议 UDP（User Datagram Protocol）是无连接的，尽最大可能交付，不可靠，没有拥塞控制，面向报文，支持一对一、一对多、多对一和多对多的交互通信 直接发消息给对方，不管对方是否在线，发消息后也不需要确认 无线（视频会议，通话），性能好，可能丢失一些数据 Java模型相关概念： 同步：当前线程要自己进行数据的读写操作（自己去银行取钱） 异步：当前线程可以去做其他事情（委托别人拿银行卡到银行取钱，然后给你） 阻塞：在数据没有的情况下，还是要继续等待着读（排队等待） 非阻塞：在数据没有的情况下，会去做其他事情，一旦有了数据再来获取（柜台取款，取个号，然后坐在椅子上做其它事，等号广播会通知你办理） Java 中的通信模型: BIO 表示同步阻塞式通信，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，可以通过线程池机制改善 同步阻塞式性能极差：大量线程，大量阻塞 伪异步通信：引入线程池，不需要一个客户端一个线程，实现线程复用来处理很多个客户端，线程可控 高并发下性能还是很差：线程数量少，数据依然是阻塞的，数据没有来线程还是要等待 NIO 表示同步非阻塞 IO，服务器实现模式为请求对应一个线程，客户端发送的连接会注册到多路复用器上，多路复用器轮询到连接有 I&#x2F;O 请求时才启动一个线程进行处理 工作原理：1 个主线程专门负责接收客户端，1 个线程轮询所有的客户端，发来了数据才会开启线程处理 同步：线程还要不断的接收客户端连接，以及处理数据 非阻塞：如果一个管道没有数据，不需要等待，可以轮询下一个管道是否有数据 AIO 表示异步非阻塞 IO，AIO 引入异步通道的概念，采用了 Proactor 模式，有效的请求才启动线程，特点是先由操作系统完成后才通知服务端程序启动线程去处理，一般适用于连接数较多且连接时间较长的应用 异步：服务端线程接收到了客户端管道以后就交给底层处理 IO 通信，线程可以做其他事情 非阻塞：底层也是客户端有数据才会处理，有了数据以后处理好通知服务器应用来启动线程进行处理 各种模型应用场景： BIO 适用于连接数目比较小且固定的架构，该方式对服务器资源要求比较高，并发局限于应用中，程序简单 NIO 适用于连接数目多且连接比较短（轻操作）的架构，如聊天服务器，并发局限于应用中，编程复杂，JDK 1.4 开始支持 AIO 适用于连接数目多且连接比较长（重操作）的架构，如相册服务器，充分调用操作系统参与并发操作，JDK 1.7 开始支持 I&#x2F;OIO模型五种模型对于一个套接字上的输入操作，第一步是等待数据从网络中到达，当数据到达时被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区 Linux 有五种 I&#x2F;O 模型： 阻塞式 I&#x2F;O 非阻塞式 I&#x2F;O I&#x2F;O 复用（select 和 poll） 信号驱动式 I&#x2F;O（SIGIO） 异步 I&#x2F;O（AIO） 五种模型对比： 同步 I&#x2F;O 包括阻塞式 I&#x2F;O、非阻塞式 I&#x2F;O、I&#x2F;O 复用和信号驱动 I&#x2F;O ，它们的主要区别在第一个阶段，非阻塞式 I&#x2F;O 、信号驱动 I&#x2F;O 和异步 I&#x2F;O 在第一阶段不会阻塞 同步 I&#x2F;O：将数据从内核缓冲区复制到应用进程缓冲区的阶段（第二阶段），应用进程会阻塞 异步 I&#x2F;O：第二阶段应用进程不会阻塞 阻塞式IO应用进程通过系统调用 recvfrom 接收数据，会被阻塞，直到数据从内核缓冲区复制到应用进程缓冲区中才返回。阻塞不意味着整个操作系统都被阻塞，其它应用进程还可以执行，只是当前阻塞进程不消耗 CPU 时间，这种模型的 CPU 利用率会比较高 recvfrom() 用于接收 Socket 传来的数据，并复制到应用进程的缓冲区 buf 中，把 recvfrom() 当成系统调用 非阻塞式应用进程通过 recvfrom 调用不停的去和内核交互，直到内核准备好数据。如果没有准备好数据，内核返回一个错误码，过一段时间应用进程再执行 recvfrom 系统调用，在两次发送请求的时间段，进程可以进行其他任务，这种方式称为轮询（polling） 由于 CPU 要处理更多的系统调用，因此这种模型的 CPU 利用率比较低 信号驱动应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，等待数据阶段应用进程是非阻塞的。当内核数据准备就绪时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中 相比于非阻塞式 I&#x2F;O 的轮询方式，信号驱动 I&#x2F;O 的 CPU 利用率更高 IO 复用IO 复用模型使用 select 或者 poll 函数等待数据，select 会监听所有注册好的 IO，等待多个套接字中的任何一个变为可读，等待过程会被阻塞，当某个套接字准备好数据变为可读时 select 调用就返回，然后调用 recvfrom 把数据从内核复制到进程中 IO 复用让单个进程具有处理多个 I&#x2F;O 事件的能力，又被称为 Event Driven I&#x2F;O，即事件驱动 I&#x2F;O 如果一个 Web 服务器没有 I&#x2F;O 复用，那么每一个 Socket 连接都要创建一个线程去处理，如果同时有几万个连接，就需要创建相同数量的线程。相比于多进程和多线程技术，I&#x2F;O 复用不需要进程线程创建和切换的开销，系统开销更小 异步 IO应用进程执行 aio_read 系统调用会立即返回，给内核传递描述符、缓冲区指针、缓冲区大小等。应用进程可以继续执行不会被阻塞，内核会在所有操作完成之后向应用进程发送信号 异步 I&#x2F;O 与信号驱动 I&#x2F;O 的区别在于，异步 I&#x2F;O 的信号是通知应用进程 I&#x2F;O 完成，而信号驱动 I&#x2F;O 的信号是通知应用进程可以开始 I&#x2F;O 多路复用select函数Socket 不是文件，只是一个标识符，但是 Unix 操作系统把所有东西都看作是文件，所以 Socket 说成 file descriptor，也就是 fd select 允许应用程序监视一组文件描述符，等待一个或者多个描述符成为就绪状态，从而完成 I&#x2F;O 操作。 1int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); fd_set 使用 bitmap 数组实现，数组大小用 FD_SETSIZE 定义，单进程只能监听少于 FD_SETSIZE 数量的描述符，32 位机默认是 1024 个，64 位机默认是 2048，可以对进行修改，然后重新编译内核 fd_set 有三种类型的描述符：readset、writeset、exceptset，对应读、写、异常条件的描述符集合 n 是监测的 socket 的最大数量 timeout 为超时参数，调用 select 会一直阻塞直到有描述符的事件到达或者等待的时间超过 timeout 1234struct timeval&#123; long tv_sec; //秒 long tv_usec;\t//微秒&#125; timeout &#x3D;&#x3D; null：等待无限长的时间 tv_sec &#x3D;&#x3D; 0 &amp;&amp; tv_usec &#x3D;&#x3D; 0：获取后直接返回，不阻塞等待 tv_sec !&#x3D; 0 || tv_usec !&#x3D; 0：等待指定时间 方法成功调用返回结果为就绪的文件描述符个数，出错返回结果为 -1，超时返回结果为 0 Linux 提供了一组宏为 fd_set 进行赋值操作： 1234int FD_ZERO(fd_set *fdset); // 将一个 fd_set 类型变量的所有值都置为 0int FD_CLR(int fd, fd_set *fdset);\t// 将一个 fd_set 类型变量的 fd 位置为 0int FD_SET(int fd, fd_set *fdset);\t// 将一个 fd_set 类型变量的 fd 位置为 1int FD_ISSET(int fd, fd_set *fdset);// 判断 fd 位是否被置为 1 示例： 12345678910111213141516171819202122232425262728293031sockfd = socket(AF_INET, SOCK_STREAM, 0);memset(&amp;addr, 0, sizeof(addr)));addr.sin_family = AF_INET;addr.sin_port = htons(2000);addr.sin_addr.s_addr = INADDR_ANY;bind(sockfd, (struct sockaddr*)&amp;addr, sizeof(addr));//绑定连接listen(sockfd, 5);//监听5个端口for(i = 0; i &lt; 5; i++) &#123;\tmemset(&amp;client, e, sizeof(client)); addrlen = sizeof(client);\tfds[i] = accept(sockfd, (struct sockaddr*)&amp;client, &amp;addrlen); //将监听的对应的文件描述符fd存入fds：[3,4,5,6,7] if(fds[i] &gt; max) max = fds[i];&#125;while(1) &#123; FD_ZERO(&amp;rset);//置为0 for(i = 0; i &lt; 5; i++) &#123; FD_SET(fds[i], &amp;rset);//对应位置1 [0001 1111 00.....]\t&#125;\tprint(&quot;round again&quot;);\tselect(max + 1, &amp;rset, NULL, NULL, NULL);//监听 for(i = 0; i &lt;5; i++) &#123; if(FD_ISSET(fds[i], &amp;rset)) &#123;//判断监听哪一个端口 memset(buffer, 0, MAXBUF); read(fds[i], buffer, MAXBUF);//进入内核态读数据 print(buffer); &#125; &#125;&#125; 参考视频：https://www.bilibili.com/video/BV19D4y1o797 流程select 调用流程图： 使用 copy_from_user 从用户空间拷贝 fd_set 到内核空间，进程阻塞 注册回调函数 _pollwait 遍历所有 fd，调用其对应的 poll 方法判断当前请求是否准备就绪，对于 socket，这个 poll 方法是 sock_poll，sock_poll 根据情况会调用到 tcp_poll、udp_poll 或者 datagram_poll，以 tcp_poll 为例，其核心实现就是 _pollwait _pollwait 把 current（调用 select 的进程）挂到设备的等待队列，不同设备有不同的等待队列，对于 tcp_poll ，其等待队列是 sk → sk_sleep（把进程挂到等待队列中并不代表进程已经睡眠），在设备收到消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时 current 便被唤醒，进入就绪队列 poll 方法返回时会返回一个描述读写操作是否就绪的 mask 掩码，根据这个 mask 掩码给 fd_set 赋值 如果遍历完所有的 fd，还没有返回一个可读写的 mask 掩码，则会调用 schedule_timeout 让 current 进程进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程，如果超过一定的超时时间（schedule_timeout）没有其他线程唤醒，则调用 select 的进程会重新被唤醒获得 CPU，进而重新遍历 fd，判断有没有就绪的 fd 把 fd_set 从内核空间拷贝到用户空间，阻塞进程继续执行 参考文章：https://www.cnblogs.com/anker/p/3265058.html 其他流程图：https://www.processon.com/view/link/5f62b9a6e401fd2ad7e5d6d1 pollpoll 的功能与 select 类似，也是等待一组描述符中的一个成为就绪状态 1int poll(struct pollfd *fds, unsigned int nfds, int timeout); poll 中的描述符是 pollfd 类型的数组，pollfd 的定义如下： 12345struct pollfd &#123; int fd; /* file descriptor */ short events; /* requested events */ short revents; /* returned events */&#125;; select 和 poll 对比： select 会修改描述符，而 poll 不会 select 的描述符类型使用数组实现，有描述符的限制；而 poll 使用链表实现，没有描述符数量的限制 poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高 select 和 poll 速度都比较慢，每次调用都需要将全部描述符数组 fd 从应用进程缓冲区复制到内核缓冲区，同时每次都需要在内核遍历传递进来的所有 fd ，这个开销在 fd 很多时会很大 几乎所有的系统都支持 select，但是只有比较新的系统支持 poll select 和 poll 的时间复杂度 O(n)，对 socket 进行扫描时是线性扫描，即采用轮询的方法，效率较低，因为并不知道具体是哪个 socket 具有事件，所以随着 fd 数量的增加会造成遍历速度慢的线性下降性能问题 poll 还有一个特点是水平触发，如果报告了 fd 后，没有被处理，那么下次 poll 时会再次报告该 fd 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定 参考文章：https://github.com/CyC2018/CS-Notes/blob/master/notes/Socket.md epoll函数epoll 使用事件的就绪通知方式，通过 epoll_ctl() 向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，一旦该 fd 就绪，内核通过 callback 回调函数将 I&#x2F;O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件就绪的描述符 123int epoll_create(int size);int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); epall_create：一个系统函数，函数将在内核空间内创建一个 epoll 数据结构，可以理解为 epoll 结构空间，返回值为 epoll 的文件描述符编号，以后有 client 连接时，向该 epoll 结构中添加监听，所以 epoll 使用一个文件描述符管理多个描述符 epall_ctl：epoll 的事件注册函数，select 函数是调用时指定需要监听的描述符和事件，epoll 先将用户感兴趣的描述符事件注册到 epoll 空间。此函数是非阻塞函数，用来增删改 epoll 空间内的描述符，参数解释： epfd：epoll 结构的进程 fd 编号，函数将依靠该编号找到对应的 epoll 结构 op：表示当前请求类型，有三个宏定义： EPOLL_CTL_ADD：注册新的 fd 到 epfd 中 EPOLL_CTL_MOD：修改已经注册的 fd 的监听事件 EPOLL_CTI_DEL：从 epfd 中删除一个 fd fd：需要监听的文件描述符，一般指 socket_fd event：告诉内核对该 fd 资源感兴趣的事件，epoll_event 的结构： 1234struct epoll_event &#123; _uint32_t events;\t/*epoll events*/ epoll_data_t data;\t/*user data variable*/&#125; events 可以是以下几个宏集合：EPOLLIN、EPOLOUT、EPOLLPRI、EPOLLERR、EPOLLHUP（挂断）、EPOLET（边缘触发）、EPOLLONESHOT（只监听一次，事件触发后自动清除该 fd，从 epoll 列表） epoll_wait：等待事件的产生，类似于 select() 调用，返回值为本次就绪的 fd 个数，直接从就绪链表获取，时间复杂度 O(1) epfd：指定感兴趣的 epoll 事件列表 events：指向一个 epoll_event 结构数组，当函数返回时，内核会把就绪状态的数据拷贝到该数组 maxevents：标明 epoll_event 数组最多能接收的数据量，即本次操作最多能获取多少就绪数据 timeout：单位为毫秒 0：表示立即返回，非阻塞调用 -1：阻塞调用，直到有用户感兴趣的事件就绪为止 大于 0：阻塞调用，阻塞指定时间内如果有事件就绪则提前返回，否则等待指定时间后返回 epoll 的描述符事件有两种触发模式：LT（level trigger）和 ET（edge trigger）： LT 模式：当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程，是默认的一种模式，并且同时支持 Blocking 和 No-Blocking ET 模式：通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高；只支持 No-Blocking，以避免由于一个 fd 的阻塞读&#x2F;阻塞写操作把处理多个文件描述符的任务饥饿 1234567891011121314151617181920212223242526272829303132333435// 创建 epoll 描述符，每个应用程序只需要一个，用于监控所有套接字int pollingfd = epoll_create(0xCAFE);if ( pollingfd &lt; 0 )// report error// 初始化 epoll 结构struct epoll_event ev = &#123; 0 &#125;;// 将连接类实例与事件相关联，可以关联任何想要的东西ev.data.ptr = pConnection1;// 监视输入，并且在事件发生后不自动重新准备描述符ev.events = EPOLLIN | EPOLLONESHOT;// 将描述符添加到监控列表中，即使另一个线程在epoll_wait中等待，描述符将被正确添加if ( epoll_ctl( epollfd, EPOLL_CTL_ADD, pConnection1-&gt;getSocket(), &amp;ev) != 0 ) // report error// 最多等待 20 个事件struct epoll_event pevents[20];// 等待10秒，检索20个并存入epoll_event数组int ready = epoll_wait(pollingfd, pevents, 20, 10000);// 检查epoll是否成功if ( ret == -1)// report error and abortelse if ( ret == 0)// timeout; no event detectedelse&#123; for (int i = 0; i &lt; ready; i+ ) &#123; if ( pevents[i].events &amp; EPOLLIN ) &#123; // 获取连接指针 Connection * c = (Connection*) pevents[i].data.ptr; c-&gt;handleReadEvent(); &#125; &#125;&#125; 流程图：https://gitee.com/seazean/images/blob/master/Java/IO-epoll%E5%8E%9F%E7%90%86%E5%9B%BE.jpg 参考视频：https://www.bilibili.com/video/BV19D4y1o797 特点epoll 的特点： epoll 仅适用于 Linux 系统 epoll 使用一个文件描述符管理多个描述符，将用户关心的文件描述符的事件存放到内核的一个事件表（个人理解成哑元节点） 没有最大描述符数量（并发连接）的限制，打开 fd 的上限远大于1024（1G 内存能监听约 10 万个端口） epoll 的时间复杂度 O(1)，epoll 理解为 event poll，不同于忙轮询和无差别轮询，调用 epoll_wait 只是轮询就绪链表。当监听列表有设备就绪时调用回调函数，把就绪 fd 放入就绪链表中，并唤醒在 epoll_wait 中阻塞的进程，所以 epoll 实际上是事件驱动（每个事件关联上fd）的，降低了 system call 的时间复杂度 epoll 内核中根据每个 fd 上的 callback 函数来实现，只有活跃的 socket 才会主动调用 callback，所以使用 epoll 没有前面两者的线性下降的性能问题，效率提高 epoll 注册新的事件是注册到到内核中 epoll 句柄中，不需要每次调用 epoll_wait 时重复拷贝，对比前面两种只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，也可以利用 mmap() 文件映射内存加速与内核空间的消息传递（只是可以用，并没有用） 前面两者要把 current 往设备等待队列中挂一次，epoll 也只把 current 往等待队列上挂一次，但是这里的等待队列并不是设备等待队列，只是一个 epoll 内部定义的等待队列，这样可以节省开销 epoll 对多线程编程更有友好，一个线程调用了 epoll_wait() 另一个线程关闭了同一个描述符，也不会产生像 select 和 poll 的不确定情况 参考文章：https://www.jianshu.com/p/dfd940e7fca2 参考文章：https://www.cnblogs.com/anker/p/3265058.html 应用应用场景： select 应用场景： select 的 timeout 参数精度为微秒，poll 和 epoll 为毫秒，因此 select 适用实时性要求比较高的场景，比如核反应堆的控制 select 可移植性更好，几乎被所有主流平台所支持 poll 应用场景：poll 没有最大描述符数量的限制，适用于平台支持并且对实时性要求不高的情况 epoll 应用场景： 运行在 Linux 平台上，有大量的描述符需要同时轮询，并且这些连接最好是长连接 需要同时监控小于 1000 个描述符，没必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势 需要监控的描述符状态变化多，而且是非常短暂的，就没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，每次对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率，并且 epoll 的描述符存储在内核，不容易调试 参考文章：https://github.com/CyC2018/CS-Notes/blob/master/notes/Socket.md 系统调用内核态用户空间：用户代码、用户堆栈 内核空间：内核代码、内核调度程序、进程描述符（内核堆栈、thread_info 进程描述符） 进程描述符和用户的进程是一一对应的 SYS_API 系统调用：如 read、write，系统调用就是 0X80 中断 进程描述符 pd：进程从用户态切换到内核态时，需要保存用户态时的上下文信息在 PCB 中 线程上下文：用户程序基地址，程序计数器、cpu cache、寄存器等，方便程序切回用户态时恢复现场 内核堆栈：系统调用函数也是要创建变量的，这些变量在内核堆栈上分配 80中断在用户程序中调用操作系统提供的核心态级别的子功能，为了系统安全需要进行用户态和内核态转换，状态的转换需要进行 CPU 中断，中断分为硬中断和软中断： 硬中断：如网络传输中，数据到达网卡后，网卡经过一系列操作后发起硬件中断 软中断：如程序运行过程中本身产生的一些中断 发起 0X80 中断 程序执行碰到除 0 异常 系统调用 system_call 函数所对应的中断指令编号是 0X80（十进制是 8×16&#x3D;128），而该指令编号对应的就是系统调用程序的入口，所以称系统调用为 80 中断 系统调用的流程： 在 CPU 寄存器里存一个系统调用号，表示哪个系统函数，比如 read 将 CPU 的临时数据都保存到 thread_info 中 执行 80 中断处理程序，找到刚刚存的系统调用号（read），先检查缓存中有没有对应的数据，没有就去磁盘中加载到内核缓冲区，然后从内核缓冲区拷贝到用户空间 最后恢复到用户态，通过 thread_info 恢复现场，用户态继续执行 参考视频：https://www.bilibili.com/video/BV19D4y1o797 零拷贝DMADMA (Direct Memory Access) ：直接存储器访问，让外部设备不通过 CPU 直接与系统内存交换数据的接口技术 作用：可以解决批量数据的输入&#x2F;输出问题，使数据的传送速度取决于存储器和外设的工作速度 把内存数据传输到网卡然后发送： 没有 DMA：CPU 读内存数据到 CPU 高速缓存，再写到网卡，这样就把 CPU 的速度拉低到和网卡一个速度 使用 DMA：把数据读到 Socket 内核缓存区（CPU 复制），CPU 分配给 DMA 开始异步操作，DMA 读取 Socket 缓冲区到 DMA 缓冲区，然后写到网卡。DMA 执行完后中断（就是通知） CPU，这时 Socket 内核缓冲区为空，CPU 从用户态切换到内核态，执行中断处理程序，将需要使用 Socket 缓冲区的阻塞进程移到就绪队列 一个完整的 DMA 传输过程必须经历 DMA 请求、DMA 响应、DMA 传输、DMA 结束四个步骤： DMA 方式是一种完全由硬件进行信息传送的控制方式，通常系统总线由 CPU 管理，在 DMA 方式中，CPU 的主存控制信号被禁止使用，CPU 把总线（地址总线、数据总线、控制总线）让出来由 DMA 控制器接管，用来控制传送的字节数、判断 DMA 是否结束、以及发出 DMA 结束信号，所以 DMA 控制器必须有以下功能： 接受外设发出的 DMA 请求，并向 CPU 发出总线接管请求 当 CPU 发出允许接管信号后，进入 DMA 操作周期 确定传送数据的主存单元地址及长度，并自动修改主存地址计数和传送长度计数 规定数据在主存和外设间的传送方向，发出读写等控制信号，执行数据传送操作 判断 DMA 传送是否结束，发出 DMA 结束信号，使 CPU 恢复正常工作状态（中断） BIO传统的 I&#x2F;O 操作进行了 4 次用户空间与内核空间的上下文切换，以及 4 次数据拷贝： JVM 发出 read 系统调用，OS 上下文切换到内核模式（切换 1）并将数据从网卡或硬盘等设备通过 DMA 读取到内核空间缓冲区（拷贝 1），内核缓冲区实际上是磁盘高速缓存（PageCache） OS 内核将数据复制到用户空间缓冲区（拷贝 2），然后 read 系统调用返回，又会导致一次内核空间到用户空间的上下文切换（切换 2） JVM 处理代码逻辑并发送 write() 系统调用，OS 上下文切换到内核模式（切换3）并从用户空间缓冲区复制数据到内核空间缓冲区（拷贝3） 将内核空间缓冲区中的数据写到 hardware（拷贝4），write 系统调用返回，导致内核空间到用户空间的再次上下文切换（切换4） 流程图中的箭头反过来也成立，可以从网卡获取数据 read 调用图示：read、write 都是系统调用指令 mmapmmap（Memory Mapped Files）内存映射加 write 实现零拷贝，零拷贝就是没有数据从内核空间复制到用户空间 用户空间和内核空间都使用内存，所以可以共享同一块物理内存地址，省去用户态和内核态之间的拷贝。写网卡时，共享空间的内容拷贝到 Socket 缓冲区，然后交给 DMA 发送到网卡，只需要 3 次复制 进行了 4 次用户空间与内核空间的上下文切换，以及 3 次数据拷贝（2 次 DMA，一次 CPU 复制）： 发出 mmap 系统调用，DMA 拷贝到内核缓冲区，映射到共享缓冲区；mmap 系统调用返回，无需拷贝 发出 write 系统调用，将数据从内核缓冲区拷贝到内核 Socket 缓冲区；write 系统调用返回，DMA 将内核空间 Socket 缓冲区中的数据传递到协议引擎 原理：利用操作系统的 Page 来实现文件到物理内存的直接映射，完成映射后对物理内存的操作会被同步到硬盘上 缺点：不可靠，写到 mmap 中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用 flush 的时候才把数据真正的写到硬盘 Java NIO 提供了 MappedByteBuffer 类可以用来实现 mmap 内存映射，MappedByteBuffer 类对象只能通过调用 FileChannel.map() 获取 sendfilesendfile 实现零拷贝，打开文件的文件描述符 fd 和 socket 的 fd 传递给 sendfile，然后经过 3 次复制和 2 次用户态和内核态的切换 原理：数据根本不经过用户态，直接从内核缓冲区进入到 Socket Buffer，由于和用户态完全无关，就减少了两次上下文切换 说明：零拷贝技术是不允许进程对文件内容作进一步的加工的，比如压缩数据再发送 sendfile2.4 之后，sendfile 实现了更简单的方式，文件到达内核缓冲区后，不必再将数据全部复制到 socket buffer 缓冲区，而是只将记录数据位置和长度相关等描述符信息保存到 socket buffer，DMA 根据 Socket 缓冲区中描述符提供的位置和偏移量信息直接将内核空间缓冲区中的数据拷贝到协议引擎上（2 次复制 2 次切换） Java NIO 对 sendfile 的支持是 FileChannel.transferTo()/transferFrom()，把磁盘文件读取 OS 内核缓冲区后的 fileChannel，直接转给 socketChannel 发送，底层就是 sendfile 参考文章：https://blog.csdn.net/hancoder/article/details/112149121 BIOInet一个 InetAddress 类的对象就代表一个 IP 地址对象 成员方法： static InetAddress getLocalHost()：获得本地主机 IP 地址对象 static InetAddress getByName(String host)：根据 IP 地址字符串或主机名获得对应的 IP 地址对象 String getHostName()：获取主机名 String getHostAddress()：获得 IP 地址字符串 12345678910111213141516171819public class InetAddressDemo &#123; public static void main(String[] args) throws Exception &#123; // 1.获取本机地址对象 InetAddress ip = InetAddress.getLocalHost(); System.out.println(ip.getHostName());//DESKTOP-NNMBHQR System.out.println(ip.getHostAddress());//192.168.11.1 // 2.获取域名ip对象 InetAddress ip2 = InetAddress.getByName(&quot;www.baidu.com&quot;); System.out.println(ip2.getHostName());//www.baidu.com System.out.println(ip2.getHostAddress());//14.215.177.38 // 3.获取公网IP对象。 InetAddress ip3 = InetAddress.getByName(&quot;182.61.200.6&quot;); System.out.println(ip3.getHostName());//182.61.200.6 System.out.println(ip3.getHostAddress());//182.61.200.6 // 4.判断是否能通： ping 5s之前测试是否可通 System.out.println(ip2.isReachable(5000)); // ping百度 &#125;&#125; UDP基本介绍UDP（User Datagram Protocol）协议的特点： 面向无连接的协议，发送端只管发送，不确认对方是否能收到，速度快，但是不可靠，会丢失数据 尽最大努力交付，没有拥塞控制 基于数据包进行数据传输，发送数据的包的大小限制 64KB 以内 支持一对一、一对多、多对一、多对多的交互通信 UDP 协议的使用场景：在线视频、网络语音、电话 实现UDPUDP 协议相关的两个类： DatagramPacket（数据包对象）：用来封装要发送或要接收的数据，比如：集装箱 DatagramSocket（发送对象）：用来发送或接收数据包，比如：码头 DatagramPacket： DatagramPacket 类： public new DatagramPacket(byte[] buf, int length, InetAddress address, int port)：创建发送端数据包对象 buf：要发送的内容，字节数组 length：要发送内容的长度，单位是字节 address：接收端的IP地址对象 port：接收端的端口号 public new DatagramPacket(byte[] buf, int length)：创建接收端的数据包对象 buf：用来存储接收到内容 length：能够接收内容的长度 DatagramPacket 类常用方法： public int getLength()：获得实际接收到的字节个数 public byte[] getData()：返回数据缓冲区 DatagramSocket： DatagramSocket 类构造方法： protected DatagramSocket()：创建发送端的 Socket 对象，系统会随机分配一个端口号 protected DatagramSocket(int port)：创建接收端的 Socket 对象并指定端口号 DatagramSocket 类成员方法： public void send(DatagramPacket dp)：发送数据包 public void receive(DatagramPacket p)：接收数据包 public void close()：关闭数据报套接字 1234567891011121314151617181920212223242526272829303132333435public class UDPClientDemo &#123; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;===启动客户端===&quot;); // 1.创建一个集装箱对象，用于封装需要发送的数据包! byte[] buffer = &quot;我学Java&quot;.getBytes(); DatagramPacket packet = new DatagramPacket(buffer,bubffer.length,InetAddress.getLoclHost,8000); // 2.创建一个码头对象 DatagramSocket socket = new DatagramSocket(); // 3.开始发送数据包对象 socket.send(packet); socket.close(); &#125;&#125;public class UDPServerDemo&#123; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;==启动服务端程序==&quot;); // 1.创建一个接收客户都端的数据包对象（集装箱） byte[] buffer = new byte[1024*64]; DatagramPacket packet = new DatagramPacket(buffer, bubffer.length); // 2.创建一个接收端的码头对象 DatagramSocket socket = new DatagramSocket(8000); // 3.开始接收 socket.receive(packet); // 4.从集装箱中获取本次读取的数据量 int len = packet.getLength(); // 5.输出数据 // String rs = new String(socket.getData(), 0, len) String rs = new String(buffer , 0 , len); System.out.println(rs); // 6.服务端还可以获取发来信息的客户端的IP和端口。 String ip = packet.getAddress().getHostAdress(); int port = packet.getPort(); socket.close(); &#125;&#125; 通讯方式UDP 通信方式： 单播：用于两个主机之间的端对端通信 组播：用于对一组特定的主机进行通信 IP : 224.0.1.0 Socket 对象 : MulticastSocket 广播：用于一个主机对整个局域网上所有主机上的数据通信 IP : 255.255.255.255 Socket 对象 : DatagramSocket TCP基本介绍TCP&#x2F;IP (Transfer Control Protocol) 协议，传输控制协议 TCP&#x2F;IP 协议的特点： 面向连接的协议，提供可靠交互，速度慢 点对点的全双工通信 通过三次握手建立连接，连接成功形成数据传输通道；通过四次挥手断开连接 基于字节流进行数据传输，传输数据大小没有限制 TCP 协议的使用场景：文件上传和下载、邮件发送和接收、远程登录 注意：TCP 不会为没有数据的 ACK 超时重传 推荐阅读：https://yuanrengu.com/2020/77eef79f.html SocketTCP 通信也叫 Socket 网络编程，只要代码基于 Socket 开发，底层就是基于了可靠传输的 TCP 通信 双向通信：Java Socket 是全双工的，在任意时刻，线路上存在 A -&gt; B 和 B -&gt; A 的双向信号传输，即使是阻塞 IO，读和写也是可以同时进行的，只要分别采用读线程和写线程即可，读不会阻塞写、写也不会阻塞读 TCP 协议相关的类： Socket：一个该类的对象就代表一个客户端程序。 ServerSocket：一个该类的对象就代表一个服务器端程序。 Socket 类： 构造方法： Socket(InetAddress address,int port)：创建流套接字并将其连接到指定 IP 指定端口号 Socket(String host, int port)：根据 IP 地址字符串和端口号创建客户端 Socket 对象 注意事项：执行该方法，就会立即连接指定的服务器，连接成功，则表示三次握手通过，反之抛出异常 常用 API： OutputStream getOutputStream()：获得字节输出流对象 InputStream getInputStream()：获得字节输入流对象 void shutdownInput()：停止接受 void shutdownOutput()：停止发送数据，终止通信 SocketAddress getRemoteSocketAddress() ：返回套接字连接到的端点的地址，未连接返回 null ServerSocket 类： 构造方法：public ServerSocket(int port) 常用 API：public Socket accept()，阻塞等待接收一个客户端的 Socket 管道连接请求，连接成功返回一个 Socket 对象 三次握手后 TCP 连接建立成功，服务器内核会把连接从 SYN 半连接队列（一次握手时在服务端建立的队列）中移出，移入 accept 全连接队列，等待进程调用 accept 函数时把连接取出。如果进程不能及时调用 accept 函数，就会造成 accept 队列溢出，最终导致建立好的 TCP 连接被丢弃 相当于客户端和服务器建立一个数据管道（虚连接，不是真正的物理连接），管道一般不用 close 实现TCP开发流程客户端的开发流程： 客户端要请求于服务端的 Socket 管道连接 从 Socket 通信管道中得到一个字节输出流 通过字节输出流给服务端写出数据 服务端的开发流程： 用 ServerSocket 注册端口 接收客户端的 Socket 管道连接 从 Socket 通信管道中得到一个字节输入流 从字节输入流中读取客户端发来的数据 如果输出缓冲区空间不够存放主机发送的数据，则会被阻塞，输入缓冲区同理 缓冲区不属于应用程序，属于内核 TCP 从输出缓冲区读取数据会加锁阻塞线程 实现通信需求一：客户端发送一行数据，服务端接收一行数据 1234567891011121314151617181920212223242526272829303132public class ClientDemo &#123; public static void main(String[] args) throws Exception &#123; // 1.客户端要请求于服务端的socket管道连接。 Socket socket = new Socket(&quot;127.0.0.1&quot;, 8080); // 2.从socket通信管道中得到一个字节输出流 OutputStream os = socket.getOutputStream(); // 3.把低级的字节输出流包装成高级的打印流。 PrintStream ps = new PrintStream(os); // 4.开始发消息出去 ps.println(&quot;我是客户端&quot;); ps.flush();//一般不关闭IO流 System.out.println(&quot;客户端发送完毕~~~~&quot;); &#125;&#125;public class ServerDemo&#123; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;----服务端启动----&quot;); // 1.注册端口: public ServerSocket(int port) ServerSocket serverSocket = new ServerSocket(8080); // 2.开始等待接收客户端的Socket管道连接。 Socket socket = serverSocket.accept(); // 3.从socket通信管道中得到一个字节输入流。 InputStream is = socket.getInputStream(); // 4.把字节输入流转换成字符输入流 BufferedReader br = new BufferedReader(new InputStreamReader(is)); // 6.按照行读取消息 。 String line; if((line = br.readLine()) != null)&#123; System.out.println(line); &#125; &#125;&#125; 需求二：客户端可以反复发送数据，服务端可以反复数据 1234567891011121314151617181920212223242526272829303132333435public class ClientDemo &#123; public static void main(String[] args) throws Exception &#123; // 1.客户端要请求于服务端的socket管道连接。 Socket socket = new Socket(&quot;127.0.0.1&quot;,8080); // 2.从socket通信管道中得到一个字节输出流 OutputStream os = socket.getOutputStream(); // 3.把低级的字节输出流包装成高级的打印流。 PrintStream ps = new PrintStream(os); // 4.开始发消息出去 while(true)&#123; Scanner sc = new Scanner(System.in); System.out.print(&quot;请说：&quot;); ps.println(sc.nextLine()); ps.flush(); &#125; &#125;&#125;public class ServerDemo&#123; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;----服务端启动----&quot;); // 1.注册端口: public ServerSocket(int port) ServerSocket serverSocket = new ServerSocket(8080); // 2.开始等待接收客户端的Socket管道连接。 Socket socket = serverSocket.accept(); // 3.从socket通信管道中得到一个字节输入流。 InputStream is = socket.getInputStream(); // 4.把字节输入流转换成字符输入流 BufferedReader br = new BufferedReader(new InputStreamReader(is)); // 6.按照行读取消息 。 String line; while((line = br.readLine()) != null)&#123; System.out.println(line); &#125; &#125;&#125; 需求三：实现一个服务端可以同时接收多个客户端的消息 123456789101112131415161718192021222324252627282930313233343536373839404142public class ClientDemo &#123; public static void main(String[] args) throws Exception &#123; Socket socket = new Socket(&quot;127.0.0.1&quot;,8080); OutputStream os = new socket.getOutputStream(); PrintStream ps = new PrintStream(os); while(true)&#123; Scanner sc = new Scanner(System.in); System.out.print(&quot;请说：&quot;); ps.println(sc.nextLine()); ps.flush(); &#125; &#125;&#125;public class ServerDemo&#123; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;----服务端启动----&quot;); ServerSocket serverSocket = new ServerSocket(8080); while(true)&#123; // 开始等待接收客户端的Socket管道连接。 Socket socket = serverSocket.accept(); // 每接收到一个客户端必须为这个客户端管道分配一个独立的线程来处理与之通信。 new ServerReaderThread(socket).start(); &#125; &#125;&#125;class ServerReaderThread extends Thread&#123; privat Socket socket; public ServerReaderThread(Socket socket)&#123;this.socket = socket;&#125; @Override public void run() &#123; try(InputStream is = socket.getInputStream(); BufferedReader br = new BufferedReader(new InputStreamReader(is)) )&#123; String line; while((line = br.readLine()) != null)&#123; sout(socket.getRemoteSocketAddress() + &quot;:&quot; + line); &#125; &#125;catch(Exception e)&#123; sout(socket.getRemoteSocketAddress() + &quot;下线了~~~~~~&quot;); &#125; &#125;&#125; 伪异步一个客户端要一个线程，并发越高系统瘫痪的越快，可以在服务端引入线程池，使用线程池来处理与客户端的消息通信 优势：不会引起系统的死机，可以控制并发线程的数量 劣势：同时可以并发的线程将受到限制 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class BIOServer &#123; public static void main(String[] args) throws Exception &#123; //线程池机制 //创建一个线程池，如果有客户端连接，就创建一个线程，与之通讯(单独写一个方法) ExecutorService newCachedThreadPool = Executors.newCachedThreadPool(); //创建ServerSocket ServerSocket serverSocket = new ServerSocket(6666); System.out.println(&quot;服务器启动了&quot;); while (true) &#123; System.out.println(&quot;线程名字 = &quot; + Thread.currentThread().getName()); //监听，等待客户端连接 System.out.println(&quot;等待连接....&quot;); final Socket socket = serverSocket.accept(); System.out.println(&quot;连接到一个客户端&quot;); //创建一个线程，与之通讯 newCachedThreadPool.execute(new Runnable() &#123; public void run() &#123; //可以和客户端通讯 handler(socket); &#125; &#125;); &#125; &#125; //编写一个handler方法，和客户端通讯 public static void handler(Socket socket) &#123; try &#123; System.out.println(&quot;线程名字 = &quot; + Thread.currentThread().getName()); byte[] bytes = new byte[1024]; //通过socket获取输入流 InputStream inputStream = socket.getInputStream(); int len; //循环的读取客户端发送的数据 while ((len = inputStream.read(bytes)) != -1) &#123; System.out.println(&quot;线程名字 = &quot; + Thread.currentThread().getName()); //输出客户端发送的数据 System.out.println(new String(bytes, 0, read)); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(&quot;关闭和client的连接&quot;); try &#123; socket.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 文件传输字节流客户端：本地图片: ‪E:\\seazean\\图片资源\\beautiful.jpg服务端：服务器路径：E:\\seazean\\图片服务器 UUID. randomUUID() : 方法生成随机的文件名 **socket.shutdownOutput()**：这个必须执行，不然服务器会一直循环等待数据，最后文件损坏，程序报错 1234567891011121314151617181920212223242526272829//常量包public class Constants &#123; public static final String SRC_IMAGE = &quot;D:\\\\seazean\\\\图片资源\\\\beautiful.jpg&quot;; public static final String SERVER_DIR = &quot;D:\\\\seazean\\\\图片服务器\\\\&quot;; public static final String SERVER_IP = &quot;127.0.0.1&quot;; public static final int SERVER_PORT = 8888;&#125;public class ClientDemo &#123; public static void main(String[] args) throws Exception &#123; Socket socket = new Socket(Constants.ERVER_IP,Constants.SERVER_PORT); BufferedOutputStream bos=new BufferedOutputStream(socket.getOutputStream()); //提取本机的图片上传给服务端。Constants.SRC_IMAGE BufferedInputStream bis = new BufferedInputStream(new FileInputStream()); byte[] buffer = new byte[1024]; int len ; while((len = bis.read(buffer)) != -1) &#123; bos.write(buffer, 0 ,len); &#125; bos.flush();// 刷新图片数据到服务端！！ socket.shutdownOutput();// 告诉服务端我的数据已经发送完毕，不要在等我了！ bis.close(); //等待着服务端的响应数据！！ BufferedReader br = new BufferedReader( new InputStreamReader(socket.getInputStream())); System.out.println(&quot;收到服务端响应：&quot;+br.readLine()); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243public class ServerDemo &#123; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;----服务端启动----&quot;); // 1.注册端口: ServerSocket serverSocket = new ServerSocket(Constants.SERVER_PORT); // 2.定义一个循环不断的接收客户端的连接请求 while(true)&#123; // 3.开始等待接收客户端的Socket管道连接。 Socket socket = serverSocket.accept(); // 4.每接收到一个客户端必须为这个客户端管道分配一个独立的线程来处理与之通信。 new ServerReaderThread(socket).start(); &#125; &#125;&#125;class ServerReaderThread extends Thread&#123; private Socket socket ; public ServerReaderThread(Socket socket)&#123;this.socket = socket;&#125; @Override public void run() &#123; try&#123; InputStream is = socket.getInputStream(); BufferedInputStream bis = new BufferedInputStream(is); BufferedOutputStream bos = new BufferedOutputStream( new FileOutputStream (Constants.SERVER_DIR+UUID.randomUUID().toString()+&quot;.jpg&quot;)); byte[] buffer = new byte[1024]; int len; while((len = bis.read(buffer)) != -1)&#123; bos.write(buffer,0,len); &#125; bos.close(); System.out.println(&quot;服务端接收完毕了！&quot;); // 4.响应数据给客户端 PrintStream ps = new PrintStream(socket.getOutputStream()); ps.println(&quot;您好，已成功接收您上传的图片！&quot;); ps.flush(); Thread.sleep(10000); &#125;catch (Exception e)&#123; sout(socket.getRemoteSocketAddress() + &quot;下线了&quot;); &#125; &#125;&#125; 数据流构造方法： DataOutputStream(OutputStream out) : 创建一个新的数据输出流，以将数据写入指定的底层输出流 DataInputStream(InputStream in) : 创建使用指定的底层 InputStream 的 DataInputStream 常用API： final void writeUTF(String str) : 使用机器无关的方式使用 UTF-8 编码将字符串写入底层输出流 final String readUTF() : 读取以 modified UTF-8 格式编码的 Unicode 字符串，返回 String 类型 1234567891011121314151617181920212223242526272829303132333435363738394041public class Client &#123; public static void main(String[] args) &#123; InputStream is = new FileInputStream(&quot;path&quot;); // 1、请求与服务端的Socket链接 Socket socket = new Socket(&quot;127.0.0.1&quot; , 8888); // 2、把字节输出流包装成一个数据输出流 DataOutputStream dos = new DataOutputStream(socket.getOutputStream()); // 3、先发送上传文件的后缀给服务端 dos.writeUTF(&quot;.png&quot;); // 4、把文件数据发送给服务端进行接收 byte[] buffer = new byte[1024]; int len; while((len = is.read(buffer)) &gt; 0 )&#123; dos.write(buffer , 0 , len); &#125; dos.flush(); Thread.sleep(10000); &#125;&#125;public class Server &#123; public static void main(String[] args) &#123; ServerSocket ss = new ServerSocket(8888); Socket socket = ss.accept(); // 1、得到一个数据输入流读取客户端发送过来的数据 DataInputStream dis = new DataInputStream(socket.getInputStream()); // 2、读取客户端发送过来的文件类型 String suffix = dis.readUTF(); // 3、定义一个字节输出管道负责把客户端发来的文件数据写出去 OutputStream os = new FileOutputStream(&quot;path&quot;+ UUID.randomUUID().toString()+suffix); // 4、从数据输入流中读取文件数据，写出到字节输出流中去 byte[] buffer = new byte[1024]; int len; while((len = dis.read(buffer)) &gt; 0)&#123; os.write(buffer,0, len); &#125; os.close(); System.out.println(&quot;服务端接收文件保存成功！&quot;); &#125;&#125; NIO基本介绍NIO的介绍： Java NIO（New IO、Java non-blocking IO），从 Java 1.4 版本开始引入的一个新的 IO API，可以替代标准的 Java IO API，NIO 支持面向缓冲区的、基于通道的 IO 操作，以更加高效的方式进行文件的读写操作 NIO 有三大核心部分：Channel（通道），Buffer（缓冲区），Selector（选择器） NIO 是非阻塞 IO，传统 IO 的 read 和 write 只能阻塞执行，线程在读写 IO 期间不能干其他事情，比如调用 socket.accept()，如果服务器没有数据传输过来，线程就一直阻塞，而 NIO 中可以配置 Socket 为非阻塞模式 NIO 可以做到用一个线程来处理多个操作的。假设有 1000 个请求过来，根据实际情况可以分配 20 或者 80 个线程来处理，不像之前的阻塞 IO 那样分配 1000 个 NIO 和 BIO 的比较： BIO 以流的方式处理数据，而 NIO 以块的方式处理数据，块 I&#x2F;O 的效率比流 I&#x2F;O 高很多 BIO 是阻塞的，NIO 则是非阻塞的 BIO 基于字节流和字符流进行操作，而 NIO 基于 Channel 和 Buffer 进行操作，数据从通道读取到缓冲区中，或者从缓冲区写入到通道中。Selector 用于监听多个通道的事件（比如：连接请求，数据到达等），因此使用单个线程就可以监听多个客户端通道 NIO BIO 面向缓冲区（Buffer） 面向流（Stream） 非阻塞（Non Blocking IO） 阻塞IO(Blocking IO) 选择器（Selectors） 实现原理NIO 三大核心部分：Channel (通道)、Buffer (缓冲区)、Selector (选择器) Buffer 缓冲区 缓冲区本质是一块可以写入数据、读取数据的内存，底层是一个数组，这块内存被包装成 NIO Buffer 对象，并且提供了方法用来操作这块内存，相比较直接对数组的操作，Buffer 的 API 更加容易操作和管理 Channel 通道 Java NIO 的通道类似流，不同的是既可以从通道中读取数据，又可以写数据到通道，流的读写通常是单向的，通道可以非阻塞读取和写入通道，支持读取或写入缓冲区，也支持异步地读写 Selector 选择器 Selector 是一个 Java NIO 组件，能够检查一个或多个 NIO 通道，并确定哪些通道已经准备好进行读取或写入，这样一个单独的线程可以管理多个 channel，从而管理多个网络连接，提高效率 NIO 的实现框架： 每个 Channel 对应一个 Buffer 一个线程对应 Selector ， 一个 Selector 对应多个 Channel（连接） 程序切换到哪个 Channel 是由事件决定的，Event 是一个重要的概念 Selector 会根据不同的事件，在各个通道上切换 Buffer 是一个内存块 ， 底层是一个数组 数据的读取写入是通过 Buffer 完成的 , BIO 中要么是输入流，或者是输出流，不能双向，NIO 的 Buffer 是可以读也可以写， flip() 切换 Buffer 的工作模式 Java NIO 系统的核心在于：通道和缓冲区，通道表示打开的 IO 设备（例如：文件、 套接字）的连接。若要使用 NIO 系统，获取用于连接 IO 设备的通道以及用于容纳数据的缓冲区，然后操作缓冲区，对数据进行处理。简而言之，Channel 负责传输， Buffer 负责存取数据 缓冲区基本介绍缓冲区（Buffer）：缓冲区本质上是一个可以读写数据的内存块，用于特定基本数据类型的容器，用于与 NIO 通道进行交互，数据是从通道读入缓冲区，从缓冲区写入通道中的 Buffer 底层是一个数组，可以保存多个相同类型的数据，根据数据类型不同 ，有以下 Buffer 常用子类：ByteBuffer、CharBuffer、ShortBuffer、IntBuffer、LongBuffer、FloatBuffer、DoubleBuffer 基本属性 容量（capacity）：作为一个内存块，Buffer 具有固定大小，缓冲区容量不能为负，并且创建后不能更改 限制 （limit）：表示缓冲区中可以操作数据的大小（limit 后数据不能进行读写），缓冲区的限制不能为负，并且不能大于其容量。写入模式，limit 等于 buffer 的容量；读取模式下，limit 等于写入的数据量 位置（position）：下一个要读取或写入的数据的索引，缓冲区的位置不能为负，并且不能大于其限制 标记（mark）与重置（reset）：标记是一个索引，通过 Buffer 中的 mark() 方法指定 Buffer 中一个特定的位置，可以通过调用 reset() 方法恢复到这个 position 位置、限制、容量遵守以下不变式： 0 &lt;&#x3D; position &lt;&#x3D; limit &lt;&#x3D; capacity 常用APIstatic XxxBuffer allocate(int capacity)：创建一个容量为 capacity 的 XxxBuffer 对象 Buffer 基本操作： 方法 说明 public Buffer clear() 清空缓冲区，不清空内容，将位置设置为零，限制设置为容量 public Buffer flip() 翻转缓冲区，将缓冲区的界限设置为当前位置，position 置 0 public int capacity() 返回 Buffer的 capacity 大小 public final int limit() 返回 Buffer 的界限 limit 的位置 public Buffer limit(int n) 设置缓冲区界限为 n public Buffer mark() 在此位置对缓冲区设置标记 public final int position() 返回缓冲区的当前位置 position public Buffer position(int n) 设置缓冲区的当前位置为n public Buffer reset() 将位置 position 重置为先前 mark 标记的位置 public Buffer rewind() 将位置设为为 0，取消设置的 mark public final int remaining() 返回当前位置 position 和 limit 之间的元素个数 public final boolean hasRemaining() 判断缓冲区中是否还有元素 public static ByteBuffer wrap(byte[] array) 将一个字节数组包装到缓冲区中 abstract ByteBuffer asReadOnlyBuffer() 创建一个新的只读字节缓冲区 public abstract ByteBuffer compact() 缓冲区当前位置与其限制（如果有）之间的字节被复制到缓冲区的开头 Buffer 数据操作： 方法 说明 public abstract byte get() 读取该缓冲区当前位置的单个字节，然后位置 + 1 public ByteBuffer get(byte[] dst) 读取多个字节到字节数组 dst 中 public abstract byte get(int index) 读取指定索引位置的字节，不移动 position public abstract ByteBuffer put(byte b) 将给定单个字节写入缓冲区的当前位置，position+1 public final ByteBuffer put(byte[] src) 将 src 字节数组写入缓冲区的当前位置 public abstract ByteBuffer put(int index, byte b) 将指定字节写入缓冲区的索引位置，不移动 position 提示：” ”，占用两个字节 读写数据使用 Buffer 读写数据一般遵循以下四个步骤： 写入数据到 Buffer 调用 flip()方法，转换为读取模式 从 Buffer 中读取数据 调用 buffer.clear() 方法清除缓冲区（不是清空了数据，只是重置指针） 12345678910111213141516171819202122232425262728293031323334353637383940414243public class TestBuffer &#123;\t@Test public void test()&#123; String str = &quot;seazean&quot;; //1. 分配一个指定大小的缓冲区 ByteBuffer buffer = ByteBuffer.allocate(1024); System.out.println(&quot;-----------------allocate()----------------&quot;); System.out.println(bufferf.position());//0 System.out.println(buffer.limit());//1024 System.out.println(buffer.capacity());//1024 //2. 利用 put() 存入数据到缓冲区中 buffer.put(str.getBytes()); System.out.println(&quot;-----------------put()----------------&quot;); System.out.println(bufferf.position());//7 System.out.println(buffer.limit());//1024 System.out.println(buffer.capacity());//1024 //3. 切换读取数据模式 buffer.flip(); System.out.println(&quot;-----------------flip()----------------&quot;); System.out.println(buffer.position());//0 System.out.println(buffer.limit());//7 System.out.println(buffer.capacity());//1024 //4. 利用 get() 读取缓冲区中的数据 byte[] dst = new byte[buffer.limit()]; buffer.get(dst); System.out.println(dst.length); System.out.println(new String(dst, 0, dst.length)); System.out.println(buffer.position());//7 System.out.println(buffer.limit());//7 //5. clear() : 清空缓冲区. 但是缓冲区中的数据依然存在，但是处于“被遗忘”状态 System.out.println(buffer.hasRemaining());//true buffer.clear(); System.out.println(buffer.hasRemaining());//true System.out.println(&quot;-----------------clear()----------------&quot;); System.out.println(buffer.position());//0 System.out.println(buffer.limit());//1024 System.out.println(buffer.capacity());//1024 &#125;&#125; 粘包拆包网络上有多条数据发送给服务端，数据之间使用 进行分隔，但这些数据在接收时，被进行了重新组合 123456// Hello,world // I&#x27;m zhangsan // How are you? ------ &gt; 黏包，半包// Hello,world I&#x27;m zhangsan Ho// w are you? 123456789101112131415161718192021222324252627public static void main(String[] args) &#123; ByteBuffer source = ByteBuffer.allocate(32); // 11 24 source.put(&quot;Hello,world I&#x27;m zhangsan Ho&quot;.getBytes()); split(source); source.put(&quot;w are you? haha! &quot;.getBytes()); split(source);&#125;private static void split(ByteBuffer source) &#123; source.flip(); int oldLimit = source.limit(); for (int i = 0; i &lt; oldLimit; i++) &#123; if (source.get(i) == &#x27; &#x27;) &#123; // 根据数据的长度设置缓冲区 ByteBuffer target = ByteBuffer.allocate(i + 1 - source.position()); // 0 ~ limit source.limit(i + 1); target.put(source); // 从source 读，向 target 写 // debugAll(target); 访问 buffer 的方法 source.limit(oldLimit); &#125; &#125; // 访问过的数据复制到开头 source.compact();&#125; 直接内存基本介绍Byte Buffer 有两种类型，一种是基于直接内存（也就是非堆内存），另一种是非直接内存（也就是堆内存） Direct Memory 优点： Java 的 NIO 库允许 Java 程序使用直接内存，使用 native 函数直接分配堆外内存 读写性能高，读写频繁的场合可能会考虑使用直接内存 大大提高 IO 性能，避免了在 Java 堆和 native 堆来回复制数据 直接内存缺点： 不能使用内核缓冲区 Page Cache 的缓存优势，无法缓存最近被访问的数据和使用预读功能 分配回收成本较高，不受 JVM 内存回收管理 可能导致 OutOfMemoryError 异常：OutOfMemoryError: Direct buffer memory 回收依赖 System.gc() 的调用，但这个调用 JVM 不保证执行、也不保证何时执行，行为是不可控的。程序一般需要自行管理，成对去调用 malloc、free 应用场景： 传输很大的数据文件，数据的生命周期很长，导致 Page Cache 没有起到缓存的作用，一般采用直接 IO 的方式 适合频繁的 IO 操作，比如网络并发场景 数据流的角度： 非直接内存的作用链：本地 IO → 内核缓冲区→ 用户（JVM）缓冲区 →内核缓冲区 → 本地 IO 直接内存是：本地 IO → 直接内存 → 本地 IO JVM 直接内存图解： 通信原理堆外内存不受 JVM GC 控制，可以使用堆外内存进行通信，防止 GC 后缓冲区位置发生变化的情况 NIO 使用的 SocketChannel 也是使用的堆外内存，源码解析： SocketChannel#write(java.nio.ByteBuffer) → SocketChannelImpl#write(java.nio.ByteBuffer) 12345public int write(ByteBuffer var1) throws IOException &#123; do &#123; var3 = IOUtil.write(this.fd, var1, -1L, nd); &#125; while(var3 == -3 &amp;&amp; this.isOpen());&#125; IOUtil#write(java.io.FileDescriptor, java.nio.ByteBuffer, long, sun.nio.ch.NativeDispatcher) 1234567891011121314static int write(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) &#123; // 【判断是否是直接内存，是则直接写出，不是则封装到直接内存】 if (var1 instanceof DirectBuffer) &#123; return writeFromNativeBuffer(var0, var1, var2, var4); &#125; else &#123; //.... // 从堆内buffer拷贝到堆外buffer ByteBuffer var8 = Util.getTemporaryDirectBuffer(var7); var8.put(var1); //... // 从堆外写到内核缓冲区 int var9 = writeFromNativeBuffer(var0, var8, var2, var4);\t&#125;&#125; 读操作相同 分配回收直接内存创建 Buffer 对象：static XxxBuffer allocateDirect(int capacity) DirectByteBuffer 源码分析： 1234567891011121314151617181920212223DirectByteBuffer(int cap) &#123; //.... long base = 0; try &#123; // 分配直接内存 base = unsafe.allocateMemory(size); &#125; // 内存赋值 unsafe.setMemory(base, size, (byte) 0); if (pa &amp;&amp; (base % ps != 0)) &#123; address = base + ps - (base &amp; (ps - 1)); &#125; else &#123; address = base; &#125; // 创建回收函数 cleaner = Cleaner.create(this, new Deallocator(base, size, cap));&#125;private static class Deallocator implements Runnable &#123; public void run() &#123; unsafe.freeMemory(address); //... &#125;&#125; 分配和回收原理： 使用了 Unsafe 对象的 allocateMemory 方法完成直接内存的分配，setMemory 方法完成赋值 ByteBuffer 的实现类内部，使用了 Cleaner（虚引用）来监测 ByteBuffer 对象，一旦 ByteBuffer 对象被垃圾回收，那么 ReferenceHandler 线程通过 Cleaner 的 clean 方法调用 Deallocator 的 run方法，最后通过 freeMemory 来释放直接内存 12345678910111213141516171819202122232425262728/** * 直接内存分配的底层原理：Unsafe */public class Demo1_27 &#123; static int _1Gb = 1024 * 1024 * 1024; public static void main(String[] args) throws IOException &#123; Unsafe unsafe = getUnsafe(); // 分配内存 long base = unsafe.allocateMemory(_1Gb); unsafe.setMemory(base, _1Gb, (byte) 0); System.in.read(); // 释放内存 unsafe.freeMemory(base); System.in.read(); &#125; public static Unsafe getUnsafe() &#123; try &#123; Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); f.setAccessible(true); Unsafe unsafe = (Unsafe) f.get(null); return unsafe; &#125; catch (NoSuchFieldException | IllegalAccessException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 共享内存FileChannel 提供 map 方法返回 MappedByteBuffer 对象，把文件映射到内存，通常情况可以映射整个文件，如果文件比较大，可以进行分段映射，完成映射后对物理内存的操作会被同步到硬盘上 FileChannel 中的成员属性： MapMode.mode：内存映像文件访问的方式，共三种： MapMode.READ_ONLY：只读，修改得到的缓冲区将导致抛出异常 MapMode.READ_WRITE：读&#x2F;写，对缓冲区的更改最终将写入文件，但此次修改对映射到同一文件的其他程序不一定是可见 MapMode.PRIVATE：私用，可读可写，但是修改的内容不会写入文件，只是 buffer 自身的改变 public final FileLock lock()：获取此文件通道的排他锁 MappedByteBuffer，可以让文件在直接内存（堆外内存）中进行修改，这种方式叫做内存映射，可以直接调用系统底层的缓存，没有 JVM 和 OS 之间的复制操作，提高了传输效率，作用： 可以用于进程间的通信，能达到共享内存页的作用，但在高并发下要对文件内存进行加锁，防止出现读写内容混乱和不一致性，Java 提供了文件锁 FileLock，但在父&#x2F;子进程中锁定后另一进程会一直等待，效率不高 读写那些太大而不能放进内存中的文件，分段映射 MappedByteBuffer 较之 ByteBuffer 新增的三个方法： final MappedByteBuffer force()：缓冲区是 READ_WRITE 模式下，对缓冲区内容的修改强制写入文件 final MappedByteBuffer load()：将缓冲区的内容载入物理内存，并返回该缓冲区的引用 final boolean isLoaded()：如果缓冲区的内容在物理内存中，则返回真，否则返回假 123456789101112131415161718192021222324public class MappedByteBufferTest &#123; public static void main(String[] args) throws Exception &#123; // 读写模式 RandomAccessFile ra = new RandomAccessFile(&quot;1.txt&quot;, &quot;rw&quot;); // 获取对应的通道 FileChannel channel = ra.getChannel(); /** * 参数1\tFileChannel.MapMode.READ_WRITE 使用的读写模式 * 参数2\t0: 文件映射时的起始位置 * 参数3\t5: 是映射到内存的大小（不是索引位置），即将 1.txt 的多少个字节映射到内存 * 可以直接修改的范围就是 0-5 * 实际类型 DirectByteBuffer */ MappedByteBuffer buffer = channel.map(FileChannel.MapMode.READ_WRITE, 0, 5); buffer.put(0, (byte) &#x27;H&#x27;); buffer.put(3, (byte) &#x27;9&#x27;); buffer.put(5, (byte) &#x27;Y&#x27;);\t//IndexOutOfBoundsException ra.close(); System.out.println(&quot;修改成功~~&quot;); &#125;&#125; 从硬盘上将文件读入内存，要经过文件系统进行数据拷贝，拷贝操作是由文件系统和硬件驱动实现。通过内存映射的方法访问硬盘上的文件，拷贝数据的效率要比 read 和 write 系统调用高： read() 是系统调用，首先将文件从硬盘拷贝到内核空间的一个缓冲区，再将这些数据拷贝到用户空间，实际上进行了两次数据拷贝 mmap() 也是系统调用，但没有进行数据拷贝，当缺页中断发生时，直接将文件从硬盘拷贝到共享内存，只进行了一次数据拷贝 注意：mmap 的文件映射，在 Full GC 时才会进行释放，如果需要手动清除内存映射文件，可以反射调用 sun.misc.Cleaner 方法 参考文章：https://www.jianshu.com/p/f90866dcbffc 通道基本介绍通道（Channel）：表示 IO 源与目标打开的连接，Channel 类似于传统的流，只不过 Channel 本身不能直接访问数据，Channel 只能与 Buffer 进行交互 NIO 的通道类似于流，但有些区别如下： 通道可以同时进行读写，而流只能读或者只能写 通道可以实现异步读写数据 通道可以从缓冲读数据，也可以写数据到缓冲 BIO 中的 Stream 是单向的，NIO 中的 Channel 是双向的，可以读操作，也可以写操作 Channel 在 NIO 中是一个接口：public interface Channel extends Closeable&#123;&#125; Channel 实现类： FileChannel：用于读取、写入、映射和操作文件的通道，只能工作在阻塞模式下 通过 FileInputStream 获取的 Channel 只能读 通过 FileOutputStream 获取的 Channel 只能写 通过 RandomAccessFile 是否能读写根据构造 RandomAccessFile 时的读写模式决定 DatagramChannel：通过 UDP 读写网络中的数据通道 SocketChannel：通过 TCP 读写网络中的数据 ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel 提示：ServerSocketChanne 类似 ServerSocket、SocketChannel 类似 Socket 常用API获取 Channel 方式： 对支持通道的对象调用 getChannel() 方法 通过通道的静态方法 open() 打开并返回指定通道 使用 Files 类的静态方法 newByteChannel() 获取字节通道 Channel 基本操作：读写都是相对于内存来看，也就是缓冲区 方法 说明 public abstract int read(ByteBuffer dst) 从 Channel 中读取数据到 ByteBuffer，从 position 开始储存 public final long read(ByteBuffer[] dsts) 将 Channel 中的数据分散到 ByteBuffer[] public abstract int write(ByteBuffer src) 将 ByteBuffer 中的数据写入 Channel，从 position 开始写出 public final long write(ByteBuffer[] srcs) 将 ByteBuffer[] 到中的数据聚集到 Channel public abstract long position() 返回此通道的文件位置 FileChannel position(long newPosition) 设置此通道的文件位置 public abstract long size() 返回此通道的文件的当前大小 SelectableChannel 的操作 API： 方法 说明 SocketChannel accept() 如果通道处于非阻塞模式，没有请求连接时此方法将立即返回 NULL，否则将阻塞直到有新的连接或发生 I&#x2F;O 错误，通过该方法返回的套接字通道将处于阻塞模式 SelectionKey register(Selector sel, int ops) 将通道注册到选择器上，并指定监听事件 SelectionKey register(Selector sel, int ops, Object att) 将通道注册到选择器上，并在当前通道绑定一个附件对象，Object 代表可以是任何类型 文件读写1234567891011121314151617181920212223242526272829303132public class ChannelTest &#123; @Test\tpublic void write() throws Exception&#123; // 1、字节输出流通向目标文件 FileOutputStream fos = new FileOutputStream(&quot;data01.txt&quot;); // 2、得到字节输出流对应的通道 【FileChannel】 FileChannel channel = fos.getChannel(); // 3、分配缓冲区 ByteBuffer buffer = ByteBuffer.allocate(1024); buffer.put(&quot;hello,黑马Java程序员！&quot;.getBytes()); // 4、把缓冲区切换成写出模式 buffer.flip(); channel.write(buffer); channel.close(); System.out.println(&quot;写数据到文件中！&quot;); &#125; @Test public void read() throws Exception &#123; // 1、定义一个文件字节输入流与源文件接通 FileInputStream fis = new FileInputStream(&quot;data01.txt&quot;); // 2、需要得到文件字节输入流的文件通道 FileChannel channel = fis.getChannel(); // 3、定义一个缓冲区 ByteBuffer buffer = ByteBuffer.allocate(1024); // 4、读取数据到缓冲区 channel.read(buffer); buffer.flip(); // 5、读取出缓冲区中的数据并输出即可 String rs = new String(buffer.array(),0,buffer.remaining()); System.out.println(rs); &#125;&#125; 文件复制Channel 的方法：sendfile 实现零拷贝 abstract long transferFrom(ReadableByteChannel src, long position, long count)：从给定的可读字节通道将字节传输到该通道的文件中 src：源通道 position：文件中要进行传输的位置，必须是非负的 count：要传输的最大字节数，必须是非负的 abstract long transferTo(long position, long count, WritableByteChannel target)：将该通道文件的字节传输到给定的可写字节通道。 position：传输开始的文件中的位置; 必须是非负的 count：要传输的最大字节数; 必须是非负的 target：目标通道 文件复制的两种方式： Buffer 使用上述两种方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class ChannelTest &#123; @Test public void copy1() throws Exception &#123; File srcFile = new File(&quot;C:\\\\壁纸.jpg&quot;); File destFile = new File(&quot;C:\\\\Users\\\\壁纸new.jpg&quot;); // 得到一个字节字节输入流 FileInputStream fis = new FileInputStream(srcFile); // 得到一个字节输出流 FileOutputStream fos = new FileOutputStream(destFile); // 得到的是文件通道 FileChannel isChannel = fis.getChannel(); FileChannel osChannel = fos.getChannel(); // 分配缓冲区 ByteBuffer buffer = ByteBuffer.allocate(1024); while(true)&#123; // 必须先清空缓冲然后再写入数据到缓冲区 buffer.clear(); // 开始读取一次数据 int flag = isChannel.read(buffer); if(flag == -1)&#123; break; &#125; // 已经读取了数据 ，把缓冲区的模式切换成可读模式 buffer.flip(); // 把数据写出到 osChannel.write(buffer); &#125; isChannel.close(); osChannel.close(); System.out.println(&quot;复制完成！&quot;); &#125; @Test\tpublic void copy02() throws Exception &#123; // 1、字节输入管道 FileInputStream fis = new FileInputStream(&quot;data01.txt&quot;); FileChannel isChannel = fis.getChannel(); // 2、字节输出流管道 FileOutputStream fos = new FileOutputStream(&quot;data03.txt&quot;); FileChannel osChannel = fos.getChannel(); // 3、复制 osChannel.transferFrom(isChannel,isChannel.position(),isChannel.size()); isChannel.close(); osChannel.close();\t&#125; @Test\tpublic void copy03() throws Exception &#123; // 1、字节输入管道 FileInputStream fis = new FileInputStream(&quot;data01.txt&quot;); FileChannel isChannel = fis.getChannel(); // 2、字节输出流管道 FileOutputStream fos = new FileOutputStream(&quot;data04.txt&quot;); FileChannel osChannel = fos.getChannel(); // 3、复制 isChannel.transferTo(isChannel.position() , isChannel.size() , osChannel); isChannel.close(); osChannel.close();\t&#125;&#125; 分散聚集分散读取（Scatter ）：是指把 Channel 通道的数据读入到多个缓冲区中去 聚集写入（Gathering ）：是指将多个 Buffer 中的数据聚集到 Channel 123456789101112131415161718192021222324252627public class ChannelTest &#123; @Test public void test() throws IOException&#123; // 1、字节输入管道 FileInputStream is = new FileInputStream(&quot;data01.txt&quot;); FileChannel isChannel = is.getChannel(); // 2、字节输出流管道 FileOutputStream fos = new FileOutputStream(&quot;data02.txt&quot;); FileChannel osChannel = fos.getChannel(); // 3、定义多个缓冲区做数据分散 ByteBuffer buffer1 = ByteBuffer.allocate(4); ByteBuffer buffer2 = ByteBuffer.allocate(1024); ByteBuffer[] buffers = &#123;buffer1 , buffer2&#125;; // 4、从通道中读取数据分散到各个缓冲区 isChannel.read(buffers); // 5、从每个缓冲区中查询是否有数据读取到了 for(ByteBuffer buffer : buffers)&#123; buffer.flip();// 切换到读数据模式 System.out.println(new String(buffer.array() , 0 , buffer.remaining())); &#125; // 6、聚集写入到通道 osChannel.write(buffers); isChannel.close(); osChannel.close(); System.out.println(&quot;文件复制~~&quot;); &#125;&#125; 选择器基本介绍选择器（Selector） 是 SelectableChannle 对象的多路复用器，Selector 可以同时监控多个通道的状况，利用 Selector 可使一个单独的线程管理多个 Channel，Selector 是非阻塞 IO 的核心 Selector 能够检测多个注册的通道上是否有事件发生（多个 Channel 以事件的方式可以注册到同一个 Selector)，如果有事件发生，就获取事件然后针对每个事件进行相应的处理，就可以只用一个单线程去管理多个通道，也就是管理多个连接和请求 只有在连接&#x2F;通道真正有读写事件发生时，才会进行读写，就大大地减少了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程 避免了多线程之间的上下文切换导致的开销 常用API创建 Selector：Selector selector = Selector.open(); 向选择器注册通道：SelectableChannel.register(Selector sel, int ops, Object att) 参数一：选择器，指定当前 Channel 注册到的选择器 参数二：选择器对通道的监听事件，监听的事件类型用四个常量表示 读 : SelectionKey.OP_READ （1） 写 : SelectionKey.OP_WRITE （4） 连接 : SelectionKey.OP_CONNECT （8） 接收 : SelectionKey.OP_ACCEPT （16） 若不止监听一个事件，使用位或操作符连接：int interest = SelectionKey.OP_READ | SelectionKey.OP_WRITE 参数三：可以关联一个附件，可以是任何对象 Selector API： 方法 说明 public static Selector open() 打开选择器 public abstract void close() 关闭此选择器 public abstract int select() 阻塞选择一组通道准备好进行 I&#x2F;O 操作的键 public abstract int select(long timeout) 阻塞等待 timeout 毫秒 public abstract int selectNow() 获取一下，不阻塞，立刻返回 public abstract Selector wakeup() 唤醒正在阻塞的 selector public abstract Set selectedKeys() 返回此选择器的选择键集 SelectionKey API: 方法 说明 public abstract void cancel() 取消该键的通道与其选择器的注册 public abstract SelectableChannel channel() 返回创建此键的通道，该方法在取消键之后仍将返回通道 public final Object attachment() 返回当前 key 关联的附件 public final boolean isAcceptable() 检测此密钥的通道是否已准备好接受新的套接字连接 public final boolean isConnectable() 检测此密钥的通道是否已完成或未完成其套接字连接操作 public final boolean isReadable() 检测此密钥的频道是否可以阅读 public final boolean isWritable() 检测此密钥的通道是否准备好进行写入 基本步骤： 12345678910//1.获取通道ServerSocketChannel ssChannel = ServerSocketChannel.open();//2.切换非阻塞模式ssChannel.configureBlocking(false);//3.绑定连接ssChannel.bin(new InetSocketAddress(9999));//4.获取选择器Selector selector = Selector.open();//5.将通道注册到选择器上，并且指定“监听接收事件”ssChannel.register(selector, SelectionKey.OP_ACCEPT); NIO实现常用API SelectableChannel_API 方法 说明 public final SelectableChannel configureBlocking(boolean block) 设置此通道的阻塞模式 public final SelectionKey register(Selector sel, int ops) 向给定的选择器注册此通道，并选择关注的的事件 SocketChannel_API： 方法 说明 public static SocketChannel open() 打开套接字通道 public static SocketChannel open(SocketAddress remote) 打开套接字通道并连接到远程地址 public abstract boolean connect(SocketAddress remote) 连接此通道的到远程地址 public abstract SocketChannel bind(SocketAddress local) 将通道的套接字绑定到本地地址 public abstract SocketAddress getLocalAddress() 返回套接字绑定的本地套接字地址 public abstract SocketAddress getRemoteAddress() 返回套接字连接的远程套接字地址 ServerSocketChannel_API： 方法 说明 public static ServerSocketChannel open() 打开服务器套接字通道 public final ServerSocketChannel bind(SocketAddress local) 将通道的套接字绑定到本地地址，并配置套接字以监听连接 public abstract SocketChannel accept() 接受与此通道套接字的连接，通过此方法返回的套接字通道将处于阻塞模式 如果 ServerSocketChannel 处于非阻塞模式，如果没有挂起连接，则此方法将立即返回 null 如果通道处于阻塞模式，如果没有挂起连接将无限期地阻塞，直到有新的连接或发生 I&#x2F;O 错误 代码实现服务端 ： 获取通道，当客户端连接服务端时，服务端会通过 ServerSocketChannel.accept 得到 SocketChannel 切换非阻塞模式 绑定连接 获取选择器 将通道注册到选择器上，并且指定监听接收事件 轮询式的获取选择器上已经准备就绪的事件 客户端： 获取通道：SocketChannel sc = SocketChannel.open(new InetSocketAddress(HOST, PORT)) 切换非阻塞模式 分配指定大小的缓冲区：ByteBuffer buffer = ByteBuffer.allocate(1024) 发送数据给服务端 37 行代码，如果判断条件改为 !&#x3D;-1，需要客户端 close 一下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class Server &#123; public static void main(String[] args)&#123; // 1、获取通道 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 2、切换为非阻塞模式 serverSocketChannel.configureBlocking(false); // 3、绑定连接的端口 serverSocketChannel.bind(new InetSocketAddress(9999)); // 4、获取选择器Selector Selector selector = Selector.open(); // 5、将通道都注册到选择器上去，并且开始指定监听接收事件 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); // 6、使用Selector选择器阻塞等待轮已经就绪好的事件 while (selector.select() &gt; 0) &#123; System.out.println(&quot;----开始新一轮的时间处理----&quot;); // 7、获取选择器中的所有注册的通道中已经就绪好的事件 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = selectionKeys.iterator(); // 8、开始遍历这些准备好的事件 while (it.hasNext()) &#123; SelectionKey key = it.next();// 提取当前这个事件 // 9、判断这个事件具体是什么 if (key.isAcceptable()) &#123; // 10、直接获取当前接入的客户端通道 SocketChannel socketChannel = serverSocketChannel.accept(); // 11 、切换成非阻塞模式 socketChannel.configureBlocking(false); /* ByteBuffer buffer = ByteBuffer.allocate(16); // 将一个 byteBuffer 作为附件【关联】到 selectionKey 上 SelectionKey scKey = sc.register(selector, 0, buffer); */ // 12、将本客户端通道注册到选择器 socketChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; // 13、获取当前选择器上的读就绪事件 SelectableChannel channel = key.channel(); SocketChannel socketChannel = (SocketChannel) channel; // 14、读取数据 ByteBuffer buffer = ByteBuffer.allocate(1024); // 获取关联的附件 // ByteBuffer buffer = (ByteBuffer) key.attachment(); int len; while ((len = socketChannel.read(buffer)) &gt; 0) &#123; buffer.flip(); System.out.println(socketChannel.getRemoteAddress() + &quot;:&quot; + new String(buffer.array(), 0, len)); buffer.clear();// 清除之前的数据 &#125; &#125; // 删除当前的 selectionKey，防止重复操作 it.remove(); &#125; &#125; &#125;&#125; 1234567891011121314151617181920public class Client &#123; public static void main(String[] args) throws Exception &#123; // 1、获取通道 SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); // 2、切换成非阻塞模式 socketChannel.configureBlocking(false); // 3、分配指定缓冲区大小 ByteBuffer buffer = ByteBuffer.allocate(1024); // 4、发送数据给服务端 Scanner sc = new Scanner(System.in); while (true)&#123; System.out.print(&quot;请说：&quot;); String msg = sc.nextLine(); buffer.put((&quot;Client：&quot; + msg).getBytes()); buffer.flip(); socketChannel.write(buffer); buffer.clear(); &#125; &#125;&#125; AIOJava AIO(NIO.2) ： AsynchronousI&#x2F;O，异步非阻塞，采用了 Proactor 模式。服务器实现模式为一个有效请求一个线程，客户端的 I&#x2F;O 请求都是由 OS 先完成了再通知服务器应用去启动线程进行处理 1234AIO异步非阻塞，基于NIO的，可以称之为NIO2.0 BIO NIO AIO Socket SocketChannel AsynchronousSocketChannelServerSocket ServerSocketChannel AsynchronousServerSocketChannel 当进行读写操作时，调用 API 的 read 或 write 方法，这两种方法均为异步的，完成后会主动调用回调函数： 对于读操作，当有流可读取时，操作系统会将可读的流传入 read 方法的缓冲区 对于写操作，当操作系统将 write 方法传递的流写入完毕时，操作系统主动通知应用程序 在 JDK1.7 中，这部分内容被称作 NIO.2，主要在 Java.nio.channels 包下增加了下面四个异步通道：AsynchronousSocketChannel、AsynchronousServerSocketChannel、AsynchronousFileChannel、AsynchronousDatagramChannel"},{"title":"SSM","path":"/wiki/javaNode/SSM.html","content":"MyBatis基本介绍ORM（Object Relational Mapping）： 对象关系映射，指的是持久化数据和实体对象的映射模式，解决面向对象与关系型数据库存在的互不匹配的现象 MyBatis： MyBatis 是一个优秀的基于 Java 的持久层框架，它内部封装了 JDBC，使开发者只需关注 SQL 语句本身，而不需要花费精力去处理加载驱动、创建连接、创建 Statement 等过程。 MyBatis 通过 XML 或注解的方式将要执行的各种 Statement 配置起来，并通过 Java 对象和 Statement 中 SQL 的动态参数进行映射生成最终执行的 SQL 语句。 MyBatis 框架执行 SQL 并将结果映射为 Java 对象并返回。采用 ORM 思想解决了实体和数据库映射的问题，对 JDBC 进行了封装，屏蔽了 JDBC 底层 API 的调用细节，使我们不用操作 JDBC API，就可以完成对数据库的持久化操作。 MyBatis 官网地址：http://www.mybatis.org/mybatis-3/ 参考视频：https://space.bilibili.com/37974444/ 基本操作相关APIResources：加载资源的工具类 InputStream getResourceAsStream(String fileName)：通过类加载器返回指定资源的字节流 参数 fileName 是放在 src 的核心配置文件名：MyBatisConfig.xml SqlSessionFactoryBuilder：构建器，用来获取 SqlSessionFactory 工厂对象 SqlSessionFactory build(InputStream is)：通过指定资源的字节输入流获取 SqlSession 工厂对象 SqlSessionFactory：获取 SqlSession 构建者对象的工厂接口 SqlSession openSession()：获取 SqlSession 构建者对象，并开启手动提交事务 SqlSession openSession(boolean)：获取 SqlSession 构建者对象，参数为 true 开启自动提交事务 SqlSession：构建者对象接口，用于执行 SQL、管理事务、接口代理 SqlSession 代表和数据库的一次会话，用完必须关闭 SqlSession 和 Connection 一样都是非线程安全，每次使用都应该去获取新的对象 注：update 数据需要提交事务，或开启默认提交 SqlSession 常用 API： 方法 说明 List selectList(String statement,Object parameter) 执行查询语句，返回List集合 T selectOne(String statement,Object parameter) 执行查询语句，返回一个结果对象 int insert(String statement,Object parameter) 执行新增语句，返回影响行数 int update(String statement,Object parameter) 执行删除语句，返回影响行数 int delete(String statement,Object parameter) 执行修改语句，返回影响行数 void commit() 提交事务 void rollback() 回滚事务 T getMapper(Class cls) 获取指定接口的代理实现类对象 void close() 释放资源 映射配置映射配置文件包含了数据和对象之间的映射关系以及要执行的 SQL 语句，放在 src 目录下 命名：StudentMapper.xml 映射配置文件的文件头： 1234&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; 根标签： ：核心根标签 namespace：属性，名称空间 功能标签： &lt; select &gt;：查询功能标签 ：新增功能标签 ：修改功能标签 ：删除功能标签 id：属性，唯一标识，配合名称空间使用 resultType：指定结果映射对象类型，和对应的方法的返回值类型（全限定名）保持一致，但是如果返回值是 List 则和其泛型保持一致 parameterType：指定参数映射对象类型，必须和对应的方法的参数类型（全限定名）保持一致 statementType：可选 STATEMENT，PREPARED 或 CALLABLE，默认值：PREPARED STATEMENT：直接操作 SQL，使用 Statement 不进行预编译，获取数据：$ PREPARED：预处理参数，使用 PreparedStatement 进行预编译，获取数据：# CALLABLE：执行存储过程，CallableStatement 参数获取方式： SQL 获取参数：#&#123;属性名&#125; 12345&lt;mapper namespace=&quot;StudentMapper&quot;&gt; &lt;select id=&quot;selectById&quot; resultType=&quot;student&quot; parameterType=&quot;int&quot;&gt; SELECT * FROM student WHERE id = #&#123;id&#125; &lt;/select&gt;&lt;mapper/&gt; 强烈推荐官方文档：https://mybatis.org/mybatis-3/zh/sqlmap-xml.html 核心配置核心配置文件包含了 MyBatis 最核心的设置和属性信息，如数据库的连接、事务、连接池信息等 命名：MyBatisConfig.xml 核心配置文件的文件头： 12&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt; 根标签： ：核心根标签 引入连接配置文件： ： 引入数据库连接配置文件标签 resource：属性，指定配置文件名 1&lt;properties resource=&quot;jdbc.properties&quot;/&gt; 调整设置 ：可以改变 Mybatis 运行时行为 起别名： ：为全类名起别名的父标签 ：为全类名起别名的子标签 type：指定全类名 alias：指定别名 ：为指定包下所有类起别名的子标签，别名就是类名，首字母小写 123456&lt;!--起别名--&gt;&lt;typeAliases&gt;\t&lt;typeAlias type=&quot;bean.Student&quot; alias=&quot;student&quot;/&gt;\t&lt;package name=&quot;com.seazean.bean&quot;/&gt; &lt;!--二选一--&gt;&lt;/typeAliase&gt; 自带别名： 别名 数据类型 string java.lang.String long java.lang.Lang int java.lang.Integer double java.lang.Double boolean java.lang.Boolean …. …… 配置环境，可以配置多个标签 ：配置数据库环境标签，default 属性指定哪个 environment ：配置数据库环境子标签，id 属性是唯一标识，与 default 对应 ：事务管理标签，type 属性默认 JDBC 事务 ：数据源标签 type 属性：POOLED 使用连接池（MyBatis 内置），UNPOOLED 不使用连接池 ：数据库连接信息标签。 name 属性取值：driver，url，username，password value 属性取值：与 name 对应 引入映射配置文件 ：引入映射配置文件标签 ：引入映射配置文件子标签 resource：属性指定映射配置文件的名称 url：引用网路路径或者磁盘路径下的 sql 映射文件 class：指定映射配置类 ：批量注册 参考官方文档：https://mybatis.org/mybatis-3/zh/configuration.html #{}和${}#{}：占位符，传入的内容会作为字符串加上引号，以预编译的方式传入，将 sql 中的 #{} 替换为 ? 号，调用 PreparedStatement 的 set 方法来赋值，有效的防止 SQL 注入，提高系统安全性 ${}：拼接符，传入的内容会直接替换拼接，不会加上引号，可能存在 sql 注入的安全隐患 能用 #{} 的地方就用 #{}，不用或少用 ${} 必须使用 ${} 的情况： 表名作参数时，如：SELECT * FROM $&#123;tableName&#125; order by 时，如：SELECT * FROM t_user ORDER BY $&#123;columnName&#125; sql 语句使用 #{}，properties 文件内容获取使用 ${} 日志文件在日常开发过程中，排查问题时需要输出 MyBatis 真正执行的 SQL 语句、参数、结果等信息，就可以借助 log4j 的功能来实现执行信息的输出。 在核心配置文件根标签内配置 log4j 1234&lt;!--配置LOG4J--&gt;&lt;settings&gt;\t&lt;setting name=&quot;logImpl&quot; value=&quot;log4j&quot;/&gt;&lt;/settings&gt; 在 src 目录下创建 log4j.properties 123456789101112# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n#输出到日志文件 #log4j.appender.file=org.apache.log4j.FileAppender #log4j.appender.file.File=../logs/iask.log #log4j.appender.file.layout=org.apache.log4j.PatternLayout #log4j.appender.file.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; %l %m%n pom.xml 12345678910&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt; 代码实现 实体类 123456public class Student &#123; private Integer id; private String name; private Integer age; .....&#125; StudentMapper 12345678910111213141516public interface StudentMapper &#123; //查询全部 public abstract List&lt;Student&gt; selectAll(); //根据id查询 public abstract Student selectById(Integer id); //新增数据 public abstract Integer insert(Student stu); //修改数据 public abstract Integer update(Student stu); //删除数据 public abstract Integer delete(Integer id);&#125; config.properties 1234driver=com.mysql.jdbc.Driverurl=jdbc:mysql://192.168.2.184:3306/db1username=rootpassword=123456 MyBatisConfig.xml 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;!--核心根标签--&gt;&lt;configuration&gt; &lt;!--引入数据库连接的配置文件--&gt; &lt;properties resource=&quot;jdbc.properties&quot;/&gt; &lt;!--配置LOG4J--&gt; &lt;settings&gt; &lt;setting name=&quot;logImpl&quot; value=&quot;log4j&quot;/&gt; &lt;/settings&gt; &lt;!--起别名--&gt; &lt;typeAliases&gt; &lt;typeAlias type=&quot;bean.Student&quot; alias=&quot;student&quot;/&gt; &lt;!--&lt;package name=&quot;bean&quot;/&gt;--&gt; &lt;/typeAliases&gt; &lt;!--配置数据库环境，可以多个环境，default指定哪个--&gt; &lt;environments default=&quot;mysql&quot;&gt; &lt;!--id属性唯一标识--&gt; &lt;environment id=&quot;mysql&quot;&gt; &lt;!--事务管理，type属性，默认JDBC事务--&gt; &lt;transactionManager type=&quot;JDBC&quot;&gt;&lt;/transactionManager&gt; &lt;!--数据源信息 type属性连接池--&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;!--property获取数据库连接的配置信息--&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!--引入映射配置文件--&gt; &lt;mappers&gt; &lt;!--mapper引入指定的映射配置 resource属性执行的映射配置文件的名称--&gt; &lt;mapper resource=&quot;StudentMapper.xml&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; StudentMapper.xml 123456789101112131415161718192021222324252627&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;StudentMapper&quot;&gt; &lt;select id=&quot;selectAll&quot; resultType=&quot;student&quot;&gt; SELECT * FROM student &lt;/select&gt; &lt;select id=&quot;selectById&quot; resultType=&quot;student&quot; parameterType=&quot;int&quot;&gt; SELECT * FROM student WHERE id = #&#123;id&#125; &lt;/select&gt; &lt;insert id=&quot;insert&quot; parameterType=&quot;student&quot;&gt; INSERT INTO student VALUES (#&#123;id&#125;,#&#123;name&#125;,#&#123;age&#125;) &lt;/insert&gt; &lt;update id=&quot;update&quot; parameterType=&quot;student&quot;&gt; UPDATE student SET name = #&#123;name&#125;, age = #&#123;age&#125; WHERE id = #&#123;id&#125; &lt;/update&gt; &lt;delete id=&quot;delete&quot; parameterType=&quot;student&quot;&gt; DELETE FROM student WHERE id = #&#123;id&#125; &lt;/delete&gt;&lt;/mapper&gt; 控制层测试代码：根据 id 查询 123456789101112131415161718192021@Testpublic void selectById() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = ssf.openSession(); //4.执行映射配置文件中的sql语句，并接收结果 Student stu = sqlSession.selectOne(&quot;StudentMapper.selectById&quot;, 3); //5.处理结果 System.out.println(stu); //6.释放资源 sqlSession.close(); is.close();&#125; 控制层测试代码：新增功能 123456789101112131415161718192021@Testpublic void insert() throws Exception&#123; //1.加载核心配置文件 //2.获取SqlSession工厂对象 //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = sqlSessionFactory.openSession(true); //4.执行映射配置文件中的sql语句，并接收结果 Student stu = new Student(5, &quot;周七&quot;, 27); int result = sqlSession.insert(&quot;StudentMapper.insert&quot;, stu); //5.提交事务 //sqlSession.commit(); //6.处理结果 System.out.println(result); //7.释放资源 sqlSession.close(); is.close();&#125; 批量操作三种方式实现批量操作： 标签属性：这种方式属于全局批量 123&lt;settings&gt; &lt;setting name=&quot;defaultExecutorType&quot; value=&quot;BATCH&quot;/&gt;&lt;/settings&gt; defaultExecutorType：配置默认的执行器 SIMPLE 就是普通的执行器（默认，每次执行都要重新设置参数） REUSE 执行器会重用预处理语句（只预设置一次参数，多次执行） BATCH 执行器不仅重用语句还会执行批量更新（只针对修改操作） SqlSession 会话内批量操作： 1234567891011121314151617181920public void testBatch() throws IOException&#123; SqlSessionFactory sqlSessionFactory = getSqlSessionFactory(); // 可以执行批量操作的sqlSession SqlSession openSession = sqlSessionFactory.openSession(ExecutorType.BATCH); long start = System.currentTimeMillis(); try&#123; EmployeeMapper mapper = openSession.getMapper(EmployeeMapper.class); for (int i = 0; i &lt; 10000; i++) &#123; mapper.addEmp(new Employee(UUID.randomUUID().toString().substring(0, 5), &quot;b&quot;, &quot;1&quot;)); &#125; openSession.commit(); long end = System.currentTimeMillis(); // 批量：（预编译sql一次==&gt;设置参数===&gt;10000次===&gt;执行1次（类似管道）） // 非批量：（预编译sql=设置参数=执行）==》10000 耗时更多 System.out.println(&quot;执行时长：&quot; + (end - start)); &#125;finally&#123; openSession.close(); &#125;&#125; Spring 配置文件方式（applicationContext.xml）： 12345&lt;!--配置一个可以进行批量执行的sqlSession --&gt;&lt;bean id=&quot;sqlSession&quot; class=&quot;org.mybatis.spring.SqlSessionTemplate&quot;&gt; &lt;constructor-arg name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactoryBean&quot;/&gt; &lt;constructor-arg name=&quot;executorType&quot; value=&quot;BATCH&quot;/&gt;&lt;/bean&gt; 12@Autowiredprivate SqlSession sqlSession; 代理开发代理规则分层思想：控制层（controller）、业务层（service）、持久层（dao） 调用流程： 传统方式实现 DAO 层，需要写接口和实现类。采用 Mybatis 的代理开发方式实现 DAO 层的开发，只需要编写 Mapper 接口（相当于 Dao 接口），由 Mybatis 框架根据接口定义创建接口的动态代理对象 接口开发方式： 定义接口 操作数据库，MyBatis 框架根据接口，通过动态代理的方式生成代理对象，负责数据库的操作 Mapper 接口开发需要遵循以下规范： Mapper.xml 文件中的 namespace 与 DAO 层 mapper 接口的全类名相同 Mapper.xml 文件中的增删改查标签的id属性和 DAO 层 Mapper 接口方法名相同 Mapper.xml 文件中的增删改查标签的 parameterType 属性和 DAO 层 Mapper 接口方法的参数相同 Mapper.xml 文件中的增删改查标签的 resultType 属性和 DAO 层 Mapper 接口方法的返回值相同 实现原理通过动态代理开发模式，只编写一个接口不写实现类，通过 getMapper() 方法最终获取到 MapperProxy 代理对象，而这个代理对象是 MyBatis 使用了 JDK 的动态代理技术生成的 动态代理实现类对象在执行方法时最终调用了 MapperMethod.execute() 方法，这个方法中通过 switch case 语句根据操作类型来判断是新增、修改、删除、查询操作，最后一步回到了 MyBatis 最原生的 SqlSession 方式来执行增删改查 代码实现： 12345678910111213141516171819202122232425262728293031323334353637public Student selectById(Integer id) &#123; Student stu = null; SqlSession sqlSession = null; InputStream is = null; try&#123; //1.加载核心配置文件 is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory s = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 sqlSession = s.openSession(true); //4.获取StudentMapper接口的实现类对象 StudentMapper mapper = sqlSession.getMapper(StudentMapper.class); //5.通过实现类对象调用方法，接收结果 stu = mapper.selectById(id); &#125; catch (Exception e) &#123; e.getMessage(); &#125; finally &#123; //6.释放资源 if(sqlSession != null) &#123; sqlSession.close(); &#125; if(is != null) &#123; try &#123; is.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; //7.返回结果 return stu;&#125; 结果映射相关标签：返回结果映射对象类型，和对应方法的返回值类型保持一致，但是如果返回值是 List 则和其泛型保持一致 ：返回一条记录的 Map，key 是列名，value 是对应的值，用来配置字段和对象属性的映射关系标签，结果映射（和 resultType 二选一） id 属性：唯一标识 type 属性：实体对象类型 autoMapping 属性：结果自动映射 内的核心配置文件标签： ：配置主键映射关系标签 ：配置非主键映射关系标签 column 属性：表中字段名称 property 属性： 实体对象变量名称 ：配置被包含单个对象的映射关系标签，嵌套封装结果集（多对一、一对一） property 属性：被包含对象的变量名，要进行映射的属性名 javaType 属性：被包含对象的数据类型，要进行映射的属性的类型（Java 中的 Bean 类） select 属性：加载复杂类型属性的映射语句的 ID，会从 column 属性指定的列中检索数据，作为参数传递给目标 select 语句 ：配置被包含集合对象的映射关系标签，嵌套封装结果集（一对多、多对多） property 属性：被包含集合对象的变量名 ofType 属性：集合中保存的对象数据类型 ：鉴别器，用来判断某列的值，根据得到某列的不同值做出不同自定义的封装行为 自定义封装规则可以将数据库中比较复杂的数据类型映射为 JavaBean 中的属性 嵌套查询子查询： 123456public class Blog &#123; private int id; private String msg; private Author author; // set + get&#125; 1234567891011&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot; autoMapping = &quot;true&quot;&gt; &lt;association property=&quot;author&quot; column=&quot;author_id&quot; javaType=&quot;Author&quot; select=&quot;selectAuthor&quot;/&gt;&lt;/resultMap&gt;&lt;select id=&quot;selectBlog&quot; resultMap=&quot;blogResult&quot;&gt; SELECT * FROM BLOG WHERE ID = #&#123;id&#125;&lt;/select&gt;&lt;select id=&quot;selectAuthor&quot; resultType=&quot;Author&quot;&gt; SELECT * FROM AUTHOR WHERE ID = #&#123;id&#125;&lt;/select&gt; 循环引用：通过缓存解决 123456&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot; autoMapping = &quot;true&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;collection property=&quot;comment&quot; ofType=&quot;Comment&quot;&gt; &lt;association property=&quot;blog&quot; javaType=&quot;Blog&quot; resultMap=&quot;blogResult&quot;/&gt;&lt;!--y--&gt; &lt;/collection&gt;&lt;/resultMap 多表查询一对一一对一实现： 数据准备 1234567891011121314CREATE TABLE person(\tid INT PRIMARY KEY AUTO_INCREMENT,\tname VARCHAR(20),\tage INT);INSERT INTO person VALUES (NULL,&#x27;张三&#x27;,23),(NULL,&#x27;李四&#x27;,24),(NULL,&#x27;王五&#x27;,25);CREATE TABLE card(\tid INT PRIMARY KEY AUTO_INCREMENT,\tnumber VARCHAR(30),\tpid INT,\tCONSTRAINT cp_fk FOREIGN KEY (pid) REFERENCES person(id));INSERT INTO card VALUES (NULL,&#x27;12345&#x27;,1),(NULL,&#x27;23456&#x27;,2),(NULL,&#x27;34567&#x27;,3); bean 类 123456789101112public class Card &#123; private Integer id; //主键id private String number; //身份证号 private Person p; //所属人的对象 ......&#125;public class Person &#123; private Integer id; //主键id private String name; //人的姓名 private Integer age; //人的年龄&#125; 配置文件 OneToOneMapper.xml，MyBatisConfig.xml 需要引入（可以把 bean 包下起别名） 12345678910111213141516171819202122232425262728&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;OneToOneMapper&quot;&gt; &lt;!--配置字段和实体对象属性的映射关系--&gt; &lt;resultMap id=&quot;oneToOne&quot; type=&quot;card&quot;&gt; &lt;!--column 表中字段名称，property 实体对象变量名称--&gt; &lt;id column=&quot;cid&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;number&quot; property=&quot;number&quot; /&gt; &lt;!-- association：配置被包含对象的映射关系 property：被包含对象的变量名 javaType：被包含对象的数据类型 --&gt; &lt;association property=&quot;p&quot; javaType=&quot;bean.Person&quot;&gt; &lt;id column=&quot;pid&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot; /&gt; &lt;result column=&quot;age&quot; property=&quot;age&quot; /&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;oneToOne&quot;&gt; &lt;!--SQL--&gt; SELECT c.id cid,number,pid,NAME,age FROM card c,person p WHERE c.pid=p.id &lt;/select&gt;&lt;/mapper&gt; 核心配置文件 MyBatisConfig.xml 123456&lt;!-- mappers引入映射配置文件 --&gt;&lt;mappers&gt; &lt;mapper resource=&quot;one_to_one/OneToOneMapper.xml&quot;/&gt; &lt;mapper resource=&quot;one_to_many/OneToManyMapper.xml&quot;/&gt; &lt;mapper resource=&quot;many_to_many/ManyToManyMapper.xml&quot;/&gt;&lt;/mappers&gt; 测试类 12345678910111213141516171819202122232425262728public class Test01 &#123; @Test public void selectAll() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = ssf.openSession(true); //4.获取OneToOneMapper接口的实现类对象 OneToOneMapper mapper = sqlSession.getMapper(OneToOneMapper.class); //5.调用实现类的方法，接收结果 List&lt;Card&gt; list = mapper.selectAll(); //6.处理结果 for (Card c : list) &#123; System.out.println(c); &#125; //7.释放资源 sqlSession.close(); is.close(); &#125;&#125; 一对多一对多实现： 数据准备 1234567891011121314CREATE TABLE classes(\tid INT PRIMARY KEY AUTO_INCREMENT,\tname VARCHAR(20));INSERT INTO classes VALUES (NULL,&#x27;程序一班&#x27;),(NULL,&#x27;程序二班&#x27;)CREATE TABLE student(\tid INT PRIMARY KEY AUTO_INCREMENT,\tname VARCHAR(30),\tage INT,\tcid INT,\tCONSTRAINT cs_fk FOREIGN KEY (cid) REFERENCES classes(id));INSERT INTO student VALUES (NULL,&#x27;张三&#x27;,23,1),(NULL,&#x27;李四&#x27;,24,1),(NULL,&#x27;王五&#x27;,25,2); bean 类 1234567891011public class Classes &#123; private Integer id; //主键id private String name; //班级名称 private List&lt;Student&gt; students; //班级中所有学生对象 ........&#125;public class Student &#123; private Integer id; //主键id private String name; //学生姓名 private Integer age; //学生年龄&#125; 映射配置文件 12345678910111213141516&lt;mapper namespace=&quot;OneToManyMapper&quot;&gt; &lt;resultMap id=&quot;oneToMany&quot; type=&quot;bean.Classes&quot;&gt; &lt;id column=&quot;cid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;cname&quot; property=&quot;name&quot;/&gt; &lt;!--collection：配置被包含的集合对象映射关系--&gt; &lt;collection property=&quot;students&quot; ofType=&quot;bean.Student&quot;&gt; &lt;id column=&quot;sid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;sname&quot; property=&quot;name&quot;/&gt; &lt;result column=&quot;sage&quot; property=&quot;age&quot;/&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;oneToMany&quot;&gt; &lt;!--SQL--&gt; SELECT c.id cid,c.name cname,s.id sid,s.name sname,s.age sage FROM classes c,student s WHERE c.id=s.cid &lt;/select&gt;&lt;/mapper&gt; 代码实现片段 1234567891011121314//4.获取OneToManyMapper接口的实现类对象OneToManyMapper mapper = sqlSession.getMapper(OneToManyMapper.class);//5.调用实现类的方法，接收结果List&lt;Classes&gt; classes = mapper.selectAll();//6.处理结果for (Classes cls : classes) &#123; System.out.println(cls.getId() + &quot;,&quot; + cls.getName()); List&lt;Student&gt; students = cls.getStudents(); for (Student student : students) &#123; System.out.println(&quot;\\t&quot; + student); &#125;&#125; 多对多学生课程例子，中间表不需要 bean 实体类 数据准备 1234567891011121314CREATE TABLE course(\tid INT PRIMARY KEY AUTO_INCREMENT,\tname VARCHAR(20));INSERT INTO course VALUES (NULL,&#x27;语文&#x27;),(NULL,&#x27;数学&#x27;);CREATE TABLE stu_cr(\tid INT PRIMARY KEY AUTO_INCREMENT,\tsid INT,\tcid INT,\tCONSTRAINT sc_fk1 FOREIGN KEY (sid) REFERENCES student(id),\tCONSTRAINT sc_fk2 FOREIGN KEY (cid) REFERENCES course(id));INSERT INTO stu_cr VALUES (NULL,1,1),(NULL,1,2),(NULL,2,1),(NULL,2,2); bean类 12345678910public class Student &#123; private Integer id; //主键id private String name; //学生姓名 private Integer age; //学生年龄 private List&lt;Course&gt; courses; // 学生所选择的课程集合&#125;public class Course &#123; private Integer id; //主键id private String name; //课程名称&#125; 配置文件 123456789101112131415&lt;mapper namespace=&quot;ManyToManyMapper&quot;&gt; &lt;resultMap id=&quot;manyToMany&quot; type=&quot;Bean.Student&quot;&gt; &lt;id column=&quot;sid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;sname&quot; property=&quot;name&quot;/&gt; &lt;result column=&quot;sage&quot; property=&quot;age&quot;/&gt; &lt;collection property=&quot;courses&quot; ofType=&quot;Bean.Course&quot;&gt; &lt;id column=&quot;cid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;cname&quot; property=&quot;name&quot;/&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;manyToMany&quot;&gt; &lt;!--SQL--&gt; SELECT sc.sid,s.name sname,s.age sage,sc.cid,c.name cname FROM student s,course c,stu_cr sc WHERE sc.sid=s.id AND sc.cid=c.id &lt;/select&gt;&lt;/mapper&gt; 鉴别器需求：如果查询结果是女性，则把部门信息查询出来，否则不查询 ；如果是男性，把 last_name 这一列的值赋值 1234567891011121314151617181920&lt;!-- column：指定要判断的列名 javaType：列值对应的java类型 --&gt;&lt;discriminator javaType=&quot;string&quot; column=&quot;gender&quot;&gt; &lt;!-- 女生 --&gt; &lt;!-- resultType不可缺少，也可以使用resutlMap --&gt; &lt;case value=&quot;0&quot; resultType=&quot;com.bean.Employee&quot;&gt; &lt;association property=&quot;dept&quot; select=&quot;com.dao.DepartmentMapper.getDeptById&quot; column=&quot;d_id&quot;&gt; &lt;/association&gt; &lt;/case&gt; &lt;!-- 男生 --&gt; &lt;case value=&quot;1&quot; resultType=&quot;com.bean.Employee&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;last_name&quot; property=&quot;lastName&quot;/&gt; &lt;result column=&quot;gender&quot; property=&quot;gender&quot;/&gt; &lt;/case&gt;&lt;/discriminator&gt; 延迟加载两种加载立即加载：只要调用方法，马上发起查询 延迟加载：在需要用到数据时才进行加载，不需要用到数据时就不加载数据，延迟加载也称懒加载 优点： 先从单表查询，需要时再从关联表去关联查询，提高数据库性能，因为查询单表要比关联查询多张表速度要快，节省资源 坏处：只有当需要用到数据时，才会进行数据库查询，这样在大批量数据查询时，查询工作也要消耗时间，所以可能造成用户等待时间变长，造成用户体验下降 核心配置文件： 标签名 描述 默认值 lazyLoadingEnabled 延迟加载的全局开关。当开启时，所有关联对象都会延迟加载，特定关联关系中可通过设置 fetchType 属性来覆盖该项的开关状态。 false aggressiveLazyLoading 开启时，任一方法的调用都会加载该对象的所有延迟加载属性。否则每个延迟加载属性会按需加载（参考 lazyLoadTriggerMethods） false 1234&lt;settings&gt; &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;false&quot;/&gt; &lt;/settings&gt; assocation分布查询：先按照身份 id 查询所属人的 id、然后根据所属人的 id 去查询人的全部信息，这就是分步查询 映射配置文件 OneToOneMapper.xml 一对一映射： column 属性表示给要调用的其它的 select 标签传入的参数 select 属性表示调用其它的 select 标签 fetchType&#x3D;”lazy” 表示延迟加载（局部配置，只有配置了这个的地方才会延迟加载） 1234567891011121314151617&lt;mapper namespace=&quot;OneToOneMapper&quot;&gt; &lt;!--配置字段和实体对象属性的映射关系--&gt; &lt;resultMap id=&quot;oneToOne&quot; type=&quot;card&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;number&quot; property=&quot;number&quot; /&gt; &lt;association property=&quot;p&quot; javaType=&quot;bean.Person&quot; column=&quot;pid&quot; select=&quot;one_to_one.PersonMapper.findPersonByid&quot; fetchType=&quot;lazy&quot;&gt; &lt;!--需要配置新的映射文件--&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;oneToOne&quot;&gt; SELECT * FROM card &lt;!--查询全部，负责根据条件直接全部加载--&gt; &lt;/select&gt;&lt;/mapper&gt; PersonMapper.xml 12345&lt;mapper namespace=&quot;one_to_one.PersonMapper&quot;&gt; &lt;select id=&quot;findPersonByid&quot; parameterType=&quot;int&quot; resultType=&quot;person&quot;&gt; SELECT * FROM person WHERE id=#&#123;pid&#125; &lt;/select&gt;&lt;/mapper&gt; PersonMapper.java 123public interface PersonMapper &#123; User findPersonByid(int id);&#125; 测试文件 1234567891011121314151617public class Test01 &#123; @Test public void selectAll() throws Exception&#123; InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); SqlSession sqlSession = ssf.openSession(true); OneToOneMapper mapper = sqlSession.getMapper(OneToOneMapper.class); // 调用实现类的方法，接收结果 List&lt;Card&gt; list = mapper.selectAll(); // 不能遍历，遍历就是相当于使用了该数据，需要加载，不遍历就是没有使用。 // 释放资源 sqlSession.close(); is.close(); &#125;&#125; collection同样在一对多关系配置的 结点中配置延迟加载策略， 结点中也有 select 属性和 column 属性 映射配置文件 OneToManyMapper.xml 一对多映射： column 是用于指定使用哪个字段的值作为条件查询 select 是用于指定查询账户的唯一标识（账户的 dao 全限定类名加上方法名称） 123456789101112131415&lt;mapper namespace=&quot;OneToManyMapper&quot;&gt; &lt;resultMap id=&quot;oneToMany&quot; type=&quot;bean.Classes&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot;/&gt; &lt;!--collection：配置被包含的集合对象映射关系--&gt; &lt;collection property=&quot;students&quot; ofType=&quot;bean.Student&quot; column=&quot;id&quot; select=&quot;one_to_one.StudentMapper.findStudentByCid&quot;&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;oneToMany&quot;&gt; SELECT * FROM classes &lt;/select&gt;&lt;/mapper&gt; StudentMapper.xml 12345&lt;mapper namespace=&quot;one_to_one.StudentMapper&quot;&gt; &lt;select id=&quot;findPersonByCid&quot; parameterType=&quot;int&quot; resultType=&quot;student&quot;&gt; SELECT * FROM person WHERE cid=#&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 注解开发单表操作注解可以简化开发操作，省略映射配置文件的编写 常用注解： @Select(“查询的 SQL 语句”)：执行查询操作注解 @Insert(“插入的 SQL 语句”)：执行新增操作注解 @Update(“修改的 SQL 语句”)：执行修改操作注解 @Delete(“删除的 SQL 语句”)：执行删除操作注解 参数注解： @Param：当 SQL 语句需要多个（大于1）参数时，用来指定参数的对应规则 核心配置文件配置映射关系： 1234567&lt;mappers&gt;\t&lt;package name=&quot;使用了注解的Mapper接口所在包&quot;/&gt;&lt;/mappers&gt;&lt;!--或者--&gt;&lt;mappers&gt; &lt;mapper class=&quot;包名.Mapper名&quot;&gt;&lt;/mapper&gt;&lt;/mappers&gt; 基本增删改查： 创建 Mapper 接口 12345678910111213141516171819package mapper;public interface StudentMapper &#123; //查询全部 @Select(&quot;SELECT * FROM student&quot;) public abstract List&lt;Student&gt; selectAll(); //新增数据 @Insert(&quot;INSERT INTO student VALUES (#&#123;id&#125;,#&#123;name&#125;,#&#123;age&#125;)&quot;) public abstract Integer insert(Student student); //修改操作 @Update(&quot;UPDATE student SET name=#&#123;name&#125;,age=#&#123;age&#125; WHERE id=#&#123;id&#125;&quot;) public abstract Integer update(Student student); //删除操作 @Delete(&quot;DELETE FROM student WHERE id=#&#123;id&#125;&quot;) public abstract Integer delete(Integer id);&#125; 修改 MyBatis 的核心配置文件 123&lt;mappers&gt;\t&lt;package name=&quot;mapper&quot;/&gt;&lt;/mappers&gt; bean类 12345public class Student &#123; private Integer id; private String name; private Integer age;&#125; 测试类 1234567891011121314151617181920212223242526@Testpublic void selectAll() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = ssf.openSession(true); //4.获取StudentMapper接口的实现类对象 StudentMapper mapper = sqlSession.getMapper(StudentMapper.class); //5.调用实现类对象中的方法，接收结果 List&lt;Student&gt; list = mapper.selectAll(); //6.处理结果 for (Student student : list) &#123; System.out.println(student); &#125; //7.释放资源 sqlSession.close(); is.close();&#125; 多表操作相关注解实现复杂关系映射之前我们可以在映射文件中通过配置 来实现，使用注解开发后，可以使用 @Results 注解，@Result 注解，@One 注解，@Many 注解组合完成复杂关系的配置 注解 说明 @Results 代替 标签，注解中使用单个 @Result 注解或者 @Result 集合使用格式：@Results({ @Result(), @Result() })或@Results({ @Result() }) @Result 代替&lt; id&gt; 和 标签，@Result 中属性介绍：column：数据库的列名 property：封装类的变量名one：需要使用 @One 注解（@Result(one &#x3D; @One)）Many：需要使用 @Many 注解（@Result(many&#x3D; @Many)） @One(一对一) 代替 标签，多表查询的关键，用来指定子查询返回单一对象select：指定调用 Mapper 接口中的某个方法使用格式：@Result(column&#x3D;””, property&#x3D;””, one&#x3D;@One(select&#x3D;””)) @Many(多对一) 代替 标签，多表查询的关键，用来指定子查询返回对象集合select：指定调用 Mapper 接口中的某个方法使用格式：@Result(column&#x3D;””, property&#x3D;””, many&#x3D;@Many(select&#x3D;””)) 一对一身份证对人 PersonMapper 接口 12345public interface PersonMapper &#123; //根据id查询 @Select(&quot;SELECT * FROM person WHERE id=#&#123;id&#125;&quot;) public abstract Person selectById(Integer id);&#125; CardMapper接口 12345678910111213141516171819public interface CardMapper &#123; //查询全部 @Select(&quot;SELECT * FROM card&quot;) @Results(&#123; @Result(column = &quot;id&quot;,property = &quot;id&quot;), @Result(column = &quot;number&quot;,property = &quot;number&quot;), @Result( property = &quot;p&quot;, // 被包含对象的变量名 javaType = Person.class, // 被包含对象的实际数据类型 column = &quot;pid&quot;, // 根据查询出的card表中的pid字段来查询person表 /* one、@One 一对一固定写法 select属性：指定调用哪个接口中的哪个方法 */ one = @One(select = &quot;one_to_one.PersonMapper.selectById&quot;) ) &#125;) public abstract List&lt;Card&gt; selectAll();&#125; 测试类（详细代码参考单表操作） 12345678//1.加载核心配置文件//2.获取SqlSession工厂对象//3.通过工厂对象获取SqlSession对象//4.获取StudentMapper接口的实现类对象CardMapper mapper = sqlSession.getMapper(CardMapper.class);//5.调用实现类对象中的方法，接收结果List&lt;Card&gt; list = mapper.selectAll(); 一对多班级和学生 StudentMapper接口 12345public interface StudentMapper &#123; //根据cid查询student表 cid是外键约束列 @Select(&quot;SELECT * FROM student WHERE cid=#&#123;cid&#125;&quot;) public abstract List&lt;Student&gt; selectByCid(Integer cid);&#125; ClassesMapper接口 123456789101112131415public interface ClassesMapper &#123; //查询全部 @Select(&quot;SELECT * FROM classes&quot;) @Results(&#123; @Result(column = &quot;id&quot;, property = &quot;id&quot;), @Result(column = &quot;name&quot;, property = &quot;name&quot;), @Result( property = &quot;students&quot;, //被包含对象的变量名 javaType = List.class, //被包含对象的实际数据类型 column = &quot;id&quot;, //根据id字段查询student表 many = @Many(select = &quot;one_to_many.StudentMapper.selectByCid&quot;) ) &#125;) public abstract List&lt;Classes&gt; selectAll();&#125; 测试类 1234//4.获取StudentMapper接口的实现类对象ClassesMapper mapper = sqlSession.getMapper(ClassesMapper.class);//5.调用实现类对象中的方法，接收结果List&lt;Classes&gt; classes = mapper.selectAll(); 多对多学生和课程 SQL 查询语句 12SELECT DISTINCT s.id,s.name,s.age FROM student s,stu_cr sc WHERE sc.sid=s.idSELECT c.id,c.name FROM stu_cr sc,course c WHERE sc.cid=c.id AND sc.sid=#&#123;id&#125; CourseMapper 接口 12345public interface CourseMapper &#123; //根据学生id查询所选课程 @Select(&quot;SELECT c.id,c.name FROM stu_cr sc,course c WHERE sc.cid=c.id AND sc.sid=#&#123;id&#125;&quot;) public abstract List&lt;Course&gt; selectBySid(Integer id);&#125; StudentMapper 接口 1234567891011121314151617public interface StudentMapper &#123; //查询全部 @Select(&quot;SELECT DISTINCT s.id,s.name,s.age FROM student s,stu_cr sc WHERE sc.sid=s.id&quot;) @Results(&#123; @Result(column = &quot;id&quot;,property = &quot;id&quot;), @Result(column = &quot;name&quot;,property = &quot;name&quot;), @Result(column = &quot;age&quot;,property = &quot;age&quot;), @Result( property = &quot;courses&quot;, //被包含对象的变量名 javaType = List.class, //被包含对象的实际数据类型 column = &quot;id&quot;, //根据查询出的student表中的id字段查询中间表和课程表 many = @Many(select = &quot;many_to_many.CourseMapper.selectBySid&quot;) ) &#125;) public abstract List&lt;Student&gt; selectAll();&#125; 测试类 1234//4.获取StudentMapper接口的实现类对象StudentMapper mapper = sqlSession.getMapper(StudentMapper.class);//5.调用实现类对象中的方法，接收结果List&lt;Student&gt; students = mapper.selectAll(); 缓存机制缓存概述缓存：缓存就是一块内存空间，保存临时数据 作用：将数据源（数据库或者文件）中的数据读取出来存放到缓存中，再次获取时直接从缓存中获取，可以减少和数据库交互的次数，提升程序的性能 缓存适用： 适用于缓存的：经常查询但不经常修改的，数据的正确与否对最终结果影响不大的 不适用缓存的：经常改变的数据 , 敏感数据（例如：股市的牌价，银行的汇率，银行卡里面的钱）等等 缓存类别： 一级缓存：SqlSession 级别的缓存，又叫本地会话缓存，自带的（不需要配置），一级缓存的生命周期与 SqlSession 一致。在操作数据库时需要构造 SqlSession 对象，在对象中有一个数据结构（HashMap）用于存储缓存数据，不同的 SqlSession 之间的缓存数据区域是互相不影响的 二级缓存：mapper（namespace）级别的缓存，二级缓存的使用，需要手动开启（需要配置）。多个 SqlSession 去操作同一个 Mapper 的 SQL 可以共用二级缓存，二级缓存是跨 SqlSession 的 开启缓存：配置核心配置文件中 标签 cacheEnabled：true 表示全局性地开启所有映射器配置文件中已配置的任何缓存，默认 true 参考文章：https://www.cnblogs.com/ysocean/p/7342498.html 一级缓存一级缓存是 SqlSession 级别的缓存 工作流程：第一次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，如果没有，从数据库查询用户信息，得到用户信息，将用户信息存储到一级缓存中；第二次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，缓存中有，直接从缓存中获取用户信息。 一级缓存的失效： SqlSession 不同 SqlSession 相同，查询条件不同时（还未缓存该数据） SqlSession 相同，手动清除了一级缓存，调用 sqlSession.clearCache() SqlSession 相同，执行 commit 操作或者执行插入、更新、删除，清空 SqlSession 中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读 Spring 整合 MyBatis 后，一级缓存作用： 未开启事务的情况，每次查询 Spring 都会创建新的 SqlSession，因此一级缓存失效 开启事务的情况，Spring 使用 ThreadLocal 获取当前资源绑定同一个 SqlSession，因此此时一级缓存是有效的 测试一级缓存存在 123456789101112131415161718192021public void testFirstLevelCache()&#123; //1. 获取sqlSession对象 SqlSession sqlSession = SqlSessionFactoryUtils.openSession(); //2. 通过sqlSession对象获取UserDao接口的代理对象 UserDao userDao1 = sqlSession.getMapper(UserDao.class); //3. 调用UserDao接口的代理对象的findById方法获取信息\tUser user1 = userDao1.findById(1);\tSystem.out.println(user1); //sqlSession.clearCache() 清空缓存 UserDao userDao2 = sqlSession.getMapper(UserDao.class); User user = userDao.findById(1); System.out.println(user2); //4.测试两次结果是否一样 System.out.println(user1 == user2);//true //5. 提交事务关闭资源 SqlSessionFactoryUtils.commitAndClose(sqlSession);&#125; 二级缓存基本介绍二级缓存是 mapper 的缓存，只要是同一个命名空间（namespace）的 SqlSession 就共享二级缓存的内容，并且可以操作二级缓存 作用：作用范围是整个应用，可以跨线程使用，适合缓存一些修改较少的数据 工作流程：一个会话查询数据，这个数据就会被放在当前会话的一级缓存中，如果会话关闭或提交一级缓存中的数据会保存到二级缓存 二级缓存的基本使用： 在 MyBatisConfig.xml 文件开启二级缓存，cacheEnabled 默认值为 true，所以这一步可以省略不配置 1234&lt;!--配置开启二级缓存--&gt;&lt;settings&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; 配置 Mapper 映射文件 &lt;cache&gt; 标签表示当前这个 mapper 映射将使用二级缓存，区分的标准就看 mapper 的 namespace 值 12345&lt;mapper namespace=&quot;dao.UserDao&quot;&gt; &lt;!--开启user支持二级缓存--&gt; &lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;6000&quot; readOnly=&quot;&quot; size=&quot;1024&quot;/&gt;\t&lt;cache&gt;&lt;/cache&gt; &lt;!--则表示所有属性使用默认值--&gt;&lt;/mapper&gt; eviction（清除策略）： LRU – 最近最少使用：移除最长时间不被使用的对象，默认 FIFO – 先进先出：按对象进入缓存的顺序来移除它们 SOFT – 软引用：基于垃圾回收器状态和软引用规则移除对象 WEAK – 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象 flushInterval（刷新间隔）：可以设置为任意的正整数， 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新 size（引用数目）：缓存存放多少元素，默认值是 1024 readOnly（只读）：可以被设置为 true 或 false 只读的缓存会给所有调用者返回缓存对象的相同实例，因此这些对象不能被修改，促进了性能提升 可读写的缓存会（通过序列化）返回缓存对象的拷贝， 速度上会慢一些，但是更安全，因此默认值是 false type：指定自定义缓存的全类名，实现 Cache 接口即可 要进行二级缓存的类必须实现 java.io.Serializable 接口，可以使用序列化方式来保存对象。 1public class User implements Serializable&#123;&#125; 相关属性 select 标签的 useCache 属性 映射文件中的 &lt;select&gt; 标签中设置 useCache=&quot;true&quot; 代表当前 statement 要使用二级缓存（默认） 注意：如果每次查询都需要最新的数据 sql，要设置成 useCache&#x3D;false，禁用二级缓存 123&lt;select id=&quot;findAll&quot; resultType=&quot;user&quot; useCache=&quot;true&quot;&gt; select * from user&lt;/select&gt; 每个增删改标签都有 flushCache 属性，默认为 true，代表在执行增删改之后就会清除一、二级缓存，保证缓存的一致性；而查询标签默认值为 false，所以查询不会清空缓存 localCacheScope：本地缓存作用域， 中的配置项，默认值为 SESSION，当前会话的所有数据保存在会话缓存中，设置为 STATEMENT 禁用一级缓存 源码解析事务提交二级缓存才生效：DefaultSqlSession 调用 commit() 时会回调 executor.commit() CachingExecutor#query()：执行查询方法，查询出的数据会先放入 entriesToAddOnCommit 集合暂存 12345678// 从二缓存中获取数据，获取不到去一级缓存获取List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key);if (list == null) &#123; // 回调 BaseExecutor#query list = delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); // 将数据放入 entriesToAddOnCommit 集合暂存，此时还没放入二级缓存 tcm.putObject(cache, key, list);&#125; commit()：事务提交，清空一级缓存，放入二级缓存，二级缓存使用 TransactionalCacheManager（tcm）管理 12345public void commit(boolean required) throws SQLException &#123; // 首先调用 BaseExecutor#commit 方法，【清空一级缓存】 delegate.commit(required); tcm.commit();&#125; TransactionalCacheManager#commit：查询出的数据放入二级缓存 123456public void commit() &#123; // 获取所有的缓存事务，挨着进行提交 for (TransactionalCache txCache : transactionalCaches.values()) &#123; txCache.commit(); &#125;&#125; 123456789public void commit() &#123; if (clearOnCommit) &#123; delegate.clear(); &#125; // 将 entriesToAddOnCommit 中的数据放入二级缓存 flushPendingEntries(); // 清空相关集合 reset();&#125; 123456private void flushPendingEntries() &#123; for (Map.Entry&lt;Object, Object&gt; entry : entriesToAddOnCommit.entrySet()) &#123; // 将数据放入二级缓存 delegate.putObject(entry.getKey(), entry.getValue()); &#125;&#125; 增删改操作会清空缓存： update()：CachingExecutor 的更新操作 12345public int update(MappedStatement ms, Object parameterObject) throws SQLException &#123; flushCacheIfRequired(ms); // 回调 BaseExecutor#update 方法，也会清空一级缓存 return delegate.update(ms, parameterObject);&#125; flushCacheIfRequired()：判断是否需要清空二级缓存 12345678private void flushCacheIfRequired(MappedStatement ms) &#123; Cache cache = ms.getCache(); // 判断二级缓存是否存在，然后判断标签的 flushCache 的值，增删改操作的 flushCache 属性默认为 true if (cache != null &amp;&amp; ms.isFlushCacheRequired()) &#123; // 清空二级缓存 tcm.clear(cache); &#125;&#125; 自定义自定义缓存 1&lt;cache type=&quot;com.domain.something.MyCustomCache&quot;/&gt; type 属性指定的类必须实现 org.apache.ibatis.cache.Cache 接口，且提供一个接受 String 参数作为 id 的构造器 123456789public interface Cache &#123; String getId(); int getSize(); void putObject(Object key, Object value); Object getObject(Object key); boolean hasKey(Object key); Object removeObject(Object key); void clear();&#125; 缓存的配置，只需要在缓存实现中添加公有的 JavaBean 属性，然后通过 cache 元素传递属性值，例如在缓存实现上调用一个名为 setCacheFile(String file) 的方法： 123&lt;cache type=&quot;com.domain.something.MyCustomCache&quot;&gt; &lt;property name=&quot;cacheFile&quot; value=&quot;/tmp/my-custom-cache.tmp&quot;/&gt;&lt;/cache&gt; 可以使用所有简单类型作为 JavaBean 属性的类型，MyBatis 会进行转换。 可以使用占位符（如 $&#123;cache.file&#125;），以便替换成在配置文件属性中定义的值 MyBatis 支持在所有属性设置完毕之后，调用一个初始化方法， 如果想要使用这个特性，可以在自定义缓存类里实现 org.apache.ibatis.builder.InitializingObject 接口 123public interface InitializingObject &#123; void initialize() throws Exception;&#125; 注意：对缓存的配置（如清除策略、可读或可读写等），不能应用于自定义缓存 对某一命名空间的语句，只会使用该命名空间的缓存进行缓存或刷新，在多个命名空间中共享相同的缓存配置和实例，可以使用 cache-ref 元素来引用另一个缓存 1&lt;cache-ref namespace=&quot;com.someone.application.data.SomeMapper&quot;/&gt; 构造语句动态 SQL基本介绍动态 SQL 是 MyBatis 强大特性之一，逻辑复杂时，MyBatis 映射配置文件中，SQL 是动态变化的，所以引入动态 SQL 简化拼装 SQL 的操作 DynamicSQL 包含的标签： if where set choose (when、otherwise) trim foreach 各个标签都可以进行灵活嵌套和组合 OGNL：Object Graphic Navigation Language（对象图导航语言），用于对数据进行访问 参考文章：https://www.cnblogs.com/ysocean/p/7289529.html where：条件标签，有动态条件则使用该标签代替 WHERE 关键字，封装查询条件 作用：如果标签返回的内容是以 AND 或 OR 开头的，标签内会剔除掉 表结构： if基本格式： 123&lt;if test=“条件判断”&gt;\t查询条件拼接&lt;/if&gt; 我们根据实体类的不同取值，使用不同的 SQL 语句来进行查询。比如在 id 如果不为空时可以根据 id 查询，如果username 不同空时还要加入用户名作为条件，这种情况在我们的多条件组合查询中经常会碰到。 UserMapper.xml 12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;mapper.UserMapper&quot;&gt; &lt;select id=&quot;selectCondition&quot; resultType=&quot;user&quot; parameterType=&quot;user&quot;&gt; SELECT * FROM user &lt;where&gt; &lt;if test=&quot;id != null &quot;&gt; id = #&#123;id&#125; &lt;/if&gt; &lt;if test=&quot;username != null &quot;&gt; AND username = #&#123;username&#125; &lt;/if&gt; &lt;if test=&quot;sex != null &quot;&gt; AND sex = #&#123;sex&#125; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt;\t&lt;/mapper&gt; MyBatisConfig.xml，引入映射配置文件 1234&lt;mappers&gt; &lt;!--mapper引入指定的映射配置 resource属性执行的映射配置文件的名称--&gt; &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt;&lt;/mappers&gt; DAO 层 Mapper 接口 1234public interface UserMapper &#123; //多条件查询 public abstract List&lt;User&gt; selectCondition(Student stu);&#125; 实现类 123456789101112131415161718192021222324252627282930313233public class DynamicTest &#123; @Test public void selectCondition() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = ssf.openSession(true); //4.获取StudentMapper接口的实现类对象 UserMapper mapper = sqlSession.getMapper(UserMapper.class); User user = new User(); user.setId(2); user.setUsername(&quot;李四&quot;); //user.setSex(男); AND 后会自动剔除 //5.调用实现类的方法，接收结果 List&lt;Student&gt; list = mapper.selectCondition(user); //6.处理结果 for (User user : list) &#123; System.out.println(user); &#125; //7.释放资源 sqlSession.close(); is.close(); &#125;&#125; set：进行更新操作的时候，含有 set 关键词，使用该标签 12345678910111213&lt;!-- 根据 id 更新 user 表的数据 --&gt;&lt;update id=&quot;updateUserById&quot; parameterType=&quot;com.ys.po.User&quot;&gt; UPDATE user u &lt;set&gt; &lt;if test=&quot;username != null and username != &#x27;&#x27;&quot;&gt; u.username = #&#123;username&#125;, &lt;/if&gt; &lt;if test=&quot;sex != null and sex != &#x27;&#x27;&quot;&gt; u.sex = #&#123;sex&#125; &lt;/if&gt; &lt;/set&gt; WHERE id=#&#123;id&#125;&lt;/update&gt; 如果第一个条件 username 为空，那么 sql 语句为：update user u set u.sex&#x3D;? where id&#x3D;? 如果第一个条件不为空，那么 sql 语句为：update user u set u.username &#x3D; ? ,u.sex &#x3D; ? where id&#x3D;? choose假如不想用到所有的查询条件，只要查询条件有一个满足即可，使用 choose 标签可以解决此类问题，类似于 Java 的 switch 语句 标签：， 12345678910111213141516&lt;select id=&quot;selectUserByChoose&quot; resultType=&quot;user&quot; parameterType=&quot;user&quot;&gt; SELECT * FROM user &lt;where&gt; &lt;choose&gt; &lt;when test=&quot;id !=&#x27;&#x27; and id != null&quot;&gt; id=#&#123;id&#125; &lt;/when&gt; &lt;when test=&quot;username !=&#x27;&#x27; and username != null&quot;&gt; AND username=#&#123;username&#125; &lt;/when&gt; &lt;otherwise&gt; AND sex=#&#123;sex&#125; &lt;/otherwise&gt; &lt;/choose&gt; &lt;/where&gt;&lt;/select&gt; 有三个条件，id、username、sex，只能选择一个作为查询条件 如果 id 不为空，那么查询语句为：select * from user where id&#x3D;? 如果 id 为空，那么看 username 是否为空 如果不为空，那么语句为：select * from user where username&#x3D;? 如果 username 为空，那么查询语句为 select * from user where sex&#x3D;? trimtrim 标记是一个格式化的标记，可以完成 set 或者是 where 标记的功能，自定义字符串截取 prefix：给拼串后的整个字符串加一个前缀，trim 标签体中是整个字符串拼串后的结果 prefixOverrides：去掉整个字符串前面多余的字符 suffix：给拼串后的整个字符串加一个后缀 suffixOverrides：去掉整个字符串后面多余的字符 改写 if + where 语句： 1234567891011&lt;select id=&quot;selectUserByUsernameAndSex&quot; resultType=&quot;user&quot; parameterType=&quot;com.ys.po.User&quot;&gt; SELECT * FROM user &lt;trim prefix=&quot;where&quot; prefixOverrides=&quot;and | or&quot;&gt; &lt;if test=&quot;username != null&quot;&gt; AND username=#&#123;username&#125; &lt;/if&gt; &lt;if test=&quot;sex != null&quot;&gt; AND sex=#&#123;sex&#125; &lt;/if&gt; &lt;/trim&gt;&lt;/select&gt; 改写 if + set 语句： 12345678910111213&lt;!-- 根据 id 更新 user 表的数据 --&gt;&lt;update id=&quot;updateUserById&quot; parameterType=&quot;com.ys.po.User&quot;&gt; UPDATE user u &lt;trim prefix=&quot;set&quot; suffixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;username != null and username != &#x27;&#x27;&quot;&gt; u.username = #&#123;username&#125;, &lt;/if&gt; &lt;if test=&quot;sex != null and sex != &#x27;&#x27;&quot;&gt; u.sex = #&#123;sex&#125;, &lt;/if&gt; &lt;/trim&gt; WHERE id=#&#123;id&#125;&lt;/update&gt; foreach基本格式： 1234&lt;foreach&gt;：循环遍历标签。适用于多个参数或者的关系。 &lt;foreach collection=“”open=“”close=“”item=“”separator=“”&gt; 获取参数&lt;/foreach&gt; 属性： collection：参数容器类型， (list-集合， array-数组) open：开始的 SQL 语句 close：结束的 SQL 语句 item：参数变量名 separator：分隔符 需求：循环执行 sql 的拼接操作，SELECT * FROM user WHERE id IN (1,2,5) UserMapper.xml片段 12345678&lt;select id=&quot;selectByIds&quot; resultType=&quot;user&quot; parameterType=&quot;list&quot;&gt; SELECT * FROM student &lt;where&gt; &lt;foreach collection=&quot;list&quot; open=&quot;id IN(&quot; close=&quot;)&quot; item=&quot;id&quot; separator=&quot;,&quot;&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/where&gt;&lt;/select&gt; 测试代码片段 1234567891011//4.获取StudentMapper接口的实现类对象UserMapper mapper = sqlSession.getMapper(UserMapper.class);List&lt;Integer&gt; ids = new ArrayList&lt;&gt;();Collections.addAll(list, 1, 2);//5.调用实现类的方法，接收结果List&lt;User&gt; list = mapper.selectByIds(ids);for (User user : list) &#123; System.out.println(user);&#125; SQL片段将一些重复性的 SQL 语句进行抽取，以达到复用的效果 格式： 12&lt;sql id=“片段唯一标识”&gt;抽取的SQL语句&lt;/sql&gt; &lt;!--抽取标签--&gt;&lt;include refid=“片段唯一标识”/&gt; &lt;!--引入标签--&gt; 使用： 12345678910&lt;sql id=&quot;select&quot;&gt;SELECT * FROM user&lt;/sql&gt;&lt;select id=&quot;selectByIds&quot; resultType=&quot;user&quot; parameterType=&quot;list&quot;&gt; &lt;include refid=&quot;select&quot;/&gt; &lt;where&gt; &lt;foreach collection=&quot;list&quot; open=&quot;id IN(&quot; close=&quot;)&quot; item=&quot;id&quot; separator=&quot;,&quot;&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/where&gt; &lt;/select&gt; 逆向工程MyBatis 逆向工程，可以针对单表自动生成 MyBatis 执行所需要的代码（mapper.java、mapper.xml、pojo…） generatorConfig.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt; &lt;generatorConfiguration&gt; &lt;context id=&quot;testTables&quot; targetRuntime=&quot;MyBatis3&quot;&gt; &lt;commentGenerator&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name=&quot;suppressAllComments&quot; value=&quot;true&quot; /&gt; &lt;/commentGenerator&gt; &lt;!--数据库连接的信息：驱动类、连接地址、用户名、密码 --&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql://localhost:3306/mybatisrelation&quot; userId=&quot;root&quot; password=&quot;root&quot;&gt; &lt;/jdbcConnection&gt; &lt;!-- 默认false，把JDBC DECIMAL 和 NUMERIC 类型解析为 Integer，为 true时把JDBC DECIMAL和NUMERIC类型解析为java.math.BigDecimal --&gt; &lt;javaTypeResolver&gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot; /&gt; &lt;/javaTypeResolver&gt; &lt;!-- targetProject:生成PO类的位置！！ --&gt; &lt;javaModelGenerator targetPackage=&quot;com.ys.po&quot; targetProject=&quot;.\\src&quot;&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot; /&gt; &lt;!-- 从数据库返回的值被清理前后的空格 --&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot; /&gt; &lt;/javaModelGenerator&gt; &lt;!-- targetProject:mapper映射文件生成的位置！！ --&gt; &lt;sqlMapGenerator targetPackage=&quot;com.ys.mapper&quot; targetProject=&quot;.\\src&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot; /&gt; &lt;/sqlMapGenerator&gt; &lt;!-- targetPackage：mapper接口生成的位置，重要！！ --&gt; &lt;javaClientGenerator type=&quot;XMLMAPPER&quot; targetPackage=&quot;com.ys.mapper&quot; targetProject=&quot;.\\src&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot; /&gt; &lt;/javaClientGenerator&gt; &lt;!-- 指定数据库表，要生成哪些表，就写哪些表，要和数据库中对应，不能写错！ --&gt; &lt;table tableName=&quot;items&quot;&gt;&lt;/table&gt; &lt;table tableName=&quot;orders&quot;&gt;&lt;/table&gt; &lt;table tableName=&quot;orderdetail&quot;&gt;&lt;/table&gt; &lt;table tableName=&quot;user&quot;&gt;&lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 生成代码： 1234567891011121314public void testGenerator() throws Exception&#123; List&lt;String&gt; warnings = new ArrayList&lt;String&gt;(); boolean overwrite = true; //指向逆向工程配置文件 File configFile = new File(GeneratorTest.class. getResource(&quot;/generatorConfig.xml&quot;).getFile()); ConfigurationParser cp = new ConfigurationParser(warnings); Configuration config = cp.parseConfiguration(configFile); DefaultShellCallback callback = new DefaultShellCallback(overwrite); MyBatisGenerator myBatisGenerator = new MyBatisGenerator(config, callback, warnings); myBatisGenerator.generate(null);&#125; 参考文章：https://www.cnblogs.com/ysocean/p/7360409.html 构建 SQL基础语法MyBatis 提供了 org.apache.ibatis.jdbc.SQL 功能类，专门用于构建 SQL 语句 方法 说明 SELECT(String… columns) 根据字段拼接查询语句 FROM(String… tables) 根据表名拼接语句 WHERE(String… conditions) 根据条件拼接语句 INSERT_INTO(String tableName) 根据表名拼接新增语句 INTO_VALUES(String… values) 根据值拼接新增语句 UPDATE(String table) 根据表名拼接修改语句 DELETE_FROM(String table) 根据表名拼接删除语句 增删改查注解： @SelectProvider：生成查询用的 SQL 语句 @InsertProvider：生成新增用的 SQL 语句 @UpdateProvider：生成修改用的 SQL 语句注解 @DeleteProvider：生成删除用的 SQL 语句注解。 type 属性：生成 SQL 语句功能类对象 method 属性：指定调用方法 基本操作 MyBatisConfig.xml 配置 1234 &lt;!-- mappers引入映射配置文件 --&gt;&lt;mappers&gt; &lt;package name=&quot;mapper&quot;/&gt;&lt;/mappers&gt; Mapper 类 123456789101112131415161718public interface StudentMapper &#123; //查询全部 @SelectProvider(type = ReturnSql.class, method = &quot;getSelectAll&quot;) public abstract List&lt;Student&gt; selectAll(); //新增数据 @InsertProvider(type = ReturnSql.class, method = &quot;getInsert&quot;) public abstract Integer insert(Student student); //修改操作 @UpdateProvider(type = ReturnSql.class, method = &quot;getUpdate&quot;) public abstract Integer update(Student student); //删除操作 @DeleteProvider(type = ReturnSql.class, method = &quot;getDelete&quot;) public abstract Integer delete(Integer id);&#125; ReturnSQL 类 123456789101112131415161718192021222324252627282930313233343536373839404142public class ReturnSql &#123; //定义方法，返回查询的sql语句 public String getSelectAll() &#123; return new SQL() &#123; &#123; SELECT(&quot;*&quot;); FROM(&quot;student&quot;); &#125; &#125;.toString(); &#125; //定义方法，返回新增的sql语句 public String getInsert(Student stu) &#123; return new SQL() &#123; &#123; INSERT_INTO(&quot;student&quot;); INTO_VALUES(&quot;#&#123;id&#125;,#&#123;name&#125;,#&#123;age&#125;&quot;); &#125; &#125;.toString(); &#125; //定义方法，返回修改的sql语句 public String getUpdate(Student stu) &#123; return new SQL() &#123; &#123; UPDATE(&quot;student&quot;); SET(&quot;name=#&#123;name&#125;&quot;,&quot;age=#&#123;age&#125;&quot;); WHERE(&quot;id=#&#123;id&#125;&quot;); &#125; &#125;.toString(); &#125; //定义方法，返回删除的sql语句 public String getDelete(Integer id) &#123; return new SQL() &#123; &#123; DELETE_FROM(&quot;student&quot;); WHERE(&quot;id=#&#123;id&#125;&quot;); &#125; &#125;.toString(); &#125;&#125; 功能实现类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class SqlTest &#123; @Test //查询全部 public void selectAll() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = sqlSessionFactory.openSession(true); //4.获取StudentMapper接口的实现类对象 StudentMapper mapper = sqlSession.getMapper(StudentMapper.class); //5.调用实现类对象中的方法，接收结果 List&lt;Student&gt; list = mapper.selectAll(); //6.处理结果 for (Student student : list) &#123; System.out.println(student); &#125; //7.释放资源 sqlSession.close(); is.close(); &#125; @Test //新增 public void insert() throws Exception&#123; //1 2 3 4获取StudentMapper接口的实现类对象 StudentMapper mapper = sqlSession.getMapper(StudentMapper.class); //5.调用实现类对象中的方法，接收结果 -&gt;6 7 Student stu = new Student(4,&quot;赵六&quot;,26); Integer result = mapper.insert(stu); &#125; @Test //修改 public void update() throws Exception&#123; //1 2 3 4 5调用实现类对象中的方法，接收结果 -&gt;6 7 Student stu = new Student(4,&quot;赵六wq&quot;,36); Integer result = mapper.update(stu); &#125; @Test //删除 public void delete() throws Exception&#123; //1 2 3 4 5 6 7 Integer result = mapper.delete(4); &#125;&#125; 运行原理运行机制 MyBatis 运行过程： 加载 MyBatis 全局配置文件，通过 XPath 方式解析 XML 配置文件，首先解析核心配置文件， 标签中配置属性项有 defaultExecutorType，用来配置指定 Executor 类型，将配置文件的信息填充到 Configuration对象。最后解析映射器配置的映射文件，并构建 MappedStatement 对象填充至 Configuration，将解析后的映射器添加到 mapperRegistry 中，用于获取代理 创建一个 DefaultSqlSession 对象，根据参数创建指定类型的 Executor，二级缓存默认开启，把 Executor 包装成缓存执行器 DefaulSqlSession 调用 getMapper()，通过 JDK 动态代理获取 Mapper 接口的代理对象 MapperProxy 执行 SQL 语句： MapperProxy.invoke() 执行代理方法，通过 MapperMethod#execute 判断执行的是增删改查中的哪个方法 查询方法调用 sqlSession.selectOne()，从 Configuration 中获取执行者对象 MappedStatement，然后 Executor 调用 executor.query 开始执行查询方法 首先通过 CachingExecutor 去二级缓存查询，查询不到去一级缓存查询，最后去数据库查询并放入一级缓存 Configuration 对象根据 标签的 statementType 属性创建 StatementHandler 对象，在 StatementHandler 的构造方法中，创建了 ParameterHandler 和 ResultSetHandler 对象 最后获取 JDBC 原生的 Connection 数据库连接对象，创建 Statement 执行者对象，然后通过 ParameterHandler 设置预编译参数，底层是 TypeHandler#setParameter 方法，然后通过 StatementHandler 回调执行者对象执行增删改查，最后调用 ResultsetHandler 处理查询结果 四大对象： StatementHandler：执行 SQL 语句的对象 ParameterHandler：设置预编译参数用的 ResultHandler：处理结果集 Executor：执行器，真正进行 Java 与数据库交互的对象 参考视频：https://www.bilibili.com/video/BV1mW411M737?p=71 获取工厂SqlSessionFactoryBuilder.build(InputStream, String, Properties)：构建工厂 XMLConfigBuilder.parse()：解析核心配置文件每个标签的信息（XPath） parseConfiguration(parser.evalNode(&quot;/configuration&quot;))：读取节点内数据， 是 MyBatis 配置文件中的顶层标签 settings = settingsAsProperties(root.evalNode(&quot;settings&quot;))：读取核心配置文件中的 标签 settingsElement(settings)：设置框架相关的属性 configuration.setCacheEnabled()：设置缓存属性，默认是 true configuration.setDefaultExecutorType()：设置 Executor 类型到 configuration，默认是 SIMPLE mapperElement(root.evalNode(&quot;mappers&quot;))：解析 mappers 信息，分为 package 和 单个注册两种 if...else...：根据映射方法选择合适的读取方式 XMLMapperBuilder.parse()：解析 mapper 的标签的信息 configurationElement(parser.evalNode(&quot;/mapper&quot;))：解析 mapper 文件，顶层节点 buildStatementFromContext(context.evalNodes(&quot;select...&quot;))：解析每个操作标签 XMLStatementBuilder.parseStatementNode()：解析操作标签的所有的属性 builderAssistant.addMappedStatement(...)：封装成 MappedStatement 对象加入 Configuration 对象，代表一个增删改查的标签 Class&lt;?&gt; mapperInterface = Resources.classForName(mapperClass)：加载 Mapper 接口 Configuration.addMappers()：将核心配置文件配置的映射器添加到 mapperRegistry 中，用来获取代理对象 MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type)：创建注解解析器 parser.parse()：解析 Mapper 接口 SqlSource sqlSource = getSqlSourceFromAnnotations()：获取 SQL 的资源对象 builderAssistant.addMappedStatement(...)：封装成 MappedStatement 对象加入 Configuration 对象 return configuration：返回配置完成的 configuration 对象 return new DefaultSqlSessionFactory(config)：返回工厂对象，包含 Configuration 对象 总结：解析 XML 是对 Configuration 中的属性进行填充，那么可以在一个类中创建 Configuration 对象，自定义其中属性的值来达到配置的效果 获取会话DefaultSqlSessionFactory.openSession()：获取 Session 对象，并且创建 Executor 对象 DefaultSqlSessionFactory.openSessionFromDataSource(…)：ExecutorType 为 Executor 的类型，TransactionIsolationLevel 为事务隔离级别，autoCommit 是否开启事务 transactionFactory.newTransaction(DataSource, IsolationLevel, boolean：事务对象 configuration.newExecutor(tx, execType)：根据参数创建指定类型的 Executor 批量操作笔记的部分有讲解到 的属性 defaultExecutorType，根据配置创建对象 二级缓存默认开启，会包装 Executor 对象 new CachingExecutor(executor) return new DefaultSqlSession(configuration, executor, autoCommit)：返回 DefaultSqlSession 对象 获取代理Configuration.getMapper(Class, SqlSession)：获取代理的 mapper 对象 MapperRegistry.getMapper(Class, SqlSession)：MapperRegistry 是 Configuration 属性，在获取工厂对象时初始化 (MapperProxyFactory&lt;T&gt;) knownMappers.get(type)：获取接口信息封装为 MapperProxyFactory 对象 mapperProxyFactory.newInstance(sqlSession)：创建代理对象 new MapperProxy&lt;&gt;(sqlSession, mapperInterface, methodCache)：包装对象 methodCache 是并发安全的 ConcurrentHashMap 集合，存放要执行的方法 MapperProxy&lt;T&gt; implements InvocationHandler 说明 MapperProxy 默认是一个 InvocationHandler 对象 Proxy.newProxyInstance()：JDK 动态代理创建 MapperProxy 对象 执行SQLMapperProxy.invoke()：执行 SQL 语句，Object 类的方法直接执行 1234567891011121314151617public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; // 当前方法是否是属于 Object 类中的方法 if (Object.class.equals(method.getDeclaringClass())) &#123; return method.invoke(this, args); // 当前方法是否是默认方法 &#125; else if (isDefaultMethod(method)) &#123; return invokeDefaultMethod(proxy, method, args); &#125; &#125; catch (Throwable t) &#123; throw ExceptionUtil.unwrapThrowable(t); &#125; // 包装成一个 MapperMethod 对象并初始化该对象 final MapperMethod mapperMethod = cachedMapperMethod(method); // 【根据 switch-case 判断使用的什么类型的 SQL 进行逻辑处理】，此处分析查询语句的查询操作 return mapperMethod.execute(sqlSession, args);&#125; sqlSession.selectOne(String, Object)：查询数据 12345678910111213public Object execute(SqlSession sqlSession, Object[] args) &#123; //..... // 解析传入的参数 Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param);&#125;// DefaultSqlSession.selectList(String, Object)public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123; // 获取执行者对象 MappedStatement ms = configuration.getMappedStatement(statement); // 开始执行查询语句，参数通过 wrapCollection() 包装成集合类 return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);&#125; Executor#query()： CachingExecutor.query()：先执行 CachingExecutor 去二级缓存获取数据 123public class CachingExecutor implements Executor &#123; private final Executor delegate; // 包装了 BaseExecutor，二级缓存不存在数据调用 BaseExecutor 查询&#125; MappedStatement.getBoundSql(parameterObject)：把 parameterObject 封装成 BoundSql 构造函数中有：this.parameterObject = parameterObject CachingExecutor.createCacheKey()：创建缓存对象 ms.getCache()：获取二级缓存 tcm.getObject(cache, key)：尝试从二级缓存中获取数据 BaseExecutor.query()：二级缓存不存在该数据，调用该方法 localCache.getObject(key) ：尝试从本地缓存（一级缓存）获取数据 BaseExecutor.queryFromDatabase()：缓存获取数据失败，开始从数据库获取数据，并放入本地缓存 SimpleExecutor.doQuery()：执行 query configuration.newStatementHandler()：创建 StatementHandler 对象 根据 标签的 statementType 属性，根据属性选择创建哪种对象 判断 BoundSql 是否被创建，没有创建会重新封装参数信息到 BoundSql StatementHandler 的构造方法中，创建了 ParameterHandler 和 ResultSetHandler 对象 interceptorChain.pluginAll(statementHandler)：拦截器链 prepareStatement()：通过 StatementHandler 创建 JDBC 原生的 Statement 对象 getConnection()：获取 JDBC 的 Connection 对象 handler.prepare()：初始化 Statement 对象 instantiateStatement(Connection connection)：Connection 中的方法实例化对象 获取普通执行者对象：Connection.createStatement() 获取预编译执行者对象：Connection.prepareStatement() handler.parameterize()：进行参数的设置 ParameterHandler.setParameters()：通过 ParameterHandler 设置参数 typeHandler.setParameter()：底层通过 TypeHandler 实现，回调 JDBC 的接口进行设置 StatementHandler.query()：调用 JDBC 原生的 PreparedStatement 执行 SQL 1234567public &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) &#123; // 获取 SQL 语句 String sql = boundSql.getSql(); statement.execute(sql); // 通过 ResultSetHandler 对象封装结果集，映射成 JavaBean return resultSetHandler.handleResultSets(statement); &#125; resultSetHandler.handleResultSets(statement)：处理结果集 handleResultSet(rsw, resultMap, multipleResults, null)：底层回调 handleRowValues()：逐行处理数据，根据是否配置了 属性选择是否使用简单结果集映射 首先判断数据是否被限制行数，然后进行结果集的映射 最后将数据存入 ResultHandler 对象，底层就是 List 集合 123456public class DefaultResultHandler implements ResultHandler&lt;Object&gt; &#123;\tprivate final List&lt;Object&gt; list; public void handleResult(ResultContext&lt;?&gt; context) &#123; list.add(context.getResultObject()); &#125;&#125; return collapseSingleResultList(multipleResults)：可能存在多个结果集的情况 localCache.putObject(key, list)：放入一级（本地）缓存 return list.get(0)：返回结果集的第一个数据 插件使用插件原理实现原理：插件是按照插件配置顺序创建层层包装对象，执行目标方法的之后，按照逆向顺序执行（栈） 在四大对象创建时： 每个创建出来的对象不是直接返回的，而是 interceptorChain.pluginAll(parameterHandler) 获取到所有 Interceptor（插件需要实现的接口），调用 interceptor.plugin(target)返回 target 包装后的对象 插件机制可以使用插件为目标对象创建一个代理对象，代理对象可以拦截到四大对象的每一个执行 123456789101112131415161718192021222324252627282930313233343536373839404142@Intercepts( &#123; @Signature(type=StatementHandler.class,method=&quot;parameterize&quot;,args=java.sql.Statement.class) &#125;)public class MyFirstPlugin implements Interceptor&#123;\t//intercept：拦截目标对象的目标方法的执行\t@Override\tpublic Object intercept(Invocation invocation) throws Throwable &#123; System.out.println(&quot;MyFirstPlugin...intercept:&quot; + invocation.getMethod()); //动态的改变一下sql运行的参数：以前1号员工，实际从数据库查询11号员工 Object target = invocation.getTarget(); System.out.println(&quot;当前拦截到的对象：&quot; + target); //拿到：StatementHandler==&gt;ParameterHandler===&gt;parameterObject //拿到target的元数据 MetaObject metaObject = SystemMetaObject.forObject(target); Object value = metaObject.getValue(&quot;parameterHandler.parameterObject&quot;); System.out.println(&quot;sql语句用的参数是：&quot; + value); //修改完sql语句要用的参数 metaObject.setValue(&quot;parameterHandler.parameterObject&quot;, 11); //执行目标方法 Object proceed = invocation.proceed(); //返回执行后的返回值 return proceed;\t&#125;\t// plugin：包装目标对象的，为目标对象创建一个代理对象\t@Override\tpublic Object plugin(Object target) &#123; //可以借助 Plugin 的 wrap 方法来使用当前 Interceptor 包装我们目标对象 System.out.println(&quot;MyFirstPlugin...plugin:mybatis将要包装的对象&quot; + target); Object wrap = Plugin.wrap(target, this); //返回为当前target创建的动态代理 return wrap;\t&#125;\t// setProperties：将插件注册时的property属性设置进来\t@Override\tpublic void setProperties(Properties properties) &#123; System.out.println(&quot;插件配置的信息：&quot; + properties);\t&#125;&#125; 核心配置文件： 1234567&lt;!--plugins：注册插件 --&gt;&lt;plugins&gt; &lt;plugin interceptor=&quot;mybatis.dao.MyFirstPlugin&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;/&gt; &lt;/plugin&gt;&lt;/plugins&gt; 分页插件 分页可以将很多条结果进行分页显示。如果当前在第一页，则没有上一页。如果当前在最后一页，则没有下一页，需要明确当前是第几页，这一页中显示多少条结果。 MyBatis 是不带分页功能的，如果想实现分页功能，需要手动编写 LIMIT 语句，不同的数据库实现分页的 SQL 语句也是不同，手写分页 成本较高。 PageHelper：第三方分页助手，将复杂的分页操作进行封装，从而让分页功能变得非常简单 分页操作开发步骤： 导入 PageHelper 的 Maven 坐标 在 MyBatis 核心配置文件中配置 PageHelper 插件 注意：分页助手的插件配置在通用 Mapper 之前 1234567&lt;plugins&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageInterceptor&quot;&gt; &lt;!-- 指定方言 --&gt; &lt;property name=&quot;dialect&quot; value=&quot;mysql&quot;/&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;mappers&gt;.........&lt;/mappers&gt; 与 MySQL 分页查询页数计算公式不同 static &lt;E&gt; Page&lt;E&gt; startPage(int pageNum, int pageSize)：pageNum第几页，pageSize页面大小 123456789@Testpublic void selectAll() &#123; //第一页：显示2条数据 PageHelper.startPage(1,2); List&lt;Student&gt; students = sqlSession.selectList(&quot;StudentMapper.selectAll&quot;); for (Student student : students) &#123; System.out.println(student); &#125;&#125; 参数获取PageInfo构造方法： PageInfo&lt;Student&gt; info = new PageInfo&lt;&gt;(list) : list 是 SQL 执行返回的结果集合，参考上一节 PageInfo相关API： startPage()：设置分页参数 PageInfo：分页相关参数功能类。 getTotal()：获取总条数 getPages()：获取总页数 getPageNum()：获取当前页 getPageSize()：获取每页显示条数 getPrePage()：获取上一页 getNextPage()：获取下一页 isIsFirstPage()：获取是否是第一页 isIsLastPage()：获取是否是最后一页 Spring概述Spring 是分层的 JavaSE&#x2F;EE 应用 full-stack 轻量级开源框架 Spring 优点： 方便解耦，简化开发 方便集成各种框架 方便程序测试 AOP 编程难过的支持 声明式事务的支持 降低 JavaEE API 的使用难度 体系结构： 参考视频：https://space.bilibili.com/37974444 IoC基本概述 IoC（Inversion Of Control）控制反转，Spring 反向控制应用程序所需要使用的外部资源 Spring 控制的资源全部放置在 Spring 容器中，该容器称为 IoC 容器（存放实例对象） 官方网站：https://spring.io/ → Projects → spring-framework → LEARN → Reference Doc 耦合（Coupling）：代码编写过程中所使用技术的结合紧密度，用于衡量软件中各个模块之间的互联程度 内聚（Cohesion）：代码编写过程中单个模块内部各组成部分间的联系，用于衡量软件中各个功能模块内部的功能联系 代码编写的目标：高内聚，低耦合。同一个模块内的各个元素之间要高度紧密，各个模块之间的相互依存度不紧密 入门项目模拟三层架构中表现层调用业务层功能 表现层：UserApp 模拟 UserServlet（使用 main 方法模拟） 业务层：UserService 步骤： 导入 spring 坐标（5.1.9.release） 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.9.RELEASE&lt;/version&gt;&lt;/dependency&gt; 编写业务层与表现层（模拟）接口与实现类 service.UserService，service.impl.UserServiceImpl 1234public interface UserService &#123;\t//业务方法 void save();&#125; 12345public class UserServiceImpl implements UserService &#123; public void save() &#123; System.out.println(&quot;user service running...&quot;); &#125;&#125; 建立 Spring 配置文件：resources.applicationContext.xml (名字一般使用该格式) 配置所需资源（Service）为 Spring 控制的资源 12345678&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 1.创建spring控制的资源--&gt; &lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot;/&gt;&lt;/beans&gt; 表现层（App）通过 Spring 获取资源（Service 实例） 123456789public class UserApp &#123; public static void main(String[] args) &#123; //2.加载配置文件 ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); //3.获取资源 UserService userService = (UserService) ctx.getBean(&quot;userService&quot;); userService.save();//user service running... &#125;&#125; XML开发bean基本属性标签： 标签， 的子标签 作用：定义 Spring 中的资源，受此标签定义的资源将受到 Spring 控制 格式： 123&lt;beans&gt;\t&lt;bean /&gt;&lt;/beans&gt; 基本属性 id：bean 的名称，通过 id 值获取 bean (首字母小写) class：bean 的类型，使用完全限定类名 name：bean 的名称，可以通过 name 值获取 bean，用于多人配合时给 bean 起别名 1&lt;bean id=&quot;beanId&quot; name=&quot;beanName1,beanName2&quot; class=&quot;ClassName&quot;&gt;&lt;/bean&gt; 1ctx.getBean(&quot;beanId&quot;) == ctx.getBean(&quot;beanName1&quot;) == ctx.getBean(&quot;beanName2&quot;) 作用范围作用：定义 bean 的作用范围 格式： 1&lt;bean scope=&quot;singleton&quot;&gt;&lt;/bean&gt; 取值： singleton：设定创建出的对象保存在 Spring 容器中，是一个单例的对象 prototype：设定创建出的对象保存在 Spring 容器中，是一个非单例（原型）的对象 request、session、application、 websocket ：设定创建出的对象放置在 web 容器对应的位置 Spring 容器中 Bean 的线程安全问题： 原型 Bean，每次创建一个新对象，线程之间并不存在 Bean 共享，所以不会有线程安全的问题 单例 Bean，所有线程共享一个单例实例 Bean，因此是存在资源的竞争，如果单例 Bean是一个无状态 Bean，也就是线程中的操作不会对 Bean 的成员执行查询以外的操作，那么这个单例 Bean 是线程安全的 解决方法：开发人员来进行线程安全的保证，最简单的办法就是把 Bean 的作用域 singleton 改为 protopyte 生命周期作用：定义 bean 对象在初始化或销毁时完成的工作 格式： 1&lt;bean init-method=&quot;init&quot; destroy-method=&quot;destroy&gt;&lt;/bean&gt; 取值：bean 对应的类中对应的具体方法名 实现接口的方式实现初始化，无需配置文件配置 init-method： 实现 InitializingBean，定义初始化逻辑 实现 DisposableBean，定义销毁逻辑 注意事项： 当 scope&#x3D;“singleton” 时，Spring 容器中有且仅有一个对象，init 方法在创建容器时仅执行一次 当 scope&#x3D;“prototype” 时，Spring 容器要创建同一类型的多个对象，init 方法在每个对象创建时均执行一次 当 scope&#x3D;“singleton” 时，关闭容器（.close()）会导致 bean 实例的销毁，调用 destroy 方法一次 当 scope&#x3D;“prototype” 时，对象的销毁由垃圾回收机制 GC 控制，destroy 方法将不会被执行 bean 配置： 12&lt;!--init-method和destroy-method用于控制bean的生命周期--&gt;&lt;bean id=&quot;userService3&quot; scope=&quot;prototype&quot; init-method=&quot;init&quot; destroy-method=&quot;destroy&quot; class=&quot;service.impl.UserServiceImpl&quot;/&gt; 业务层实现类： 1234567891011121314151617public class UserServiceImpl implements UserService&#123; public UserServiceImpl()&#123; System.out.println(&quot; constructor is running...&quot;); &#125; public void init()&#123; System.out.println(&quot;init....&quot;); &#125; public void destroy()&#123; System.out.println(&quot;destroy....&quot;); &#125; public void save() &#123; System.out.println(&quot;user service running...&quot;); &#125;&#125; 测试类： 1UserService userService = (UserService)ctx.getBean(&quot;userService3&quot;); 创建方式 静态工厂 作用：定义 bean 对象创建方式，使用静态工厂的形式创建 bean，兼容早期遗留系统的升级工作 格式： 1&lt;bean class=&quot;FactoryClassName&quot; factory-method=&quot;factoryMethodName&quot;&gt;&lt;/bean&gt; 取值：工厂 bean 中用于获取对象的静态方法名 注意事项：class 属性必须配置成静态工厂的类名 bean 配置： 12&lt;!--静态工厂创建 bean--&gt;&lt;bean id=&quot;userService4&quot; class=&quot;service.UserServiceFactory&quot; factory-method=&quot;getService&quot;/&gt; 工厂类： 123456public class UserServiceFactory &#123; public static UserService getService()&#123; System.out.println(&quot;factory create object...&quot;); return new UserServiceImpl(); &#125;&#125; 测试类： 1UserService userService = (UserService)ctx.getBean(&quot;userService4&quot;); 实例工厂 作用：定义 bean 对象创建方式，使用实例工厂的形式创建 bean，兼容早期遗留系统的升级工作 格式： 1&lt;bean factory-bean=&quot;factoryBeanId&quot; factory-method=&quot;factoryMethodName&quot;&gt;&lt;/bean&gt; 注意事项： 使用实例工厂创建 bean 首先需要将实例工厂配置 bean，交由 Spring 进行管理 factory-bean 是实例工厂的 Id bean 配置： 123&lt;!--实例工厂创建 bean，依赖工厂对象对应的 bean--&gt;&lt;bean id=&quot;factoryBean&quot; class=&quot;service.UserServiceFactory2&quot;/&gt;&lt;bean id=&quot;userService5&quot; factory-bean=&quot;factoryBean&quot; factory-method=&quot;getService&quot;/&gt; 工厂类： 123456public class UserServiceFactory2 &#123; public UserService getService()&#123; System.out.println(&quot; instance factory create object...&quot;); return new UserServiceImpl(); &#125;&#125; 获取BeanApplicationContext 子类相关API： 方法 说明 String[] getBeanDefinitionNames() 获取 Spring 容器中定义的所有 JavaBean 的名称 BeanDefinition getBeanDefinition(String beanName) 返回给定 bean 名称的 BeanDefinition String[] getBeanNamesForType(Class&lt;?&gt; type) 获取 Spring 容器中指定类型的所有 JavaBean 的名称 Environment getEnvironment() 获取与此组件关联的环境 DI依赖注入 IoC（Inversion Of Control）控制翻转，Spring 反向控制应用程序所需要使用的外部资源 DI（Dependency Injection）依赖注入，应用程序运行依赖的资源由 Spring 为其提供，资源进入应用程序的方式称为注入，简单说就是利用反射机制为类的属性赋值的操作 IoC 和 DI 的关系：IoC 与 DI 是同一件事站在不同角度看待问题 set 注入标签： 标签， 的子标签 作用：使用 set 方法的形式为 bean 提供资源 格式： 12345&lt;bean&gt;\t&lt;property /&gt; &lt;property /&gt; .....&lt;/bean&gt; 基本属性： name：对应 bean 中的属性名，要注入的变量名，要求该属性必须提供可访问的 set 方法（严格规范此名称是 set 方法对应名称，首字母必须小写） value：设定非引用类型属性对应的值，不能与 ref 同时使用 ref：设定引用类型属性对应 bean 的 id ，不能与 value 同时使用 1&lt;property name=&quot;propertyName&quot; value=&quot;propertyValue&quot; ref=&quot;beanId&quot;/&gt; 代码实现： DAO 层：要注入的资源 123public interface UserDao &#123; public void save();&#125; 12345public class UserDaoImpl implements UserDao&#123; public void save()&#123; System.out.println(&quot;user dao running...&quot;); &#125;&#125; Service 业务层 123public interface UserService &#123; public void save();&#125; 12345678910111213141516171819public class UserServiceImpl implements UserService &#123;\tprivate UserDao userDao; private int num; //1.对需要进行注入的变量添加set方法 public void setUserDao(UserDao userDao) &#123; this.userDao = userDao; &#125; public void setNum(int num) &#123; this.num = num; &#125; public void save() &#123; System.out.println(&quot;user service running...&quot; + num); userDao.save(); bookDao.save(); &#125;&#125; 配置 applicationContext.xml 12345678&lt;!--2.将要注入的资源声明为bean--&gt;&lt;bean id=&quot;userDao&quot; class=&quot;dao.impl.UserDaoImpl&quot;/&gt;&lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot;&gt;\t&lt;!--3.将要注入的引用类型的变量通过property属性进行注入，--&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;property name=&quot;num&quot; value=&quot;666&quot;/&gt;&lt;/bean&gt; 测试类 1234567public class UserApp &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); UserService userService = (UserService) ctx.getBean(&quot;userService&quot;); userService.save(); &#125;&#125; 构造注入标签： 标签， 的子标签 作用：使用构造方法的形式为 bean 提供资源，兼容早期遗留系统的升级工作 格式： 1234&lt;bean&gt;\t&lt;constructor-arg /&gt; .....&lt;!--一个bean可以有多个constructor-arg标签--&gt;&lt;/bean&gt; 属性： name：对应bean中的构造方法所携带的参数名 value：设定非引用类型构造方法参数对应的值，不能与 ref 同时使用 ref：设定引用类型构造方法参数对应 bean 的 id ，不能与 value 同时使用 type：设定构造方法参数的类型，用于按类型匹配参数或进行类型校验 index：设定构造方法参数的位置，用于按位置匹配参数，参数 index 值从 0 开始计数 12&lt;constructor-arg name=&quot;argsName&quot; value=&quot;argsValue&quot; /&gt;&lt;constructor-arg index=&quot;arg-index&quot; type=&quot;arg-type&quot; ref=&quot;beanId&quot;/&gt; 代码实现： DAO 层：要注入的资源 1234567891011121314public class UserDaoImpl implements UserDao&#123; private String username; private String pwd; private String driver; public UserDaoImpl(String driver,String username, String pwd) &#123; this.driver = driver; this.username = username; this.pwd = pwd; &#125; public void save()&#123; System.out.println(&quot;user dao running...&quot;+username+&quot; &quot;+pwd+&quot; &quot;+driver); &#125;&#125; Service 业务层：参考 set 注入 配置 applicationContext.xml 123456789101112&lt;bean id=&quot;userDao&quot; class=&quot;dao.impl.UserDaoImpl&quot;&gt; &lt;!--使用构造方法进行注入，需要保障注入的属性与bean中定义的属性一致--&gt;\t&lt;!--一致指顺序一致或类型一致或使用index解决该问题--&gt; &lt;constructor-arg index=&quot;2&quot; value=&quot;123&quot;/&gt; &lt;constructor-arg index=&quot;1&quot; value=&quot;root&quot;/&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot;&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;property name=&quot;num&quot; value=&quot;666&quot;/&gt;&lt;/bean&gt; 方式二：使用 UserServiceImpl 的构造方法注入 1234&lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot;&gt;\t&lt;constructor-arg name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt;\t&lt;constructor-arg name=&quot;num&quot; value=&quot;666666&quot;/&gt;&lt;/bean&gt; 测试类：参考 set 注入 集合注入标签： ， 或 标签的子标签 作用：注入集合数据类型属性 格式： 123&lt;property&gt;\t&lt;list&gt;&lt;/list&gt;&lt;/property&gt; 代码实现： DAO 层：要注入的资源 123public interface BookDao &#123; public void save();&#125; 1234567891011121314151617181920212223242526272829303132333435363738public class BookDaoImpl implements BookDao &#123; private ArrayList al; private Properties properties; private int[] arr; private HashSet hs; private HashMap hm ; public void setAl(ArrayList al) &#123; this.al = al; &#125; public void setProperties(Properties properties) &#123; this.properties = properties; &#125; public void setArr(int[] arr) &#123; this.arr = arr; &#125; public void setHs(HashSet hs) &#123; this.hs = hs; &#125; public void setHm(HashMap hm) &#123; this.hm = hm; &#125; public void save() &#123; System.out.println(&quot;book dao running...&quot;); System.out.println(&quot;ArrayList:&quot; + al); System.out.println(&quot;Properties:&quot; + properties); for (int i = 0; i &lt; arr.length; i++) &#123; System.out.println(arr[i]); &#125; System.out.println(&quot;HashSet:&quot; + hs); System.out.println(&quot;HashMap:&quot; + hm); &#125;&#125; Service 业务层 1234567891011121314public class UserServiceImpl implements UserService &#123; private BookDao bookDao; public UserServiceImpl() &#123;&#125; public void setBookDao(BookDao bookDao) &#123; this.bookDao = bookDao; &#125; public void save() &#123; System.out.println(&quot;user service running...&quot; + num); bookDao.save(); &#125;&#125; 配置 applicationContext.xml 123456789101112131415161718192021222324252627282930313233343536&lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot;&gt; &lt;property name=&quot;bookDao&quot; ref=&quot;bookDao&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;bookDao&quot; class=&quot;dao.impl.BookDaoImpl&quot;&gt; &lt;property name=&quot;al&quot;&gt; &lt;list&gt; &lt;value&gt;seazean&lt;/value&gt; &lt;value&gt;66666&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;properties&quot;&gt; &lt;props&gt; &lt;prop key=&quot;name&quot;&gt;seazean666&lt;/prop&gt; &lt;prop key=&quot;value&quot;&gt;666666&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name=&quot;arr&quot;&gt; &lt;array&gt; &lt;value&gt;123456&lt;/value&gt; &lt;value&gt;66666&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;property name=&quot;hs&quot;&gt; &lt;set&gt; &lt;value&gt;seazean&lt;/value&gt; &lt;value&gt;66666&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=&quot;hm&quot;&gt; &lt;map&gt; &lt;entry key=&quot;name&quot; value=&quot;seazean66666&quot;/&gt; &lt;entry key=&quot;value&quot; value=&quot;6666666666&quot;/&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt; P标签：&lt;p:propertyName&gt;，&lt;p:propertyName-ref&gt; 作用：为 bean 注入属性值 格式： 1&lt;bean p:propertyName=&quot;propertyValue&quot; p:propertyName-ref=&quot;beanId&quot;/&gt; 开启 p 命令空间：开启 Spring 对 p 命令空间的的支持，在 beans 标签中添加对应空间支持 1234567&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt;&lt;/beans&gt; 实例： 1234567&lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot; p:userDao-ref=&quot;userDao&quot; p:bookDao-ref=&quot;bookDao&quot; p:num=&quot;10&quot;\t/&gt; SpELSpring 提供了对 EL 表达式的支持，统一属性注入格式 作用：为 bean 注入属性值 格式： 1&lt;property value=&quot;EL&quot;&gt; 注意：所有属性值不区分是否引用类型，统一使用value赋值 所有格式统一使用 value&#x3D;“#{}” 常量 #{10} #{3.14} #{2e5} #{‘it’} 引用 bean #{beanId} 引用 bean 属性 #{beanId.propertyName} 引用 bean 方法 beanId.methodName().method2() 引用静态方法 T(java.lang.Math).PI 运算符支持 #{3 lt 4 &#x3D;&#x3D; 4 ge 3} 正则表达式支持 #{user.name matches‘[a-z]{6,}’} 集合支持 #{likes[3]} 实例： 12345&lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot;&gt; &lt;property name=&quot;userDao&quot; value=&quot;#&#123;userDao&#125;&quot;/&gt; &lt;property name=&quot;bookDao&quot; value=&quot;#&#123;bookDao&#125;&quot;/&gt; &lt;property name=&quot;num&quot; value=&quot;#&#123;666666666&#125;&quot;/&gt; &lt;/bean&gt; propSpring 提供了读取外部 properties 文件的机制，使用读取到的数据为 bean 的属性赋值 操作步骤： 准备外部 properties 文件 开启 context 命名空间支持 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd &quot;&gt; 加载指定的 properties 文件 1&lt;context:property-placeholder location=&quot;classpath:data.properties&quot; /&gt; 使用加载的数据 1&lt;property name=&quot;propertyName&quot; value=&quot;$&#123;propertiesName&#125;&quot;/&gt; 注意：如果需要加载所有的 properties 文件，可以使用 *.properties 表示加载所有的 properties 文件 注意：读取数据使用 ${propertiesName} 格式进行，其中 propertiesName 指 properties 文件中的属性名 代码实现： data.properties 12username=rootpwd=123456 DAO 层：注入的资源 123public interface UserDao &#123; public void save();&#125; 123456789101112131415public class UserDaoImpl implements UserDao&#123; private String userName; private String password; public void setUserName(String userName) &#123; this.userName = userName; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public void save()&#123; System.out.println(&quot;user dao running...&quot;+userName+&quot; &quot;+password); &#125;&#125; Service 业务层 12345678910public class UserServiceImpl implements UserService &#123; private UserDao userDao; public void setUserDao(UserDao userDao) &#123; this.userDao = userDao; &#125; public void save() &#123; System.out.println(&quot;user service running...&quot;); userDao.save(); &#125;&#125; applicationContext.xml 12345678910&lt;context:property-placeholder location=&quot;classpath:*.properties&quot;/&gt;&lt;bean id=&quot;userDao&quot; class=&quot;dao.impl.UserDaoImpl&quot;&gt; &lt;property name=&quot;userName&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;pwd&#125;&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot;&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt;&lt;/bean&gt; 测试类 1234567public class UserApp &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); UserService userService = (UserService) ctx.getBean(&quot;userService&quot;); userService.save(); &#125;&#125; import标签：，标签的子标签 作用：在当前配置文件中导入其他配置文件中的项 格式： 123&lt;beans&gt; &lt;import /&gt;&lt;/beans&gt; 属性： resource：加载的配置文件名 1&lt;import resource=“config2.xml&quot;/&gt; Spring 容器加载多个配置文件： applicationContext-book.xml 123&lt;bean id=&quot;bookDao&quot; class=&quot;dao.impl.BookDaoImpl&quot;&gt; &lt;property name=&quot;num&quot; value=&quot;1&quot;/&gt;&lt;/bean&gt; applicationContext-user.xml 123456789&lt;bean id=&quot;userDao&quot; class=&quot;dao.impl.UserDaoImpl&quot;&gt; &lt;property name=&quot;userName&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;pwd&#125;&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot;&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;property name=&quot;bookDao&quot; ref=&quot;bookDao&quot;/&gt;&lt;/bean&gt; applicationContext.xml 123456&lt;import resource=&quot;applicationContext-user.xml&quot;/&gt;&lt;import resource=&quot;applicationContext-book.xml&quot;/&gt;&lt;bean id=&quot;bookDao&quot; class=&quot;com.seazean.dao.impl.BookDaoImpl&quot;&gt; &lt;property name=&quot;num&quot; value=&quot;2&quot;/&gt;&lt;/bean&gt; 测试类 12new ClassPathXmlApplicationContext(&quot;applicationContext-user.xml&quot;,&quot;applicationContext-book.xml&quot;);new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); Spring 容器中的 bean 定义冲突问题 同 id 的 bean，后定义的覆盖先定义的 导入配置文件可以理解为将导入的配置文件复制粘贴到对应位置，程序执行选择最下面的配置使用 导入配置文件的顺序与位置不同可能会导致最终程序运行结果不同 三方资源Druid第三方资源配置 pom.xml 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.16&lt;/version&gt;&lt;/dependency&gt; applicationContext.xml 1234567&lt;!--加载druid资源--&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://192.168.2.185:3306/spring_db&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;/&gt;&lt;/bean&gt; App.java 123ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);DruidDataSource datasource = (DruidDataSource) ctx.getBean(&quot;datasource&quot;);System.out.println(datasource); MybatisMybatis 核心配置文件消失 环境 environment 转换成数据源对象 映射 Mapper 扫描工作交由 Spring 处理 类型别名交由 Spring 处理 DAO 接口不需要创建实现类，MyBatis-Spring 提供了一个动态代理的实现 MapperFactoryBean，这个类可以让直接注入数据映射器接口到 service 层 bean 中，底层将会动态代理创建类 整合原理：利用 Spring 框架的 SPI 机制，在 META-INF 目录的 spring.handlers 中给 Spring 容器中导入 NamespaceHandler 类 NamespaceHandler 的 init 方法注册 bean 信息的解析器 MapperScannerBeanDefinitionParser 解析器在 Spring 容器创建过程中去解析 mapperScanner 标签，解析出的属性填充到 MapperScannerConfigurer 中 MapperScannerConfigurer 实现了 BeanDefinitionRegistryPostProcessor 接口，重写 postProcessBeanDefinitionRegistry() 方法，可以扫描到 MyBatis 的 Mapper 注解开发注解驱动XML启动注解扫描，加载类中配置的注解项： 1&lt;context:component-scan base-package=&quot;packageName1,packageName2&quot;/&gt; 说明： 在进行包扫描时，会对配置的包及其子包中所有文件进行扫描，多个包采用,隔开 扫描过程是以文件夹递归迭代的形式进行的 扫描过程仅读取合法的 Java 文件 扫描时仅读取 Spring 可识别的注解 扫描结束后会将可识别的有效注解转化为 Spring 对应的资源加入 IoC 容器 从加载效率上来说注解优于 XML 配置文件 注解：启动时使用注解的形式替代 xml 配置，将 Spring 配置文件从工程中消除，简化书写 缺点：为了达成注解驱动的目的，可能会将原先很简单的书写，变的更加复杂。XML 中配置第三方开发的资源是很方便的，但使用注解驱动无法在第三方开发的资源中进行编辑，因此会增大开发工作量 纯注解注解配置类 名称：@Configuration、@ComponentScan 类型：类注解 作用：设置当前类为 Spring 核心配置加载类 格式： 1234@Configuration@ComponentScan(&#123;&quot;scanPackageName1&quot;,&quot;scanPackageName2&quot;&#125;)public class SpringConfigClassName&#123;&#125; 说明： 核心配合类用于替换 Spring 核心配置文件，此类可以设置空的，不设置变量与属性 bean 扫描工作使用注解 @ComponentScan 替代，多个包用 &#123;&#125; 和 , 隔开 加载纯注解格式上下文对象，需要使用 AnnotationConfigApplicationContext 12345678910111213141516171819202122@Configurationpublic class SpringConfig &#123; @Bean public Person person() &#123; return new Person1(&quot;lisi&quot;, 20); &#125;&#125;public class MainTest &#123; public static void main(String[] args) &#123; ApplicationContext applicationContext = new AnnotationConfigApplicationContext(SpringConfig.class); //方式一：名称对应类名 Person bean = applicationContext.getBean(Person.class); System.out.println(bean); //方式二：名称对应方法名 Person bean1 = (Person) applicationContext.getBean(&quot;person1&quot;); //方法三：指定名称@Bean(&quot;person2&quot;) &#125;&#125; 扫描器组件扫描过滤器 开发过程中，需要根据需求加载必要的 bean，排除指定 bean 名称：@ComponentScan 类型：类注解 作用：设置 Spring 配置加载类扫描规则 格式： 12345678@ComponentScan( value = &#123;&quot;dao&quot;,&quot;service&quot;&#125;, //设置基础扫描路径 excludeFilters = //设置过滤规则，当前为排除过滤\t@ComponentScan.Filter( //设置过滤器 type= FilterType.ANNOTATION, //设置过滤方式为按照注解进行过滤 classes = Service.class) //设置具体的过滤项，过滤所有@Service修饰的bean )) 属性： includeFilters：设置包含性过滤器 excludeFilters：设置排除性过滤器 type：设置过滤器类型 基本注解设置 bean名称：@Component、@Controller、@Service、@Repository 类型：类注解，写在类定义上方 作用：设置该类为 Spring 管理的 bean 格式： 12@Componentpublic class ClassName&#123;&#125; 说明：@Controller、@Service 、@Repository 是 @Component 的衍生注解，功能同 @Component 属性： value（默认）：定义 bean 的访问 id 作用范围名称：@Scope 类型：类注解，写在类定义上方 作用：设置该类作为 bean 对应的 scope 属性 格式： 12@Scopepublic class ClassName&#123;&#125; 相关属性 value（默认）：定义 bean 的作用域，默认为 singleton，非单例取值 prototype 生命周期名称：@PostConstruct、@PreDestroy 类型：方法注解，写在方法定义上方 作用：设置该类作为 bean 对应的生命周期方法 示例： 12345678910111213141516//定义bean，后面添加bean的id@Component(&quot;userService&quot;)//定义bean的作用域@Scope(&quot;singleton&quot;)public class UserServiceImpl implements UserService &#123; //初始化 @PostConstruct public void init()&#123; System.out.println(&quot;user service init...&quot;); &#125;\t//销毁 @PreDestroy public void destroy()&#123; System.out.println(&quot;user service destroy...&quot;); &#125;&#125; 一个对象的执行顺序：Constructor &gt;&gt; @Autowired（注入属性） &gt;&gt; @PostConstruct（初始化逻辑） 加载资源名称：@Bean 类型：方法注解 作用：设置该方法的返回值作为 Spring 管理的 bean 格式： 12@Bean(&quot;dataSource&quot;)public DruidDataSource createDataSource() &#123; return ……; &#125; 说明： 因为第三方 bean 无法在其源码上进行修改，使用 @Bean 解决第三方 bean 的引入问题 该注解用于替代 XML 配置中的静态工厂与实例工厂创建 bean，不区分方法是否为静态或非静态 @Bean 所在的类必须被 Spring 扫描加载，否则该注解无法生效 相关属性 value（默认）：定义 bean 的访问 id initMethod：声明初始化方法 destroyMethod：声明销毁方法 属性注入基本类型名称：@Value 类型：属性注解、方法注解 作用：设置对应属性的值或对方法进行传参 格式： 123//@Value(&quot;$&#123;jdbc.username&#125;&quot;)@Value(&quot;root&quot;)private String username; 说明： value 值仅支持非引用类型数据，赋值时对方法的所有参数全部赋值 value 值支持读取 properties 文件中的属性值，通过类属性将 properties 中数据传入类中 value 值支持 SpEL @value 注解如果添加在属性上方，可以省略 set 方法（set 方法的目的是为属性赋值） 相关属性： value（默认）：定义对应的属性值或参数值 自动装配属性注入名称：@Autowired、@Qualifier 类型：属性注解、方法注解 作用：设置对应属性的对象、对方法进行引用类型传参 格式： 123@Autowired(required = false)@Qualifier(&quot;userDao&quot;)private UserDao userDao; 说明： @Autowired 默认按类型装配，指定 @Qualifier 后可以指定自动装配的 bean 的 id 相关属性： required：为 true （默认）表示注入 bean 时该 bean 必须存在，不然就会注入失败抛出异常；为 false 表示注入时该 bean 存在就注入，不存在就忽略跳过 注意：在使用 @Autowired 时，首先在容器中查询对应类型的 bean，如果查询结果刚好为一个，就将该 bean 装配给 @Autowired 指定的数据，如果查询的结果不止一个，那么 @Autowired 会根据名称来查找，如果查询的结果为空，那么会抛出异常 解决方法：使用 required &#x3D; false 优先注入名称：@Primary 类型：类注解 作用：设置类对应的 bean 按类型装配时优先装配 范例： 12@Primarypublic class ClassName&#123;&#125; 说明： @Autowired 默认按类型装配，当出现相同类型的 bean，使用 @Primary 提高按类型自动装配的优先级，多个 @Primary 会导致优先级设置无效 注解对比名称：@Inject、@Named、@Resource @Inject 与 @Named 是 JSR330 规范中的注解，功能与 @Autowired 和 @Qualifier 完全相同，适用于不同架构场景 @Resource 是 JSR250 规范中的注解，可以简化书写格式 @Resource 相关属性 name：设置注入的 bean 的 id type：设置注入的 bean 的类型，接收的参数为 Class 类型 @Autowired 和 @Resource之间的区别： @Autowired 默认是按照类型装配注入，默认情况下它要求依赖对象必须存在（可以设置 required 属性为 false） @Resource 默认按照名称装配注入，只有当找不到与名称匹配的 bean 才会按照类型来装配注入 静态注入Spring 容器管理的都是实例对象，**@Autowired 依赖注入的都是容器内的对象实例**，在 Java 中 static 修饰的静态属性（变量和方法）是属于类的，而非属于实例对象 当类加载器加载静态变量时，Spring 上下文尚未加载，所以类加载器不会在 Bean 中正确注入静态类 1234567891011@Componentpublic class TestClass &#123; @Autowired private static Component component; // 调用静态组件的方法 public static void testMethod() &#123; component.callTestMethod()； &#125; &#125;// 编译正常，但运行时报java.lang.NullPointerException，所以在调用testMethod()方法时，component变量还没被初始化 解决方法： @Autowired 注解到类的构造函数上，Spring 扫描到 Component 的 Bean，然后赋给静态变量 component 12345678910111213@Componentpublic class TestClass &#123; private static Component component; @Autowired public TestClass(Component component) &#123; TestClass.component = component; &#125; public static void testMethod() &#123; component.callTestMethod()； &#125;&#125; @Autowired 注解到静态属性的 setter 方法上 使用 @PostConstruct 注解一个方法，在方法内为 static 静态成员赋值 使用 Spring 框架工具类获取 bean，定义成局部变量使用 1234567public class TestClass &#123; // 调用静态组件的方法 public static void testMethod() &#123; Component component = SpringApplicationContextUtil.getBean(&quot;component&quot;); component.callTestMethod(); &#125;&#125; 参考文章：http://jessehzx.top/2018/03/18/spring-autowired-static-field/ 文件读取名称：@PropertySource 类型：类注解 作用：加载 properties 文件中的属性值 格式： 12345@PropertySource(value = &quot;classpath:filename.properties&quot;)public class ClassName &#123; @Value(&quot;$&#123;propertiesAttributeName&#125;&quot;) private String attributeName;&#125; 说明： 不支持 * 通配符，加载后，所有 Spring 控制的 bean 中均可使用对应属性值，加载多个需要用 &#123;&#125; 和 , 隔开 相关属性 value（默认）：设置加载的 properties 文件名 ignoreResourceNotFound：如果资源未找到，是否忽略，默认为 false 加载控制依赖加载@DependsOn 名称：@DependsOn 类型：类注解、方法注解 作用：控制 bean 的加载顺序，使其在指定 bean 加载完毕后再加载 格式： 123@DependsOn(&quot;beanId&quot;)public class ClassName &#123;&#125; 说明： 配置在方法上，使 @DependsOn 指定的 bean 优先于 @Bean 配置的 bean 进行加载 配置在类上，使 @DependsOn 指定的 bean 优先于当前类中所有 @Bean 配置的 bean 进行加载 配置在类上，使 @DependsOn 指定的 bean 优先于 @Component 等配置的 bean 进行加载 相关属性 value（默认）：设置当前 bean 所依赖的 bean 的 id @Order 名称：@Order 类型：配置类注解 作用：控制配置类的加载顺序，值越小越先加载 格式： 123@Order(1)public class SpringConfigClassName &#123;&#125; @Lazy 名称：@Lazy 类型：类注解、方法注解 作用：控制 bean 的加载时机，使其延迟加载，获取的时候加载 格式： 123@Lazypublic class ClassName &#123;&#125; 应用场景@DependsOn 微信订阅号，发布消息和订阅消息的 bean 的加载顺序控制（先开订阅，再发布） 双 11 活动，零点前是结算策略 A，零点后是结算策略 B，策略 B 操作的数据为促销数据，策略 B 加载顺序与促销数据的加载顺序 @Lazy 程序灾难出现后对应的应急预案处理是启动容器时加载时机 @Order 多个种类的配置出现后，优先加载系统级的，然后加载业务级的，避免细粒度的加载控制 整合资源导入名称：@Import 类型：类注解 作用：导入第三方 bean 作为 Spring 控制的资源，这些类都会被 Spring 创建并放入 ioc 容器 格式： 1234@Configuration@Import(OtherClassName.class)public class ClassName &#123;&#125; 说明： @Import 注解在同一个类上，仅允许添加一次，如果需要导入多个，使用数组的形式进行设定 在被导入的类中可以继续使用 @Import 导入其他资源 @Bean 所在的类可以使用导入的形式进入 Spring 容器，无需声明为 bean Druid 加载资源 123456789101112@Componentpublic class JDBCConfig &#123; @Bean(&quot;dataSource&quot;) public static DruidDataSource getDataSource() &#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); ds.setUrl(&quot;jdbc:mysql://192.168.2.185:3306/spring_db&quot;); ds.setUsername(&quot;root&quot;); ds.setPassword(&quot;123456&quot;); return ds; &#125;&#125; 导入资源 12345@Configuration@ComponentScan(value = &#123;&quot;service&quot;,&quot;dao&quot;&#125;)@Import(JDBCConfig.class)public class SpringConfig &#123;&#125; 测试 12DruidDataSource dataSource = (DruidDataSource) ctx.getBean(&quot;dataSource&quot;);System.out.println(dataSource); JunitSpring 接管 Junit 的运行权，使用 Spring 专用的 Junit 类加载器，为 Junit 测试用例设定对应的 Spring 容器 注意： 从 Spring5.0 以后，要求 Junit 的版本必须是4.12及以上 Junit 仅用于单元测试，不能将 Junit 的测试类配置成 Spring 的 bean，否则该配置将会被打包进入工程中 test &#x2F; java &#x2F; service &#x2F; UserServiceTest 12345678910111213//设定spring专用的类加载器@RunWith(SpringJUnit4ClassRunner.class)//设定加载的spring上下文对应的配置@ContextConfiguration(classes = SpringConfig.class)public class UserServiceTest &#123; @Autowired private AccountService accountService; @Test public void testFindById() &#123; Account account = accountService.findById(1); Assert.assertEquals(&quot;Mike&quot;, account.getName()); &#125;&#125; pom.xml 12345678910&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.1.9.RELEASE&lt;/version&gt;&lt;/dependency&gt; IoC原理核心类BeanFactoryApplicationContext： ApplicationContext 是一个接口，提供了访问 Spring 容器的 API ClassPathXmlApplicationContext 是一个类，实现了上述功能 ApplicationContext 的顶层接口是 BeanFactory BeanFactory 定义了 bean 相关的最基本操作 ApplicationContext 在 BeanFactory 基础上追加了若干新功能 ApplicationContext 和 BeanFactory对比： BeanFactory 和 ApplicationContext 是 Spring 的两大核心接口，都可以当做 Spring 的容器 BeanFactory 是 Spring 里面最底层的接口，是 IoC 的核心，定义了 IoC 的基本功能，包含了各种 Bean 的定义、加载、实例化，依赖注入和生命周期管理。ApplicationContext 接口作为 BeanFactory 的子类，除了提供 BeanFactory 所具有的功能外，还提供了更完整的框架功能： 继承 MessageSource，因此支持国际化 资源文件访问，如 URL 和文件（ResourceLoader）。 载入多个（有继承关系）上下文（即加载多个配置文件） ，使得每一个上下文都专注于一个特定的层次，比如应用的 web 层 提供在监听器中注册 bean 的事件 BeanFactory 创建的 bean 采用延迟加载形式，只有在使用到某个 Bean 时（调用 getBean），才对该 Bean 进行加载实例化（Spring 早期使用该方法获取 bean），这样就不能提前发现一些存在的 Spring 的配置问题；ApplicationContext 是在容器启动时，一次性创建了所有的 Bean，容器启动时，就可以发现 Spring 中存在的配置错误，这样有利于检查所依赖属性是否注入 ApplicationContext 启动后预载入所有的单实例 Bean，所以程序启动慢，运行时速度快 两者都支持 BeanPostProcessor、BeanFactoryPostProcessor 的使用，但两者之间的区别是：BeanFactory 需要手动注册，而 ApplicationContext 则是自动注册 FileSystemXmlApplicationContext：加载文件系统中任意位置的配置文件，而 ClassPathXmlAC 只能加载类路径下的配置文件 BeanFactory 的成员属性： 1String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; 区分是 FactoryBean 还是创建的 Bean，加上 &amp; 代表是工厂，getBean 将会返回工厂 FactoryBean：如果某个 bean 的配置非常复杂，或者想要使用编码的形式去构建它，可以提供一个构建该 bean 实例的工厂，这个工厂就是 FactoryBean 接口实现类，FactoryBean 接口实现类也是需要 Spring 管理 这里产生两种对象，一种是 FactoryBean 接口实现类（IOC 管理），另一种是 FactoryBean 接口内部管理的对象 获取 FactoryBean 接口实现类，使用 getBean 时传的 beanName 需要带 &amp; 开头 获取 FactoryBean 内部管理的对象，不需要带 &amp; 开头 BeanFactory 的基本使用： 123Resource res = new ClassPathResource(&quot;applicationContext.xml&quot;);BeanFactory bf = new XmlBeanFactory(res);UserService userService = (UserService)bf.getBean(&quot;userService&quot;); FactoryBeanFactoryBean：对单一的 bean 的初始化过程进行封装，达到简化配置的目的 FactoryBean与 BeanFactory 区别： FactoryBean：封装单个 bean 的创建过程，就是工厂的 Bean BeanFactory：Spring 容器顶层接口，定义了 bean 相关的获取操作 代码实现： FactoryBean，实现类一般是 MapperFactoryBean，创建 DAO 层接口的实现类 12345678910111213141516public class EquipmentDaoImplFactoryBean implements FactoryBean &#123; @Override\t//获取Bean public Object getObject() throws Exception &#123; return new EquipmentDaoImpl(); &#125; @Override\t//获取bean的类型 public Class&lt;?&gt; getObjectType() &#123; return null; &#125; @Override\t//是否单例 public boolean isSingleton() &#123; return false; &#125;&#125; MapperFactoryBean 继承 SqlSessionDaoSupport，可以获取 SqlSessionTemplate，完成 MyBatis 的整合 12345678910public abstract class SqlSessionDaoSupport extends DaoSupport &#123; private SqlSessionTemplate sqlSessionTemplate;\t// 获取 SqlSessionTemplate 对象\tpublic void setSqlSessionFactory(SqlSessionFactory sqlSessionFactory) &#123; if (this.sqlSessionTemplate == null || sqlSessionFactory != this.sqlSessionTemplate.getSqlSessionFactory()) &#123; this.sqlSessionTemplate = createSqlSessionTemplate(sqlSessionFactory); &#125; &#125;&#125; 过滤器数据准备 DAO 层 UserDao、AccountDao、BookDao、EquipmentDao 123public interface UserDao &#123;\tpublic void save();&#125; 1234567@Component(&quot;userDao&quot;)public class UserDaoImpl implements UserDao &#123; public void save() &#123; System.out.println(&quot;user dao running...&quot;); &#125;&#125; Service 业务层 123public interface UserService &#123; public void save();&#125; 12345678910@Service(&quot;userService&quot;)public class UserServiceImpl implements UserService &#123; @Autowired private UserDao userDao;//...........BookDao等 public void save() &#123; System.out.println(&quot;user service running...&quot;); userDao.save(); &#125;&#125; 过滤器名称：TypeFilter 类型：接口 作用：自定义类型过滤器 示例： config &#x2F; filter &#x2F; MyTypeFilter 1234567891011121314151617181920212223242526public class MyTypeFilter implements TypeFilter &#123; @Override /** * metadataReader:读取到的当前正在扫描的类的信息 * metadataReaderFactory:可以获取到任何其他类的信息 */ //加载的类满足要求，匹配成功 public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; //获取当前类注解的信息 AnnotationMetadata am = metadataReader.getAnnotationMetadata(); //获取当前正在扫描的类的类信息 ClassMetadata classMetadata = metadataReader.getClassMetadata(); //获取当前类资源（类的路径） Resource resource = metadataReader.getResource(); //通过类的元数据获取类的名称 String className = classMetadata.getClassName(); //如果加载的类名满足过滤器要求，返回匹配成功 if(className.equals(&quot;service.impl.UserServiceImpl&quot;))&#123; //返回true表示匹配成功，返回false表示匹配失败。此处仅确认匹配结果，不会确认是排除还是加入，排除/加入由配置项决定，与此处无关 return true; &#125; return false; &#125;&#125; SpringConfig 1234567891011@Configuration//设置排除bean，排除的规则是自定义规则（FilterType.CUSTOM），具体的规则定义为MyTypeFilter@ComponentScan( value = &#123;&quot;dao&quot;,&quot;service&quot;&#125;, excludeFilters = @ComponentScan.Filter( type= FilterType.CUSTOM, classes = MyTypeFilter.class ))public class SpringConfig &#123;&#125; 导入器bean 只有通过配置才可以进入 Spring 容器，被 Spring 加载并控制 配置 bean 的方式如下： XML 文件中使用 标签配置 使用 @Component 及衍生注解配置 导入器可以快速高效导入大量 bean，替代 @Import({a.class,b.class})，无需在每个类上添加 @Bean 名称： ImportSelector 类型：接口 作用：自定义bean导入器 selector &#x2F; MyImportSelector 12345678910111213141516public class MyImportSelector implements ImportSelector&#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123;// 1.编程形式加载一个类// return new String[]&#123;&quot;dao.impl.BookDaoImpl&quot;&#125;;// 2.加载import.properties文件中的单个类名// ResourceBundle bundle = ResourceBundle.getBundle(&quot;import&quot;);// String className = bundle.getString(&quot;className&quot;);// 3.加载import.properties文件中的多个类名 ResourceBundle bundle = ResourceBundle.getBundle(&quot;import&quot;); String className = bundle.getString(&quot;className&quot;); return className.split(&quot;,&quot;); &#125;&#125; import.properties 12345678#2.加载import.properties文件中的单个类名#className=dao.impl.BookDaoImpl#3.加载import.properties文件中的多个类名#className=dao.impl.BookDaoImpl,dao.impl.AccountDaoImpl#4.导入包中的所有类path=dao.impl.* SpringConfig 12345@Configuration@ComponentScan(&#123;&quot;dao&quot;,&quot;service&quot;&#125;)@Import(MyImportSelector.class)public class SpringConfig &#123;&#125; 注册器可以取代 ComponentScan 扫描器 名称：ImportBeanDefinitionRegistrar 类型：接口 作用：自定义 bean 定义注册器 registrar &#x2F; MyImportBeanDefinitionRegistrar 1234567891011121314151617181920212223public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123;/** * AnnotationMetadata:当前类的注解信息 * BeanDefinitionRegistry:BeanDefinition注册类，把所有需要添加到容器中的bean调用registerBeanDefinition手工注册进来 */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; //自定义注册器 //1.开启类路径bean定义扫描器，需要参数bean定义注册器BeanDefinitionRegistry，需要制定是否使用默认类型过滤器 ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(registry,false); //2.添加包含性加载类型过滤器（可选，也可以设置为排除性加载类型过滤器） scanner.addIncludeFilter(new TypeFilter() &#123; @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; //所有匹配全部成功，此处应该添加实际的业务判定条件 return true; &#125; &#125;); //设置扫描路径 scanner.addExcludeFilter(tf);//排除 scanner.scan(&quot;dao&quot;,&quot;service&quot;); &#125;&#125; SpringConfig 1234@Configuration@Import(MyImportBeanDefinitionRegistrar.class)public class SpringConfig &#123;&#125; 处理器通过创建类继承相应的处理器的接口，重写后置处理的方法，来实现拦截 Bean 的生命周期来实现自己自定义的逻辑 BeanPostProcessor：bean 后置处理器，bean 创建对象初始化前后进行拦截工作的 BeanFactoryPostProcessor：beanFactory 的后置处理器 加载时机：在 BeanFactory 初始化之后调用，来定制和修改 BeanFactory 的内容；所有的 bean 定义已经保存加载到 beanFactory，但是 bean 的实例还未创建 执行流程： ioc 容器创建对象 invokeBeanFactoryPostProcessors(beanFactory)：执行 BeanFactoryPostProcessor 在 BeanFactory 中找到所有类型是 BeanFactoryPostProcessor 的组件，并执行它们的方法 在初始化创建其他组件前面执行 BeanDefinitionRegistryPostProcessor： 加载时机：在所有 bean 定义信息将要被加载，但是 bean 实例还未创建，优先于 BeanFactoryPostProcessor 执行；利用 BeanDefinitionRegistryPostProcessor 给容器中再额外添加一些组件 执行流程： ioc 容器创建对象 refresh() → invokeBeanFactoryPostProcessors(beanFactory) 从容器中获取到所有的 BeanDefinitionRegistryPostProcessor 组件 依次触发所有的 postProcessBeanDefinitionRegistry() 方法 再来触发 postProcessBeanFactory() 方法 监听器基本概述ApplicationListener：监听容器中发布的事件，完成事件驱动模型开发 1public interface ApplicationListener&lt;E extends ApplicationEvent&gt; 所以监听 ApplicationEvent 及其下面的子事件 应用监听器步骤： 写一个监听器（ApplicationListener实现类）来监听某个事件（ApplicationEvent及其子类） 把监听器加入到容器 @Component 只要容器中有相关事件的发布，就能监听到这个事件； * ContextRefreshedEvent：容器刷新完成（所有 bean 都完全创建）会发布这个事件 * ContextClosedEvent：关闭容器会发布这个事件 发布一个事件：`applicationContext.publishEvent()` 12345678@Componentpublic class MyApplicationListener implements ApplicationListener&lt;ApplicationEvent&gt; &#123;\t//当容器中发布此事件以后，方法触发\t@Override\tpublic void onApplicationEvent(ApplicationEvent event) &#123; System.out.println(&quot;收到事件：&quot; + event);\t&#125;&#125; 实现原理ContextRefreshedEvent 事件： 容器初始化过程中执行 initApplicationEventMulticaster()：初始化事件多播器 先去容器中查询 id = applicationEventMulticaster 的组件，有直接返回 没有就执行 this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory) 并且加入到容器中 以后在其他组件要派发事件，自动注入这个 applicationEventMulticaster 容器初始化过程执行 registerListeners() 注册监听器 从容器中获取所有监听器：getBeanNamesForType(ApplicationListener.class, true, false) 将 listener 注册到 ApplicationEventMulticaster 容器刷新完成：finishRefresh() → publishEvent(new ContextRefreshedEvent(this)) 发布 ContextRefreshedEvent 事件： 获取事件的多播器（派发器）：getApplicationEventMulticaster() multicastEvent 派发事件 获取到所有的 ApplicationListener 遍历 ApplicationListener 如果有 Executor，可以使用 Executor 异步派发 Executor executor = getTaskExecutor() 没有就同步执行 listener 方法 invokeListener(listener, event)，拿到 listener 回调 onApplicationEvent 容器关闭会发布 ContextClosedEvent 注解实现注解：@EventListener 基本使用： 1234567@Servicepublic class UserService&#123; @EventListener(classes=&#123;ApplicationEvent.class&#125;)\tpublic void listen(ApplicationEvent event)&#123; System.out.println(&quot;UserService。。监听到的事件：&quot; + event);\t&#125;&#125; 原理：使用 EventListenerMethodProcessor 处理器来解析方法上的 @EventListener，Spring 扫描使用注解的方法，并为之创建一个监听对象 SmartInitializingSingleton 原理：afterSingletonsInstantiated() IOC 容器创建对象并 refresh() finishBeanFactoryInitialization(beanFactory)：初始化剩下的单实例 bean 先创建所有的单实例 bean：getBean() 获取所有创建好的单实例 bean，判断是否是 SmartInitializingSingleton 类型的，如果是就调用 afterSingletonsInstantiated() AOP基本概述AOP（Aspect Oriented Programing）：面向切面编程，一种编程范式，指导开发者如何组织程序结构 AOP 弥补了 OOP 的不足，基于 OOP 基础之上进行横向开发： uOOP 规定程序开发以类为主体模型，一切围绕对象进行，完成某个任务先构建模型 uAOP 程序开发主要关注基于 OOP 开发中的共性功能，一切围绕共性功能进行，完成某个任务先构建可能遇到的所有共性功能（当所有功能都开发出来也就没有共性与非共性之分），将软件开发由手工制作走向半自动化&#x2F;全自动化阶段，实现“插拔式组件体系结构”搭建 AOP 作用： 提高代码的可重用性 业务代码编码更简洁 业务代码维护更高效 业务功能扩展更便捷 核心概念概念详解 Joinpoint（连接点）：就是方法 Pointcut（切入点）：就是挖掉共性功能的方法 Advice（通知）：就是共性功能，最终以一个方法的形式呈现 Aspect（切面）：就是共性功能与挖的位置的对应关系 Target（目标对象）：就是挖掉功能的方法对应的类产生的对象，这种对象是无法直接完成最终工作的 Weaving（织入）：就是将挖掉的功能回填的动态过程 Proxy（代理）：目标对象无法直接完成工作，需要对其进行功能回填，通过创建原始对象的代理对象实现 Introduction（引入&#x2F;引介）：就是对原始对象无中生有的添加成员变量或成员方法 入门项目开发步骤： 开发阶段 制作程序 将非共性功能开发到对应的目标对象类中，并制作成切入点方法 将共性功能独立开发出来，制作成通知 在配置文件中，声明切入点 在配置文件中，声明切入点与通知间的关系（含通知类型），即切面 运行阶段（AOP 完成） Spring 容器加载配置文件，监控所有配置的切入点方法的执行 当监控到切入点方法被运行，使用代理机制，动态创建目标对象的代理对象，根据通知类别，在代理对象的对应位置将通知对应的功能织入，完成完整的代码逻辑并运行 导入坐标 pom.xml 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.9.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.9.4&lt;/version&gt;&lt;/dependency&gt; 业务层抽取通用代码 service &#x2F; UserServiceImpl 123public interface UserService &#123; public void save();&#125; 1234567public class UserServiceImpl implements UserService &#123; @Override public void save() &#123; //System.out.println(&quot;共性功能&quot;); System.out.println(&quot;user service running...&quot;); &#125;&#125; aop.AOPAdvice 1234567//1.制作通知类，在类中定义一个方法用于完成共性功能public class AOPAdvice &#123; //共性功能抽取后职称独立的方法 public void function()&#123; System.out.println(&quot;共性功能&quot;); &#125;&#125; 把通知加入spring容器管理，配置aop applicationContext.xml 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop https://www.springframework.org/schema/aop/spring-aop.xsd &quot;&gt; &lt;!--原始Spring控制资源--&gt; &lt;bean id=&quot;userService&quot; class= &quot;service.impl.UserServiceImpl&quot;/&gt; &lt;!--2.配置共性功能成功spring控制的资源--&gt; &lt;bean id=&quot;myAdvice&quot; class=&quot;aop.AOPAdvice&quot;/&gt; &lt;!--3.开启AOP命名空间: beans标签内--&gt; &lt;!--4.配置AOP--&gt; &lt;aop:config&gt; &lt;!--5.配置切入点--&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* *..*(..))&quot;/&gt; &lt;!--6.配置切面（切入点与通知的关系）--&gt; &lt;aop:aspect ref=&quot;myAdvice&quot;&gt; &lt;!--7.配置具体的切入点对应通知中那个操作方法--&gt; &lt;aop:before method=&quot;function&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 测试类 1234567public class App &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); UserService userService = (UserService) ctx.getBean(&quot;userService&quot;); userService.save();//先输出共性功能，然后 user service running... &#125;&#125; XML开发AspectJAspect（切面）用于描述切入点与通知间的关系，是 AOP 编程中的一个概念 AspectJ 是基于 java 语言对 Aspect 的实现 AOPconfig标签：aop:config， 的子标签 作用：设置 AOP 格式： 12345&lt;beans&gt; &lt;aop:config&gt;……&lt;/aop:config&gt; &lt;aop:config&gt;……&lt;/aop:config&gt; &lt;!--一个beans标签中可以配置多个aop:config标签--&gt;&lt;/beans&gt; pointcut标签：aop:pointcut，归属于 aop:config 标签和 aop:aspect 标签 作用：设置切入点 格式： 123456&lt;aop:config&gt; &lt;aop:pointcut id=&quot;pointcutId&quot; expression=&quot;……&quot;/&gt; &lt;aop:aspect&gt; &lt;aop:pointcut id=&quot;pointcutId&quot; expression=&quot;……&quot;/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 说明： 一个 aop:config 标签中可以配置多个 aop:pointcut 标签，且该标签可以配置在 aop:aspect 标签内 属性： id ：识别切入点的名称 expression ：切入点表达式 aspect标签：aop:aspect，aop:config 的子标签 作用：设置具体的 AOP 通知对应的切入点（切面） 格式： 12345&lt;aop:config&gt; &lt;aop:aspect ref=&quot;beanId&quot;&gt;……&lt;/aop:aspect&gt; &lt;aop:aspect ref=&quot;beanId&quot;&gt;……&lt;/aop:aspect&gt; &lt;!--一个aop:config标签中可以配置多个aop:aspect标签--&gt;&lt;/aop:config&gt; 属性： ref ：通知所在的 bean 的 id Pointcut切入点切入点描述的是某个方法 切入点表达式是一个快速匹配方法描述的通配格式，类似于正则表达式 表达式格式： 1关键字(访问修饰符 返回值 包名.类名.方法名(参数)异常名) 示例： 12//匹配UserService中只含有一个参数的findById方法execution(public User service.UserService.findById(int)) 格式解析： 关键字：描述表达式的匹配模式（参看关键字列表） 访问修饰符：方法的访问控制权限修饰符 类名：方法所在的类（此处可以配置接口名称） 异常：方法定义中指定抛出的异常 关键字： execution ：匹配执行指定方法 args ：匹配带有指定参数类型的方法 within、this、target、@within、@target、@args、@annotation、bean、reference pointcut等 通配符： *：单个独立的任意符号，可以独立出现，也可以作为前缀或者后缀的匹配符出现 12//匹配com.seazean包下的任意包中的UserService类或接口中所有find开头的带有一个任意参数的方法execution(public * com.seazean.*.UserService.find*(*) .. ：多个连续的任意符号，可以独立出现，常用于简化包名与参数 12//匹配com包下的任意包中的UserService类或接口中所有名称为findById参数任意数量和类型的方法execution(public User com..UserService.findById(..)) +：专用于匹配子类类型 12//匹配任意包下的Service结尾的类或者接口的子类或者实现类execution(* *..*Service+.*(..)) 逻辑运算符： &amp;&amp;：连接两个切入点表达式，表示两个切入点表达式同时成立的匹配 ||：连接两个切入点表达式，表示两个切入点表达式成立任意一个的匹配 ! ：连接单个切入点表达式，表示该切入点表达式不成立的匹配 示例： 1234567891011121314151617181920execution(* *(..)) //前三个都是匹配全部execution(* *..*(..))execution(* *..*.*(..))execution(public * *..*.*(..))execution(public int *..*.*(..))execution(public void *..*.*(..))execution(public void com..*.*(..)) execution(public void com..service.*.*(..))execution(public void com.seazean.service.*.*(..))execution(public void com.seazean.service.User*.*(..))execution(public void com.seazean.service.*Service.*(..))execution(public void com.seazean.service.UserService.*(..))execution(public User com.seazean.service.UserService.find*(..))\t//find开头execution(public User com.seazean.service.UserService.*Id(..)) //Iexecution(public User com.seazean.service.UserService.findById(..))execution(public User com.seazean.service.UserService.findById(int))execution(public User com.seazean.service.UserService.findById(int,int))execution(public User com.seazean.service.UserService.findById(int,*))execution(public User com.seazean.service.UserService.findById())execution(List com.seazean.service.*Service+.findAll(..)) 配置方式XML 配置规则： 企业开发命名规范严格遵循规范文档进行 先为方法配置局部切入点，再抽取类中公共切入点，最后抽取全局切入点 代码走查过程中检测切入点是否存在越界性包含 代码走查过程中检测切入点是否存在非包含性进驻 设定 AOP 执行检测程序，在单元测试中监控通知被执行次数与预计次数是否匹配（不绝对正确：加进一个不该加的，删去一个不该删的相当于结果不变） 设定完毕的切入点如果发生调整务必进行回归测试 1234567891011121314&lt;aop:config&gt; &lt;!--1.配置公共切入点--&gt; &lt;aop:pointcut id=&quot;pt1&quot; expression=&quot;execution(* *(..))&quot;/&gt; &lt;aop:aspect ref=&quot;myAdvice&quot;&gt; &lt;!--2.配置局部切入点--&gt; &lt;aop:pointcut id=&quot;pt2&quot; expression=&quot;execution(* *(..))&quot;/&gt; &lt;!--引用公共切入点--&gt; &lt;aop:before method=&quot;logAdvice&quot; pointcut-ref=&quot;pt1&quot;/&gt; &lt;!--引用局部切入点--&gt; &lt;aop:before method=&quot;logAdvice&quot; pointcut-ref=&quot;pt2&quot;/&gt; &lt;!--3.直接配置切入点--&gt; &lt;aop:before method=&quot;logAdvice&quot; pointcut=&quot;execution(* *(..))&quot;/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; Advice通知类型AOP 的通知类型共5种：前置通知，后置通知、返回后通知、抛出异常后通知、环绕通知 before标签：aop:before，aop:aspect的子标签 作用：设置前置通知 前置通知：原始方法执行前执行，如果通知中抛出异常，阻止原始方法运行 应用：数据校验 格式： 1234&lt;aop:aspect ref=&quot;adviceId&quot;&gt; &lt;aop:before method=&quot;methodName&quot; pointcut=&quot;execution(* *(..))&quot;/&gt; &lt;!--一个aop:aspect标签中可以配置多个aop:before标签--&gt;&lt;/aop:aspect&gt; 基本属性： method：在通知类中设置当前通知类别对应的方法 pointcut：设置当前通知对应的切入点表达式，与pointcut-ref属性冲突 pointcut-ref：设置当前通知对应的切入点id，与pointcut属性冲突 after标签：aop:after，aop:aspect的子标签 作用：设置后置通知 后置通知：原始方法执行后执行，无论原始方法中是否出现异常，都将执行通知 应用：现场清理 格式： 1234&lt;aop:aspect ref=&quot;adviceId&quot;&gt; &lt;aop:after method=&quot;methodName&quot; pointcut=&quot;execution(* *(..))&quot;/&gt; &lt;!--一个aop:aspect标签中可以配置多个aop:after标签--&gt;&lt;/aop:aspect&gt; 基本属性： method：在通知类中设置当前通知类别对应的方法 pointcut：设置当前通知对应的切入点表达式，与pointcut-ref属性冲突 pointcut-ref：设置当前通知对应的切入点id，与pointcut属性冲突 after-r标签：aop:after-returning，aop:aspect的子标签 作用：设置返回后通知 返回后通知：原始方法正常执行完毕并返回结果后执行，如果原始方法中抛出异常，无法执行 应用：返回值相关数据处理 格式： 1234&lt;aop:aspect ref=&quot;adviceId&quot;&gt; &lt;aop:after-returning method=&quot;methodName&quot; pointcut=&quot;execution(* *(..))&quot;/&gt; &lt;!--一个aop:aspect标签中可以配置多个aop:after-returning标签--&gt;&lt;/aop:aspect&gt; 基本属性： method：在通知类中设置当前通知类别对应的方法 pointcut：设置当前通知对应的切入点表达式，与pointcut-ref属性冲突 pointcut-ref：设置当前通知对应的切入点id，与pointcut属性冲突 returning：设置接受返回值的参数，与通知类中对应方法的参数一致 after-t标签：aop:after-throwing，aop:aspect的子标签 作用：设置抛出异常后通知 抛出异常后通知：原始方法抛出异常后执行，如果原始方法没有抛出异常，无法执行 应用：对原始方法中出现的异常信息进行处理 格式： 1234&lt;aop:aspect ref=&quot;adviceId&quot;&gt; &lt;aop:after-throwing method=&quot;methodName&quot; pointcut=&quot;execution(* *(..))&quot;/&gt; &lt;!--一个aop:aspect标签中可以配置多个aop:after-throwing标签--&gt;&lt;/aop:aspect&gt; 基本属性： method：在通知类中设置当前通知类别对应的方法 pointcut：设置当前通知对应的切入点表达式，与pointcut-ref属性冲突 pointcut-ref：设置当前通知对应的切入点id，与pointcut属性冲突 throwing：设置接受异常对象的参数，与通知类中对应方法的参数一致 around标签：aop:around，aop:aspect的子标签 作用：设置环绕通知 环绕通知：在原始方法执行前后均有对应执行执行，还可以阻止原始方法的执行 应用：功能强大，可以做任何事情 格式： 1234&lt;aop:aspect ref=&quot;adviceId&quot;&gt; &lt;aop:around method=&quot;methodName&quot; pointcut=&quot;execution(* *(..))&quot;/&gt; &lt;!--一个aop:aspect标签中可以配置多个aop:around标签--&gt;&lt;/aop:aspect&gt; 基本属性： method ：在通知类中设置当前通知类别对应的方法 pointcut ：设置当前通知对应的切入点表达式，与pointcut-ref属性冲突 pointcut-ref ：设置当前通知对应的切入点id，与pointcut属性冲突 环绕通知的开发方式（参考通知顺序章节）： 环绕通知是在原始方法的前后添加功能，在环绕通知中，存在对原始方法的显式调用 1234public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; Object ret = pjp.proceed(); return ret;&#125; 环绕通知方法相关说明： 方法须设定 Object 类型的返回值，否则会拦截原始方法的返回。如果原始方法返回值类型为 void，通知方法也可以设定返回值类型为 void，最终返回 null 方法需在第一个参数位置设定 ProceedingJoinPoint 对象，通过该对象调用 proceed() 方法，实现对原始方法的调用。如省略该参数，原始方法将无法执行 使用 proceed() 方法调用原始方法时，因无法预知原始方法运行过程中是否会出现异常，强制抛出 Throwable 对象，封装原始方法中可能出现的异常信息 通知顺序当同一个切入点配置了多个通知时，通知会存在运行的先后顺序，该顺序以通知配置的顺序为准。 AOPAdvice 123456789101112131415161718192021public class AOPAdvice &#123; public void before()&#123; System.out.println(&quot;before...); &#125; public void after()&#123; System.out.println(&quot;after...&quot;); &#125; public void afterReturing()&#123; System.out.println(&quot;afterReturing...&quot;); &#125; public void afterThrowing()&#123; System.out.println(&quot;afterThrowing...&quot;); &#125; public Object around(ProceedingJoinPoint pjp) &#123; System.out.println(&quot;around before...&quot;); //对原始方法的调用 Object ret = pjp.proceed(); System.out.println(&quot;around after...&quot;+ret); return ret; &#125;&#125; applicationContext.xml 顺序执行 12345678910&lt;aop:config&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* *..*(..))&quot;/&gt; &lt;aop:aspect ref=&quot;myAdvice&quot;&gt; &lt;aop:before method=&quot;before&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;aop:after method=&quot;after&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;aop:after-returning method=&quot;afterReturing&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;aop:after-throwing method=&quot;afterThrowing&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 获取数据参数第一种方式： 设定通知方法第一个参数为 JoinPoint，通过该对象调用 getArgs() 方法，获取原始方法运行的参数数组 123public void before(JoinPoint jp) throws Throwable &#123; Object[] args = jp.getArgs();&#125; 所有的通知均可以获取参数，环绕通知使用ProceedingJoinPoint.getArgs()方法 第二种方式： 设定切入点表达式为通知方法传递参数（锁定通知变量名） 流程图： 解释： &amp;amp 代表并且 &amp; 输出结果：a &#x3D; param1 b &#x3D; param2 第三种方式： 设定切入点表达式为通知方法传递参数（改变通知变量名的定义顺序） 流程图： 解释：输出结果 a &#x3D; param2 b &#x3D; param1 返回值环绕通知和返回后通知可以获取返回值，后置通知不一定，其他类型获取不到 第一种方式：适用于返回后通知（after-returning） 设定返回值变量名 原始方法： 1234567public class UserServiceImpl implements UserService &#123; @Override public int save() &#123; System.out.println(&quot;user service running...&quot;); return 100; &#125;&#125; AOP 配置： 1234&lt;aop:aspect ref=&quot;myAdvice&quot;&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* *(..))&quot;/&gt; &lt;aop:after-returning method=&quot;afterReturning&quot; pointcut-ref=&quot;pt&quot; returning=&quot;ret&quot;/&gt;&lt;/aop:aspect&gt; 通知类： 12345public class AOPAdvice &#123; public void afterReturning(Object ret) &#123; System.out.println(&quot;return:&quot; + ret); &#125;&#125; 第二种：适用于环绕通知（around） 在通知类的方法中调用原始方法获取返回值 原始方法： 1234567public class UserServiceImpl implements UserService &#123; @Override public int save() &#123; System.out.println(&quot;user service running...&quot;); return 100; &#125;&#125; AOP 配置： 1234&lt;aop:aspect ref=&quot;myAdvice&quot;&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* *(..)) &quot;/&gt; &lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;pt&quot; /&gt;&lt;/aop:aspect&gt; 通知类： 123456public class AOPAdvice &#123; public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; Object ret = pjp.proceed(); return ret; &#125;&#125; 测试类： 12345678public class App &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); UserService userService = (UserService) ctx.getBean(&quot;userService&quot;); int ret = userService.save(); System.out.println(&quot;app.....&quot; + ret); &#125;&#125; 异常环绕通知和抛出异常后通知可以获取异常，后置通知不一定，其他类型获取不到 第一种：适用于返回后通知（after-throwing） 设定异常对象变量名 原始方法 1234567public class UserServiceImpl implements UserService &#123; @Override\tpublic void save() &#123; System.out.println(&quot;user service running...&quot;); int i = 1/0; &#125;&#125; AOP 配置 1234&lt;aop:aspect ref=&quot;myAdvice&quot;&gt;\t&lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* *(..)) &quot;/&gt; &lt;aop:after-throwing method=&quot;afterThrowing&quot; pointcut-ref=&quot;pt&quot; throwing=&quot;t&quot;/&gt;&lt;/aop:aspect&gt; 通知类 123public void afterThrowing(Throwable t)&#123; System.out.println(t.getMessage());&#125; 第二种：适用于环绕通知（around） 在通知类的方法中调用原始方法捕获异常 原始方法： 1234567public class UserServiceImpl implements UserService &#123; @Override\tpublic void save() &#123; System.out.println(&quot;user service running...&quot;); int i = 1/0; &#125;&#125; AOP 配置： 1234&lt;aop:aspect ref=&quot;myAdvice&quot;&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* *(..)) &quot;/&gt; &lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;pt&quot; /&gt;&lt;/aop:aspect&gt; 通知类：try……catch……捕获异常后，ret为null 123456789public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; Object ret = pjp.proceed();\t//对此处调用进行try……catch……捕获异常，或抛出异常 /* try &#123; ret = pjp.proceed(); &#125; catch (Throwable throwable) &#123; System.out.println(&quot;around exception...&quot; + throwable.getMessage()); &#125;*/ return ret;&#125; 测试类 1userService.delete(); 获取全部 UserService 1234567public interface UserService &#123; public void save(int i, int m); public int update(); public void delete();&#125; 123456789101112131415161718public class UserServiceImpl implements UserService &#123; @Override public void save(int i, int m) &#123; System.out.println(&quot;user service running...&quot; + i + &quot;,&quot; + m); &#125; @Override public int update() &#123; System.out.println(&quot;user service update running...&quot;); return 100; &#125; @Override public void delete() &#123; System.out.println(&quot;user service delete running...&quot;); int i = 1 / 0; &#125;&#125; AOPAdvice 123456789101112131415161718192021222324252627282930313233public class AOPAdvice &#123; public void before(JoinPoint jp)&#123; //通过JoinPoint参数获取调用原始方法所携带的参数 Object[] args = jp.getArgs(); System.out.println(&quot;before...&quot;+args[0]); &#125; public void after(JoinPoint jp)&#123; Object[] args = jp.getArgs(); System.out.println(&quot;after...&quot;+args[0]); &#125; public void afterReturing(Object ret)&#123; System.out.println(&quot;afterReturing...&quot;+ret); &#125; public void afterThrowing(Throwable t)&#123; System.out.println(&quot;afterThrowing...&quot;+t.getMessage()); &#125; public Object around(ProceedingJoinPoint pjp) &#123; System.out.println(&quot;around before...&quot;); Object ret = null; try &#123; //对原始方法的调用 ret = pjp.proceed(); &#125; catch (Throwable throwable) &#123; System.out.println(&quot;around...exception....&quot;+throwable.getMessage()); &#125; System.out.println(&quot;around after...&quot;+ret); return ret; &#125;&#125; applicationContext.xml 123456789101112131415161718192021222324252627&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop https://www.springframework.org/schema/aop/spring-aop.xsd &quot;&gt; &lt;bean id=&quot;userService&quot; class=&quot;service.impl.UserServiceImpl&quot;/&gt; &lt;bean id=&quot;myAdvice&quot; class=&quot;aop.AOPAdvice&quot;/&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* *..*(..))&quot;/&gt; &lt;aop:aspect ref=&quot;myAdvice&quot;&gt; &lt;aop:before method=&quot;before&quot; pointcut=&quot;pt&quot;/&gt; &lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;aop:after method=&quot;after&quot; pointcut=&quot;pt&quot;/&gt; &lt;aop:after-returning method=&quot;afterReturning&quot; pointcut-ref=&quot;pt&quot; returning=&quot;ret&quot;/&gt; &lt;aop:after-throwing method=&quot;afterThrowing&quot; pointcut-ref=&quot;pt&quot; throwing=&quot;t&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 测试类 12345678910public class App &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); UserService userService = (UserService) ctx.getBean(&quot;userService&quot;);// userService.save(666, 888);// int ret = userService.update();// System.out.println(&quot;app.....&quot; + ret); userService.delete(); &#125;&#125; 注解开发AOP注解AOP 注解简化 XML： 注意事项： 切入点最终体现为一个方法，无参无返回值，无实际方法体内容，但不能是抽象方法 引用切入点时必须使用方法调用名称，方法后面的 () 不能省略 切面类中定义的切入点只能在当前类中使用，如果想引用其他类中定义的切入点使用“类名.方法名()”引用 可以在通知类型注解后添加参数，实现 XML 配置中的属性，例如 after-returning 后的 returning 性 启动注解XML开启 AOP 注解支持： 12&lt;aop:aspectj-autoproxy/&gt;&lt;context:component-scan base-package=&quot;aop,config,service&quot;/&gt;&lt;!--启动Spring扫描--&gt; 开发步骤： 导入坐标（伴随 spring-context 坐标导入已经依赖导入完成） 开启 AOP 注解支持 配置切面 @Aspect 定义专用的切入点方法，并配置切入点 @Pointcut 为通知方法配置通知类型及对应切入点 @Before 纯注解注解：@EnableAspectJAutoProxy 位置：Spring 注解配置类定义上方 作用：设置当前类开启 AOP 注解驱动的支持，加载 AOP 注解 格式： 12345@Configuration@ComponentScan(&quot;com.seazean&quot;)@EnableAspectJAutoProxypublic class SpringConfig &#123;&#125; 基本注解Aspect注解：@Aspect 位置：类定义上方 作用：设置当前类为切面类 格式： 123@Aspectpublic class AopAdvice &#123;&#125; Pointcut注解：@Pointcut 位置：方法定义上方 作用：使用当前方法名作为切入点引用名称 格式： 123@Pointcut(&quot;execution(* *(..))&quot;)public void pt() &#123;&#125; 说明：被修饰的方法忽略其业务功能，格式设定为无参无返回值的方法，方法体内空实现（非抽象） Before注解：@Before 位置：方法定义上方 作用：标注当前方法作为前置通知 格式： 1234@Before(&quot;pt()&quot;)public void before(JoinPoint joinPoint)&#123; //joinPoint.getArgs();&#125; 注意：多个参数时，JoinPoint参数一定要在第一位 After注解：@After 位置：方法定义上方 作用：标注当前方法作为后置通知 格式： 123@After(&quot;pt()&quot;)public void after()&#123;&#125; AfterR注解：@AfterReturning 位置：方法定义上方 作用：标注当前方法作为返回后通知 格式： 123@AfterReturning(value=&quot;pt()&quot;, returning = &quot;result&quot;)public void afterReturning(Object result) &#123;&#125; 特殊参数： returning ：设定使用通知方法参数接收返回值的变量名 AfterT注解：@AfterThrowing 位置：方法定义上方 作用：标注当前方法作为异常后通知 格式： 123@AfterThrowing(value=&quot;pt()&quot;, throwing = &quot;t&quot;)public void afterThrowing(Throwable t)&#123;&#125; 特殊参数： throwing ：设定使用通知方法参数接收原始方法中抛出的异常对象名 Around注解：@Around 位置：方法定义上方 作用：标注当前方法作为环绕通知 格式： 12345@Around(&quot;pt()&quot;)public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; Object ret = pjp.proceed(); return ret;&#125; 执行顺序AOP 使用 XML 配置情况下，通知的执行顺序由配置顺序决定，在注解情况下由于不存在配置顺序的概念，参照通知所配置的方法名字符串对应的编码值顺序，可以简单理解为字母排序 同一个通知类中，相同通知类型以方法名排序为准 12345@Before(&quot;aop.AOPPointcut.pt()&quot;)public void aop001Log()&#123;&#125;@Before(&quot;aop.AOPPointcut.pt()&quot;)public void aop002Exception()&#123;&#125; 不同通知类中，以类名排序为准 使用 @Order 注解通过变更 bean 的加载顺序改变通知的加载顺序 12345@Component@Aspect@Order(1) //先执行public class AOPAdvice2 &#123;&#125; 12345@Component@Aspect@Order(2) public class AOPAdvice1 &#123;//默认执行此通知&#125; AOP 原理静态代理装饰者模式（Decorator Pattern）：在不惊动原始设计的基础上，为其添加功能 1234567891011121314public class UserServiceDecorator implements UserService&#123; private UserService userService; public UserServiceDecorator(UserService userService) &#123; this.userService = userService; &#125; public void save() &#123; //原始调用 userService.save(); //增强功能（后置） System.out.println(&quot;后置增强功能&quot;); &#125;&#125; ProxyJDKProxy 动态代理是针对对象做代理，要求原始对象具有接口实现，并对接口方法进行增强，因为代理类继承Proxy 静态代理和动态代理的区别： 静态代理是在编译时就已经将接口、代理类、被代理类的字节码文件确定下来 动态代理是程序在运行后通过反射创建字节码文件交由 JVM 加载 1234567891011121314151617181920public class UserServiceJDKProxy &#123; public static UserService createUserServiceJDKProxy(UserService userService) &#123; UserService service = (UserService) Proxy.newProxyInstance( userService.getClass().getClassLoader(),//获取被代理对象的类加载器 userService.getClass().getInterfaces(),\t//获取被代理对象实现的接口 new InvocationHandler() &#123; //对原始方法执行进行拦截并增强 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if (method.getName().equals(&quot;save&quot;)) &#123; System.out.println(&quot;前置增强&quot;); Object ret = method.invoke(userService, args); System.out.println(&quot;后置增强&quot;); return ret; &#125; return null; &#125; &#125;); return service; &#125;&#125; CGLIBCGLIB（Code Generation Library）：Code 生成类库 CGLIB 特点： CGLIB 动态代理不限定是否具有接口，可以对任意操作进行增强 CGLIB 动态代理无需要原始被代理对象，动态创建出新的代理对象 CGLIB 继承被代理类，如果代理类是 final 则不能实现 CGLIB 类 JDKProxy 仅对接口方法做增强，CGLIB 对所有方法做增强，包括 Object 类中的方法（toString、hashCode） 返回值类型采用多态向下转型，所以需要设置父类类型 需要对方法进行判断是否是 save，来选择性增强 1234567891011121314151617181920212223242526public class UserServiceImplCglibProxy &#123; public static UserService createUserServiceCglibProxy(Class cls)&#123; //1.创建Enhancer对象（可以理解为内存中动态创建了一个类的字节码） Enhancer enhancer = new Enhancer(); //2.设置Enhancer对象的父类是指定类型UserServerImpl enhancer.setSuperclass(cls); //3.设置回调方法 enhancer.setCallback(new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method m, Object[] args, MethodProxy mp) throws Throwable &#123; //o是被代理出的类创建的对象，所以使用MethodProxy调用，并且是调用父类 //通过调用父类的方法实现对原始方法的调用 Object ret = methodProxy.invokeSuper(o, args); //后置增强内容,需要判断是都是save方法 if (method.getName().equals(&quot;save&quot;)) &#123; System.out.println(&quot;I love Java&quot;); &#125; return ret; &#125; &#125;); //使用Enhancer对象创建对应的对象 return (UserService)enhancer.create(); &#125;&#125; Test类 123456public class App &#123; public static void main(String[] args) &#123; UserService userService = UserServiceCglibProxy.createUserServiceCglibProxy(UserServiceImpl.class); userService.save(); &#125;&#125; 代理选择Spirng 可以通过配置的形式控制使用的代理形式，Spring 会先判断是否实现了接口，如果实现了接口就使用 JDK 动态代理，如果没有实现接口则使用 CGLIB 动态代理，通过配置可以修改为使用 CGLIB XML 配置 12&lt;!--XML配置AOP--&gt;&lt;aop:config proxy-target-class=&quot;false&quot;&gt;&lt;/aop:config&gt; XML 注解支持 12&lt;!--注解配置AOP--&gt;&lt;aop:aspectj-autoproxy proxy-target-class=&quot;false&quot;/&gt; 注解驱动 12//修改为使用 cglib 创建代理对象@EnableAspectJAutoProxy(proxyTargetClass = true) JDK 动态代理和 CGLIB 动态代理的区别： JDK 动态代理只能对实现了接口的类生成代理，没有实现接口的类不能使用。 CGLIB 动态代理即使被代理的类没有实现接口也可以使用，因为 CGLIB 动态代理是使用继承被代理类的方式进行扩展 CGLIB 动态代理是通过继承的方式，覆盖被代理类的方法来进行代理，所以如果方法是被 final 修饰的话，就不能进行代理 织入时机 事务事务机制事务介绍事务：数据库中多个操作合并在一起形成的操作序列，事务特征（ACID） 作用： 当数据库操作序列中个别操作失败时，提供一种方式使数据库状态恢复到正常状态（A），保障数据库即使在异常状态下仍能保持数据一致性（C）（要么操作前状态，要么操作后状态） 当出现并发访问数据库时，在多个访问间进行相互隔离，防止并发访问操作结果互相干扰（I） Spring 事务一般加到业务层，对应着业务的操作，Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的，Spring 只提供统一事务管理接口 Spring 在事务开始时，根据当前环境中设置的隔离级别，调整数据库隔离级别，由此保持一致。程序是否支持事务首先取决于数据库 ，比如 MySQL ，如果是 Innodb 引擎，是支持事务的；如果 MySQL 使用 MyISAM 引擎，那从根上就是不支持事务的 保证原子性： 要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚 在 MySQL 中，恢复机制是通过回滚日志（undo log） 实现，所有事务进行的修改都会先先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常的话，直接利用回滚日志中的信息将数据回滚到修改之前的样子即可 回滚日志会先于数据持久化到磁盘上，这样保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务 隔离级别TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT：使用后端数据库默认的隔离级别，MySQL 默认采用的 REPEATABLE_READ 隔离级别，Oracle 默认采用的 READ_COMMITTED隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE：最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别 MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 分布式事务：允许多个独立的事务资源（transactional resources）参与到一个全局的事务中。事务资源通常是关系型数据库系统，但也可以是其他类型的资源，全局事务要求在其中的所有参与的事务要么都提交，要么都回滚，这对于事务原有的 ACID 要求又有了提高 在使用分布式事务时，InnoDB 存储引擎的事务隔离级别必须设置为 SERIALIZABLE 传播行为事务传播行为是为了解决业务层方法之间互相调用的事务问题，也就是方法嵌套： 当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。 例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行 123456789101112//外层事务 Service A 的 aMethod 调用内层 Service B 的 bMethodclass A &#123; @Transactional(propagation=propagation.xxx) public void aMethod &#123; B b = new B(); b.bMethod(); &#125;&#125;class B &#123; @Transactional(propagation=propagation.xxx) public void bMethod &#123;&#125;&#125; 支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务则加入该事务；如果当前没有事务则创建一个新的事务 内外层是相同的事务，在 aMethod 或者在 bMethod 内的任何地方出现异常，事务都会被回滚 工作流程： 线程执行到 serviceA.aMethod() 时，其实是执行的代理 serviceA 对象的 aMethod 首先执行事务增强器逻辑（环绕增强），提取事务标签属性，检查当前线程是否绑定 connection 数据库连接资源，没有就调用 datasource.getConnection()，设置事务提交为手动提交 autocommit(false) 执行其他增强器的逻辑，然后调用 target 的目标方法 aMethod() 方法，进入 serviceB 的逻辑 serviceB 也是先执行事务增强器的逻辑，提取事务标签属性，但此时会检查到线程绑定了 connection，检查注解的传播属性，所以调用 DataSourceUtils.getConnection(datasource) 共享该连接资源，执行完相关的增强和 SQL 后，发现事务并不是当前方法开启的，可以直接返回上层 serviceA.aMethod() 继续执行，执行完增强后进行提交事务或回滚事务 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起 内外层是不同的事务，如果 bMethod 已经提交，如果 aMethod 失败回滚 ，bMethod 不会回滚 如果 bMethod 失败回滚，ServiceB 抛出的异常被 ServiceA 捕获，如果 B 抛出的异常是 A 会回滚的异常，aMethod 事务需要回滚，否则仍然可以提交 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务（两个事务没有关系）来运行 如果 ServiceB 异常回滚，可以通过 try-catch 机制执行 ServiceC 如果 ServiceB 提交， ServiceA 可以根据具体的配置决定是 commit 还是 rollback 应用场景：在查询数据的时候要向数据库中存储一些日志，系统不希望存日志的行为影响到主逻辑，可以使用该传播 requied：必须的、supports：支持的、mandatory：强制的、nested：嵌套的 超时属性事务超时，指一个事务所允许执行的最长时间，如果超过该时间限制事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒，默认值为 -1 只读属性对于只有读取数据查询的事务，可以指定事务类型为 readonly，即只读事务；只读事务不涉及数据的修改，数据库会提供一些优化手段，适合用在有多条数据库查询操作的方法中 读操作为什么需要启用事务支持： MySQL 默认对每一个新建立的连接都启用了 autocommit 模式，在该模式下，每一个发送到 MySQL 服务器的 SQL 语句都会在一个单独的事务中进行处理，执行结束后会自动提交事务，并开启一个新的事务 执行多条查询语句，如果方法加上了 @Transactional 注解，这个方法执行的所有 SQL 会被放在一个事务中，如果声明了只读事务的话，数据库就会去优化它的执行，并不会带来其他的收益。如果不加 @Transactional，每条 SQL 会开启一个单独的事务，中间被其它事务修改了数据，比如在前条 SQL 查询之后，后条 SQL 查询之前，数据被其他用户改变，则这次整体的统计查询将会出现读数据不一致的状态 核心对象事务对象J2EE 开发使用分层设计的思想进行，对于简单的业务层转调数据层的单一操作，事务开启在业务层或者数据层并无太大差别，当业务中包含多个数据层的调用时，需要在业务层开启事务，对数据层中多个操作进行组合并归属于同一个事务进行处理 Spring 为业务层提供了整套的事务解决方案： PlatformTransactionManager TransactionDefinition TransactionStatus PTMPlatformTransactionManager，平台事务管理器实现类： DataSourceTransactionManager 适用于 Spring JDBC 或 MyBatis HibernateTransactionManager 适用于 Hibernate3.0 及以上版本 JpaTransactionManager 适用于 JPA JdoTransactionManager 适用于 JDO JtaTransactionManager 适用于 JTA 管理器： JPA（Java Persistence API）Java EE 标准之一，为 POJO 提供持久化标准规范，并规范了持久化开发的统一 API，符合 JPA 规范的开发可以在不同的 JPA 框架下运行 非持久化一个字段： 12345static String transient1; // not persistent because of staticfinal String transient2 = “Satish”; // not persistent because of finaltransient String transient3; // not persistent because of transient@TransientString transient4; // not persistent because of @Transient JDO（Java Data Object）是 Java 对象持久化规范，用于存取某种数据库中的对象，并提供标准化 API。JDBC 仅针对关系数据库进行操作，JDO 可以扩展到关系数据库、XML、对象数据库等，可移植性更强 JTA（Java Transaction API）Java EE 标准之一，允许应用程序执行分布式事务处理。与 JDBC 相比，JDBC 事务则被限定在一个单一的数据库连接，而一个 JTA 事务可以有多个参与者，比如 JDBC 连接、JDO 都可以参与到一个 JTA 事务中 此接口定义了事务的基本操作： 方法 说明 TransactionStatus getTransaction(TransactionDefinition definition) 获取事务 void commit(TransactionStatus status) 提交事务 void rollback(TransactionStatus status) 回滚事务 DefinitionTransactionDefinition 此接口定义了事务的基本信息： 方法 说明 String getName() 获取事务定义名称 boolean isReadOnly() 获取事务的读写属性 int getIsolationLevel() 获取事务隔离级别 int getTimeout() 获取事务超时时间 int getPropagationBehavior() 获取事务传播行为特征 StatusTransactionStatus 此接口定义了事务在执行过程中某个时间点上的状态信息及对应的状态操作： 方法 说明 boolean isNewTransaction() 获取事务是否处于新开始事务状态 voin flush() 刷新事务状态 boolean isCompleted() 获取事务是否处于已完成状态 boolean hasSavepoint() 获取事务是否具有回滚储存点 boolean isRollbackOnly() 获取事务是否处于回滚状态 void setRollbackOnly() 设置事务处于回滚状态 编程式控制方式编程式、声明式（XML）、声明式（注解） 环境准备银行转账业务 包装类 123456public class Account implements Serializable &#123; private Integer id; private String name; private Double money; .....&#125; DAO层接口：AccountDao 1234567public interface AccountDao &#123; //入账操作\tname:入账用户名\tmoney:入账金额 void inMoney(@Param(&quot;name&quot;) String name, @Param(&quot;money&quot;) Double money); //出账操作\tname:出账用户名\tmoney:出账金额 void outMoney(@Param(&quot;name&quot;) String name, @Param(&quot;money&quot;) Double money);&#125; 业务层接口提供转账操作：AccountService 1234public interface AccountService &#123;\t//转账操作\toutName:出账用户名\tinName:入账用户名\tmoney:转账金额\tpublic void transfer(String outName,String inName,Double money);&#125; 业务层实现提供转账操作：AccountServiceImpl 1234567891011public class AccountServiceImpl implements AccountService &#123; private AccountDao accountDao; public void setAccountDao(AccountDao accountDao) &#123; this.accountDao = accountDao; &#125; @Override public void transfer(String outName,String inName,Double money)&#123; accountDao.inMoney(outName,money); accountDao.outMoney(inName,money);\t&#125;&#125; 映射配置文件：dao &#x2F; AccountDao.xml 123456789&lt;mapper namespace=&quot;dao.AccountDao&quot;&gt; &lt;update id=&quot;inMoney&quot;&gt; UPDATE account SET money = money + #&#123;money&#125; WHERE name = #&#123;name&#125; &lt;/update&gt; &lt;update id=&quot;outMoney&quot;&gt; UPDATE account SET money = money - #&#123;money&#125; WHERE name = #&#123;name&#125; &lt;/update&gt;&lt;/mapper&gt; jdbc.properties 1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://192.168.2.185:3306/spring_dbjdbc.username=rootjdbc.password=1234 核心配置文件：applicationContext.xml 123456789101112131415161718192021&lt;context:property-placeholder location=&quot;classpath:*.properties&quot;/&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;accountService&quot; class=&quot;service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;/&gt;&lt;/bean&gt;&lt;bean class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;domain&quot;/&gt;&lt;/bean&gt;&lt;!--扫描映射配置和Dao--&gt;&lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;dao&quot;/&gt;&lt;/bean&gt; 测试类 123ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;ap...xml&quot;);AccountService accountService = (AccountService) ctx.getBean(&quot;accountService&quot;);accountService.transfer(&quot;Jock1&quot;, &quot;Jock2&quot;, 100d); 编程式编程式事务就是代码显式的给出事务的开启和提交 修改业务层实现提供转账操作：AccountServiceImpl 123456789101112131415public void transfer(String outName,String inName,Double money)&#123; //1.创建事务管理器， DataSourceTransactionManager dstm = new DataSourceTransactionManager(); //2.为事务管理器设置与数据层相同的数据源 dstm.setDataSource(dataSource); //3.创建事务定义对象 TransactionDefinition td = new DefaultTransactionDefinition(); //4.创建事务状态对象，用于控制事务执行，【开启事务】 TransactionStatus ts = dstm.getTransaction(td); accountDao.inMoney(inName,money); int i = 1/0; //模拟业务层事务过程中出现错误 accountDao.outMoney(outName,money); //5.提交事务 dstm.commit(ts);&#125; 配置 applicationContext.xml 12345&lt;!--添加属性注入--&gt;&lt;bean id=&quot;accountService&quot; class=&quot;service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;/&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt; AOP改造 将业务层的事务处理功能抽取出来制作成 AOP 通知，利用环绕通知运行期动态织入 12345678910111213141516171819202122public class TxAdvice &#123; private DataSource dataSource; public void setDataSource(DataSource dataSource) &#123; this.dataSource = dataSource; &#125; public Object tx(ProceedingJoinPoint pjp) throws Throwable &#123; //开启事务 PlatformTransactionManager ptm = new DataSourceTransactionManager(dataSource); //事务定义 TransactionDefinition td = new DefaultTransactionDefinition(); //事务状态 TransactionStatus ts = ptm.getTransaction(td); //pjp.getArgs()标准写法，也可以不加，同样可以传递参数 Object ret = pjp.proceed(pjp.getArgs()); //提交事务 ptm.commit(ts); return ret; &#125;&#125; 配置 applicationContext.xml，要开启 AOP 空间 1234567891011121314151617&lt;!--修改bean的属性注入--&gt;&lt;bean id=&quot;accountService&quot; class=&quot;service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;/&gt;&lt;/bean&gt;&lt;!--配置AOP通知类，并注入dataSource--&gt;&lt;bean id=&quot;txAdvice&quot; class=&quot;aop.TxAdvice&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt;&lt;!--使用环绕通知将通知类织入到原始业务对象执行过程中--&gt;&lt;aop:config&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* *..transfer(..))&quot;/&gt; &lt;aop:aspect ref=&quot;txAdvice&quot;&gt; &lt;aop:around method=&quot;tx&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 修改业务层实现提供转账操作：AccountServiceImpl 123456789101112public class AccountServiceImpl implements AccountService &#123; private AccountDao accountDao; public void setAccountDao(AccountDao accountDao) &#123; this.accountDao = accountDao; &#125; @Override public void transfer(String outName,String inName,Double money)&#123; accountDao.inMoney(outName,money); //int i = 1 / 0; accountDao.outMoney(inName,money);\t&#125;&#125; 声明式XMLtx使用删除 TxAdvice 通知类，开启 tx 命名空间，配置 applicationContext.xml 123456789101112131415161718&lt;!--配置平台事务管理器--&gt;&lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt;&lt;!--定义事务管理的通知类--&gt;&lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;!--定义控制的事务--&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;transfer&quot; read-only=&quot;false&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!--使用aop:advisor在AOP配置中引用事务专属通知类，底层invoke调用--&gt;&lt;aop:config&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* service.*Service.*(..))&quot;/&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;pt&quot;/&gt;&lt;/aop:config&gt; aop:advice 与 aop:advisor 区别 aop:advice 配置的通知类可以是普通 Java 对象，不实现接口，也不使用继承关系 aop:advisor 配置的通知类必须实现通知接口，底层 invoke 调用 MethodBeforeAdvice AfterReturningAdvice ThrowsAdvice pom.xml 文件引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;5.1.9.RELEASE&lt;/version&gt;&lt;/dependency&gt; tx配置advice标签：tx:advice，beans 的子标签 作用：专用于声明事务通知 格式： 1234&lt;beans&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;/tx:advice&gt;&lt;/beans&gt; 基本属性： id：用于配置 aop 时指定通知器的 id transaction-manager：指定事务管理器 bean attributes类型：tx:attributes，tx:advice 的子标签 作用：定义通知属性 格式： 1234&lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;tx:attributes&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; method标签：tx:method，tx:attribute 的子标签 作用：设置具体的事务属性 格式： 1234567&lt;tx:attributes&gt; &lt;!--标准格式--&gt; &lt;tx:method name=&quot;*&quot; read-only=&quot;false&quot;/&gt; &lt;tx:method name=&quot;get*&quot; read-only=&quot;true&quot;/&gt; &lt;tx:method name=&quot;find*&quot; read-only=&quot;true&quot;/&gt;&lt;/tx:attributes&gt;&lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* service.*Service.*(..))&quot;/&gt;&lt;!--标准--&gt; 说明：通常事务属性会配置多个，包含 1 个读写的全事务属性，1 个只读的查询类事务属性 属性： name：待添加事务的方法名表达式（支持 * 通配符） read-only：设置事务的读写属性，true 为只读，false 为读写 timeout：设置事务的超时时长，单位秒，-1 为无限长 isolation：设置事务的隔离界别，该隔离级设定是基于 Spring 的设定，非数据库端 no-rollback-for：设置事务中不回滚的异常，多个异常使用 , 分隔 rollback-for：设置事务中必回滚的异常，多个异常使用 , 分隔 propagation：设置事务的传播行为 注解开启注解XML标签：tx:annotation-driven 归属：beans 标签 作用：开启事务注解驱动，并指定对应的事务管理器 范例： 1&lt;tx:annotation-driven transaction-manager=&quot;txManager&quot;/&gt; 纯注解名称：@EnableTransactionManagement 类型：类注解，Spring 注解配置类上方 作用：开启注解驱动，等同 XML 格式中的注解驱动 范例： 1234567@Configuration@ComponentScan(&quot;com.seazean&quot;)@PropertySource(&quot;classpath:jdbc.properties&quot;)@Import(&#123;JDBCConfig.class,MyBatisConfig.class,TransactionManagerConfig.class&#125;)@EnableTransactionManagementpublic class SpringConfig &#123;&#125; 123456public class TransactionManagerConfig &#123; @Bean //自动装配 public PlatformTransactionManager getTransactionManager(@Autowired DataSource dataSource)&#123; return new DataSourceTransactionManager(dataSource); &#125;&#125; 配置注解名称：@Transactional 类型：方法注解，类注解，接口注解 作用：设置当前类&#x2F;接口中所有方法或具体方法开启事务，并指定相关事务属性 范例： 123456789@Transactional( readOnly = false, timeout = -1, isolation = Isolation.DEFAULT, rollbackFor = &#123;ArithmeticException.class, IOException.class&#125;, noRollbackFor = &#123;&#125;, propagation = Propagation.REQUIRES_NEW)public void addAccount&#123;&#125; 说明： @Transactional 注解只有作用到 public 方法上事务才生效 不推荐在接口上使用 @Transactional 注解 原因：在接口上使用注解，只有在使用基于接口的代理（JDK）时才会生效，因为注解是不能继承的，这就意味着如果正在使用基于类的代理（CGLIB）时，那么事务的设置将不能被基于类的代理所识别 正确的设置 @Transactional 的 rollbackFor 和 propagation 属性，否则事务可能会回滚失败 默认情况下，事务只有遇到运行期异常 和 Error 会导致事务回滚，但是在遇到检查型（Checked）异常时不会回滚 继承自 RuntimeException 或 error 的是非检查型异常，比如空指针和索引越界，而继承自 Exception 的则是检查型异常，比如 IOException、ClassNotFoundException，RuntimeException 本身继承 Exception 非检查型类异常可以不用捕获，而检查型异常则必须用 try 语句块把异常交给上级方法，这样事务才能有效 事务不生效的问题 情况 1：确认创建的 MySQL 数据库表引擎是 InnoDB，MyISAM 不支持事务 情况 2：注解到 protected，private 方法上事务不生效，但不会报错 原因：理论上而言，不用 public 修饰，也可以用 aop 实现事务的功能，但是方法私有化让其他业务无法调用 AopUtils.canApply：methodMatcher.matches(method, targetClass) --true--&gt; return trueTransactionAttributeSourcePointcut.matches() ，AbstractFallbackTransactionAttributeSource 中 getTransactionAttribute 方法调用了其本身的 computeTransactionAttribute 方法，当加了事务注解的方法不是 public 时，该方法直接返回 null，所以造成增强不匹配 123456private TransactionAttribute computeTransactionAttribute(Method method, Class&lt;?&gt; targetClass) &#123; // Don&#x27;t allow no-public methods as required. if (allowPublicMethodsOnly() &amp;&amp; !Modifier.isPublic(method.getModifiers())) &#123; return null; &#125;&#125; 情况 3：注解所在的类没有被加载成 Bean 情况 4：在业务层捕捉异常后未向上抛出，事务不生效 原因：在业务层捕捉并处理了异常（try..catch）等于把异常处理掉了，Spring 就不知道这里有错，也不会主动去回滚数据，推荐做法是在业务层统一抛出异常，然后在控制层统一处理 情况 5：遇到检测异常时，也无法回滚 原因：Spring 的默认的事务规则是遇到运行异常（RuntimeException）和程序错误（Error）才会回滚。想针对检测异常进行事务回滚，可以在 @Transactional 注解里使用 rollbackFor 属性明确指定异常 情况 6：Spring 的事务传播策略在内部方法调用时将不起作用，在一个 Service 内部，事务方法之间的嵌套调用，普通方法和事务方法之间的嵌套调用，都不会开启新的事务，事务注解要加到调用方法上才生效 原因：Spring 的事务都是使用 AOP 代理的模式，动态代理 invoke 后会调用原始对象，而原始对象在去调用方法时是不会触发拦截器，就是一个方法调用本对象的另一个方法，所以事务也就无法生效 123456@Transactionalpublic int add()&#123; update();&#125;//注解添加在update方法上无效，需要添加到add()方法上public int update()&#123;&#125; 情况 7：注解在接口上，代理对象是 CGLIB 使用注解 Dao 层 1234567public interface AccountDao &#123; @Update(&quot;update account set money = money + #&#123;money&#125; where name = #&#123;name&#125;&quot;) void inMoney(@Param(&quot;name&quot;) String name, @Param(&quot;money&quot;) Double money); @Update(&quot;update account set money = money - #&#123;money&#125; where name = #&#123;name&#125;&quot;) void outMoney(@Param(&quot;name&quot;) String name, @Param(&quot;money&quot;) Double money);&#125; 业务层 123456789101112public interface AccountService &#123; //对当前方法添加事务，该配置将替换接口的配置 @Transactional( readOnly = false, timeout = -1, isolation = Isolation.DEFAULT, rollbackFor = &#123;&#125;,//java.lang.ArithmeticException.class, IOException.class noRollbackFor = &#123;&#125;, propagation = Propagation.REQUIRED ) public void transfer(String outName, String inName, Double money);&#125; 123456789public class AccountServiceImpl implements AccountService &#123; @Autowired private AccountDao accountDao; public void transfer(String outName, String inName, Double money) &#123; accountDao.inMoney(outName,money); //int i = 1/0; accountDao.outMoney(inName,money); &#125;&#125; 添加文件 Spring.config、Mybatis.config、JDBCConfig (参考ioc_Mybatis)、TransactionManagerConfig 1234567@Configuration@ComponentScan(&#123;&quot;&quot;,&quot;&quot;,&quot;&quot;&#125;)@PropertySource(&quot;classpath:jdbc.properties&quot;)@Import(&#123;JDBCConfig.class,MyBatisConfig.class&#125;)@EnableTransactionManagementpublic class SpringConfig &#123;&#125; 模板对象Spring 模板对象：TransactionTemplate、JdbcTemplate、RedisTemplate、RabbitTemplate、JmsTemplate、HibernateTemplate、RestTemplate JdbcTemplate：提供标准的 sql 语句操作API NamedParameterJdbcTemplate：提供标准的具名 sql 语句操作API RedisTemplate： 1234567public void changeMoney(Integer id, Double money) &#123; redisTemplate.opsForValue().set(&quot;account:id:&quot;+id,money);&#125;public Double findMondyById(Integer id) &#123; Object money = redisTemplate.opsForValue().get(&quot;account:id:&quot; + id); return new Double(money.toString());&#125; 原理XML三大对象： BeanDefinition：是 Spring 中极其重要的一个概念，存储了 bean 对象的所有特征信息，如是否单例、是否懒加载、factoryBeanName 等，和 bean 的关系就是类与对象的关系，一个不同的 bean 对应一个 BeanDefinition BeanDefinationRegistry：存放 BeanDefination 的容器，是一种键值对的形式，通过特定的 Bean 定义的 id，映射到相应的 BeanDefination，BeanFactory 的实现类同样继承 BeanDefinationRegistry 接口，拥有保存 BD 的能力 BeanDefinitionReader：读取配置文件，XML 用 Dom4j 解析，注解用 IO 流加载解析 程序： 12BeanFactory bf = new XmlBeanFactory(new ClassPathResource(&quot;applicationContext.xml&quot;));UserService userService1 = (UserService)bf.getBean(&quot;userService&quot;); 源码解析： 123456789public XmlBeanFactory(Resource resource, BeanFactory parentBeanFactory) &#123; super(parentBeanFactory); this.reader.loadBeanDefinitions(resource);&#125;public int loadBeanDefinitions(Resource resource) &#123; //将 resource 包装成带编码格式的 EncodedResource //EncodedResource 中 getReader()方法，调用java.io包下的 转换流 创建指定编码的输入流对象 return loadBeanDefinitions(new EncodedResource(resource));&#125; XmlBeanDefinitionReader.loadBeanDefinitions()：把 Resource 解析成 BeanDefinition 对象 currentResources = this.resourcesCurrentlyBeingLoaded.get()：拿到当前线程已经加载过的所有 EncodedResoure 资源，用 ThreadLocal 保证线程安全 if (currentResources == null)：判断 currentResources 是否为空，为空则进行初始化 if (!currentResources.add(encodedResource))：如果已经加载过该资源会报错，防止重复加载 inputSource = new InputSource(inputStream)：资源对象包装成 InputSource，InputSource 是 SAX 中的资源对象，用来进行 XML 文件的解析 return doLoadBeanDefinitions()：加载返回 currentResources.remove(encodedResource)：加载完成移除当前 encodedResource resourcesCurrentlyBeingLoaded.remove()：ThreadLocal 为空时移除元素，防止内存泄露 XmlBeanDefinitionReader.doLoadBeanDefinitions(inputSource, resource)：真正的加载函数 Document doc = doLoadDocument(inputSource, resource)：转换成有层次结构的 Document 对象 getEntityResolver()：获取用来解析 DTD、XSD 约束的解析器 getValidationModeForResource(resource)：获取验证模式 int count = registerBeanDefinitions(doc, resource)：将 Document 解析成 BD 对象，注册（添加）到 BeanDefinationRegistry 中，返回新注册的数量 createBeanDefinitionDocumentReader()：创建 DefaultBeanDefinitionDocumentReader 对象 getRegistry().getBeanDefinitionCount()：获取解析前 BeanDefinationRegistry 中的 bd 数量 registerBeanDefinitions(doc, readerContext)：注册 BD this.readerContext = readerContext：保存上下文对象 doRegisterBeanDefinitions(doc.getDocumentElement())：真正的注册 BD 函数 doc.getDocumentElement()：拿出顶层标签 return getRegistry().getBeanDefinitionCount() - countBefore：返回新加入的数量 DefaultBeanDefinitionDocumentReader.doRegisterBeanDefinitions()：注册 BD 到 BR createDelegate(getReaderContext(), root, parent)：beans 是标签的解析器对象 delegate.isDefaultNamespace(root)：判断 beans 标签是否是默认的属性 root.getAttribute(PROFILE_ATTRIBUTE)：解析 profile 属性 preProcessXml(root)：解析前置处理，自定义实现 parseBeanDefinitions(root, this.delegate)：解析 beans 标签中的子标签 parseDefaultElement(ele, delegate)：如果是默认的标签，用该方法解析子标签 判断标签名称，进行相应的解析 processBeanDefinition(ele, delegate)： delegate.parseCustomElement(ele)：解析自定义的标签 postProcessXml(root)：解析后置处理 DefaultBeanDefinitionDocumentReader.processBeanDefinition()：解析 bean 标签并注册到注册中心 delegate.parseBeanDefinitionElement(ele)：解析 bean 标签封装为 BeanDefinitionHolder if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty())：条件一成立说明 name 没有值，条件二成立说明别名有值 beanName = aliases.remove(0)：拿别名列表的第一个元素当作 beanName parseBeanDefinitionElement(ele, beanName, containingBean)：解析 bean 标签 parseState.push(new BeanEntry(beanName))：当前解析器的状态设置为 BeanEntry class 和 parent 属性存在一个，parent 是作为父标签为了被继承 createBeanDefinition(className, parent)：设置了class 的 GenericBeanDefinition对象 parseBeanDefinitionAttributes()：解析 bean 标签的属性 接下来解析子标签 beanName = this.readerContext.generateBeanName(beanDefinition)：生成 className + # + 序号的名称赋值给 beanName return new BeanDefinitionHolder(beanDefinition, beanName, aliases)：包装成 BeanDefinitionHolder registerBeanDefinition(bdHolder, getReaderContext().getRegistry())：注册到容器 beanName = definitionHolder.getBeanName()：获取beanName this.beanDefinitionMap.put(beanName, beanDefinition)：添加到注册中心 getReaderContext().fireComponentRegistered()：发送注册完成事件 说明：源码部分的笔记不一定适合所有人阅读，作者采用流水线式去解析重要的代码，解析的结构类似于树状，如果视觉疲劳可以去网上参考一些博客和流程图学习源码。 IOC容器启动Spring IOC 容器是 ApplicationContext 或者 BeanFactory，使用多个 Map 集合保存单实例 Bean，环境信息等资源，不同层级有不同的容器，比如整合 SpringMVC 的父子容器（先看 Bean 部分的源码解析再回看容器） ClassPathXmlApplicationContext 与 AnnotationConfigApplicationContext 差不多： 12345public AnnotationConfigApplicationContext(Class&lt;?&gt;... annotatedClasses) &#123; this(); register(annotatedClasses);// 解析配置类，封装成一个 BeanDefinitionHolder，并注册到容器 refresh();// 加载刷新容器中的 Bean&#125; 123456public AnnotationConfigApplicationContext() &#123; // 注册 Spring 的注解解析器到容器 this.reader = new AnnotatedBeanDefinitionReader(this); // 实例化路径扫描器，用于对指定的包目录进行扫描查找 bean 对象 this.scanner = new ClassPathBeanDefinitionScanner(this);&#125; AbstractApplicationContext.refresh()： prepareRefresh()：刷新前的预处理 this.startupDate = System.currentTimeMillis()：设置容器的启动时间 initPropertySources()：初始化一些属性设置，可以自定义个性化的属性设置方法 getEnvironment().validateRequiredProperties()：检查环境变量 earlyApplicationEvents= new LinkedHashSet&lt;ApplicationEvent&gt;()：保存容器中早期的事件 obtainFreshBeanFactory()：获取一个全新的 BeanFactory 接口实例，如果容器中存在工厂实例直接销毁 refreshBeanFactory()：创建 BeanFactory，设置序列化 ID、读取 BeanDefinition 并加载到工厂 if (hasBeanFactory())：applicationContext 内部拥有一个 beanFactory 实例，需要将该实例完全释放销毁 destroyBeans()：销毁原 beanFactory 实例，将 beanFactory 内部维护的单实例 bean 全部清掉，如果哪个 bean 实现了 Disposablejie接口，还会进行 bean distroy 方法的调用处理 this.singletonsCurrentlyInDestruction = true：设置当前 beanFactory 状态为销毁状态 String[] disposableBeanNames：获取销毁集合中的 bean，如果当前 bean 有析构函数就会在销毁集合 destroySingleton(disposableBeanNames[i])：遍历所有的 disposableBeans，执行销毁方法 removeSingleton(beanName)：清除三级缓存和 registeredSingletons 中的当前 beanName 的数据 this.disposableBeans.remove(beanName)：从销毁集合中清除，每个 bean 只能 destroy 一次 destroyBean(beanName, disposableBean)：销毁 bean dependentBeanMap 记录了依赖当前 bean 的其他 bean 信息，因为依赖的对象要被回收了，所以依赖当前 bean 的其他对象都要执行 destroySingleton，遍历 dependentBeanMap 执行销毁 bean.destroy()：解决完成依赖后，执行 DisposableBean 的 destroy 方法 this.dependenciesForBeanMap.remove(beanName)：保存当前 bean 依赖了谁，直接清除 进行一些集合和缓存的清理工作 closeBeanFactory()：将容器内部的 beanFactory 设置为空，重新创建 beanFactory = createBeanFactory()：创建新的 DefaultListableBeanFactory 对象 beanFactory.setSerializationId(getId())：进行 ID 的设置，可以根据 ID 获取 BeanFactory 对象 customizeBeanFactory(beanFactory)：设置是否允许覆盖和循环引用 loadBeanDefinitions(beanFactory)：加载 BeanDefinition 信息，注册 BD注册到 BeanFactory 中 this.beanFactory = beanFactory：把 beanFactory 填充至容器中 getBeanFactory()：返回创建的 DefaultListableBeanFactory 对象，该对象继承 BeanDefinitionRegistry prepareBeanFactory(beanFactory)：BeanFactory 的预准备工作，向容器中添加一些组件 setBeanClassLoader(getClassLoader())：给当前 bf 设置一个类加载器，加载 bd 的 class 信息 setBeanExpressionResolver()：设置 EL 表达式解析器 addPropertyEditorRegistrar：添加一个属性编辑器，解决属性注入时的格式转换 addBeanPostProcessor()：添加后处理器，主要用于向 bean 内部注入一些框架级别的实例 ignoreDependencyInterface()：设置忽略自动装配的接口，bean 内部的这些类型的字段 不参与依赖注入 registerResolvableDependency()：注册一些类型依赖关系 addBeanPostProcessor()：将配置的监听者注册到容器中，当前 bean 实现 ApplicationListener 接口就是监听器事件 beanFactory.registerSingleton()：添加一些系统信息 postProcessBeanFactory(beanFactory)：BeanFactory 准备工作完成后进行的后置处理工作，扩展方法 invokeBeanFactoryPostProcessors(beanFactory)：执行 BeanFactoryPostProcessor 的方法 processedBeans = new HashSet&lt;&gt;()：存储已经执行过的 BeanFactoryPostProcessor 的 beanName if (beanFactory instanceof BeanDefinitionRegistry)：当前 BeanFactory 是 bd 的注册中心，bd 全部注册到 bf for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors)：遍历所有的 bf 后置处理器 if (postProcessor instanceof BeanDefinitionRegistryPostProcessor)：是 Registry 类的后置处理器 registryProcessor.postProcessBeanDefinitionRegistry(registry)：向 bf 中注册一些 bd registryProcessors.add(registryProcessor)：添加到 BeanDefinitionRegistryPostProcessor 集合 regularPostProcessors.add(postProcessor)：添加到 BeanFactoryPostProcessor 集合 逻辑到这里已经获取到所有 BeanDefinitionRegistryPostProcessor 和 BeanFactoryPostProcessor 接口类型的后置处理器 首先回调 BeanDefinitionRegistryPostProcessor 类的后置处理方法 postProcessBeanDefinitionRegistry() 获取实现了 PriorityOrdered（主排序接口）接口的 bdrpp，进行 sort 排序，然后全部执行并放入已经处理过的集合 再执行实现了 Ordered（次排序接口）接口的 bdrpp 最后执行没有实现任何优先级或者是顺序接口 bdrpp，boolean reiterate = true 控制 while 是否需要再次循环，循环内是查找并执行 bdrpp 后处理器的 registry 相关的接口方法，接口方法执行以后会向 bf 内注册 bd，注册的 bd 也有可能是 bdrpp 类型，所以需要该变量控制循环 processedBeans.add(ppName)：已经执行过的后置处理器存储到该集合中，防止重复执行 invokeBeanFactoryPostProcessors()：bdrpp 继承了 BeanFactoryPostProcessor，有 postProcessBeanFactory 方法 执行普通 BeanFactoryPostProcessor 的相关 postProcessBeanFactory 方法，按照主次无次序执行 if (processedBeans.contains(ppName))：会过滤掉已经执行过的后置处理器 beanFactory.clearMetadataCache()：清除缓存中合并的 Bean 定义，因为后置处理器可能更改了元数据 以上是 BeanFactory 的创建及预准备工作，接下来进入 Bean 的流程 registerBeanPostProcessors(beanFactory)：注册 Bean 的后置处理器，为了干预 Spring 初始化 bean 的流程，这里仅仅是向容器中注入而非使用 beanFactory.getBeanNamesForType(BeanPostProcessor.class)：获取配置中实现了 BeanPostProcessor 接口类型 int beanProcessorTargetCount：后置处理器的数量，已经注册的 + 未注册的 + 即将要添加的一个 beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker())：添加一个检查器 BeanPostProcessorChecker.postProcessAfterInitialization()：初始化后的后处理器方法 !(bean instanceof BeanPostProcessor) ：当前 bean 类型是普通 bean，不是后置处理器 !isInfrastructureBean(beanName)：成立说明当前 beanName 是用户级别的 bean 不是 Spring 框架的 this.beanFactory.getBeanPostProcessorCount() &lt; this.beanPostProcessorTargetCount：BeanFactory 上面注册后处理器数量 &lt; 后处理器数量，说明后处理框架尚未初始化完成 for (String ppName : postProcessorNames)：遍历 PostProcessor 集合，根据实现不同的顺序接口添加到不同集合 sortPostProcessors(priorityOrderedPostProcessors, beanFactory)：实现 PriorityOrdered 接口的后处理器排序 registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors)：注册到 beanFactory 中 接着排序注册实现 Ordered 接口的后置处理器，然后注册普通的（ 没有实现任何优先级接口）后置处理器 最后排序 MergedBeanDefinitionPostProcessor 类型的处理器，根据实现的排序接口，排序完注册到 beanFactory 中 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext))：重新注册 ApplicationListenerDetector 后处理器，用于在 Bean 创建完成后检查是否属于 ApplicationListener 类型，如果是就把 Bean 放到监听器容器中保存起来 initMessageSource()：初始化 MessageSource 组件，主要用于做国际化功能，消息绑定与消息解析 if (beanFactory.containsLocalBean(MESSAGE_SOURCE_BEAN_NAME))：容器是否含有名称为 messageSource 的 bean beanFactory.getBean(MESSAGE_SOURCE_BEAN_NAME, MessageSource.class)：如果有证明用户自定义了该类型的 bean，获取后直接赋值给 this.messageSource dms = new DelegatingMessageSource()：容器中没有就新建一个赋值 initApplicationEventMulticaster()：初始化事件传播器，在注册监听器时会用到 if (beanFactory.containsLocalBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME))：条件成立说明用户自定义了事件传播器，可以实现 ApplicationEventMulticaster 接口编写自己的事件传播器，通过 bean 的方式提供给 Spring 如果有就直接从容器中获取；如果没有则创建一个 SimpleApplicationEventMulticaster 注册 onRefresh()：留给用户去实现，可以硬编码提供一些组件，比如提供一些监听器 registerListeners()：注册通过配置提供的 Listener，这些监听器最终注册到 ApplicationEventMulticaster 内 for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) ：注册编码实现的监听器 getBeanNamesForType(ApplicationListener.class, true, false)：注册通过配置提供的 Listener multicastEvent(earlyEvent)：发布前面步骤产生的事件 applicationEvents Executor executor = getTaskExecutor()：获取线程池，有线程池就异步执行，没有就同步执行 finishBeanFactoryInitialization()：实例化非懒加载状态的单实例 beanFactory.freezeConfiguration()：冻结配置信息，就是冻结 BD 信息，冻结后无法再向 bf 内注册 bd beanFactory.preInstantiateSingletons()：实例化 non-lazy-init singletons for (String beanName : beanNames)：遍历容器内所有的 beanDefinitionNames getMergedLocalBeanDefinition(beanName)：获取与父类合并后的对象（Bean → 获取流程部分详解此函数） if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit())：BD 对应的 Class 满足非抽象、单实例，非懒加载，需要预先实例化 if (isFactoryBean(beanName))：BD 对应的 Class 是 factoryBean 对象 getBean(FACTORY_BEAN_PREFIX + beanName)：获取工厂 FactoryBean 实例本身 isEagerInit：控制 FactoryBean 内部管理的 Bean 是否也初始化 getBean(beanName)：初始化 Bean，获取 Bean 详解此函数 getBean(beanName)：不是工厂 bean 直接获取 for (String beanName : beanNames)：检查所有的 Bean 是否实现 SmartInitializingSingleton 接口，实现了就执行 afterSingletonsInstantiated()，进行一些创建后的操作 finishRefresh()：完成刷新后做的一些事情，主要是启动生命周期 clearResourceCaches()：清空上下文缓存 initLifecycleProcessor()：初始化和生命周期有关的后置处理器，容器的生命周期 if (beanFactory.containsLocalBean(LIFECYCLE_PROCESSOR_BEAN_NAME))：成立说明自定义了生命周期处理器 defaultProcessor = new DefaultLifecycleProcessor()：Spring 默认提供的生命周期处理器 beanFactory.registerSingleton()：将生命周期处理器注册到 bf 的一级缓存和注册单例集合中 getLifecycleProcessor().onRefresh()：获取该**生命周期后置处理器回调 onRefresh()**，调用 startBeans(true) lifecycleBeans = getLifecycleBeans()：获取到所有实现了 Lifecycle 接口的对象包装到 Map 内，key 是beanName， value 是 Lifecycle 对象 int phase = getPhase(bean)：获取当前 Lifecycle 的 phase 值，当前生命周期对象可能依赖其他生命周期对象的执行结果，所以需要 phase 决定执行顺序，数值越低的优先执行 LifecycleGroup group = phases.get(phase)：把 phsae 相同的 Lifecycle 存入 LifecycleGroup if (group == null)：group 为空则创建，初始情况下是空的 group.add(beanName, bean)：将当前 Lifecycle 添加到当前 phase 值一样的 group 内 Collections.sort(keys)：从小到大排序，按优先级启动 phases.get(key).start()：遍历所有的 Lifecycle 对象开始启动 doStart(this.lifecycleBeans, member.name, this.autoStartupOnly)：底层调用该方法启动 bean = lifecycleBeans.remove(beanName)： 确保 Lifecycle 只被启动一次，在一个分组内被启动了在其他分组内就看不到 Lifecycle 了 dependenciesForBean = getBeanFactory().getDependenciesForBean(beanName)：获取当前即将被启动的 Lifecycle 所依赖的其他 beanName，需要先启动所依赖的 bean，才能启动自身 if ()：传入的参数 autoStartupOnly 为 true 表示启动 isAutoStartUp 为 true 的 SmartLifecycle 对象，不会启动普通的生命周期的对象；false 代表全部启动 bean.start()：调用启动方法 publishEvent(new ContextRefreshedEvent(this))：发布容器刷新完成事件 liveBeansView.registerApplicationContext(this)：暴露 Mbean 补充生命周期 stop() 方法的调用 DefaultLifecycleProcessor.stop()：调用 DefaultLifecycleProcessor.stopBeans() 获取到所有实现了 Lifecycle 接口的对象并按 phase 数值分组的 keys.sort(Collections.reverseOrder())：按 phase 降序排序 Lifecycle 接口，最先启动的最晚关闭（责任链？） phases.get(key).stop()：遍历所有的 Lifecycle 对象开始停止 latch = new CountDownLatch(this.smartMemberCount)：创建 CountDownLatch，设置 latch 内部的值为当前分组内的 smartMemberCount 的数量 countDownBeanNames = Collections.synchronizedSet(new LinkedHashSet&lt;&gt;())：保存当前正在处理关闭的smartLifecycle 的 BeanName for (LifecycleGroupMember member : this.members)：处理本分组内需要关闭的 Lifecycle doStop(this.lifecycleBeans, member.name, latch, countDownBeanNames)：真正的停止方法 getBeanFactory().getDependentBeans(beanName)：获取依赖当前 Lifecycle 的其他对象的 beanName，因为当前的 Lifecycle 即将要关闭了，所有的依赖了当前 Lifecycle 的 bean 也要关闭 countDownBeanNames.add(beanName)：将当前 SmartLifecycle beanName 添加到 countDownBeanNames 集合内，该集合表示正在关闭的 SmartLifecycle bean.stop()：调用停止的方法 获取Bean单实例：在容器启动时创建对象 多实例：在每次获取的时候创建对象 获取流程：获取 Bean 时先从单例池获取，如果没有则进行第二次获取，并带上工厂类去创建并添加至单例池 Java 启动 Spring 代码： 12ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);UserService userService = (UserService) context.getBean(&quot;userService&quot;); AbstractBeanFactory.doGetBean()：获取 Bean，context.getBean() 追踪到此 beanName = transformedBeanName(name)：name 可能是一个别名，重定向出来真实 beanName；也可能是一个 &amp; 开头的 name，说明要获取的 bean 实例对象，是一个 FactoryBean 对象（IOC 原理 → 核心类） BeanFactoryUtils.transformedBeanName(name)：判断是哪种 name，返回截取 &amp; 以后的 name 并放入缓存 transformedBeanNameCache.computeIfAbsent：缓存是并发安全集合，key &#x3D;&#x3D; null || value &#x3D;&#x3D; null 时 put 成功 do while 循环一直去除 &amp; 直到不再含有 &amp; canonicalName(name)：aliasMap 保存别名信息，其中的 do while 逻辑是迭代查找，比如 A 别名叫做 B，但是 B 又有别名叫 C， aliasMap 为 {“C”:”B”, “B”:”A”}，get(C) 最后返回的是 A DefaultSingletonBeanRegistry.getSingleton()：第一次获取从缓存池获取（循环依赖详解此代码） 缓存中有数据进行 getObjectForBeanInstance() 获取可使用的 Bean（本节结束部分详解此函数） 缓存中没有数据进行下面的逻辑进行创建 if(isPrototypeCurrentlyInCreation(beanName))：检查 bean 是否在原型（Prototype）正在被创建的集合中，如果是就报错，说明产生了循环依赖，原型模式解决不了循环依赖 原因：先加载 A，把 A 加入集合，A 依赖 B 去加载 B，B 又依赖 A，去加载 A，发现 A 在正在创建集合中，产生循环依赖 markBeanAsCreated(beanName)：把 bean 标记为已经创建，防止其他线程重新创建 Bean mbd = getMergedLocalBeanDefinition(beanName)：获取合并父 BD 后的 BD 对象，BD 是直接继承的，合并后的 BD 信息是包含父类的 BD 信息 this.mergedBeanDefinitions.get(beanName)：从缓存中获取 if(bd.getParentName()==null)：beanName 对应 BD 没有父 BD 就不用处理继承，封装为 RootBeanDefinition 返回 parentBeanName = transformedBeanName(bd.getParentName())：处理父 BD 的 name 信息 if(!beanName.equals(parentBeanName))：一般情况父子 BD 的名称不同 pbd = getMergedBeanDefinition(parentBeanName)：递归调用，最终返回父 BD 的父 BD 信息 mbd = new RootBeanDefinition(pbd)：按照父 BD 信息创建 RootBeanDefinition 对象 mbd.overrideFrom(bd)：子 BD 信息覆盖 mbd，因为是要以子 BD 为基准，不存在的才去父 BD 寻找（类似 Java 继承） this.mergedBeanDefinitions.put(beanName, mbd)：放入缓存 checkMergedBeanDefinition()：判断当前 BD 是否为抽象 BD，抽象 BD 不能创建实例，只能作为父 BD 被继承 mbd.getDependsOn()：获取 bean 标签 depends-on if(dependsOn != null)：遍历所有的依赖加载，解决不了循环依赖 isDependent(beanName, dep)：判断循环依赖，出现循环依赖问题报错 两个 Map：&lt;bean name=&quot;A&quot; depends-on=&quot;B&quot; ...&gt; dependentBeanMap：记录依赖了当前 beanName 的其他 beanName（谁依赖我，我记录谁） dependenciesForBeanMap：记录当前 beanName 依赖的其它 beanName 以 B 为视角 dependentBeanMap {“B”：{“A”}}，以 A 为视角 dependenciesForBeanMap {“A” :{“B”}} canonicalName(beanName)：处理 bean 的 name dependentBeans = this.dependentBeanMap.get(canonicalName)：获取依赖了当前 bean 的 name if (dependentBeans.contains(dependentBeanName))：依赖了当前 bean 的集合中是否有该 name，有就产生循环依赖 进行递归处理所有的引用：假如 &lt;bean name=&quot;A&quot; dp=&quot;B&quot;&gt; &lt;bean name=&quot;B&quot; dp=&quot;C&quot;&gt; &lt;bean name=&quot;C&quot; dp=&quot;A&quot;&gt; 123dependentBeanMap=&#123;A:&#123;C&#125;, B:&#123;A&#125;, C:&#123;B&#125;&#125; // C 依赖 A 判断谁依赖了C 递归判断 谁依赖了BisDependent(C, A) → C#dependentBeans=&#123;B&#125; → isDependent(B, A); → B#dependentBeans=&#123;A&#125; //返回true registerDependentBean(dep, beanName)：把 bean 和依赖注册到两个 Map 中，注意参数的位置，被依赖的在前 getBean(dep)：先加载依赖的 Bean，又进入 doGetBean() 的逻辑 if (mbd.isSingleton())：判断 bean 是否是单例的 bean getSingleton(String, ObjectFactory&lt;?&gt;)：第二次获取，传入一个工厂对象，这个方法更倾向于创建实例并返回 1234sharedInstance = getSingleton(beanName, () -&gt; &#123; return createBean(beanName, mbd, args);//创建，跳转生命周期 //lambda表达式，调用了ObjectFactory的getObject()方法，实际回调接口实现的是 createBean()方法进行创建对象&#125;); singletonObjects.get(beanName)：从一级缓存检查是否已经被加载，单例模式复用已经创建的 bean this.singletonsCurrentlyInDestruction：容器销毁时会设置这个属性为 true，这时就不能再创建 bean 实例了 beforeSingletonCreation(beanName)：检查构造注入的依赖，构造参数注入产生的循环依赖无法解决 !this.singletonsCurrentlyInCreation.add(beanName)：将当前 beanName 放入到正在创建中单实例集合，放入成功说明没有产生循环依赖，失败则产生循环依赖，进入判断条件内的逻辑抛出异常 原因：加载 A，向正在创建集合中添加了 {A}，根据 A 的构造方法实例化 A 对象，发现 A 的构造方法依赖 B，然后加载 B，B 构造方法的参数依赖于 A，又去加载 A 时来到当前方法，因为创建中集合已经存在 A，所以添加失败 singletonObject = singletonFactory.getObject()：创建 bean（生命周期部分详解） 创建完成以后，Bean 已经初始化好，是一个完整的可使用的 Bean afterSingletonCreation(beanName)：从正在创建中的集合中移出 addSingleton(beanName, singletonObject)：添加一级缓存单例池中，从二级三级缓存移除 bean = getObjectForBeanInstance：单实例可能是普通单实例或者 FactoryBean，如果是 FactoryBean 实例，需要判断 name 是带 &amp; 还是不带 &amp;，带 &amp; 说明 getBean 获取 FactoryBean 对象，否则是获取 FactoryBean 内部管理的实例 参数 name 是未处理 &amp; 的 name，beanName 是处理过 &amp; 和别名后的 name if(BeanFactoryUtils.isFactoryDereference(name))：判断 doGetBean 中参数 name 前是否带 &amp;，不是处理后的 if(!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name))：Bean 是普通单实例或者是 FactoryBean 就可以直接返回，否则进入下面的获取 FactoryBean 内部管理的实例的逻辑 getCachedObjectForFactoryBean(beanName)：尝试到缓存获取，获取到直接返回，获取不到进行下面逻辑 if (mbd == null &amp;&amp; containsBeanDefinition(beanName))：Spring 中有当前 beanName 的 BeanDefinition 信息 mbd = getMergedLocalBeanDefinition(beanName)：获取合并后的 BeanDefinition mbd.isSynthetic()：默认值是 false 表示这是一个用户对象，如果是 true 表示是系统对象 object = getObjectFromFactoryBean(factory, beanName, !synthetic)：从工厂内获取实例 factory.isSingleton() &amp;&amp; containsSingleton(beanName)：工厂内部维护的对象是单实例并且一级缓存存在该 bean 首先去缓存中获取，获取不到就使用工厂获取然后放入缓存，进行循环依赖判断 else if (mbd.isPrototype())：bean 是原型的 bean beforePrototypeCreation(beanName)：当前线程正在创建的原型对象 beanName 存入 prototypesCurrentlyInCreation curVal = this.prototypesCurrentlyInCreation.get()：获取当前线程的正在创建的原型类集合 this.prototypesCurrentlyInCreation.set(beanName)：集合为空就把当前 beanName 加入 if (curVal instanceof String)：已经有线程相关原型类创建了，把当前的创建的加进去 createBean(beanName, mbd, args)：创建原型类对象，不需要三级缓存 afterPrototypeCreation(beanName)：从正在创建中的集合中移除该 beanName， 与 beforePrototypeCreation逻辑相反 convertIfNecessary()：依赖检查，检查所需的类型是否与实际 bean 实例的类型匹配 return (T) bean：返回创建完成的 bean 生命周期四个阶段Bean 的生命周期：实例化 instantiation，填充属性 populate，初始化 initialization，销毁 destruction AbstractAutowireCapableBeanFactory.createBean()：进入 Bean 生命周期的流程 resolvedClass = resolveBeanClass(mbd, beanName)：判断 mdb 中的 class 是否已经加载到 JVM，如果未加载则使用类加载器将 beanName 加载到 JVM中并返回 class 对象 if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null)：条件成立封装 mbd 并把 resolveBeanClass 设置到 bd 中 条件二：mbd 在 resolveBeanClass 之前是否有 class 条件三：mbd 有 className bean = resolveBeforeInstantiation(beanName, mbdToUse)：实例化前的后置处理器返回一个代理实例对象（不是 AOP） 自定义类继承 InstantiationAwareBeanPostProcessor，重写 postProcessBeforeInstantiation 方法，方法逻辑为创建对象 并配置文件 &lt;bean class=&quot;intefacePackage.MyInstantiationAwareBeanPostProcessor&quot;&gt; 导入为 bean 条件成立，短路操作，直接 return bean Object beanInstance = doCreateBean(beanName, mbdToUse, args)：Do it AbstractAutowireCapableBeanFactory.doCreateBean(beanName, RootBeanDefinition, Object[] args)：创建 Bean BeanWrapper instanceWrapper = null：Spring 给所有创建的 Bean 实例包装成 BeanWrapper，内部最核心的方法是获取实例，提供了一些额外的接口方法，比如属性访问器 instanceWrapper = this.factoryBeanInstanceCache.remove(beanName)：单例对象尝试从缓存中获取，会移除缓存 createBeanInstance()：缓存中没有实例就进行创建实例（逻辑复杂，下一小节详解） if (!mbd.postProcessed)：每个 bean 只进行一次该逻辑 applyMergedBeanDefinitionPostProcessors()：后置处理器，合并 bd 信息，接下来要属性填充了 AutowiredAnnotationBeanPostProcessor.postProcessMergedBeanDefinition()：后置处理逻辑（@Autowired） metadata = findAutowiringMetadata(beanName, beanType, null)：提取当前 bean 整个继承体系内的 @Autowired、@Value、@Inject 信息，存入一个 InjectionMetadata 对象，保存着当前 bean 信息和要自动注入的字段信息 12private final Class&lt;?&gt; targetClass; //当前 bean private final Collection&lt;InjectedElement&gt; injectedElements;\t//要注入的信息集合 metadata = buildAutowiringMetadata(clazz)：查询当前 clazz 感兴趣的注解信息 ReflectionUtils.doWithLocalFields()：提取字段的注解的信息 findAutowiredAnnotation(field)：代表感兴趣的注解就是那三种注解，获取这三种注解的元数据 ReflectionUtils.doWithLocalMethods()：提取方法的注解的信息 do&#123;&#125; while (targetClass != null &amp;&amp; targetClass != Object.class)：循环从父类中解析，直到 Object 类 this.injectionMetadataCache.put(cacheKey, metadata)：存入缓存 mbd.postProcessed = true：设置为 true，下次访问该逻辑不会再进入 earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)：单例、解决循环引用、是否在单例正在创建集合中 12345if (earlySingletonExposure) &#123; // 【放入三级缓存一个工厂对象，用来获取提前引用】 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); // lamda 表达式，用来获取提前引用，循环依赖部分详解该逻辑&#125; populateBean(beanName, mbd, instanceWrapper)：**属性填充，依赖注入，整体逻辑是先处理标签再处理注解，填充至 pvs 中，最后通过 apply 方法最后完成属性依赖注入到 BeanWrapper ** if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName))：实例化后的后置处理器，默认返回 true，可以自定义类继承 InstantiationAwareBeanPostProcessor 修改后置处理方法的返回值为 false，使 continueWithPropertyPopulation 为 false，会导致直接返回，不进行属性的注入 if (!continueWithPropertyPopulation)：自定义方法返回值会造成该条件成立，逻辑为直接返回，不进行依赖注入 PropertyValues pvs = (mbd.hasPropertyValues() ? mbd.getPropertyValues() : null)：处理依赖注入逻辑开始 mbd.getResolvedAutowireMode() == ?：根据 bean 标签配置的 autowire 判断是 BY_NAME 或者 BY_TYPE autowireByName(beanName, mbd, bw, newPvs)：根据字段名称去获取依赖的 bean，还没注入，只是添加到 pvs propertyNames = unsatisfiedNonSimpleProperties(mbd, bw)：bean 实例中有该字段和该字段的 setter 方法，但是在 bd 中没有 property 属性 拿到配置的 property 信息和 bean 的所有字段信息 pd.getWriteMethod() != null：当前字段是否有 set 方法，配置类注入的方式需要 set 方法 !isExcludedFromDependencyCheck(pd)：当前字段类型是否在忽略自动注入的列表中 !pvs.contains(pd.getName()：当前字段不在 xml 或者其他方式的配置中，也就是 bd 中不存在对应的 property !BeanUtils.isSimpleProperty(pd.getPropertyType()：是否是基本数据类型和内置的几种数据类型，基本数据类型不允许自动注入 if (containsBean(propertyName))：BeanFactory 中存在当前 property 的 bean 实例，说明找到对应的依赖数据 getBean(propertyName)：拿到 propertyName 对应的 bean 实例 pvs.add(propertyName, bean)：填充到 pvs 中 registerDependentBean(propertyName, beanName))：添加到两个依赖 Map（dependsOn）中 autowireByType(beanName, mbd, bw, newPvs)：根据字段类型去查找依赖的 bean desc = new AutowireByTypeDependencyDescriptor(methodParam, eager)：依赖描述信息 resolveDependency(desc, beanName, autowiredBeanNames, converter)：根据描述信息，查找依赖对象，容器中没有对应的实例但是有对应的 BD，会调用 getBean(Type) 获取对象 pvs = newPvs：newPvs 是处理了依赖数据后的 pvs，所以赋值给 pvs hasInstAwareBpps：表示当前是否有 InstantiationAwareBeanPostProcessors 的后置处理器（Autowired） pvsToUse = ibp.postProcessProperties(pvs, bw.getWrappedInstance(), beanName)：**@Autowired 注解的注入**，这个传入的 pvs 对象，最后原封不动的返回，不会添加东西 findAutowiringMetadata()：包装着当前 bd 需要注入的注解信息集合，三种注解的元数据，直接缓存获取 InjectionMetadata.InjectedElement.inject()：遍历注解信息解析后注入到 Bean，方法和字段的注入实现不同 以字段注入为例： value = resolveFieldValue(field, bean, beanName)：处理字段属性值 value = beanFactory.resolveDependency()：解决依赖 result = doResolveDependency()：真正处理自动注入依赖的逻辑 Object shortcut = descriptor.resolveShortcut(this)：默认返回 null Object value = getAutowireCandidateResolver().getSuggestedValue(descriptor)：获取 @Value 的值 converter.convertIfNecessary(value, type, descriptor.getTypeDescriptor())：如果 value 不是 null，就直接进行类型转换返回数据 matchingBeans = findAutowireCandidates(beanName, type, descriptor)：如果 value 是空说明字段是引用类型，获取 @Autowired 的 Bean 12345// addCandidateEntry() → Object beanInstance = descriptor.resolveCandidate()public Object resolveCandidate(String beanName, Class&lt;?&gt; requiredType, BeanFactory beanFactory) throws BeansException &#123;\t// 获取 bean return beanFactory.getBean(beanName);&#125; ReflectionUtils.makeAccessible(field)：修改访问权限 field.set(bean, value)：获取属性访问器为此 field 对象赋值 applyPropertyValues()：将所有解析的 PropertyValues 的注入至 BeanWrapper 实例中（深拷贝） if (pvs.isEmpty())：注解 @Autowired 和 @Value 标注的信息在后置处理的逻辑注入完成，此处为空直接返回 下面的逻辑进行 XML 配置的属性的注入，首先获取转换器进行数据转换，然后获取 WriteMethod (set) 方法进行反射调用，完成属性的注入 initializeBean(String,Object,RootBeanDefinition)：初始化，分为配置文件和实现接口两种方式 invokeAwareMethods(beanName, bean)：根据 bean 是否实现 Aware 接口执行初始化的方法 wrappedBean = applyBeanPostProcessorsBeforeInitialization：初始化前的后置处理器，可以继承接口重写方法 processor.postProcessBeforeInitialization()：执行后置处理的方法，默认返回 bean 本身 if (current == null) return result：重写方法返回 null，会造成后置处理的短路，直接返回 invokeInitMethods(beanName, wrappedBean, mbd)：反射执行初始化方法 isInitializingBean = (bean instanceof InitializingBean)：初始化方法的定义有两种方式，一种是自定义类实现 InitializingBean 接口，另一种是配置文件配置 &lt;bean id&#x3D;”…” class&#x3D;”…” init-method&#x3D;”init”&#x2F; &gt; isInitializingBean &amp;&amp; (mbd == null || !mbd.isExternallyManagedInitMethod(&quot;afterPropertiesSet&quot;))： 条件一：当前 bean 是不是实现了 InitializingBean 条件二：InitializingBean 接口中的方法 afterPropertiesSet，判断该方法是否是容器外管理的方法 if (mbd != null &amp;&amp; bean.getClass() != NullBean.class)：成立说明是配置文件的方式 if(!(接口条件))表示如果通过接口实现了初始化方法的话，就不会在调用配置类中 init-method 定义的方法 ((InitializingBean) bean).afterPropertiesSet()：调用方法 invokeCustomInitMethod：执行自定义的方法 initMethodName = mbd.getInitMethodName()：获取方法名 Method initMethod = ()：根据方法名获取到 init-method 方法 methodToInvoke = ClassUtils.getInterfaceMethodIfPossible(initMethod)：将方法转成从接口层面获取 ReflectionUtils.makeAccessible(methodToInvoke)：访问权限设置成可访问 methodToInvoke.invoke(bean)：反射调用初始化方法，以当前 bean 为角度去调用 wrappedBean = applyBeanPostProcessorsAfterInitialization：初始化后的后置处理器 AbstractAutoProxyCreator.postProcessAfterInitialization()：如果 Bean 被子类标识为要代理的 bean，则使用配置的拦截器创建代理对象，AOP 部分详解 如果不存在循环依赖，创建动态代理 bean 在此处完成；否则真正的创建阶段是在属性填充时获取提前引用的阶段，循环依赖详解，源码分析： 123456789101112131415// 该集合用来避免重复将某个 bean 生成代理对象，private final Map&lt;Object, Object&gt; earlyProxyReferences = new ConcurrentHashMap&lt;&gt;(16);public Object postProcessAfterInitialization(@Nullable Object bean,String bN)&#123; if (bean != null) &#123; // cacheKey 是 beanName 或者加上 &amp; Object cacheKey = getCacheKey(bean.getClass(), beanName);y if (this.earlyProxyReferences.remove(cacheKey) != bean) &#123; // 去提前代理引用池中寻找该key，不存在则创建代理 // 如果存在则证明被代理过，则判断是否是当前的 bean，不是则创建代理 return wrapIfNecessary(bean, bN, cacheKey); &#125; &#125; return bean;&#125; if (earlySingletonExposure)：是否允许提前引用 earlySingletonReference = getSingleton(beanName, false)：从二级缓存获取实例，放入一级缓存是在 doGetBean 中的sharedInstance &#x3D; getSingleton() 逻辑中，此时在 createBean 的逻辑还没有返回，所以一级缓存没有 if (earlySingletonReference != null)：当前 bean 实例从二级缓存中获取到了，说明产生了循环依赖，在属性填充阶段会提前调用三级缓存中的工厂生成 Bean 的代理对象（或原始实例），放入二级缓存中，然后使用原始 bean 继续执行初始化 if (exposedObject == bean)：初始化后的 bean &#x3D;&#x3D; 创建的原始实例，条件成立的两种情况：当前的真实实例不需要被代理；当前实例存在循环依赖已经被提前代理过了，初始化时的后置处理器直接返回 bean 原实例 exposedObject = earlySingletonReference：把代理后的 Bean 传给 exposedObject 用来返回，因为只有代理对象才封装了拦截器链，main 方法中用代理对象调用方法时会进行增强，代理是对原始对象的包装，所以这里返回的代理对象中含有完整的原实例（属性填充和初始化后的），是一个完整的代理对象，返回后外层方法会将当前 Bean 放入一级缓存 else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName))：是否有其他 bean 依赖当前 bean，执行到这里说明是不存在循环依赖、存在增强代理的逻辑，也就是正常的逻辑 dependentBeans = getDependentBeans(beanName)：取到依赖当前 bean 的其他 beanName if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean))：判断 dependentBean 是否创建完成 if (!this.alreadyCreated.contains(beanName))：成立当前 bean 尚未创建完成，当前 bean 是依赖exposedObject 的 bean，返回 true return false：创建完成返回 false actualDependentBeans.add(dependentBean)：创建完成的 dependentBean 加入该集合 if (!actualDependentBeans.isEmpty())：条件成立说明有依赖于当前 bean 的 bean 实例创建完成，但是当前的 bean 还没创建完成返回，依赖当前 bean 的外部 bean 持有的是不完整的 bean，所以需要报错 registerDisposableBeanIfNecessary：判断当前 bean 是否需要注册析构函数回调，当容器销毁时进行回调 if (!mbd.isPrototype() &amp;&amp; requiresDestruction(bean, mbd)) 如果是原型 prototype 不会注册析构回调，不会回调该函数，对象的回收由 JVM 的 GC 机制完成 requiresDestruction()： DisposableBeanAdapter.hasDestroyMethod(bean, mbd)：bd 中定义了 DestroyMethod 返回 true hasDestructionAwareBeanPostProcessors()：后处理器框架决定是否进行析构回调 registerDisposableBean()：条件成立进入该方法，给当前单实例注册回调适配器，适配器内根据当前 bean 实例是继承接口（DisposableBean）还是自定义标签来判定具体调用哪个方法实现 this.disposableBeans.put(beanName, bean)：向销毁集合添加实例 创建实例AbstractAutowireCapableBeanFactory.createBeanInstance(beanName, RootBeanDefinition, Object[] args) resolveBeanClass(mbd, beanName)：确保 Bean 的 Class 真正的被加载 判断类的访问权限是不是 public，不是进入下一个判断，是否允许访问类的 non-public 的构造方法，不允许则报错 Supplier&lt;?&gt; instanceSupplier = mbd.getInstanceSupplier()：获取创建实例的函数，可以自定义，没有进入下面的逻辑 if (mbd.getFactoryMethodName() != null)：判断 bean 是否设置了 factory-method 属性，优先使用 ，设置了该属性进入 factory-method 方法创建实例 resolved = false：代表 bd 对应的构造信息是否已经解析成可以反射调用的构造方法 autowireNecessary = false：是否自动匹配构造方法 if(mbd.resolvedConstructorOrFactoryMethod != null)：获取 bd 的构造信息转化成反射调用的 method 信息 method 为 null 则 resolved 和 autowireNecessary 都为默认值 false autowireNecessary = mbd.constructorArgumentsResolved：构造方法有参数，设置为 true bd 对应的构造信息解析完成，可以直接反射调用构造方法了： return autowireConstructor(beanName, mbd, null, null)：有参构造，根据参数匹配最优的构造器创建实例 return instantiateBean(beanName, mbd)：无参构造方法通过反射创建实例 SimpleInstantiationStrategy.instantiate()：真正用来实例化的函数（无论如何都会走到这一步） if (!bd.hasMethodOverrides())：没有方法重写覆盖 BeanUtils.instantiateClass(constructorToUse)：调用 Constructor.newInstance() 实例化 instantiateWithMethodInjection(bd, beanName, owner)：有方法重写采用 CGLIB 实例化 BeanWrapper bw = new BeanWrapperImpl(beanInstance)：包装成 BeanWrapper 类型的对象 return bw：返回实例 ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName)：**@Autowired 注解**，对应的后置处理器 AutowiredAnnotationBeanPostProcessor 逻辑 配置了 lookup 的相关逻辑 this.candidateConstructorsCache.get(beanClass)：从缓存中获取构造方法，第一次获取为 null，进入下面逻辑 rawCandidates = beanClass.getDeclaredConstructors()：获取所有的构造器 Constructor&lt;?&gt; requiredConstructor = null：唯一的选项构造器，**@Autowired(required &#x3D; “true”)** 时有值 for (Constructor&lt;?&gt; candidate : rawCandidates)：遍历所有的构造器： ann = findAutowiredAnnotation(candidate)：有三种注解中的一个会返回注解的属性 遍历 this.autowiredAnnotationTypes 中的三种注解： 123this.autowiredAnnotationTypes.add(Autowired.class);//！！！！！！！！！！！！！！this.autowiredAnnotationTypes.add(Value.class);this.autowiredAnnotationTypes.add(...ClassUtils.forName(&quot;javax.inject.Inject&quot;)); AnnotatedElementUtils.getMergedAnnotationAttributes(ao, type)：获取注解的属性 if (attributes != null) return attributes：任意一个注解属性不为空就注解返回 if (ann == null)：注解属性为空 userClass = ClassUtils.getUserClass(beanClass)：如果当前 beanClass 是代理对象，方法上就已经没有注解了，所以获取原始的用户类型重新获取该构造器上的注解属性（事务注解失效也是这个原理） if (ann != null)：注解属性不为空了 required = determineRequiredStatus(ann)：获取 required 属性的值 !ann.containsKey(this.requiredParameterName) || ：判断属性是否包含 required，不包含进入后面逻辑 this.requiredParameterValue == ann.getBoolean(this.requiredParameterName)：获取属性值返回 if (required)：代表注解 @Autowired(required &#x3D; true) if (!candidates.isEmpty())：true 代表只能有一个构造方法，构造集合不是空代表可选的构造器不唯一，报错 requiredConstructor = candidate：把构造器赋值给 requiredConstructor candidates.add(candidate)：把当前构造方法添加至 candidates 集合 if(candidate.getParameterCount() == 0)：当前遍历的构造器的参数为 0 代表没有参数，是默认构造器，赋值给 defaultConstructor candidateConstructors = candidates.toArray(new Constructor&lt;?&gt;[0])：将构造器转成数组返回 if(ctors != null)：条件成立代表指定了构造方法数组 mbd.getResolvedAutowireMode() == AUTOWIRE_CONSTRUCTOR： 标签内 autowiremode 的属性值，默认是 no，AUTOWIRE_CONSTRUCTOR 代表选择最优的构造方法 mbd.hasConstructorArgumentValues()：bean 信息中是否配置了构造参数的值 !ObjectUtils.isEmpty(args)：getBean 时，指定了参数 arg return autowireConstructor(beanName, mbd, ctors, args)：选择最优的构造器进行创建实例（复杂，不建议研究） beanFactory.initBeanWrapper(bw)：向 BeanWrapper 中注册转换器，向工厂中注册属性编辑器 Constructor&lt;?&gt; constructorToUse = null：实例化反射构造器 ArgumentsHolder argsHolderToUse：实例化时真正去用的参数，并持有对象 rawArguments 是转换前的参数，arguments 是类型转换完成的参数 Object[] argsToUse：参数实例化时使用的参数 Object[] argsToResolve：表示构造器参数做转换后的参数引用 if (constructorToUse != null &amp;&amp; mbd.constructorArgumentsResolved)： 条件一成立说明当前 bd 生成的实例不是第一次，缓存中有解析好的构造器方法可以直接拿来反射调用 条件二成立说明构造器参数已经解析过了 argsToUse = resolvePreparedArguments()：argsToResolve 不是完全解析好的，还需要继续解析 if (constructorToUse == null || argsToUse == null)：条件成立说明缓存机制失败，进入构造器匹配逻辑 Constructor&lt;?&gt;[] candidates = chosenCtors：chosenCtors 只有在构造方法上有 autowaire 三种注解时才有数据 if (candidates == null)：candidates 为空就根据 beanClass 是否允许访问非公开的方法来获取构造方法 if (candidates.length == 1 &amp;&amp; explicitArgs == null &amp;&amp; !mbd.hasConstructorArgumentValues())：默认无参 bw.setBeanInstance(instantiate())：使用无参构造器反射调用，创建出实例对象，设置到 BeanWrapper 中去 boolean autowiring：需要选择最优的构造器 cargs = mbd.getConstructorArgumentValues()：获取参数值 resolvedValues = new ConstructorArgumentValues()：获取已经解析后的构造器参数值 final Map&lt;Integer, ValueHolder&gt; indexedArgumentValues：key 是 index， value 是值 final List&lt;ValueHolder&gt; genericArgumentValues：没有 index 的值 minNrOfArgs = resolveConstructorArguments(..,resolvedValues)：从 bd 中解析并获取构造器参数的个数 valueResolver.resolveValueIfNecessary()：将引用转换成真实的对象 resolvedValueHolder.setSource(valueHolder)：将对象填充至 ValueHolder 中 resolvedValues.addIndexedArgumentValue()：将参数值封装至 resolvedValues 中 AutowireUtils.sortConstructors(candidates)：排序规则 public &gt; 非公开的 &gt; 参数多的 &gt; 参数少的 int minTypeDiffWeight = Integer.MAX_VALUE：值越低说明构造器参数列表类型和构造参数的匹配度越高 Set&lt;Constructor&lt;?&gt;&gt; ambiguousConstructors：模棱两可的构造器，两个构造器匹配度相等时放入 for (Constructor&lt;?&gt; candidate : candidates)：遍历筛选出 minTypeDiffWeight 最低的构造器 Class&lt;?&gt;[] paramTypes = candidate.getParameterTypes()：获取当前处理的构造器的参数类型 if()：candidates 是排过序的，当前筛选出来的构造器的优先级一定是优先于后面的 constructor if (paramTypes.length &lt; minNrOfArgs)：需求的小于给的，不匹配 int typeDiffWeight：获取匹配度 mbd.isLenientConstructorResolution()：true 表示 ambiguousConstructors 允许有数据，false 代表不允许有数据，有数据就报错（LenientConstructorResolution：宽松的构造函数解析） argsHolder.getTypeDifferenceWeight(paramTypes)：选择参数转换前和转换后匹配度最低的，循环向父类中寻找该方法，直到寻找到 Obejct 类 if (typeDiffWeight &lt; minTypeDiffWeight)：条件成立说明当前循环处理的构造器更优 else if (constructorToUse != null &amp;&amp; typeDiffWeight == minTypeDiffWeight)：当前处理的构造器的计算出来的 DiffWeight 与上一次筛选出来的最优构造器的值一致，说明有模棱两可的情况 if (constructorToUse == null)：未找到可以使用的构造器，报错 else if (ambiguousConstructors != null &amp;&amp; !mbd.isLenientConstructorResolution())：模棱两可有数据，LenientConstructorResolution &#x3D;&#x3D; false，所以报错 argsHolderToUse.storeCache(mbd, constructorToUse)：匹配成功，进行缓存，方便后来者使用该 bd 实例化 bw.setBeanInstance(instantiate(beanName, mbd, constructorToUse, argsToUse))：匹配成功调用 instantiate 创建出实例对象，设置到 BeanWrapper 中去 return instantiateBean(beanName, mbd)：默认走到这里 循环依赖循环引用循环依赖：是一个或多个对象实例之间存在直接或间接的依赖关系，这种依赖关系构成一个环形调用 Spring 循环依赖有四种： DependsOn 依赖加载【无法解决】（两种 Map） 原型模式 Prototype 循环依赖【无法解决】（正在创建集合） 单例 Bean 循环依赖：构造参数产生依赖【无法解决】（正在创建集合，getSingleton() 逻辑中） 单例 Bean 循环依赖：setter 产生依赖【可以解决】 解决循环依赖：提前引用，提前暴露创建中的 Bean Spring 先实例化 A，拿到 A 的构造方法反射创建出来 A 的早期实例对象，这个对象被包装成 ObjectFactory 对象，放入三级缓存 处理 A 的依赖数据，检查发现 A 依赖 B 对象，所以 Spring 就会去根据 B 类型到容器中去 getBean(B)，这里产生递归 拿到 B 的构造方法，进行反射创建出来 B 的早期实例对象，也会把 B 包装成 ObjectFactory 对象，放到三级缓存，处理 B 的依赖数据，检查发现 B 依赖了 A 对象，然后 Spring 就会去根据 A 类型到容器中去 getBean(A.class) 这时从三级缓存中获取到 A 的早期对象进入属性填充 循环依赖的三级缓存： 12345678//一级缓存：存放所有初始化完成单实例 bean，单例池，key是beanName，value是对应的单实例对象引用private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);//二级缓存：存放实例化未进行初始化的 Bean，提前引用池private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;&gt;(16);/** Cache of singleton factories: bean name to ObjectFactory. 3*/private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16); 为什么需要三级缓存？ 循环依赖解决需要提前引用动态代理对象，AOP 动态代理是在 Bean 初始化后的后置处理中进行，这时的 bean 已经是成品对象。因为需要提前进行动态代理，三级缓存的 ObjectFactory 提前产生需要代理的对象，把提前引用放入二级缓存 如果只有二级缓存，提前引用就直接放入了一级缓存，然后 Bean 初始化完成后又会放入一级缓存，产生数据覆盖，导致提前引用的对象和一级缓存中的并不是同一个对象 一级缓存只能存放完整的单实例，为了保证 Bean 的生命周期不被破坏，不能将未初始化的 Bean 暴露到一级缓存 若存在循环依赖，后置处理不创建代理对象，真正创建代理对象的过程是在 getBean(B) 的阶段中 三级缓存一定会创建提前引用吗？ 出现循环依赖就会去三级缓存获取提前引用，不出现就不会，走正常的逻辑，创建完成直接放入一级缓存 存在循环依赖，就创建代理对象放入二级缓存，如果没有增强方法就返回 createBeanInstance 创建的实例，因为 addSingletonFactory 参数中传入了实例化的 Bean，在 singletonFactory.getObject() 中返回给 singletonObject，所以存在循环依赖就一定会使用工厂，但是不一定创建的是代理对象，不需要增强就是原始对象 wrapIfNecessary 一定创建代理对象吗？（AOP 动态代理部分有源码解析） 存在增强器会创建动态代理，不需要增强就不需要创建动态代理对象 存在循环依赖会提前增强，初始化后不需要增强 什么时候将 Bean 的引用提前暴露给第三级缓存的 ObjectFactory 持有？ 实例化之后，依赖注入之前 1createBeanInstance -&gt; addSingletonFactory -&gt; populateBean 源码解析假如 A 依赖 B，B 依赖 A 当 A 创建实例后填充属性前，执行： 1addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)) 1234567891011121314// 添加给定的单例工厂以构建指定的单例protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(singletonFactory, &quot;Singleton factory must not be null&quot;); synchronized (this.singletonObjects) &#123; // 单例池包含该Bean说明已经创建完成，不需要循环依赖 if (!this.singletonObjects.containsKey(beanName)) &#123; //加入三级缓存 this.singletonFactories.put(beanName,singletonFactory); this.earlySingletonObjects.remove(beanName); // 从二级缓存移除，因为三个Map中都是一个对象，不能同时存在！ this.registeredSingletons.add(beanName); &#125; &#125;&#125; 填充属性时 A 依赖 B，这时需要 getBean(B)，也会把 B 的工厂放入三级缓存，接着 B 填充属性时发现依赖 A，去进行**第一次 ** getSingleton(A) 123456789101112131415161718192021222324252627public Object getSingleton(String beanName) &#123; return getSingleton(beanName, true);//为true代表允许拿到早期引用。&#125;protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; // 在一级缓存中获取 beanName 对应的单实例对象。 Object singletonObject = this.singletonObjects.get(beanName); // 单实例确实尚未创建；单实例正在创建，发生了循环依赖 if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; // 从二级缓存获取 singletonObject = this.earlySingletonObjects.get(beanName); // 二级缓存不存在，并且允许获取早期实例对象，去三级缓存查看 if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; // 从三级缓存获取工厂对象，并得到 bean 的提前引用 singletonObject = singletonFactory.getObject(); // 【缓存升级】，放入二级缓存，提前引用池 this.earlySingletonObjects.put(beanName, singletonObject); // 从三级缓存移除该对象 this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject;&#125; 从三级缓存获取 A 的 Bean：singletonFactory.getObject()，调用了 lambda 表达式的 getEarlyBeanReference 方法： 1234567public Object getEarlyBeanReference(Object bean, String beanName) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); // 【向提前引用代理池 earlyProxyReferences 中添加该 Bean，防止对象被重新代理】 this.earlyProxyReferences.put(cacheKey, bean); // 创建代理对象，createProxy return wrapIfNecessary(bean, beanName, cacheKey);&#125; B 填充了 A 的提前引用后会继续初始化直到完成，返回原始 A 的逻辑继续执行 AOP注解原理@EnableAspectJAutoProxy：AOP 注解驱动，给容器中导入 AspectJAutoProxyRegistrar 12345678910@Import(AspectJAutoProxyRegistrar.class)public @interface EnableAspectJAutoProxy &#123; // 是否强制使用 CGLIB 创建代理对象 // 配置文件方式：&lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt;\tboolean proxyTargetClass() default false; // 将当前代理对象暴露到上下文内，方便代理对象内部的真实对象拿到代理对象 // 配置文件方式：&lt;aop:aspectj-autoproxy expose-proxy=&quot;true&quot;/&gt;\tboolean exposeProxy() default false;&#125; AspectJAutoProxyRegistrar 在用来向容器中注册 AnnotationAwareAspectJAutoProxyCreator，以 BeanDefiantion 形式存在，在容器初始化时加载。AnnotationAwareAspectJAutoProxyCreator 间接实现了 InstantiationAwareBeanPostProcessor，Order 接口，该类会在 Bean 的实例化和初始化的前后起作用 工作流程：创建 IOC 容器，调用 refresh() 刷新容器，registerBeanPostProcessors(beanFactory) 阶段，通过 getBean() 创建 AnnotationAwareAspectJAutoProxyCreator 对象，在生命周期的初始化方法中执行回调 initBeanFactory() 方法初始化注册三个工具类：BeanFactoryAdvisorRetrievalHelperAdapter、ReflectiveAspectJAdvisorFactory、BeanFactoryAspectJAdvisorsBuilderAdapter 后置处理Bean 初始化完成的执行后置处理器的方法： 123456789101112public Object postProcessAfterInitialization(@Nullable Object bean,String bN)&#123; if (bean != null) &#123; // cacheKey 是 【beanName 或者加上 &amp; 的 beanName】 Object cacheKey = getCacheKey(bean.getClass(), beanName); if (this.earlyProxyReferences.remove(cacheKey) != bean) &#123; // 去提前代理引用池中寻找该 key，不存在则创建代理 // 如果存在则证明被代理过，则判断是否是当前的 bean，不是则创建代理 return wrapIfNecessary(bean, bN, cacheKey); &#125; &#125; return bean;&#125; AbstractAutoProxyCreator.wrapIfNecessary()：根据通知创建动态代理，没有通知直接返回原实例 1234567891011121314151617181920212223242526272829303132333435363738protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; // 条件一般不成立，很少使用 TargetSourceCreator 去创建对象 BeforeInstantiation 阶段，doCreateBean 之前的阶段 if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; // advisedBeans 集合保存的是 bean 是否被增强过了 // 条件成立说明当前 beanName 对应的实例不需要被增强处理，判断是在 BeforeInstantiation 阶段做的 if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; // 条件一：判断当前 bean 类型是否是基础框架类型，这个类的实例不能被增强 // 条件二：shouldSkip 判断当前 beanName 是否是 .ORIGINAL 结尾，如果是就跳过增强逻辑，直接返回 if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; // 【查找适合当前 bean 实例的增强方法】（下一节详解） Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); // 条件成立说明上面方法查询到适合当前class的通知 if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.put(cacheKey, Boolean.TRUE); // 根据查询到的增强创建代理对象（下一节详解） // 参数一：目标对象 // 参数二：beanName // 参数三：匹配当前目标对象 clazz 的 Advisor 数据 Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); // 保存代理对象类型 this.proxyTypes.put(cacheKey, proxy.getClass()); // 返回代理对象 return proxy; &#125;\t// 执行到这里说明没有查到通知，当前 bean 不需要增强 this.advisedBeans.put(cacheKey, Boolean.FALSE); // 【返回原始的 bean 实例】 return bean;&#125; 获取通知AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean()：查找适合当前类实例的增强，并进行排序 12345678910protected Object[] getAdvicesAndAdvisorsForBean(Class&lt;?&gt; beanClass, String beanName, @Nullable TargetSource targetSource) &#123;\t// 查询适合当前类型的增强通知 List&lt;Advisor&gt; advisors = findEligibleAdvisors(beanClass, beanName); if (advisors.isEmpty()) &#123; // 增强为空直接返回 null，不需要创建代理 return DO_NOT_PROXY; &#125; // 不是空，转成数组返回 return advisors.toArray();&#125; AbstractAdvisorAutoProxyCreator.findEligibleAdvisors()： candidateAdvisors = findCandidateAdvisors()：获取当前容器内可以使用（所有）的 advisor，调用的是 AnnotationAwareAspectJAutoProxyCreator 类的方法，每个方法对应一个 Advisor advisors = super.findCandidateAdvisors()：查询出 XML 配置的所有 Advisor 类型 advisorNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors()：通过 BF 查询出来 BD 配置的 class 中 是 Advisor 子类的 BeanName advisors.add()：使用 Spring 容器获取当前这个 Advisor 类型的实例 advisors.addAll(....buildAspectJAdvisors())：获取所有添加 @Aspect 注解类中的 Advisor buildAspectJAdvisors()：构建的方法，把 Advice 封装成 Advisor beanNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors(this.beanFactory, Object.class, true, false)：获取出容器内 Object 所有的 beanName，就是全部的 for (String beanName : beanNames)：遍历所有的 beanName，判断每个 beanName 对应的 Class 是否是 Aspect 类型，就是加了 @Aspect 注解的类 factory = new BeanFactoryAspectInstanceFactory(this.beanFactory, beanName)：使用工厂模式管理 Aspect 的元数据，关联的真实 @Aspect 注解的实例对象 classAdvisors = this.advisorFactory.getAdvisors(factory)：添加了 @Aspect 注解的类的通知信息 aspectClass：@Aspect 标签的类的 class for (Method method : getAdvisorMethods(aspectClass))：遍历不包括 @Pointcut 注解的方法 Advisor advisor = getAdvisor(method, lazySingletonAspectInstanceFactory, advisors.size(), aspectName)：将当前 method 包装成 Advisor 数据 AspectJExpressionPointcut expressionPointcut = getPointcut()：获取切点表达式 return new InstantiationModelAwarePointcutAdvisorImpl()：把 method 中 Advice 包装成 Advisor，Spring 中每个 Advisor 内部一定是持有一个 Advice 的，Advice 内部最重要的数据是当前 method 和aspectInstanceFactory，工厂用来获取实例 this.instantiatedAdvice = instantiateAdvice(this.declaredPointcut)：实例化 Advice 对象，逻辑是获取注解信息，根据注解的不同生成对应的 Advice 对象 advisors.addAll(classAdvisors)：保存通过 @Aspect 注解定义的 Advisor 数据 this.aspectBeanNames = aspectNames：将所有 @Aspect 注解 beanName 缓存起来，表示提取 Advisor 工作完成 return advisors：返回 Advisor 列表 eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, ...)：选出匹配当前类的增强 if (candidateAdvisors.isEmpty())：条件成立说明当前 Spring 没有可以操作的 Advisor List&lt;Advisor&gt; eligibleAdvisors = new ArrayList&lt;&gt;()：存放匹配当前 beanClass 的 Advisors 信息 for (Advisor candidate : candidateAdvisors)：遍历所有的 Advisor if (canApply(candidate, clazz, hasIntroductions))：判断遍历的 advisor 是否匹配当前的 class，匹配就加入集合 if (advisor instanceof PointcutAdvisor)：创建的 advisor 是 InstantiationModelAwarePointcutAdvisorImpl 类型 PointcutAdvisor pca = (PointcutAdvisor) advisor：封装当前 Advisor return canApply(pca.getPointcut(), targetClass, hasIntroductions)：重载该方法 if (!pc.getClassFilter().matches(targetClass))：类不匹配 Pointcut 表达式，直接返回 false methodMatcher = pc.getMethodMatcher()：获取 Pointcut 方法匹配器，类匹配进行类中方法的匹配 Set&lt;Class&lt;?&gt;&gt; classes：保存目标对象 class 和目标对象父类超类的接口和自身实现的接口 if (!Proxy.isProxyClass(targetClass))：判断当前实例是不是代理类，确保 class 内存储的数据包括目标对象的class 而不是代理类的 class for (Class&lt;?&gt; clazz : classes)：检查目标 class 和上级接口的所有方法，查看是否会被方法匹配器匹配，如果有一个方法匹配成功，就说明目标对象 AOP 代理需要增强 specificMethod = AopUtils.getMostSpecificMethod(method, targetClass)：方法可能是接口的，判断当前类有没有该方法 return (specificMethod != method &amp;&amp; matchesMethod(specificMethod))：类和方法的匹配，不包括参数 extendAdvisors(eligibleAdvisors)：在 eligibleAdvisors 列表的索引 0 的位置添加 DefaultPointcutAdvisor，封装了 ExposeInvocationInterceptor 拦截器 eligibleAdvisors = sortAdvisors(eligibleAdvisors)：对拦截器进行排序，数值越小优先级越高，高的排在前面 实现 Ordered 或 PriorityOrdered 接口，PriorityOrdered 的级别要优先于 Ordered，使用 OrderComparator 比较器 使用 @Order（Spring 规范）或 @Priority（JDK 规范）注解，使用 AnnotationAwareOrderComparator 比较器 ExposeInvocationInterceptor 实现了 PriorityOrdered ，所以总是排在第一位，MethodBeforeAdviceInterceptor 没实现任何接口，所以优先级最低，排在最后 return eligibleAdvisors：返回拦截器链 创建代理AbstractAutoProxyCreator.createProxy()：根据增强方法创建代理对象 ProxyFactory proxyFactory = new ProxyFactory()：无参构造 ProxyFactory，此处讲解一下两种有参构造方法： public ProxyFactory(Object target)： 123456public ProxyFactory(Object target) &#123;\t// 将目标对象封装成 SingletonTargetSource 保存到父类的字段中 setTarget(target); // 获取目标对象 class 所有接口保存到 AdvisedSupport 中的 interfaces 集合中 setInterfaces(ClassUtils.getAllInterfaces(target));&#125; ClassUtils.getAllInterfaces(target) 底层调用 getAllInterfacesForClassAsSet(java.lang.Class&lt;?&gt;, java.lang.ClassLoader)： if (clazz.isInterface() &amp;&amp; isVisible(clazz, classLoader))： 条件一：判断当前目标对象是接口 条件二：检查给定的类在给定的 ClassLoader 中是否可见 Class&lt;?&gt;[] ifcs = current.getInterfaces()：拿到自己实现的接口，拿不到接口实现的接口 current = current.getSuperclass()：递归寻找父类的接口，去获取父类实现的接口 public ProxyFactory(Class&lt;?&gt; proxyInterface, Interceptor interceptor)： 123456public ProxyFactory(Class&lt;?&gt; proxyInterface, Interceptor interceptor) &#123; // 添加一个代理的接口 addInterface(proxyInterface); // 添加通知，底层调用 addAdvisor addAdvice(interceptor);&#125; addAdvisor(pos, new DefaultPointcutAdvisor(advice))：Spring 中 Advice 对应的接口就是 Advisor，Spring 使用 Advisor 包装 Advice 实例 proxyFactory.copyFrom(this)：填充一些信息到 proxyFactory if (!proxyFactory.isProxyTargetClass())：条件成立说明 proxyTargetClass 为 false（默认），两种配置方法： &lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt; ：强制使用 CGLIB @EnableAspectJAutoProxy(proxyTargetClass = true) if (shouldProxyTargetClass(beanClass, beanName))：如果 bd 内有 preserveTargetClass &#x3D; true ，那么这个 bd 对应的 class 创建代理时必须使用 CGLIB，条件成立设置 proxyTargetClass 为 true evaluateProxyInterfaces(beanClass, proxyFactory)：根据目标类判定是否可以使用 JDK 动态代理 targetInterfaces = ClassUtils.getAllInterfacesForClass()：获取当前目标对象 class 和父类的全部实现接口 boolean hasReasonableProxyInterface = false：实现的接口中是否有一个合理的接口 if (!isConfigurationCallbackInterface(ifc) &amp;&amp; !isInternalLanguageInterface(ifc) &amp;&amp; ifc.getMethods().length &gt; 0)：遍历所有的接口，如果有任意一个接口满足条件，设置 hRPI 变量为 true 条件一：判断当前接口是否是 Spring 生命周期内会回调的接口 条件二：接口不能是 GroovyObject、Factory、MockAccess 类型的 条件三：找到一个可以使用的被代理的接口 if (hasReasonableProxyInterface)：有合理的接口，将这些接口设置到 proxyFactory 内 proxyFactory.setProxyTargetClass(true)：没有合理的代理接口，强制使用 CGLIB 创建对象 advisors = buildAdvisors(beanName, specificInterceptors)：匹配目标对象 clazz 的 Advisors，填充至 ProxyFactory proxyFactory.setPreFiltered(true)：设置为 true 表示传递给 proxyFactory 的 Advisors 信息做过基础类和方法的匹配 return proxyFactory.getProxy(getProxyClassLoader())：创建代理对象 123public Object getProxy() &#123; return createAopProxy().getProxy();&#125; DefaultAopProxyFactory.createAopProxy(AdvisedSupport config)：参数是一个配置对象，保存着创建代理需要的生产资料，会加锁创建，保证线程安全 12345678910111213141516171819public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; // 条件二为 true 代表强制使用 CGLIB 动态代理 if (config.isOptimize() || config.isProxyTargetClass() || // 条件三：被代理对象没有实现任何接口或者只实现了 SpringProxy 接口，只能使用 CGLIB 动态代理 hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;&quot;); &#125; // 条件成立说明 target 【是接口或者是已经被代理过的类型】，只能使用 JDK 动态代理 if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config);\t// 使用 JDK 动态代理 &#125; return new ObjenesisCglibAopProxy(config);\t// 使用 CGLIB 动态代理 &#125; else &#123; return new JdkDynamicAopProxy(config); // 【有接口的情况下只能使用 JDK 动态代理】 &#125;&#125; JdkDynamicAopProxy.getProxy(java.lang.ClassLoader)：获取 JDK 的代理对象 12345678910111213141516public JdkDynamicAopProxy(AdvisedSupport config) throws AopConfigException &#123; // 配置类封装到 JdkDynamicAopProxy.advised 属性中 this.advised = config;&#125;public Object getProxy(@Nullable ClassLoader classLoader) &#123; // 获取需要代理的接口数组 Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); // 查找当前所有的需要代理的接口，看是否有 equals 方法和 hashcode 方法，如果有就做一个标记 findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); // 该方法最终返回一个代理类对象 return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this); // classLoader：类加载器 proxiedInterfaces：生成的代理类，需要实现的接口集合 // this JdkDynamicAopProxy 实现了 InvocationHandler&#125; AopProxyUtils.completeProxiedInterfaces(this.advised, true)：获取代理的接口数组，并添加 SpringProxy 接口 specifiedInterfaces = advised.getProxiedInterfaces()：从 ProxyFactory 中拿到所有的 target 提取出来的接口 if (specifiedInterfaces.length == 0)：如果没有实现接口，检查当前 target 是不是接口或者已经是代理类，封装到 ProxyFactory 的 interfaces 集合中 addSpringProxy = !advised.isInterfaceProxied(SpringProxy.class)：判断目标对象所有接口中是否有 SpringProxy 接口，没有的话需要添加，这个接口标识这个代理类型是 Spring 管理的 addAdvised = !advised.isOpaque() &amp;&amp; !advised.isInterfaceProxied(Advised.class)：判断目标对象的所有接口，是否已经有 Advised 接口 addDecoratingProxy = (decoratingProxy &amp;&amp; !advised.isInterfaceProxied(DecoratingProxy.class))：判断目标对象的所有接口，是否已经有 DecoratingProxy 接口 int nonUserIfcCount = 0：非用户自定义的接口数量，接下来要添加上面的三个接口了 proxiedInterfaces = new Class&lt;?&gt;[specifiedInterfaces.length + nonUserIfcCount]：创建一个新的 class 数组，长度是原目标对象提取出来的接口数量和 Spring 追加的数量，然后进行 System.arraycopy 拷贝到新数组中 int index = specifiedInterfaces.length：获取原目标对象提取出来的接口数量，当作 index if(addSpringProxy)：根据上面三个布尔值把接口添加到新数组中 return proxiedInterfaces：返回追加后的接口集合 JdkDynamicAopProxy.findDefinedEqualsAndHashCodeMethods()：查找在任何定义在接口中的 equals 和 hashCode 方法 for (Class&lt;?&gt; proxiedInterface : proxiedInterfaces)：遍历所有的接口 Method[] methods = proxiedInterface.getDeclaredMethods()：获取接口中的所有方法 for (Method method : methods)：遍历所有的方法 if (AopUtils.isEqualsMethod(method))：当前方法是 equals 方法，把 equalsDefined 置为 true if (AopUtils.isHashCodeMethod(method))：当前方法是 hashCode 方法，把 hashCodeDefined 置为 true if (this.equalsDefined &amp;&amp; this.hashCodeDefined)：如果有一个接口中有这两种方法，直接返回 方法增强main() 函数中调用用户方法，会进入代理对象的 invoke 方法 JdkDynamicAopProxy 类中的 invoke 方法是真正执行代理方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// proxy：代理对象，method：目标对象的方法，args：目标对象方法对应的参数public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object oldProxy = null; boolean setProxyContext = false; // advised 就是初始化 JdkDynamicAopProxy 对象时传入的变量 TargetSource targetSource = this.advised.targetSource; Object target = null; try &#123; // 条件成立说明代理类实现的接口没有定义 equals 方法，并且当前 method 调用 equals 方法， // 就调用 JdkDynamicAopProxy 提供的 equals 方法 if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123; return equals(args[0]); &#125; //..... Object retVal; // 需不需要暴露当前代理对象到 AOP 上下文内 if (this.advised.exposeProxy) &#123; // 【把代理对象设置到上下文环境】 oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; // 根据 targetSource 获取真正的代理对象 target = targetSource.getTarget(); Class&lt;?&gt; targetClass = (target != null ? target.getClass() : null); // 查找【适合该方法的增强】，首先从缓存中查找，查找不到进入主方法【下文详解】 List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); // 拦截器链是空，说明当前 method 不需要被增强 if (chain.isEmpty()) &#123; Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); &#125; else &#123; // 有匹配当前 method 的方法拦截器，要做增强处理，把方法信息封装到方法调用器里 MethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // 【拦截器链驱动方法，核心】 retVal = invocation.proceed(); &#125; Class&lt;?&gt; returnType = method.getReturnType(); if (retVal != null &amp;&amp; retVal == target &amp;&amp; returnType != Object.class &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; // 如果目标方法返回目标对象，这里做个普通替换返回代理对象 retVal = proxy; &#125; // 返回执行的结果 return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; targetSource.releaseTarget(target); &#125; // 如果允许了提前暴露，这里需要设置为初始状态 if (setProxyContext) &#123; // 当前代理对象已经完成工作，【把原始对象设置回上下文】 AopContext.setCurrentProxy(oldProxy); &#125; &#125;&#125; this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass)：查找适合该方法的增强，首先从缓存中查找，获取通知时是从全部增强中获取适合当前类的，这里是从当前类的中获取适合当前方法的增强 AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance()：向容器注册适配器，可以将非 Advisor 类型的增强，包装成为 Advisor，将 Advisor 类型的增强提取出来对应的 MethodInterceptor instance = new DefaultAdvisorAdapterRegistry()：该对象向容器中注册了 MethodBeforeAdviceAdapter、AfterReturningAdviceAdapter、ThrowsAdviceAdapter 三个适配器 Advisor 中持有 Advice 对象 123public interface Advisor &#123;\tAdvice getAdvice();&#125; advisors = config.getAdvisors()：获取 ProxyFactory 内部持有的增强信息 interceptorList = new ArrayList&lt;&gt;(advisors.length)：拦截器列表有 5 个，1 个 ExposeInvocation和 4 个增强器 actualClass = (targetClass != null ? targetClass : method.getDeclaringClass())：真实的目标对象类型 Boolean hasIntroductions = null：引介增强，不关心 for (Advisor advisor : advisors)：遍历所有的 advisor 增强 if (advisor instanceof PointcutAdvisor)：条件成立说明当前 Advisor 是包含切点信息的，进入匹配逻辑 pointcutAdvisor = (PointcutAdvisor) advisor：转成可以获取到切点信息的接口 if(config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(actualClass))：当前代理被预处理，或者当前被代理的 class 对象匹配当前 Advisor 成功，只是 class 匹配成功 mm = pointcutAdvisor.getPointcut().getMethodMatcher()：获取切点的方法匹配器，不考虑引介增强 match = mm.matches(method, actualClass)：静态匹配成功返回 true，只关注于处理类及其方法，不考虑参数 if (match)：如果静态切点检查是匹配的，在运行的时候才进行动态切点检查，会考虑参数匹配（代表传入了参数）。如果静态匹配失败，直接不需要进行参数匹配，提高了工作效率 interceptors = registry.getInterceptors(advisor)：提取出当前 advisor 内持有的 advice 信息 Advice advice = advisor.getAdvice()：获取增强方法 if (advice instanceof MethodInterceptor)：当前 advice 是 MethodInterceptor 直接加入集合 for (AdvisorAdapter adapter : this.adapters)：遍历三个适配器进行匹配（初始化时创建的），匹配成功创建对应的拦截器返回，以 MethodBeforeAdviceAdapter 为例 if (adapter.supportsAdvice(advice))：判断当前 advice 是否是对应的 MethodBeforeAdvice interceptors.add(adapter.getInterceptor(advisor))：条件成立就往拦截器链中添加 advisor advice = (MethodBeforeAdvice) advisor.getAdvice()：获取增强方法 return new MethodBeforeAdviceInterceptor(advice)：封装成 MethodBeforeAdviceInterceptor 返回 interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm))：向拦截器链添加动态匹配器 interceptorList.addAll(Arrays.asList(interceptors))：将当前 advisor 内部的方法拦截器追加到 interceptorList interceptors = registry.getInterceptors(advisor)：进入 else 的逻辑，说明当前 Advisor 匹配全部 class 的全部 method，全部加入到 interceptorList return interceptorList：返回 method 方法的拦截器链 retVal &#x3D; invocation.proceed()：拦截器链驱动方法 if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1)：条件成立说明方法拦截器全部都已经调用过了（index 从 - 1 开始累加），接下来需要执行目标对象的目标方法 return invokeJoinpoint()：调用连接点（目标）方法 this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex)：获取下一个方法拦截器 if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher)：需要运行时匹配 if (dm.methodMatcher.matches(this.method, targetClass, this.arguments))：判断是否匹配成功 return dm.interceptor.invoke(this)：匹配成功，执行方法 return proceed()：匹配失败跳过当前拦截器 return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this)：一般方法拦截器都会执行到该方法，此方法内继续执行 proceed() 完成责任链的驱动，直到最后一个 MethodBeforeAdviceInterceptor 调用前置通知，然后调用 mi.proceed()，发现是最后一个拦截器就直接执行连接点（目标方法），return 到上一个拦截器的 mi.proceed() 处，依次返回到责任链的上一个拦截器执行通知方法 图示先从上往下建立链，然后从下往上依次执行，责任链模式 正常执行：（环绕通知）→ 前置通知 → 目标方法 → 后置通知 → 返回通知 出现异常：（环绕通知）→ 前置通知 → 目标方法 → 后置通知 → 异常通知 MethodBeforeAdviceInterceptor 源码： 123456public Object invoke(MethodInvocation mi) throws Throwable &#123; // 先执行通知方法，再驱动责任链 this.advice.before(mi.getMethod(), mi.getArguments(), mi.getThis()); // 开始驱动目标方法执行，执行完后返回到这，然后继续向上层返回 return mi.proceed();&#125; AfterReturningAdviceInterceptor 源码：没有任何异常处理机制，直接抛给上层 123456public Object invoke(MethodInvocation mi) throws Throwable &#123; // 先驱动责任链，再执行通知方法 Object retVal = mi.proceed(); this.advice.afterReturning(retVal, mi.getMethod(), mi.getArguments(), mi.getThis()); return retVal;&#125; AspectJAfterThrowingAdvice 执行异常处理： 12345678910111213public Object invoke(MethodInvocation mi) throws Throwable &#123; try &#123; // 默认直接驱动责任链 return mi.proceed(); &#125; catch (Throwable ex) &#123; // 出现错误才执行该方法 if (shouldInvokeOnThrowing(ex)) &#123; invokeAdviceMethod(getJoinPointMatch(), null, ex); &#125; throw ex; &#125;&#125; 参考视频：https://www.bilibili.com/video/BV1gW411W7wy 事务解析方法标签解析1&lt;tx:annotation-driven transaction-manager=&quot;txManager&quot;/&gt; 容器启动时会根据注解注册对应的解析器： 1234567891011public class TxNamespaceHandler extends NamespaceHandlerSupport &#123; public void init() &#123; registerBeanDefinitionParser(&quot;advice&quot;, new TxAdviceBeanDefinitionParser()); // 注册解析器 registerBeanDefinitionParser(&quot;annotation-driven&quot;, new AnnotationDrivenBeanDefinitionParser()); registerBeanDefinitionParser(&quot;jta-transaction-manager&quot;, new JtaTransactionManagerBeanDefinitionParser());\t&#125;&#125;protected final void registerBeanDefinitionParser(String elementName, BeanDefinitionParser parser) &#123; this.parsers.put(elementName, parser);&#125; 获取对应的解析器 NamespaceHandlerSupport#findParserForElement： 1234567private BeanDefinitionParser findParserForElement(Element element, ParserContext parserContext) &#123; String localName = parserContext.getDelegate().getLocalName(element); // 获取对应的解析器 BeanDefinitionParser parser = this.parsers.get(localName);\t// ... return parser;&#125; 调用解析器的方法对 XML 文件进行解析： 123456789101112131415161718public BeanDefinition parse(Element element, ParserContext parserContext) &#123;\t// 向Spring容器注册了一个 BD -&gt; TransactionalEventListenerFactory.class registerTransactionalEventListenerFactory(parserContext); String mode = element.getAttribute(&quot;mode&quot;); if (&quot;aspectj&quot;.equals(mode)) &#123; // mode=&quot;aspectj&quot; registerTransactionAspect(element, parserContext); if (ClassUtils.isPresent(&quot;javax.transaction.Transactional&quot;, getClass().getClassLoader())) &#123; registerJtaTransactionAspect(element, parserContext); &#125; &#125; else &#123; // mode=&quot;proxy&quot;，默认逻辑，不配置 mode 时 // 用来向容器中注入一些 BeanDefinition，包括事务增强器、事务拦截器、注解解析器 AopAutoProxyConfigurer.configureAutoProxyCreator(element, parserContext); &#125; return null;&#125; 注解解析@EnableTransactionManagement 导入 TransactionManagementConfigurationSelector，该类给 Spring 容器中两个组件： 12345678910111213protected String[] selectImports(AdviceMode adviceMode) &#123; switch (adviceMode) &#123; // 导入 AutoProxyRegistrar 和 ProxyTransactionManagementConfiguration（默认） case PROXY: return new String[] &#123;AutoProxyRegistrar.class.getName(), ProxyTransactionManagementConfiguration.class.getName()&#125;; // 导入 AspectJTransactionManagementConfiguration（与声明式事务无关） case ASPECTJ: return new String[] &#123;determineTransactionAspectClass()&#125;; default: return null; &#125;&#125; AutoProxyRegistrar：给容器中注册 InfrastructureAdvisorAutoProxyCreator，利用后置处理器机制拦截 bean 以后包装并返回一个代理对象，代理对象中保存所有的拦截器，利用拦截器的链式机制依次进入每一个拦截器中进行拦截执行（就是 AOP 原理） ProxyTransactionManagementConfiguration：是一个 Spring 的事务配置类，注册了三个 Bean： BeanFactoryTransactionAttributeSourceAdvisor：事务驱动，利用注解 @Bean 把该类注入到容器中，该增强器有两个字段： TransactionAttributeSource：解析事务注解的相关信息，真实类型是 AnnotationTransactionAttributeSource，构造方法中注册了三个注解解析器，解析 Spring、JTA、Ejb3 三种类型的事务注解 TransactionInterceptor：事务拦截器，代理对象执行拦截器方法时，调用 TransactionInterceptor 的 invoke 方法，底层调用TransactionAspectSupport.invokeWithinTransaction()，通过 PlatformTransactionManager 控制着事务的提交和回滚，所以事务的底层原理就是通过 AOP 动态织入，进行事务开启和提交 注解解析器 SpringTransactionAnnotationParser 解析 @Transactional 注解： 1234567891011121314151617181920212223242526272829303132333435protected TransactionAttribute parseTransactionAnnotation(AnnotationAttributes attributes) &#123; RuleBasedTransactionAttribute rbta = new RuleBasedTransactionAttribute();\t// 从注解信息中获取传播行为 Propagation propagation = attributes.getEnum(&quot;propagation&quot;); rbta.setPropagationBehavior(propagation.value()); // 获取隔离界别 Isolation isolation = attributes.getEnum(&quot;isolation&quot;); rbta.setIsolationLevel(isolation.value()); rbta.setTimeout(attributes.getNumber(&quot;timeout&quot;).intValue()); // 从注解信息中获取 readOnly 参数 rbta.setReadOnly(attributes.getBoolean(&quot;readOnly&quot;)); // 从注解信息中获取 value 信息并且设置 qualifier，表示当前事务指定使用的【事务管理器】 rbta.setQualifier(attributes.getString(&quot;value&quot;));\t// 【存放的是 rollback 条件】，回滚规则放在这个集合 List&lt;RollbackRuleAttribute&gt; rollbackRules = new ArrayList&lt;&gt;(); // 表示事务碰到哪些指定的异常才进行回滚，不指定的话默认是 RuntimeException/Error 非检查型异常菜回滚 for (Class&lt;?&gt; rbRule : attributes.getClassArray(&quot;rollbackFor&quot;)) &#123; rollbackRules.add(new RollbackRuleAttribute(rbRule)); &#125; // 与 rollbackFor 功能相同 for (String rbRule : attributes.getStringArray(&quot;rollbackForClassName&quot;)) &#123; rollbackRules.add(new RollbackRuleAttribute(rbRule)); &#125; // 表示事务碰到指定的 exception 实现对象不进行回滚，否则碰到其他的class就进行回滚 for (Class&lt;?&gt; rbRule : attributes.getClassArray(&quot;noRollbackFor&quot;)) &#123; rollbackRules.add(new NoRollbackRuleAttribute(rbRule)); &#125; for (String rbRule : attributes.getStringArray(&quot;noRollbackForClassName&quot;)) &#123; rollbackRules.add(new NoRollbackRuleAttribute(rbRule)); &#125; // 设置回滚规则 rbta.setRollbackRules(rollbackRules); return rbta;&#125; 驱动方法TransactionInterceptor 事务拦截器的核心驱动方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public Object invoke(MethodInvocation invocation) throws Throwable &#123; // targetClass 是需要被事务增强器增强的目标类，invocation.getThis() → 目标对象 → 目标类 Class&lt;?&gt; targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null);\t// 参数一是目标方法，参数二是目标类，参数三是方法引用，用来触发驱动方法 return invokeWithinTransaction(invocation.getMethod(), targetClass, invocation::proceed);&#125;protected Object invokeWithinTransaction(Method method, @Nullable Class&lt;?&gt; targetClass, final InvocationCallback invocation) throws Throwable &#123; // 事务属性源信息 TransactionAttributeSource tas = getTransactionAttributeSource(); // 提取 @Transactional 注解信息，txAttr 是注解信息的承载对象 final TransactionAttribute txAttr = (tas != null ? tas.getTransactionAttribute(method, targetClass) : null); // 获取 Spring 配置的事务管理器 // 首先会检查是否通过XML或注解配置 qualifier，没有就尝试去容器获取，一般情况下为 DatasourceTransactionManager final PlatformTransactionManager tm = determineTransactionManager(txAttr); // 权限定类名.方法名，该值用来当做事务名称使用 final String joinpointIdentification = methodIdentification(method, targetClass, txAttr); // 条件成立说明是【声明式事务】 if (txAttr == null || !(tm instanceof CallbackPreferringPlatformTransactionManager)) &#123; // 用来【开启事务】 TransactionInfo txInfo = createTransactionIfNecessary(tm, txAttr, joinpointIdentification); Object retVal; try &#123; // This is an 【around advice】: Invoke the next interceptor in the chain. // 环绕通知，执行目标方法（方法引用方式，invocation::proceed，还是调用 proceed） retVal = invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; // 执行业务代码时抛出异常，执行回滚逻辑 completeTransactionAfterThrowing(txInfo, ex); throw ex; &#125; finally &#123; // 清理事务的信息 cleanupTransactionInfo(txInfo); &#125; // 提交事务的入口 commitTransactionAfterReturning(txInfo); return retVal; &#125; else &#123; // 编程式事务，省略 &#125;&#125; 开启事务事务绑定创建事务的方法： 1234567891011121314151617181920212223242526272829303132protected TransactionInfo createTransactionIfNecessary(@Nullable PlatformTransactionManager tm, @Nullable TransactionAttribute txAttr, final String joinpointIdentification) &#123; // If no name specified, apply method identification as transaction name. if (txAttr != null &amp;&amp; txAttr.getName() == null) &#123; // 事务的名称： 类的权限定名.方法名 txAttr = new DelegatingTransactionAttribute(txAttr) &#123; @Override public String getName() &#123; return joinpointIdentification; &#125; &#125;; &#125; TransactionStatus status = null; if (txAttr != null) &#123; if (tm != null) &#123; // 通过事务管理器根据事务属性创建事务状态对象，事务状态对象一般情况下包装着 事务对象，当然也有可能是null // 方法上的注解为 @Transactional(propagation = NOT_SUPPORTED || propagation = NEVER) 时 // 【下一小节详解】 status = tm.getTransaction(txAttr); &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Skipping transactional joinpoint [&quot; + joinpointIdentification + &quot;] because no transaction manager has been configured&quot;); &#125; &#125; &#125; // 包装成一个上层的事务上下文对象 return prepareTransactionInfo(tm, txAttr, joinpointIdentification, status);&#125; TransactionAspectSupport#prepareTransactionInfo：为事务的属性和状态准备一个事务信息对象 TransactionInfo txInfo = new TransactionInfo(tm, txAttr, joinpointIdentification)：创建事务信息对象 txInfo.newTransactionStatus(status)：填充事务的状态信息 txInfo.bindToThread()：利用 ThreadLocal 把当前事务信息绑定到当前线程，不同的事务信息会形成一个栈的结构 this.oldTransactionInfo = transactionInfoHolder.get()：获取其他事务的信息存入 oldTransactionInfo transactionInfoHolder.set(this)：将当前的事务信息设置到 ThreadLocalMap 中 事务创建1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public final TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException &#123; // 获取事务的对象 Object transaction = doGetTransaction(); boolean debugEnabled = logger.isDebugEnabled(); if (definition == null) &#123; // Use defaults if no transaction definition given. definition = new DefaultTransactionDefinition(); &#125;\t// 条件成立说明当前是事务重入的情况，事务中有 ConnectionHolder 对象 if (isExistingTransaction(transaction)) &#123; // a方法开启事务，a方法内调用b方法，b方法仍然加了 @Transactional 注解，需要检查传播行为 return handleExistingTransaction(definition, transaction, debugEnabled); &#125; // 逻辑到这说明当前线程没有连接资源，一个连接对应一个事务，没有连接就相当于没有开启事务 // 检查事务的延迟属性 if (definition.getTimeout() &lt; TransactionDefinition.TIMEOUT_DEFAULT) &#123; throw new InvalidTimeoutException(&quot;Invalid transaction timeout&quot;, definition.getTimeout()); &#125; // 传播行为是 MANDATORY，没有事务就抛出异常 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_MANDATORY) &#123; throw new IllegalTransactionStateException(); &#125; // 需要开启事务的传播行为 else if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; // 什么也没挂起，因为线程并没有绑定事务 SuspendedResourcesHolder suspendedResources = suspend(null); try &#123; // 是否支持同步线程事务，一般是 true boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); // 新建一个事务状态信息 DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, suspendedResources); // 【启动事务】 doBegin(transaction, definition); // 设置线程上下文变量，方便程序运行期间获取当前事务的一些核心的属性，initSynchronization() 启动同步 prepareSynchronization(status, definition); return status; &#125; catch (RuntimeException | Error ex) &#123; // 恢复现场 resume(null, suspendedResources); throw ex; &#125; &#125; // 不支持事务的传播行为 else &#123; // Create &quot;empty&quot; transaction: no actual transaction, but potentially synchronization. boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); // 创建事务状态对象 // 参数2 transaction 是 null 说明当前事务状态是未手动开启事，线程上未绑定任何的连接资源，业务程序执行时需要先去 datasource 获取的 conn，是自动提交事务的，不需要 Spring 再提交事务 // 参数6 suspendedResources 是 null 说明当前事务状态未挂起任何事务，当前事务执行到后置处理时不需要恢复现场 return prepareTransactionStatus(definition, null, true, newSynchronization, debugEnabled, null); &#125;&#125; DataSourceTransactionManager#doGetTransaction：真正获取事务的方法 DataSourceTransactionObject txObject = new DataSourceTransactionObject()：创建事务对象 txObject.setSavepointAllowed(isNestedAllowed())：设置事务对象是否支持保存点，由事务管理器控制（默认不支持） ConnectionHolder conHolder = TransactionSynchronizationManager.getResource(obtainDataSource())： 从 ThreadLocal 中获取 conHolder 资源，可能拿到 null 或者不是 null 是 null：举例 12@Transactionpublic void a() &#123;...b.b()....&#125; 不是 null：执行 b 方法事务增强的前置逻辑时，可以拿到 a 放进去的 conHolder 资源 12@Transactionpublic void b() &#123;....&#125; txObject.setConnectionHolder(conHolder, false)：将 ConnectionHolder 保存到事务对象内，参数二是 false 代表连接资源是上层事务共享的，不是新建的连接资源 return txObject：返回事务的对象 DataSourceTransactionManager#doBegin：事务开启的逻辑 txObject = (DataSourceTransactionObject) transaction：强转为事务对象 事务中没有数据库连接资源就要分配： Connection newCon = obtainDataSource().getConnection()：获取 JDBC 原生的数据库连接对象 txObject.setConnectionHolder(new ConnectionHolder(newCon), true)：代表是新开启的事务，新建的连接对象 previousIsolationLevel = DataSourceUtils.prepareConnectionForTransaction(con, definition)：修改连接属性 if (definition != null &amp;&amp; definition.isReadOnly())：注解（或 XML）配置了只读属性，需要设置 if (..definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT)：注解配置了隔离级别 int currentIsolation = con.getTransactionIsolation()：获取连接的隔离界别 previousIsolationLevel = currentIsolation：保存之前的隔离界别，返回该值 con.setTransactionIsolation(definition.getIsolationLevel())：将当前连接设置为配置的隔离界别 txObject.setPreviousIsolationLevel(previousIsolationLevel)：将 Conn 原来的隔离级别保存到事务对象，为了释放 Conn 时重置回原状态 if (con.getAutoCommit())：默认会成立，说明还没开启事务 txObject.setMustRestoreAutoCommit(true)：保存 Conn 原来的事务状态 con.setAutoCommit(false)：开启事务，JDBC 原生的方式 txObject.getConnectionHolder().setTransactionActive(true)：表示 Holder 持有的 Conn 已经手动开启事务了 TransactionSynchronizationManager.bindResource(obtainDataSource(), txObject.getConnectionHolder())：将 ConnectionHolder 对象绑定到 ThreadLocal 内，数据源为 key，为了方便获取手动开启事务的连接对象去执行 SQL 事务重入事务重入的核心处理逻辑： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private TransactionStatus handleExistingTransaction( TransactionDefinition definition, Object transaction, boolean debugEnabled)&#123;\t// 传播行为是 PROPAGATION_NEVER，需要以非事务方式执行操作，如果当前事务存在则【抛出异常】 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) &#123; throw new IllegalTransactionStateException(); &#125;\t// 传播行为是 PROPAGATION_NOT_SUPPORTED，以非事务方式运行，如果当前存在事务，则【把当前事务挂起】 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) &#123; // 挂起事务 Object suspendedResources = suspend(transaction); boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); // 创建一个非事务的事务状态对象返回 return prepareTransactionStatus(definition, null, false, newSynchronization, debugEnabled, suspendedResources); &#125;\t// 开启新事物的逻辑 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) &#123; // 【挂起当前事务】 SuspendedResourcesHolder suspendedResources = suspend(transaction); // 【开启新事物】 &#125;\t// 传播行为是 PROPAGATION_NESTED，嵌套事务 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; // Spring 默认不支持内嵌事务 // 【开启方式】：&lt;property name=&quot;nestedTransactionAllowed&quot; value=&quot;true&quot;&gt; if (!isNestedTransactionAllowed()) &#123; throw new NestedTransactionNotSupportedException(); &#125; if (useSavepointForNestedTransaction()) &#123; // 为当前方法创建一个 TransactionStatus 对象， DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); // 创建一个 JDBC 的保存点 status.createAndHoldSavepoint(); // 不需要使用同步，直接返回 return status; &#125; else &#123; // Usually only for JTA transaction，开启一个新事务 &#125; &#125; // Assumably PROPAGATION_SUPPORTS or PROPAGATION_REQUIRED，【使用当前的事务】 boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); return prepareTransactionStatus(definition, transaction, false, newSynchronization, debugEnabled, null);&#125; 挂起恢复AbstractPlatformTransactionManager#suspend：挂起事务，并获得一个上下文信息对象 12345678910111213141516171819202122232425protected final SuspendedResourcesHolder suspend(@Nullable Object transaction) &#123; // 事务是同步状态的 if (TransactionSynchronizationManager.isSynchronizationActive()) &#123; List&lt;TransactionSynchronization&gt; suspendedSynchronizations = doSuspendSynchronization(); try &#123; Object suspendedResources = null; if (transaction != null) &#123; // do it suspendedResources = doSuspend(transaction); &#125; //将上层事务绑定在线程上下文的变量全部取出来 //... // 通过被挂起的资源和上层事务的上下文变量，创建一个【SuspendedResourcesHolder】返回 return new SuspendedResourcesHolder(suspendedResources, suspendedSynchronizations, name, readOnly, isolationLevel, wasActive); &#125; //...&#125;protected Object doSuspend(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; // 将当前方法的事务对象 connectionHolder 属性置为 null，不和上层共享资源 // 当前方法有可能是不开启事务或者要开启一个独立的事务 txObject.setConnectionHolder(null); // 【解绑在线程上的事务】 return TransactionSynchronizationManager.unbindResource(obtainDataSource());&#125; AbstractPlatformTransactionManager#resume：恢复现场，根据挂起资源去恢复线程上下文信息 1234567891011121314151617181920protected final void resume(Object transaction, SuspendedResourcesHolder resourcesHolder) &#123; if (resourcesHolder != null) &#123; // 获取被挂起的事务资源 Object suspendedResources = resourcesHolder.suspendedResources; if (suspendedResources != null) &#123; //绑定上一个事务的 ConnectionHolder 到线程上下文 doResume(transaction, suspendedResources); &#125; List&lt;TransactionSynchronization&gt; suspendedSynchronizations = resourcesHolder.suspendedSynchronizations; if (suspendedSynchronizations != null) &#123; //.... // 将线程上下文变量恢复为上一个事务的挂起现场 doResumeSynchronization(suspendedSynchronizations); &#125; &#125;&#125;protected void doResume(@Nullable Object transaction, Object suspendedResources) &#123; // doSuspend 的逆动作，【绑定资源】 TransactionSynchronizationManager.bindResource(obtainDataSource(), suspendedResources);&#125; 提交回滚回滚方式1234567891011121314151617181920212223242526protected void completeTransactionAfterThrowing(@Nullable TransactionInfo txInfo, Throwable ex) &#123; // 事务状态信息不为空进入逻辑 if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) &#123; // 条件二成立 说明目标方法抛出的异常需要回滚事务 if (txInfo.transactionAttribute != null &amp;&amp; txInfo.transactionAttribute.rollbackOn(ex)) &#123; try &#123; // 事务管理器的回滚方法 txInfo.getTransactionManager().rollback(txInfo.getTransactionStatus()); &#125; catch (TransactionSystemException ex2) &#123;&#125; &#125; else &#123; // 执行到这里，说明当前事务虽然抛出了异常，但是该异常并不会导致整个事务回滚 try &#123; // 提交事务 txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); &#125; catch (TransactionSystemException ex2) &#123;&#125; &#125; &#125;&#125;public boolean rollbackOn(Throwable ex) &#123; // 继承自 RuntimeException 或 error 的是【非检查型异常】，才会归滚事务 // 如果配置了其他回滚错误，会获取到回滚规则 rollbackRules 进行判断 return (ex instanceof RuntimeException || ex instanceof Error);&#125; 123456789public final void rollback(TransactionStatus status) throws TransactionException &#123; // 事务已经完成不需要回滚 if (status.isCompleted()) &#123; throw new IllegalTransactionStateException(); &#125; DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; // 开始回滚事务 processRollback(defStatus, false);&#125; AbstractPlatformTransactionManager#processRollback：事务回滚 triggerBeforeCompletion(status)：用来做扩展逻辑，回滚前的前置处理 if (status.hasSavepoint())：条件成立说明当前事务是一个内嵌事务，当前方法只是复用了上层事务的一个内嵌事务 status.rollbackToHeldSavepoint()：内嵌事务加入事务时会创建一个保存点，此时恢复至保存点 if (status.isNewTransaction())：说明事务是当前连接开启的，需要去回滚事务 doRollback(status)：真正的的回滚函数 DataSourceTransactionObject txObject = status.getTransaction()：获取事务对象 Connection con = txObject.getConnectionHolder().getConnection()：获取连接对象 con.rollback()：JDBC 的方式回滚事务 else：当前方法是共享的上层的事务，和上层使用同一个 Conn 资源，共享的事务不能直接回滚，应该交给上层处理 doSetRollbackOnly(status)：设置 con.rollbackOnly &#x3D; true，线程回到上层事务 commit 时会检查该字段，然后执行回滚操作 triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK)：回滚的后置处理 cleanupAfterCompletion(status)：清理和恢复现场 提交方式123456protected void commitTransactionAfterReturning(@Nullable TransactionInfo txInfo) &#123; if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) &#123; // 事务管理器的提交方法 txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); &#125;&#125; 123456789101112131415161718192021public final void commit(TransactionStatus status) throws TransactionException &#123; // 已经完成的事务不需要提交了 if (status.isCompleted()) &#123; throw new IllegalTransactionStateException(); &#125; DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; // 条件成立说明是当前的业务强制回滚 if (defStatus.isLocalRollbackOnly()) &#123; // 回滚逻辑， processRollback(defStatus, false); return; &#125;\t// 成立说明共享当前事务的【下层事务逻辑出错，需要回滚】 if (!shouldCommitOnGlobalRollbackOnly() &amp;&amp; defStatus.isGlobalRollbackOnly()) &#123; // 如果当前事务还是事务重入，会继续抛给上层，最上层事务会进行真实的事务回滚操作 processRollback(defStatus, true); return; &#125;\t// 执行提交 processCommit(defStatus);&#125; AbstractPlatformTransactionManager#processCommit：事务提交 prepareForCommit(status)：前置处理 if (status.hasSavepoint())：条件成立说明当前事务是一个内嵌事务，只是复用了上层事务 status.releaseHeldSavepoint()：清理保存点，因为没有发生任何异常，所以保存点没有存在的意义了 if (status.isNewTransaction())：说明事务是归属于当前连接的，需要去提交事务 doCommit(status)：真正的提交函数 Connection con = txObject.getConnectionHolder().getConnection()：获取连接对象 con.commit()：JDBC 的方式提交事务 doRollbackOnCommitException(status, ex)：提交事务出错后进行回滚 cleanupAfterCompletion(status)：清理和恢复现场 清理现场恢复上层事务： 12345678910protected void cleanupTransactionInfo(@Nullable TransactionInfo txInfo) &#123; if (txInfo != null) &#123; // 从当前线程的 ThreadLocal 获取上层的事务信息，将当前事务出栈，继续执行上层事务 txInfo.restoreThreadLocalStatus(); &#125;&#125;private void restoreThreadLocalStatus() &#123; // Use stack to restore old transaction TransactionInfo. transactionInfoHolder.set(this.oldTransactionInfo);&#125; 当前层级事务结束时的清理： 12345678910111213141516171819private void cleanupAfterCompletion(DefaultTransactionStatus status) &#123; // 设置当前方法的事务状态为完成状态 status.setCompleted(); if (status.isNewSynchronization()) &#123; // 清理线程上下文变量以及扩展点注册的 sync TransactionSynchronizationManager.clear(); &#125; // 事务是当前线程开启的 if (status.isNewTransaction()) &#123; // 解绑资源 doCleanupAfterCompletion(status.getTransaction()); &#125; // 条件成立说明当前事务执行的时候，【挂起了一个上层的事务】 if (status.getSuspendedResources() != null) &#123; Object transaction = (status.hasTransaction() ? status.getTransaction() : null); // 恢复上层事务现场 resume(transaction, (SuspendedResourcesHolder) status.getSuspendedResources()); &#125;&#125; DataSourceTransactionManager#doCleanupAfterCompletion：清理工作 TransactionSynchronizationManager.unbindResource(obtainDataSource())：解绑数据库资源 if (txObject.isMustRestoreAutoCommit())：是否恢复连接，Conn 归还到 DataSource，归还前需要恢复到申请时的状态 con.setAutoCommit(true)：恢复连接为自动提交 DataSourceUtils.resetConnectionAfterTransaction(con, txObject.getPreviousIsolationLevel())：恢复隔离级别 DataSourceUtils.releaseConnection(con, this.dataSource)：将连接归还给数据库连接池 txObject.getConnectionHolder().clear()：清理 ConnectionHolder 资源 注解Component@Component 解析流程： 注解类启动容器的时，注册 ClassPathBeanDefinitionScanner 到容器，用来扫描 Bean 的相关信息 123456789101112protected Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Set&lt;BeanDefinitionHolder&gt; beanDefinitions = new LinkedHashSet&lt;&gt;(); // 遍历指定的所有的包，【这就相当于扫描了】 for (String basePackage : basePackages) &#123; // 读取当前包下的资源装换为 BeanDefinition，字节流的方式 Set&lt;BeanDefinition&gt; candidates = findCandidateComponents(basePackage); for (BeanDefinition candidate : candidates) &#123; // 遍历，封装，类似于 XML 的解析方式，注册到容器中 registerBeanDefinition(definitionHolder, this.registry) &#125; return beanDefinitions;&#125; ClassPathScanningCandidateComponentProvider.findCandidateComponents() 12345678public Set&lt;BeanDefinition&gt; findCandidateComponents(String basePackage) &#123; if (this.componentsIndex != null &amp;&amp; indexSupportsIncludeFilters()) &#123; return addCandidateComponentsFromIndex(this.componentsIndex, basePackage); &#125; else &#123; return scanCandidateComponents(basePackage); &#125;&#125; 1private Set&lt;BeanDefinition&gt; scanCandidateComponents(String basePackage) &#123;&#125; String packageSearchPath = ResourcePatternResolver.CLASSPATH_ALL_URL_PREFIX resolveBasePackage(basePackage) + &#39;/&#39; + this.resourcePattern ：将 package 转化为 ClassLoader 类资源搜索路径 packageSearchPath，例如：com.sea.spring.boot 转化为 classpath*:com/sea/spring/boot/**/*.class resources = getResourcePatternResolver().getResources(packageSearchPath)：加载路径下的资源 for (Resource resource : resources) ：遍历所有的资源 metadataReader = getMetadataReaderFactory().getMetadataReader(resource)：获取元数据阅读器 if (isCandidateComponent(metadataReader))：当前类不匹配任何排除过滤器，并且匹配一个包含过滤器，返回 true includeFilters 由 registerDefaultFilters() 设置初始值，方法有 @Component，没有 @Service，因为 @Component 是 @Service 的元注解，Spring 在读取 @Service 时也读取了元注解，并将 @Service 作为 @Component 处理 1this.includeFilters.add(new AnnotationTypeFilter(Component.class)) 12345@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Component\t// 拥有了 Component 功能public @interface Service &#123;&#125; candidates.add(sbd)：添加到返回结果的 list 参考文章：https://my.oschina.net/floor/blog/4325651 Autowired打开 @Autowired 源码，注释上写 Please consult the javadoc for the AutowiredAnnotationBeanPostProcessor AutowiredAnnotationBeanPostProcessor 间接实现 InstantiationAwareBeanPostProcessor，就具备了实例化前后（而不是初始化前后）管理对象的能力，实现了 BeanPostProcessor，具有初始化前后管理对象的能力，实现 BeanFactoryAware，具备随时拿到 BeanFactory 的能力，所以这个类具备一切后置处理器的能力 在容器启动，为对象赋值的时候，遇到 @Autowired 注解，会用后置处理器机制，来创建属性的实例，然后再利用反射机制，将实例化好的属性，赋值给对象上，这就是 Autowired 的原理 作用时机： Spring 在每个 Bean 实例化之后，调用 AutowiredAnnotationBeanPostProcessor 的 postProcessMergedBeanDefinition() 方法，查找该 Bean 是否有 @Autowired 注解，进行相关元数据的获取 Spring 在每个 Bean 调用 populateBean() 进行属性注入的时候，即调用 postProcessProperties() 方法，查找该 Bean 属性是否有 @Autowired 注解，进行相关数据的填充 MVC基本介绍SpringMVC：是一种基于 Java 实现 MVC 模型的轻量级 Web 框架 SpringMVC 优点： 使用简单 性能突出（对比现有的框架技术） 灵活性强 软件开发三层架构： 表现层：负责数据展示 业务层：负责业务处理 数据层：负责数据操作 MVC（Model View Controller），一种用于设计创建Web应用程序表现层的模式 Model（模型）：数据模型，用于封装数据 View（视图）：页面视图，用于展示数据 jsp html Controller（控制器）：处理用户交互的调度器，用于根据用户需求处理程序逻辑 Servlet SpringMVC 参考视频：https://space.bilibili.com/37974444/ 基本配置入门项目流程分析： 服务器启动 加载 web.xml 中 DispatcherServlet 读取 spring-mvc.xml 中的配置，加载所有 controller 包中所有标记为 bean 的类 读取 bean 中方法上方标注 @RequestMapping 的内容 处理请求 DispatcherServlet 配置拦截所有请求 &#x2F; 使用请求路径与所有加载的 @RequestMapping 的内容进行比对 执行对应的方法 根据方法的返回值在 webapp 目录中查找对应的页面并展示 代码实现： pom.xml 导入坐标 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;demo&lt;/groupId&gt;&lt;artifactId&gt;spring_base_config&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;war&lt;/packaging&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- servlet3.0规范的坐标 --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!--jsp坐标--&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!--spring的坐标--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.9.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!--springmvc的坐标--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.1.9.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;!--构建--&gt;&lt;build&gt; &lt;!--设置插件--&gt; &lt;plugins&gt; &lt;!--具体的插件配置--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;80&lt;/port&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 设定具体 Controller，控制层 java &#x2F; controller &#x2F; UserController 1234567891011@Controller //@Component衍生注解public class UserController &#123; //设定当前方法的访问映射地址，等同于Servlet在web.xml中的配置 @RequestMapping(&quot;/save&quot;) //设置当前方法返回值类型为String，用于指定请求完成后跳转的页面 public String save()&#123; System.out.println(&quot;user mvc controller is running ...&quot;); //设定具体跳转的页面 return &quot;success.jsp&quot;; &#125;&#125; webapp &#x2F; WEB-INF &#x2F; web.xml，配置SpringMVC核心控制器，请求转发到对应的具体业务处理器Controller中（等同于Servlet配置） 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;!--配置Servlet--&gt; &lt;servlet&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--加载Spring控制文件--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; resouces &#x2F; spring-mvc.xml 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!--扫描加载所有的控制类--&gt; &lt;context:component-scan base-package=&quot;controller&quot;/&gt;&lt;/beans&gt; 加载控制Controller 加载控制：SpringMVC 的处理器对应的 bean 必须按照规范格式开发，未避免加入无效的 bean 可通过 bean 加载过滤器进行包含设定或排除设定，表现层 bean 标注通常设定为 @Controller resources &#x2F; spring-mvc.xml 配置 12345&lt;context:component-scan base-package=&quot;com.seazean&quot;&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt;&lt;/context:component-scan&gt; 静态资源加载（webapp 目录下的相关资源），spring-mvc.xml 配置，开启 mvc 命名空间 1234567&lt;!--放行指定类型静态资源配置方式--&gt;&lt;mvc:resources mapping=&quot;/img/**&quot; location=&quot;/img/&quot;/&gt; &lt;!--webapp/img/资源--&gt;&lt;mvc:resources mapping=&quot;/js/**&quot; location=&quot;/js/&quot;/&gt;&lt;mvc:resources mapping=&quot;/css/**&quot; location=&quot;/css/&quot;/&gt;&lt;!--SpringMVC 提供的通用资源放行方式，建议选择--&gt;&lt;mvc:default-servlet-handler/&gt; 中文乱码处理 SpringMVC 提供专用的中文字符过滤器，用于处理乱码问题。配置在 web.xml 里面 12345678910111213&lt;!--乱码处理过滤器，与Servlet中使用的完全相同，差异之处在于处理器的类由Spring提供--&gt;&lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 注解驱动WebApplicationContext，生成 Spring 核心容器（主容器&#x2F;父容器&#x2F;根容器） 父容器：Spring 环境加载后形成的容器，包含 Spring 环境下的所有的 bean 子容器：当前 mvc 环境加载后形成的容器，不包含 Spring 环境下的 bean 子容器可以访问父容器中的资源，父容器不可以访问子容器的资源 EnableWebMvc 注解作用： 支持 ConversionService 的配置，可以方便配置自定义类型转换器 支持 @NumberFormat 注解格式化数字类型 支持 @DateTimeFormat 注解格式化日期数据，日期包括 Date、Calendar 支持 @Valid 的参数校验（需要导入 JSR-303 规范） 配合第三方 jar 包和 SpringMVC 提供的注解读写 XML 和 JSON 格式数据 纯注解开发： 使用注解形式转化 SpringMVC 核心配置文件为配置类 java &#x2F; config &#x2F; SpringMVCConfiguration.java 1234567891011121314@Configuration@ComponentScan(value = &quot;com.seazean&quot;, includeFilters = @ComponentScan.Filter( type=FilterType.ANNOTATION, classes = &#123;Controller.class&#125; ) )//等同于&lt;mvc:annotation-driven/&gt;，还不完全相同@EnableWebMvcpublic class SpringMVCConfiguration implements WebMvcConfigurer&#123; //注解配置通用放行资源的格式 建议使用 @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123; configurer.enable(); &#125;&#125; 基于 servlet3.0 规范，自定义 Servlet 容器初始化配置类，加载 SpringMVC 核心配置类 1234567891011121314151617181920212223242526272829303132333435public class ServletContainersInitConfig extends AbstractDispatcherServletInitializer &#123; //创建Servlet容器时，使用注解方式加载SPRINGMVC配置类中的信息， //并加载成WEB专用的ApplicationContext对象该对象放入了ServletContext范围， //在整个WEB容器中可以随时获取调用 @Override protected WebApplicationContext createServletApplicationContext() &#123; A.C.W.A ctx = new AnnotationConfigWebApplicationContext(); ctx.register(SpringMVCConfiguration.class); return ctx; &#125; //注解配置映射地址方式，服务于SpringMVC的核心控制器DispatcherServlet @Override protected String[] getServletMappings() &#123; return new String[]&#123;&quot;/&quot;&#125;; &#125; @Override protected WebApplicationContext createRootApplicationContext() &#123; return null; &#125; //乱码处理作为过滤器，在servlet容器启动时进行配置 @Override public void onStartup(ServletContext servletContext) throws ServletException &#123; super.onStartup(servletContext); CharacterEncodingFilter cef = new CharacterEncodingFilter(); cef.setEncoding(&quot;UTF-8&quot;); FilterRegistration.Dynamic registration = servletContext.addFilter(&quot;characterEncodingFilter&quot;, cef); registration.addMappingForUrlPatterns(EnumSet.of( DispatcherType.REQUEST, DispatcherType.FORWARD, DispatcherType.INCLUDE), false,&quot;/*&quot;); &#125;&#125; 请求映射名称：@RequestMapping 类型：方法注解、类注解 位置：处理器类中的方法定义上方、处理器类定义上方 方法注解 作用：绑定请求地址与对应处理方法间的关系 无类映射地址访问格式： http://localhost/requestURL2 1234567@Controllerpublic class UserController &#123; @RequestMapping(&quot;/requestURL2&quot;) public String requestURL2() &#123; return &quot;page.jsp&quot;; &#125;&#125; 类注解 作用：为当前处理器中所有方法设定公共的访问路径前缀 带有类映射地址访问格式，将类映射地址作为前缀添加在实际映射地址前面：**&#x2F;user&#x2F;requestURL1** 最终返回的页面如果未设定绝对访问路径，将从类映射地址所在目录中查找 webapp&#x2F;user&#x2F;page.jsp 12345678@Controller@RequestMapping(&quot;/user&quot;)public class UserController &#123; @RequestMapping(&quot;/requestURL2&quot;) public String requestURL2() &#123; return &quot;page.jsp&quot;; &#125;&#125; 常用属性 1234567891011@RequestMapping( value=&quot;/requestURL3&quot;, //设定请求路径，与path属性、 value属性相同 method = RequestMethod.GET, //设定请求方式 params = &quot;name&quot;, //设定请求参数条件 headers = &quot;content-type=text/*&quot;, //设定请求消息头条件 consumes = &quot;text/*&quot;, //用于指定可以接收的请求正文类型（MIME类型） produces = &quot;text/*&quot; //用于指定可以生成的响应正文类型（MIME类型）)public String requestURL3() &#123; return &quot;/page.jsp&quot;;&#125; 基本操作请求处理普通类型SpringMVC 将传递的参数封装到处理器方法的形参中，达到快速访问参数的目的 访问 URL：http://localhost/requestParam1?name=seazean&amp;age=14 12345678@Controllerpublic class UserController &#123; @RequestMapping(&quot;/requestParam1&quot;) public String requestParam1(String name ,int age)&#123; System.out.println(&quot;name=&quot; + name + &quot;,age=&quot; + age); return &quot;page.jsp&quot;; &#125;&#125; 123456&lt;%@page pageEncoding=&quot;UTF-8&quot; language=&quot;java&quot; contentType=&quot;text/html;UTF-8&quot; %&gt;&lt;html&gt;&lt;body&gt;\t&lt;h1&gt;请求参数测试页面&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; @RequestParam 的使用： 类型：形参注解 位置：处理器类中的方法形参前方 作用：绑定请求参数与对应处理方法形参间的关系 访问 URL：http://localhost/requestParam2?userName=Jock 12345678@RequestMapping(&quot;/requestParam2&quot;)public String requestParam2(@RequestParam( name = &quot;userName&quot;, required = true,\t//为true代表必须有参数 defaultValue = &quot;s&quot;) String name)&#123; System.out.println(&quot;name=&quot; + name); return &quot;page.jsp&quot;;&#125; POJO类型简单类型当 POJO 中使用简单类型属性时， 参数名称与 POJO 类属性名保持一致 访问 URL： http://localhost/requestParam3?name=seazean&amp;age=14 12345@RequestMapping(&quot;/requestParam3&quot;)public String requestParam3(User user)&#123; System.out.println(&quot;name=&quot; + user.getName()); return &quot;page.jsp&quot;;&#125; 12345public class User &#123; private String name; private Integer age; //......&#125; 参数冲突当 POJO 类型属性与其他形参出现同名问题时，将被同时赋值，建议使用 @RequestParam 注解进行区分 访问 URL： http://localhost/requestParam4?name=seazean&amp;age=14 12345@RequestMapping(&quot;/requestParam4&quot;)public String requestParam4(User user, String age)&#123; System.out.println(&quot;user.age=&quot; + user.getAge() + &quot;,age=&quot; + age);//14 14 return &quot;page.jsp&quot;;&#125; 复杂类型当 POJO 中出现对象属性时，参数名称与对象层次结构名称保持一致 访问 URL： http://localhost/requestParam5?address.province=beijing 12345@RequestMapping(&quot;/requestParam5&quot;)public String requestParam5(User user)&#123; System.out.println(&quot;user.address=&quot; + user.getAddress().getProvince()); return &quot;page.jsp&quot;;&#125; 12345public class User &#123; private String name; private Integer age; private Address address; //....&#125; 12345public class Address &#123; private String province; private String city; private String address;&#125; 容器类型POJO 中出现集合类型的处理方式 通过 URL 地址中同名参数，可以为 POJO 中的集合属性进行赋值，集合属性要求保存简单数据 访问 URL：http://localhost/requestParam6?nick=Jock1&amp;nick=Jockme&amp;nick=zahc 123456@RequestMapping(&quot;/requestParam6&quot;)public String requestParam6(User user)&#123; System.out.println(&quot;user=&quot; + user); //user = User&#123;name=&#x27;null&#x27;,age=null,nick=&#123;Jock1,Jockme,zahc&#125;&#125; return &quot;page.jsp&quot;;&#125; 12345public class User &#123; private String name; private Integer age; private List&lt;String&gt; nick;&#125; POJO 中出现 List 保存对象数据，参数名称与对象层次结构名称保持一致，使用数组格式描述集合中对象的位置访问 URL：http://localhost/requestParam7?addresses[0].province=bj&amp;addresses[1].province=tj 123456@RequestMapping(&quot;/requestParam7&quot;)public String requestParam7(User user)&#123; System.out.println(&quot;user.addresses=&quot; + user.getAddress()); //&#123;Address&#123;provice=bj,city=&#x27;null&#x27;,address=&#x27;null&#x27;&#125;&#125;,&#123;Address&#123;....&#125;&#125; return &quot;page.jsp&quot;;&#125; 12345public class User &#123; private String name; private Integer age; private List&lt;Address&gt; addresses;&#125; POJO 中出现 Map 保存对象数据，参数名称与对象层次结构名称保持一致，使用映射格式描述集合中对象位置 URL: http://localhost/requestParam8?addressMap[’home’].province=bj&amp;addressMap[’job’].province=tj 123456@RequestMapping(&quot;/requestParam8&quot;)public String requestParam8(User user)&#123; System.out.println(&quot;user.addressMap=&quot; + user.getAddressMap()); //user.addressMap=&#123;home=Address&#123;p=,c=,a=&#125;,job=Address&#123;....&#125;&#125; return &quot;page.jsp&quot;;&#125; 1234public class User &#123; private Map&lt;String,Address&gt; addressMap; //....&#125; 数组集合数组类型请求参数名与处理器方法形参名保持一致，且请求参数数量＞ 1个 访问 URL： http://localhost/requestParam9?nick=Jockme&amp;nick=zahc 12345@RequestMapping(&quot;/requestParam9&quot;)public String requestParam9(String[] nick)&#123; System.out.println(nick[0] + &quot;,&quot; + nick[1]); return &quot;page.jsp&quot;;&#125; 集合类型保存简单类型数据，请求参数名与处理器方法形参名保持一致，且请求参数数量＞ 1个 访问 URL： http://localhost/requestParam10?nick=Jockme&amp;nick=zahc 12345@RequestMapping(&quot;/requestParam10&quot;)public String requestParam10(@RequestParam(&quot;nick&quot;) List&lt;String&gt; nick)&#123; System.out.println(nick); return &quot;page.jsp&quot;;&#125; 注意： SpringMVC 默认将 List 作为对象处理，赋值前先创建对象，然后将 nick 作为对象的属性进行处理。List 是接口无法创建对象，报无法找到构造方法异常；修复类型为可创建对象的 ArrayList 类型后，对象可以创建但没有 nick 属性，因此数据为空解决方法：需要告知 SpringMVC 的处理器 nick 是一组数据，而不是一个单一属性。通过 @RequestParam 注解，将数量大于 1 个 names 参数打包成参数数组后， SpringMVC 才能识别该数据格式，并判定形参类型是否为数组或集合，并按数组或集合对象的形式操作数据 转换器类型开启转换配置：&lt;mvc:annotation-driven /&gt; 作用：提供 Controller 请求转发，Json 自动转换等功能 如果访问 URL：http://localhost/requestParam1?name=seazean&amp;age=seazean，会出现报错，类型转化异常 12345@RequestMapping(&quot;/requestParam1&quot;)public String requestParam1(String name ,int age)&#123; System.out.println(&quot;name=&quot; + name + &quot;,age=&quot; + age); return &quot;page.jsp&quot;;&#125; SpringMVC 对接收的数据进行自动类型转换，该工作通过 Converter 接口实现： 标量转换器 集合、数组相关转换器 默认转换器 日期 如果访问 URL：http://localhost/requestParam11?date=1999-09-09 会报错，所以需要日期类型转换 声明自定义的转换格式并覆盖系统转换格式，配置 resources &#x2F; spring-mvc.xml 12345678910111213141516&lt;!--5.启用自定义Converter--&gt;&lt;mvc:annotation-driven conversion-service=&quot;conversionService&quot;/&gt;&lt;!--1.设定格式类型Converter，注册为Bean，受SpringMVC管理--&gt;&lt;bean id=&quot;conversionService&quot; class=&quot;org.springframework.format.support.FormattingConversionServiceFactoryBean&quot;&gt; &lt;!--2.自定义Converter格式类型设定，该设定使用的是同类型覆盖的思想--&gt; &lt;property name=&quot;formatters&quot;&gt; &lt;!--3.使用set保障相同类型的转换器仅保留一个，避免冲突--&gt; &lt;set&gt; &lt;!--4.设置具体的格式类型--&gt; &lt;bean class=&quot;org.springframework.format.datetime.DateFormatter&quot;&gt; &lt;!--5.类型规则--&gt; &lt;property name=&quot;pattern&quot; value=&quot;yyyy-MM-dd&quot;/&gt; &lt;/bean&gt; &lt;/set&gt; &lt;/property&gt;&lt;/bean&gt; @DateTimeFormat类型：形参注解、成员变量注解位置：形参前面 或 成员变量上方作用：为当前参数或变量指定类型转换规则 1234public String requestParam12(@DateTimeFormat(pattern = &quot;yyyy-MM-dd&quot;) Date date)&#123; System.out.println(&quot;date=&quot; + date); return &quot;page.jsp&quot;;&#125; 12@DateTimeFormat(pattern = &quot;yyyy-MM-dd&quot;)private Date date; 依赖注解驱动支持，xml 开启配置： 1&lt;mvc:annotation-driven /&gt; 自定义自定义类型转换器，实现 Converter 接口或者直接容器中注入： 方式一： 1234567891011121314151617181920212223242526272829303132333435363738394041424344 public class WebConfig implements WebMvcConfigurer &#123; @Bean public WebMvcConfigurer webMvcConfigurer() &#123; return new WebMvcConfigurer() &#123; @Override public void addFormatters(FormatterRegistry registry) &#123; registry.addConverter(new Converter&lt;String, Date&gt;() &#123; @Override public Pet convert(String source) &#123; DateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); Date date = null; //类型转换器无法预计使用过程中出现的异常，因此必须在类型转换器内部捕获， //不允许抛出，框架无法预计此类异常如何处理 try &#123; date = df.parse(source); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; return date; &#125; &#125;); &#125; &#125; &#125;* 方式二： ```java //本例中的泛型填写的是String，Date，最终出现字符串转日期时，该类型转换器生效 public class MyDateConverter implements Converter&lt;String, Date&gt; &#123; //重写接口的抽象方法，参数由泛型决定 public Date convert(String source) &#123; DateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); Date date = null; //类型转换器无法预计使用过程中出现的异常，因此必须在类型转换器内部捕获， //不允许抛出，框架无法预计此类异常如何处理 try &#123; date = df.parse(source); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; return date; &#125; &#125; 配置 resources &#x2F; spring-mvc.xml，注册自定义转换器，将功能加入到 SpringMVC 转换服务 ConverterService 中 1234567891011121314151617&lt;!--1.将自定义Converter注册为Bean，受SpringMVC管理--&gt;&lt;bean id=&quot;myDateConverter&quot; class=&quot;converter.MyDateConverter&quot;/&gt;&lt;!--2.设定自定义Converter服务bean--&gt;&lt;bean id=&quot;conversionService&quot; class=&quot;org.springframework.context.support.ConversionServiceFactoryBean&quot;&gt; &lt;!--3.注入所有的自定义Converter，该设定使用的是同类型覆盖的思想--&gt; &lt;property name=&quot;converters&quot;&gt; &lt;!--4.set保障同类型转换器仅保留一个，去重规则以Converter&lt;S,T&gt;的泛型为准--&gt; &lt;set&gt; &lt;!--5.具体的类型转换器--&gt; &lt;ref bean=&quot;myDateConverter&quot;/&gt; &lt;/set&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!--开启注解驱动，加载自定义格式化转换器对应的类型转换服务--&gt;&lt;mvc:annotation-driven conversion-service=&quot;conversionService&quot;/&gt; 使用转换器 12345@RequestMapping(&quot;/requestParam12&quot;)public String requestParam12(Date date)&#123; System.out.println(date); return &quot;page.jsp&quot;;&#125; 响应处理页面跳转请求转发和重定向： 请求转发： 12345678@Controllerpublic class UserController &#123; @RequestMapping(&quot;/showPage1&quot;)\tpublic String showPage1() &#123; System.out.println(&quot;user mvc controller is running ...&quot;); return &quot;forward:/WEB-INF/page/page.jsp;\t&#125;&#125; 请求重定向： 12345@RequestMapping(&quot;/showPage2&quot;)public String showPage2() &#123; System.out.println(&quot;user mvc controller is running ...&quot;); return &quot;redirect:/WEB-INF/page/page.jsp&quot;;//不能访问WEB-INF下的资源&#125; 页面访问快捷设定（InternalResourceViewResolver）： 展示页面的保存位置通常固定且结构相似，可以设定通用的访问路径简化页面配置，配置 spring-mvc.xml： 1234&lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;&lt;/bean&gt; 简化 12345678910111213141516@RequestMapping(&quot;/showPage3&quot;)public String showPage3() &#123; System.out.println(&quot;user mvc controller is running...&quot;); return &quot;page&quot;;&#125;@RequestMapping(&quot;/showPage4&quot;)public String showPage4() &#123; System.out.println(&quot;user mvc controller is running...&quot;); return &quot;forward:page&quot;;&#125;@RequestMapping(&quot;/showPage5&quot;)public String showPage5() &#123; System.out.println(&quot;user mvc controller is running...&quot;); return &quot;redirect:page&quot;;&#125; 如果未设定了返回值，使用 void 类型，则默认使用访问路径作页面地址的前缀后缀 12345//最简页面配置方式，使用访问路径作为页面名称，省略返回值@RequestMapping(&quot;/showPage6&quot;)public void showPage6() &#123; System.out.println(&quot;user mvc controller is running ...&quot;);&#125; 数据跳转ModelAndView 是 SpringMVC 提供的一个对象，该对象可以用作控制器方法的返回值（Model 同），实现携带数据跳转 作用： 设置数据，向请求域对象中存储数据 设置视图，逻辑视图 代码实现： 使用 HttpServletRequest 类型形参进行数据传递 12345678@Controllerpublic class BookController &#123; @RequestMapping(&quot;/showPageAndData1&quot;) public String showPageAndData1(HttpServletRequest request) &#123; request.setAttribute(&quot;name&quot;,&quot;seazean&quot;); return &quot;page&quot;; &#125;&#125; 使用 Model 类型形参进行数据传递 12345678910@RequestMapping(&quot;/showPageAndData2&quot;)public String showPageAndData2(Model model) &#123; model.addAttribute(&quot;name&quot;,&quot;seazean&quot;); Book book = new Book(); book.setName(&quot;SpringMVC入门实战&quot;); book.setPrice(66.6d); //添加数据的方式，key对value model.addAttribute(&quot;book&quot;,book); return &quot;page&quot;;&#125; 1234public class Book &#123; private String name; private Double price;&#125; 使用 ModelAndView 类型形参进行数据传递，将该对象作为返回值传递给调用者 123456789101112131415@RequestMapping(&quot;/showPageAndData3&quot;)public ModelAndView showPageAndData3(ModelAndView modelAndView) &#123; //ModelAndView mav = new ModelAndView(); 替换形参中的参数 Book book = new Book(); book.setName(&quot;SpringMVC入门案例&quot;); book.setPrice(66.66d); //添加数据的方式，key对value modelAndView.addObject(&quot;book&quot;,book); modelAndView.addObject(&quot;name&quot;,&quot;Jockme&quot;); //设置页面的方式，该方法最后一次执行的结果生效 modelAndView.setViewName(&quot;page&quot;); //返回值设定成ModelAndView对象 return modelAndView;&#125; ModelAndView 扩展 12345678910111213//ModelAndView对象支持转发的手工设定，该设定不会启用前缀后缀的页面拼接格式@RequestMapping(&quot;/showPageAndData4&quot;)public ModelAndView showPageAndData4(ModelAndView modelAndView) &#123; modelAndView.setViewName(&quot;forward:/WEB-INF/page/page.jsp&quot;); return modelAndView;&#125;//ModelAndView对象支持重定向的手工设定，该设定不会启用前缀后缀的页面拼接格式@RequestMapping(&quot;/showPageAndData5&quot;)public ModelAndView showPageAndData6(ModelAndView modelAndView) &#123; modelAndView.setViewName(&quot;redirect:page.jsp&quot;); return modelAndView;&#125; JSON注解：@ResponseBody 作用：将 Controller 的方法返回的对象通过适当的转换器转换为指定的格式之后，写入到 Response 的 body 区。如果返回值是字符串，那么直接将字符串返回客户端；如果是一个对象，会将对象转化为 JSON，返回客户端 注意：当方法上面没有写 ResponseBody，底层会将方法的返回值封装为 ModelAndView 对象 使用 HttpServletResponse 对象响应数据 1234567@Controllerpublic class AccountController &#123; @RequestMapping(&quot;/showData1&quot;) public void showData1(HttpServletResponse response) throws IOException &#123; response.getWriter().write(&quot;message&quot;); &#125;&#125; 使用 @ResponseBody 将返回的结果作为响应内容（页面显示），而非响应的页面名称 12345@RequestMapping(&quot;/showData2&quot;)@ResponseBodypublic String showData2()&#123; return &quot;&#123;&#x27;name&#x27;:&#x27;Jock&#x27;&#125;&quot;;&#125; 使用 jackson 进行 json 数据格式转化 导入坐标： 123456789101112131415161718&lt;!--json相关坐标3个--&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 12345678910@RequestMapping(&quot;/showData3&quot;)@ResponseBodypublic String showData3() throws JsonProcessingException &#123; Book book = new Book(); book.setName(&quot;SpringMVC入门案例&quot;); book.setPrice(66.66d); ObjectMapper om = new ObjectMapper(); return om.writeValueAsString(book);&#125; 使用 SpringMVC 提供的消息类型转换器将对象与集合数据自动转换为 JSON 数据 123456789//使用SpringMVC注解驱动，对标注@ResponseBody注解的控制器方法进行结果转换，由于返回值为引用类型，自动调用jackson提供的类型转换器进行格式转换@RequestMapping(&quot;/showData4&quot;)@ResponseBodypublic Book showData4() &#123; Book book = new Book(); book.setName(&quot;SpringMVC入门案例&quot;); book.setPrice(66.66d); return book;&#125; 手工添加信息类型转换器 123456789&lt;bean class=&quot;org.springframework.web.servlet.mvc.method. annotation.RequestMappingHandlerAdapter&quot;&gt; &lt;property name=&quot;messageConverters&quot;&gt; &lt;list&gt; &lt;bean class=&quot;org.springframework.http.converter. json.MappingJackson2HttpMessageConverter&quot;/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean 使用 SpringMVC 注解驱动： 12&lt;!--开启springmvc注解驱动，对@ResponseBody的注解进行格式增强，追加其类型转换的功能，具体实现由MappingJackson2HttpMessageConverter进行--&gt;&lt;mvc:annotation-driven/&gt; 转换集合类型数据 12345678910111213141516@RequestMapping(&quot;/showData5&quot;)@ResponseBodypublic List showData5() &#123; Book book1 = new Book(); book1.setName(&quot;SpringMVC入门案例&quot;); book1.setPrice(66.66d); Book book2 = new Book(); book2.setName(&quot;SpringMVC入门案例&quot;); book2.setPrice(66.66d); ArrayList al = new ArrayList(); al.add(book1); al.add(book2); return al;&#125; Restful基本介绍Rest（REpresentational State Transfer）：表现层状态转化，定义了资源在网络传输中以某种表现形式进行状态转移，即网络资源的访问方式 资源：把真实的对象数据称为资源，一个资源既可以是一个集合，也可以是单个个体；每一种资源都有特定的 URI（统一资源标识符）与之对应，如果获取这个资源，访问这个 URI 就可以，比如获取特定的班级 /class/12；资源也可以包含子资源，比如 /classes/classId/teachers 某个指定班级的所有老师 表现形式：资源是一种信息实体，它可以有多种外在表现形式，把资源具体呈现出来的形式比如 json、xml、image、txt 等等叫做它的”表现层&#x2F;表现形式” 状态转移：描述的服务器端资源的状态，比如增删改查（通过 HTTP 动词实现）引起资源状态的改变，互联网通信协议 HTTP 协议，是一个无状态协议，所有的资源状态都保存在服务器端 访问方式Restful 是按照 Rest 风格访问网络资源 传统风格访问路径：http://localhost/user/get?id=1 Rest 风格访问路径：http://localhost/user/1 优点：隐藏资源的访问行为，通过地址无法得知做的是何种操作，书写简化 Restful 请求路径简化配置方式：@RestController = @Controller + @ResponseBody 相关注解：@GetMapping 注解是 @RequestMapping 注解的衍生，所以效果是一样的，建议使用 @GetMapping @GetMapping(&quot;/poll&quot;) &#x3D; @RequestMapping(value = &quot;/poll&quot;,method = RequestMethod.GET) 12345@RequestMapping(method = RequestMethod.GET) // @GetMapping 就拥有了 @RequestMapping 的功能public @interface GetMapping &#123; @AliasFor(annotation = RequestMapping.class)\t// 与 RequestMapping 相通\tString name() default &quot;&quot;;&#125; @PostMapping(&quot;/push&quot;) &#x3D; @RequestMapping(value = &quot;/push&quot;,method = RequestMethod.POST) 过滤器：HiddenHttpMethodFilter 是 SpringMVC 对 Restful 风格的访问支持的过滤器 代码实现： restful.jsp： 页面表单使用隐藏域提交请求类型，参数名称固定为 _method，必须配合提交类型 method&#x3D;post 使用 GET 请求通过地址栏可以发送，也可以通过设置 form 的请求方式提交 POST 请求必须通过 form 的请求方式提交 1234567&lt;h1&gt;restful风格请求表单&lt;/h1&gt;&lt;!--切换请求路径为restful风格--&gt;&lt;form action=&quot;/user&quot; method=&quot;post&quot;&gt;\t&lt;!--一隐藏域，切换为PUT请求或DELETE请求，但是form表单的提交方式method属性必须填写post--&gt;\t&lt;input name=&quot;_method&quot; type=&quot;hidden&quot; value=&quot;PUT&quot;/&gt;\t&lt;input value=&quot;REST-PUT 提交&quot; type=&quot;submit&quot;/&gt;&lt;/form&gt; java &#x2F; controller &#x2F; UserController 123456789101112131415161718192021222324252627@RestController //设置rest风格的控制器@RequestMapping(&quot;/user/&quot;)\t//设置公共访问路径，配合下方访问路径使用public class UserController &#123; @GetMapping(&quot;/user&quot;) //@RequestMapping(value = &quot;/user&quot;,method = RequestMethod.GET) public String getUser()&#123; return &quot;GET-张三&quot;; &#125; @PostMapping(&quot;/user&quot;) //@RequestMapping(value = &quot;/user&quot;,method = RequestMethod.POST) public String saveUser()&#123; return &quot;POST-张三&quot;; &#125; @PutMapping(&quot;/user&quot;) //@RequestMapping(value = &quot;/user&quot;,method = RequestMethod.PUT) public String putUser()&#123; return &quot;PUT-张三&quot;; &#125; @DeleteMapping(&quot;/user&quot;) //@RequestMapping(value = &quot;/user&quot;,method = RequestMethod.DELETE) public String deleteUser()&#123; return &quot;DELETE-张三&quot;; &#125;&#125; 配置拦截器 web.xml 123456789&lt;!--配置拦截器，解析请求中的参数_method，否则无法发起PUT请求与DELETE请求，配合页面表单使用--&gt;&lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt;&lt;/filter-mapping&gt; 参数注解Restful 开发中的参数注解 123@GetMapping(&quot;&#123;id&#125;&quot;)public String getMessage(@PathVariable(&quot;id&quot;) Integer id)&#123;&#125; 使用 @PathVariable 注解获取路径上配置的具名变量，一般在有多个参数的时候添加 其他注解： @RequestHeader：获取请求头 @RequestParam：获取请求参数（指问号后的参数，url?a&#x3D;1&amp;b&#x3D;2） @CookieValue：获取 Cookie 值 @RequestAttribute：获取 request 域属性 @RequestBody：获取请求体 [POST] @MatrixVariable：矩阵变量 @ModelAttribute：自定义类型变量 1234567891011121314151617181920212223242526272829303132333435@RestController\t@RequestMapping(&quot;/user/&quot;)public class UserController &#123; //rest风格访问路径简化书写方式，配合类注解@RequestMapping使用 @RequestMapping(&quot;&#123;id&#125;&quot;) public String restLocation2(@PathVariable Integer id)&#123; System.out.println(&quot;restful is running ....get:&quot; + id); return &quot;success.jsp&quot;; &#125; //@RequestMapping(value = &quot;&#123;id&#125;&quot;,method = RequestMethod.GET) @GetMapping(&quot;&#123;id&#125;&quot;) public String get(@PathVariable Integer id)&#123; System.out.println(&quot;restful is running ....get:&quot; + id); return &quot;success.jsp&quot;; &#125; @PostMapping(&quot;&#123;id&#125;&quot;) public String post(@PathVariable Integer id)&#123; System.out.println(&quot;restful is running ....post:&quot; + id); return &quot;success.jsp&quot;; &#125; @PutMapping(&quot;&#123;id&#125;&quot;) public String put(@PathVariable Integer id)&#123; System.out.println(&quot;restful is running ....put:&quot; + id); return &quot;success.jsp&quot;; &#125; @DeleteMapping(&quot;&#123;id&#125;&quot;) public String delete(@PathVariable Integer id)&#123; System.out.println(&quot;restful is running ....delete:&quot; + id); return &quot;success.jsp&quot;; &#125;&#125; 识别原理表单提交要使用 REST 时，会带上 _method=PUT，请求过来被 HiddenHttpMethodFilter 拦截，进行过滤操作 org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal()： 1234567891011121314151617181920212223242526272829303132public class HiddenHttpMethodFilter extends OncePerRequestFilter &#123; // 兼容的请求 PUT、DELETE、PATCH private static final List&lt;String&gt; ALLOWED_METHODS = Collections.unmodifiableList(Arrays.asList(HttpMethod.PUT.name(), HttpMethod.DELETE.name(), HttpMethod.PATCH.name())); // 隐藏域的名字\tpublic static final String DEFAULT_METHOD_PARAM = &quot;_method&quot;;\tprivate String methodParam = DEFAULT_METHOD_PARAM; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; HttpServletRequest requestToUse = request; // 请求必须是 POST， if (&quot;POST&quot;.equals(request.getMethod()) &amp;&amp; request.getAttribute(WebUtils.ERROR_EXCEPTION_ATTRIBUTE) == null) &#123; // 获取标签中 name=&quot;_method&quot; 的 value 值 String paramValue = request.getParameter(this.methodParam); if (StringUtils.hasLength(paramValue)) &#123; // 转成大写 String method = paramValue.toUpperCase(Locale.ENGLISH); // 兼容的请求方式 if (ALLOWED_METHODS.contains(method)) &#123; // 包装请求 requestToUse = new HttpMethodRequestWrapper(request, method); &#125; &#125; &#125; // 过滤器链放行的时候用wrapper。以后的方法调用getMethod是调用requesWrapper的 filterChain.doFilter(requestToUse, response); &#125;&#125; Rest 使用客户端工具，如 Postman 可直接发送 put、delete 等方式请求不被过滤 改变默认的 _method 的方式： 1234567891011@Configuration(proxyBeanMethods = false)public class WebConfig&#123; //自定义filter @Bean public HiddenHttpMethodFilter hiddenHttpMethodFilter()&#123; HiddenHttpMethodFilter methodFilter = new HiddenHttpMethodFilter(); //通过set 方法自定义 methodFilter.setMethodParam(&quot;_m&quot;); return methodFilter; &#125; &#125; ServletSpringMVC 提供访问原始 Servlet 接口的功能 SpringMVC 提供访问原始 Servlet 接口 API 的功能，通过形参声明即可 12345678910@RequestMapping(&quot;/servletApi&quot;)public String servletApi(HttpServletRequest request, HttpServletResponse response, HttpSession session)&#123; System.out.println(request); System.out.println(response); System.out.println(session); request.setAttribute(&quot;name&quot;,&quot;seazean&quot;); System.out.println(request.getAttribute(&quot;name&quot;)); return &quot;page.jsp&quot;;&#125; Head 数据获取快捷操作方式名称：@RequestHeader类型：形参注解位置：处理器类中的方法形参前方作用：绑定请求头数据与对应处理方法形参间的关系范例： 12345快捷操作方式@RequestMapping(&quot;/headApi&quot;)public String headApi(@RequestHeader(&quot;Accept-Language&quot;) String headMsg)&#123; System.out.println(headMsg); return &quot;page&quot;;&#125; Cookie 数据获取快捷操作方式名称：@CookieValue类型：形参注解位置：处理器类中的方法形参前方作用：绑定请求 Cookie 数据与对应处理方法形参间的关系范例： 12345@RequestMapping(&quot;/cookieApi&quot;)public String cookieApi(@CookieValue(&quot;JSESSIONID&quot;) String jsessionid)&#123; System.out.println(jsessionid); return &quot;page&quot;;&#125; Session 数据获取名称：@SessionAttribute类型：形参注解位置：处理器类中的方法形参前方作用：绑定请求Session数据与对应处理方法形参间的关系范例： 1234567891011@RequestMapping(&quot;/sessionApi&quot;)public String sessionApi(@SessionAttribute(&quot;name&quot;) String name)&#123; System.out.println(name); return &quot;page.jsp&quot;;&#125;//用于在session中放入数据@RequestMapping(&quot;/setSessionData&quot;)public String setSessionData(HttpSession session)&#123; session.setAttribute(&quot;name&quot;,&quot;seazean&quot;); return &quot;page&quot;;&#125; Session 数据设置名称：@SessionAttributes类型：类注解位置：处理器类上方作用：声明放入session范围的变量名称，适用于Model类型数据传参范例： 1234567891011121314151617181920@Controller//设定当前类中名称为age和gender的变量放入session范围，不常用@SessionAttributes(names = &#123;&quot;age&quot;,&quot;gender&quot;&#125;)public class ServletController &#123;\t//将数据放入session存储范围，Model对象实现数据set，@SessionAttributes注解实现范围设定 @RequestMapping(&quot;/setSessionData2&quot;) public String setSessionDate2(Model model) &#123; model.addAttribute(&quot;age&quot;,39); model.addAttribute(&quot;gender&quot;,&quot;男&quot;); return &quot;page&quot;; &#125; @RequestMapping(&quot;/sessionApi&quot;) public String sessionApi(@SessionAttribute(&quot;age&quot;) int age, @SessionAttribute(&quot;gender&quot;) String gender)&#123; System.out.println(name); System.out.println(age); return &quot;page&quot;; &#125;&#125; spring-mvc.xml 配置 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;com.seazean&quot;/&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/page/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;mvc:annotation-driven/&gt;&lt;/beans&gt; 运行原理技术架构组件介绍核心组件： DispatcherServlet：核心控制器， 是 SpringMVC 的核心，整体流程控制的中心，所有的请求第一步都先到达这里，由其调用其它组件处理用户的请求，它就是在 web.xml 配置的核心 Servlet，有效的降低了组件间的耦合性 HandlerMapping：处理器映射器， 负责根据请求找到对应具体的 Handler 处理器，SpringMVC 中针对配置文件方式、注解方式等提供了不同的映射器来处理 Handler：处理器，其实就是 Controller，业务处理的核心类，通常由开发者编写，并且必须遵守 Controller 开发的规则，这样适配器才能正确的执行。例如实现 Controller 接口，将 Controller 注册到 IOC 容器中等 HandlAdapter：处理器适配器，根据映射器中找到的 Handler，通过 HandlerAdapter 去执行 Handler，这是适配器模式的应用 View Resolver：视图解析器， 将 Handler 中返回的逻辑视图（ModelAndView）解析为一个具体的视图（View）对象 View：视图， View 最后对页面进行渲染将结果返回给用户，SpringMVC 框架提供了很多的 View 视图类型，包括：jstlView、freemarkerView、pdfView 等 优点： 与 Spring 集成，更好的管理资源 有很多参数解析器和视图解析器，支持的数据类型丰富 将映射器、处理器、视图解析器进行解耦，分工明确 工作原理在 Spring 容器初始化时会建立所有的 URL 和 Controller 的对应关系，保存到 Map&lt;URL, Controller&gt; 中，这样 request 就能快速根据 URL 定位到 Controller： 在 Spring IOC 容器初始化完所有单例 bean 后 SpringMVC 会遍历所有的 bean，获取 Controller 中对应的 URL（这里获取 URL 的实现类有多个，用于处理不同形式配置的 Controller） 将每一个 URL 对应一个 Controller 存入 Map&lt;URL, Controller&gt; 中 注意：将 @Controller 注解换成 @Component，启动时不会报错，但是在浏览器中输入路径时会出现 404，说明 Spring 没有对所有的 bean 进行 URL 映射 一个 Request 来了： 监听端口，获得请求：Tomcat 监听 8080 端口的请求处理，根据路径调用了 web.xml 中配置的核心控制器 DispatcherServlet，DispatcherServlet#doDispatch 是核心调度方法 首先根据 URI 获取 HandlerMapping 处理器映射器，RequestMappingHandlerMapping 用来处理 @RequestMapping 注解的映射规则，其中保存了所有 handler 的映射规则，最后包装成一个拦截器链返回，拦截器链对象持有 HandlerMapping。如果没有合适的处理请求的 HandlerMapping，说明请求处理失败，设置响应码 404 返回 根据映射器获取当前 handler，处理器适配器执行处理方法，适配器根据请求的 URL 去 handler 中寻找对应的处理方法： 创建 ModelAndViewContainer (mav) 对象，用来填充数据，然后通过不同的参数解析器去解析 URL 中的参数，完成数据解析绑定，然后执行真正的 Controller 方法，完成 handle 处理 方法执行完对返回值进行处理，没添加 @ResponseBody 注解的返回值使用视图处理器处理，把视图名称设置进入 mav 中 对添加了 @ResponseBody 注解的 Controller 的按照普通的返回值进行处理，首先进行内容协商，找到一种浏览器可以接受（请求头 Accept）的并且服务器可以生成的数据类型，选择合适数据转换器，设置响应头中的数据类型，然后写出数据 最后把 ModelAndViewContainer 和 ModelMap 中的数据封装到 ModelAndView 对象返回 视图解析，根据返回值创建视图，请求转发 View 实例为 InternalResourceView，重定向 View 实例为 RedirectView。最后调用 view.render 进行页面渲染，结果派发 请求转发时请求域中的数据不丢失，会把 ModelAndView 的数据设置到请求域中，获取 Servlet 原生的 RequestDispatcher，调用 RequestDispatcher#forward 实现转发 重定向会造成请求域中的数据丢失，使用 Servlet 原生方式实现重定向 HttpServletResponse#sendRedirect 调度函数请求进入原生的 HttpServlet 的 doGet() 方法处理，调用子类 FrameworkServlet 的 doGet() 方法，最终调用 DispatcherServlet 的 doService() 方法，为请求设置相关属性后调用 doDispatch()，请求和响应的以参数的形式传入 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// request 和 response 为 Java 原生的类protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; // 文件上传请求 boolean multipartRequestParsed = false; // 异步管理器 WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; // 文件上传相关请求 processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // 找到当前请求使用哪个 HandlerMapping （Controller 的方法）处理，返回执行链 mappedHandler = getHandler(processedRequest); // 没有合适的处理请求的方式 HandlerMapping，请求失败，直接返回 404 if (mappedHandler == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // 根据映射器获取当前 handler 处理器适配器，用来【处理当前的请求】 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // 获取发出此次请求的方式 String method = request.getMethod(); // 判断请求是不是 GET 方法 boolean isGet = HttpMethod.GET.matches(method); if (isGet || HttpMethod.HEAD.matches(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; // 拦截器链的前置处理 if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // 执行处理方法，返回的是 ModelAndView 对象，封装了所有的返回值数据 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; // 设置视图名字 applyDefaultViewName(processedRequest, mv); // 执行拦截器链中的后置处理方法 mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; // 处理程序调用的结果，进行结果派发 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; //....&#125; 笔记参考视频：https://www.bilibili.com/video/BV19K4y1L7MT 请求映射映射器doDispatch() 中调用 getHandler 方法获取所有的映射器 总体流程： 所有的请求映射都在 HandlerMapping 中，RequestMappingHandlerMapping 处理 @RequestMapping 注解的映射规则 遍历所有的 HandlerMapping 看是否可以匹配当前请求，匹配成功后返回，匹配失败设置 HTTP 404 响应码 用户可以自定义的映射处理，也可以给容器中放入自定义 HandlerMapping 访问 URL：http://localhost:8080/user 123456789@GetMapping(&quot;/user&quot;)public String getUser()&#123; return &quot;GET&quot;;&#125;@PostMapping(&quot;/user&quot;)public String postUser()&#123; return &quot;POST&quot;;&#125;//。。。。。 HandlerMapping 处理器映射器，保存了所有 @RequestMapping 和 handler 的映射规则 12345678910111213protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123; if (this.handlerMappings != null) &#123; // 遍历所有的 HandlerMapping for (HandlerMapping mapping : this.handlerMappings) &#123; // 尝试去每个 HandlerMapping 中匹配当前请求的处理 HandlerExecutionChain handler = mapping.getHandler(request); if (handler != null) &#123; return handler; &#125; &#125; &#125; return null;&#125; mapping.getHandler(request)：调用 AbstractHandlerMapping#getHandler Object handler = getHandlerInternal(request)：获取映射器，底层调用 RequestMappingInfoHandlerMapping 类的方法，又调用 AbstractHandlerMethodMapping#getHandlerInternal String lookupPath = initLookupPath(request)：地址栏的 URI，这里的 lookupPath 为 &#x2F;user this.mappingRegistry.acquireReadLock()：加读锁防止其他线程并发修改 handlerMethod = lookupHandlerMethod(lookupPath, request)：获取当前 HandlerMapping 中的映射规则 directPathMatches = this.mappingRegistry.getMappingsByDirectPath(lookupPath)：获取当前的映射器与当前请求的 URI 有关的所有映射规则 addMatchingMappings(directPathMatches, matches, request)：匹配某个映射规则 for (T mapping : mappings)：遍历所有的映射规则 match = getMatchingMapping(mapping, request)：去匹配每一个映射规则，匹配失败返回 null matches.add(new Match())：匹配成功后封装成匹配器添加到匹配集合中 matches.sort(comparator)：匹配集合排序 Match bestMatch = matches.get(0)：匹配完成只剩一个，直接获取返回对应的处理方法 if (matches.size() &gt; 1)：当有多个映射规则符合请求时，报错 return bestMatch.getHandlerMethod()：返回匹配器中的处理方法 executionChain = getHandlerExecutionChain(handler, request)：为当前请求和映射器的构建一个拦截器链 for (HandlerInterceptor interceptor : this.adaptedInterceptors)：遍历所有的拦截器 chain.addInterceptor(interceptor)：把所有的拦截器添加到 HandlerExecutionChain 中，形成拦截器链 return executionChain：返回拦截器链，HandlerMapping 是链的 handler 成员属性 适配器doDispatch() 中调用 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()) 1234567891011121314protected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException &#123; if (this.handlerAdapters != null) &#123; // 遍历所有的 HandlerAdapter for (HandlerAdapter adapter : this.handlerAdapters) &#123; // 判断当前适配器是否支持当前 handle if (adapter.supports(handler)) &#123; // 返回的是 【RequestMappingHandlerAdapter】 // AbstractHandlerMethodAdapter#supports -&gt; RequestMappingHandlerAdapter return adapter; &#125; &#125; &#125; throw new ServletException();&#125; 方法执行实例代码： 1234567@GetMapping(&quot;/params&quot;)public String param(Map&lt;String, Object&gt; map, Model model, HttpServletRequest request) &#123; map.put(&quot;k1&quot;, &quot;v1&quot;); // 都可以向请求域中添加数据 model.addAttribute(&quot;k2&quot;, &quot;v2&quot;);\t// 它们两个都在数据封装在 【BindingAwareModelMap】，继承自 LinkedHashMap request.setAttribute(&quot;m&quot;, &quot;HelloWorld&quot;); return &quot;forward:/success&quot;;&#125; doDispatch() 中调用 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()) 使用适配器执行方法 AbstractHandlerMethodAdapter#handle → RequestMappingHandlerAdapter#handleInternal → invokeHandlerMethod： 1234567891011121314151617181920212223242526272829303132333435363738394041424344protected ModelAndView invokeHandlerMethod(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception &#123;\t// 封装成 SpringMVC 的接口，用于通用 Web 请求拦截器，使能够访问通用请求元数据，而不是用于实际处理请求 ServletWebRequest webRequest = new ServletWebRequest(request, response); try &#123; // WebDataBinder 用于【从 Web 请求参数到 JavaBean 对象的数据绑定】，获取创建该实例的工厂 WebDataBinderFactory binderFactory = getDataBinderFactory(handlerMethod); // 创建 Model 实例，用于向模型添加属性 ModelFactory modelFactory = getModelFactory(handlerMethod, binderFactory); // 方法执行器 ServletInvocableHandlerMethod invocableMethod = createInvocableHandlerMethod(handlerMethod); // 参数解析器，有很多 if (this.argumentResolvers != null) &#123; invocableMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers); &#125; // 返回值处理器，也有很多 if (this.returnValueHandlers != null) &#123; invocableMethod.setHandlerMethodReturnValueHandlers(this.returnValueHandlers); &#125; // 设置数据绑定器 invocableMethod.setDataBinderFactory(binderFactory); // 设置参数检查器 invocableMethod.setParameterNameDiscoverer(this.parameterNameDiscoverer); // 新建一个 ModelAndViewContainer 并进行初始化和一些属性的填充 ModelAndViewContainer mavContainer = new ModelAndViewContainer(); // 设置一些属性 // 【执行目标方法】 invocableMethod.invokeAndHandle(webRequest, mavContainer); // 异步请求 if (asyncManager.isConcurrentHandlingStarted()) &#123; return null; &#125; // 【获取 ModelAndView 对象，封装了 ModelAndViewContainer】 return getModelAndView(mavContainer, modelFactory, webRequest); &#125; finally &#123; webRequest.requestCompleted(); &#125;&#125; ServletInvocableHandlerMethod#invokeAndHandle：执行目标方法 returnValue = invokeForRequest(webRequest, mavContainer, providedArgs)：执行自己写的 controller 方法，返回的就是自定义方法中 return 的值 Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs)：参数处理的逻辑，遍历所有的参数解析器解析参数或者将 URI 中的参数进行绑定，绑定完成后开始执行目标方法 parameters = getMethodParameters()：获取此处理程序方法的方法参数的详细信息 Object[] args = new Object[parameters.length]：存放所有的参数 for (int i = 0; i &lt; parameters.length; i++)：遍历所有的参数 args[i] = findProvidedArgument(parameter, providedArgs)：获取调用方法时提供的参数，一般是空 if (!this.resolvers.supportsParameter(parameter))：获取可以解析当前参数的参数解析器 return getArgumentResolver(parameter) != null：获取参数的解析是否为空 for (HandlerMethodArgumentResolver resolver : this.argumentResolvers)：遍历容器内所有的解析器 if (resolver.supportsParameter(parameter))：是否支持当前参数 PathVariableMethodArgumentResolver#supportsParameter：解析标注 @PathVariable 注解的参数 ModelMethodProcessor#supportsParameter：解析 Map 和 Model 类型的参数，Model 和 Map 的作用一样 ExpressionValueMethodArgumentResolver#supportsParameter：解析标注 @Value 注解的参数 RequestParamMapMethodArgumentResolver#supportsParameter：解析标注 @RequestParam 注解 RequestPartMethodArgumentResolver#supportsParameter：解析文件上传的信息 ModelAttributeMethodProcessor#supportsParameter：解析标注 @ModelAttribute 注解或者不是简单类型 子类 ServletModelAttributeMethodProcessor 是解析自定义类型 JavaBean 的解析器 简单类型有 Void、Enum、Number、CharSequence、Date、URI、URL、Locale、Class args[i] = this.resolvers.resolveArgument()：开始解析参数，每个参数使用的解析器不同 resolver = getArgumentResolver(parameter)：获取参数解析器 return resolver.resolveArgument()：开始解析 PathVariableMapMethodArgumentResolver#resolveArgument：@PathVariable，包装 URI 中的参数为 Map MapMethodProcessor#resolveArgument：调用 mavContainer.getModel() 返回默认 BindingAwareModelMap 对象 ModelAttributeMethodProcessor#resolveArgument：自定义的 JavaBean 的绑定封装，下一小节详解 return doInvoke(args)：真正的执行 Controller 方法 Method method = getBridgedMethod()：从 HandlerMethod 获取要反射执行的方法 ReflectionUtils.makeAccessible(method)：破解权限 method.invoke(getBean(), args)：执行方法，getBean 获取的是标记 @Controller 的 Bean 类，其中包含执行方法 进行返回值的处理，响应部分详解，处理完成进入下面的逻辑 RequestMappingHandlerAdapter#getModelAndView：获取 ModelAndView 对象 modelFactory.updateModel(webRequest, mavContainer)：Model 数据升级到会话域（请求域中的数据在重定向时丢失） updateBindingResult(request, defaultModel)：把绑定的数据添加到 BindingAwareModelMap 中 if (mavContainer.isRequestHandled())：判断请求是否已经处理完成了 ModelMap model = mavContainer.getModel()：获取包含 Controller 方法参数的 BindingAwareModelMap（本节开头） mav = new ModelAndView()：把 ModelAndViewContainer 和 ModelMap 中的数据封装到 ModelAndView if (!mavContainer.isViewReference())：是否是通过名称指定视图引用 if (model instanceof RedirectAttributes)：判断 model 是否是重定向数据，如果是进行重定向逻辑 return mav：任何方法执行都会返回 ModelAndView 对象 参数解析解析自定义的 JavaBean 为例，调用 ModelAttributeMethodProcessor#resolveArgument 处理参数的方法，通过合适的类型转换器把 URL 中的参数转换以后，利用反射获取 set 方法，注入到 JavaBean Person.java： 1234567@Data@Component\t//加入到容器中public class Person &#123; private String userName; private Integer age; private Date birth;&#125; Controller： 12345678@RestController\t//返回的数据不是页面public class ParameterController &#123; // 数据绑定：页面提交的请求数据（GET、POST）都可以和对象属性进行绑定 @GetMapping(&quot;/saveuser&quot;) public Person saveuser(Person person)&#123; return person; &#125;&#125; 访问 URL：http://localhost:8080/saveuser?userName=zhangsan&amp;age=20 进入源码：ModelAttributeMethodProcessor#resolveArgument name = ModelFactory.getNameForParameter(parameter)：获取名字，此例就是 person ann = parameter.getParameterAnnotation(ModelAttribute.class)：是否有 ModelAttribute 注解 if (mavContainer.containsAttribute(name))：ModelAndViewContainer 中是否包含 person 对象 attribute = createAttribute()：创建一个实例，空的 Person 对象 binder = binderFactory.createBinder(webRequest, attribute, name)：Web 数据绑定器，可以利用 Converters 将请求数据转成指定的数据类型，绑定到 JavaBean 中 bindRequestParameters(binder, webRequest)：利用反射向目标对象填充数据 servletBinder = (ServletRequestDataBinder) binder：类型强转 servletBinder.bind(servletRequest)：绑定数据 mpvs = new MutablePropertyValues(request.getParameterMap())：获取请求 URI 参数中的 k-v 键值对 addBindValues(mpvs, request)：子类可以用来为请求添加额外绑定值 doBind(mpvs)：真正的绑定的方法，调用 applyPropertyValues 应用参数值，然后调用 setPropertyValues 方法 AbstractPropertyAccessor#setPropertyValues()： List&lt;PropertyValue&gt; propertyValues：获取到所有的参数的值，就是 URI 上的所有的参数值 for (PropertyValue pv : propertyValues)：遍历所有的参数值 setPropertyValue(pv)：填充到空的 Person 实例中 nestedPa = getPropertyAccessorForPropertyPath(propertyName)：获取属性访问器 tokens = getPropertyNameTokens()：获取元数据的信息 nestedPa.setPropertyValue(tokens, pv)：填充数据 processLocalProperty(tokens, pv)：处理属性 if (!Boolean.FALSE.equals(pv.conversionNecessary))：数据是否需要转换了 if (pv.isConverted())：数据已经转换过了，转换了直接赋值，没转换进行转换 oldValue = ph.getValue()：获取未转换的数据 valueToApply = convertForProperty()：进行数据转换 TypeConverterDelegate#convertIfNecessary：进入该方法的逻辑 if (conversionService.canConvert(sourceTypeDesc, typeDescriptor))：判断能不能转换 GenericConverter converter = getConverter(sourceType, targetType)：获取类型转换器 converter = this.converters.find(sourceType, targetType)：寻找合适的转换器 sourceCandidates = getClassHierarchy(sourceType.getType())：原数据类型 targetCandidates = getClassHierarchy(targetType.getType())：目标数据类型 123for (Class&lt;?&gt; sourceCandidate : sourceCandidates) &#123; //双重循环遍历，寻找合适的转换器 for (Class&lt;?&gt; targetCandidate : targetCandidates) &#123; GenericConverter converter = getRegisteredConverter(..)：匹配类型转换器 return converter：返回转换器 conversionService.convert(newValue, sourceTypeDesc, typeDescriptor)：开始转换 converter = getConverter(sourceType, targetType)：获取可用的转换器 result = ConversionUtils.invokeConverter()：执行转换方法 converter.convert()：调用转换器的转换方法（GenericConverter#convert） return handleResult(sourceType, targetType, result)：返回结果 ph.setValue(valueToApply)：设置 JavaBean 属性（BeanWrapperImpl.BeanPropertyHandler） Method writeMethod：获取写数据方法 Class&lt;?&gt; cls = getClass0()：获取 Class 对象 writeMethodName = Introspector.SET_PREFIX + getBaseName()：set 前缀 + 属性名 writeMethod = Introspector.findMethod(cls, writeMethodName, 1, args)：获取只包含一个参数的 set 方法 setWriteMethod(writeMethod)：加入缓存 ReflectionUtils.makeAccessible(writeMethod)：设置访问权限 writeMethod.invoke(getWrappedInstance(), value)：执行方法 bindingResult = binder.getBindingResult()：获取绑定的结果 mavContainer.addAllAttributes(bindingResultModel)：把所有填充的参数放入 ModelAndViewContainer return attribute：返回填充后的 Person 对象 响应处理响应数据以 Person 为例： 123456789@ResponseBody // 利用返回值处理器里面的消息转换器进行处理，而不是视图@GetMapping(value = &quot;/person&quot;)public Person getPerson()&#123; Person person = new Person(); person.setAge(28); person.setBirth(new Date()); person.setUserName(&quot;zhangsan&quot;); return person;&#125; 直接进入方法执行完后的逻辑 ServletInvocableHandlerMethod#invokeAndHandle： 1234567891011121314151617181920212223242526272829public void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123;\t// 【执行目标方法】，return person 对象 Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); // 设置状态码 setResponseStatus(webRequest); // 判断方法是否有返回值 if (returnValue == null) &#123; if (isRequestNotModified(webRequest) || getResponseStatus() != null || mavContainer.isRequestHandled()) &#123; disableContentCachingIfNecessary(webRequest); mavContainer.setRequestHandled(true); return; &#125; &#125;\t// 返回值是字符串 else if (StringUtils.hasText(getResponseStatusReason())) &#123; // 设置请求处理完成 mavContainer.setRequestHandled(true); return;\t// 设置请求没有处理完成，还需要进行返回值的逻辑 mavContainer.setRequestHandled(false); Assert.state(this.returnValueHandlers != null, &quot;No return value handlers&quot;); try &#123; // 【返回值的处理】 this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); &#125; catch (Exception ex) &#123;&#125;&#125; 没有加 @ResponseBody 注解的返回数据按照视图处理的逻辑，ViewNameMethodReturnValueHandler（视图详解） 此例是加了注解的，返回的数据不是视图，HandlerMethodReturnValueHandlerComposite#handleReturnValue： 12345678910public void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) &#123;\t// 获取合适的返回值处理器 HandlerMethodReturnValueHandler handler = selectHandler(returnValue, returnType); if (handler == null) &#123; throw new IllegalArgumentException(); &#125; // 使用处理器处理返回值（详解源码中的这两个函数） handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest);&#125; HandlerMethodReturnValueHandlerComposite#selectHandler：获取合适的返回值处理器 boolean isAsyncValue = isAsyncReturnValue(value, returnType)：是否是异步请求 for (HandlerMethodReturnValueHandler handler : this.returnValueHandlers)：遍历所有的返回值处理器 RequestResponseBodyMethodProcessor#supportsReturnType：处理标注 @ResponseBody 注解的返回值 ModelAndViewMethodReturnValueHandler#supportsReturnType：处理返回值类型是 ModelAndView 的处理器 ModelAndViewResolverMethodReturnValueHandler#supportsReturnType：直接返回 true，处理所有数据 RequestResponseBodyMethodProcessor#handleReturnValue：处理返回值，要进行内容协商 mavContainer.setRequestHandled(true)：设置请求处理完成 inputMessage = createInputMessage(webRequest)：获取输入的数据 outputMessage = createOutputMessage(webRequest)：获取输出的数据 writeWithMessageConverters(returnValue, returnType, inputMessage, outputMessage)：使用消息转换器进行写出 if (value instanceof CharSequence)：判断返回的数据是不是字符类型 body = value：把 value 赋值给 body，此时 body 中就是自定义方法执行完后的 Person 对象 if (isResourceType(value, returnType))：当前数据是不是流数据 MediaType selectedMediaType：内容协商后选择使用的类型，浏览器和服务器都支持的媒体（数据）类型 MediaType contentType = outputMessage.getHeaders().getContentType()：获取响应头的数据 if (contentType != null &amp;&amp; contentType.isConcrete())：判断当前响应头中是否已经有确定的媒体类型 selectedMediaType = contentType：前置处理已经使用了媒体类型，直接继续使用该类型 acceptableTypes = getAcceptableMediaTypes(request)：获取浏览器支持的媒体类型，请求头字段 this.contentNegotiationManager.resolveMediaTypes()：调用该方法 for(ContentNegotiationStrategy strategy:this.strategies)：默认策略是提取请求头的字段的内容，策略类为HeaderContentNegotiationStrategy，可以配置添加其他类型的策略 List&lt;MediaType&gt; mediaTypes = strategy.resolveMediaTypes(request)：解析 Accept 字段存储为 List headerValueArray = request.getHeaderValues(HttpHeaders.ACCEPT)：获取请求头中 Accept 字段 List&lt;MediaType&gt; mediaTypes = MediaType.parseMediaTypes(headerValues)：解析成 List 集合 MediaType.sortBySpecificityAndQuality(mediaTypes)：按照相对品质因数 q 降序排序 producibleTypes = getProducibleMediaTypes(request, valueType, targetType)：服务器能生成的媒体类型 request.getAttribute(HandlerMapping.PRODUCIBLE_MEDIA_TYPES_ATTRIBUTE)：从请求域获取默认的媒体类型 for (HttpMessageConverter&lt;?&gt; converter : this.messageConverters)：遍历所有的消息转换器 converter.canWrite(valueClass, null)：是否支持当前的类型 result.addAll(converter.getSupportedMediaTypes())：把当前 MessageConverter 支持的所有类型放入 result List&lt;MediaType&gt; mediaTypesToUse = new ArrayList&lt;&gt;()：存储最佳匹配的集合 内容协商： 12345678for (MediaType requestedType : acceptableTypes) &#123; // 遍历所有浏览器能接受的媒体类型 for (MediaType producibleType : producibleTypes) &#123; // 遍历所有服务器能产出的 if (requestedType.isCompatibleWith(producibleType)) &#123;\t// 判断类型是否匹配，最佳匹配 // 数据协商匹配成功，一般有多种 mediaTypesToUse.add(getMostSpecificMediaType(requestedType, producibleType)); &#125; &#125;&#125; MediaType.sortBySpecificityAndQuality(mediaTypesToUse)：按照相对品质因数 q 排序，降序排序，越大的越好 for (MediaType mediaType : mediaTypesToUse)：遍历所有的最佳匹配，选择一种赋值给选择的类型 selectedMediaType = selectedMediaType.removeQualityValue()：媒体类型去除相对品质因数 for (HttpMessageConverter&lt;?&gt; converter : this.messageConverters)：遍历所有的 HTTP 数据转换器 GenericHttpMessageConverter genericConverter：MappingJackson2HttpMessageConverter 可以将对象写为 JSON ((GenericHttpMessageConverter) converter).canWrite()：判断转换器是否可以写出给定的类型 AbstractJackson2HttpMessageConverter#canWrit if (!canWrite(mediaType))：是否可以写出指定类型 MediaType.ALL.equalsTypeAndSubtype(mediaType)：是不是 */* 类型 getSupportedMediaTypes()：支持 application/json 和 application/*+json 两种类型 return true：返回 true objectMapper = selectObjectMapper(clazz, mediaType)：选择可以使用的 objectMapper causeRef = new AtomicReference&lt;&gt;()：获取并发安全的引用 if (objectMapper.canSerialize(clazz, causeRef))：objectMapper 可以序列化当前类 return true：返回 true body = getAdvice().beforeBodyWrite()：获取要响应的所有数据，就是 Person 对象 addContentDispositionHeader(inputMessage, outputMessage)：检查路径 genericConverter.write(body, targetType, selectedMediaType, outputMessage)：调用消息转换器的 write 方法 AbstractGenericHttpMessageConverter#write：该类的方法 addDefaultHeaders(headers, t, contentType)：设置响应头中的数据类型 writeInternal(t, type, outputMessage)：数据写出为 JSON 格式 Object value = object：value 引用 Person 对象 ObjectWriter objectWriter = objectMapper.writer()：获取 ObjectWriter 对象 objectWriter.writeValue(generator, value)：使用 ObjectWriter 写出数据为 JSON 协商策略开启基于请求参数的内容协商模式：（SpringBoot 方式） 1spring.mvc.contentnegotiation:favor-parameter: true # 开启请求参数内容协商模式 发请求： http://localhost:8080/person?format=json，解析 format 策略类为 ParameterContentNegotiationStrategy，运行流程如下： acceptableTypes = getAcceptableMediaTypes(request)：获取浏览器支持的媒体类型 mediaTypes = strategy.resolveMediaTypes(request)：解析请求 URL 参数中的数据 return resolveMediaTypeKey(webRequest, getMediaTypeKey(webRequest))： getMediaTypeKey(webRequest)： request.getParameter(getParameterName())：获取 URL 中指定的需求的数据类型 getParameterName()：获取参数的属性名 format getParameter()：获取 URL 中 format 对应的数据 resolveMediaTypeKey()：解析媒体类型，封装成集合 自定义内容协商策略： 123456789101112131415161718192021222324252627public class WebConfig implements WebMvcConfigurer &#123; @Bean public WebMvcConfigurer webMvcConfigurer() &#123; return new WebMvcConfigurer() &#123; @Override\t//自定义内容协商策略 public void configureContentNegotiation(ContentNegotiationConfigurer configurer) &#123; Map&lt;String, MediaType&gt; mediaTypes = new HashMap&lt;&gt;(); mediaTypes.put(&quot;json&quot;, MediaType.APPLICATION_JSON); mediaTypes.put(&quot;xml&quot;,MediaType.APPLICATION_XML); mediaTypes.put(&quot;person&quot;,MediaType.parseMediaType(&quot;application/x-person&quot;)); // 指定支持解析哪些参数对应的哪些媒体类型 ParameterContentNegotiationStrategy parameterStrategy = new ParameterContentNegotiationStrategy(mediaTypes); // 请求头解析 HeaderContentNegotiationStrategy headStrategy = new HeaderContentNegotiationStrategy(); // 添加到容器中，即可以解析请求头 又可以解析请求参数 configurer.strategies(Arrays.asList(parameterStrategy,headStrategy)); &#125; @Override // 自定义消息转换器 public void extendMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; converters.add(new GuiguMessageConverter()); &#125; &#125; &#125;&#125; 也可以自定义 HttpMessageConverter，实现 HttpMessageConverter 接口重写方法即可 视图解析返回解析请求处理： 12345@GetMapping(&quot;/params&quot;)public String param()&#123;\treturn &quot;forward:/success&quot;; //return &quot;redirect:/success&quot;;&#125; 进入执行方法逻辑 ServletInvocableHandlerMethod#invokeAndHandle，进入 this.returnValueHandlers.handleReturnValue： 12345678910public void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) &#123;\t// 获取合适的返回值处理器：调用 if (handler.supportsReturnType(returnType))判断是否支持 HandlerMethodReturnValueHandler handler = selectHandler(returnValue, returnType); if (handler == null) &#123; throw new IllegalArgumentException(); &#125; // 使用处理器处理返回值 handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest);&#125; ViewNameMethodReturnValueHandler#supportsReturnType： 12345public boolean supportsReturnType(MethodParameter returnType) &#123; Class&lt;?&gt; paramType = returnType.getParameterType(); // 返回值是否是 void 或者是 CharSequence 字符序列，这里是字符序列 return (void.class == paramType || CharSequence.class.isAssignableFrom(paramType));&#125; ViewNameMethodReturnValueHandler#handleReturnValue： 12345678910111213141516171819public void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception &#123;\t// 返回值是字符串，是 return &quot;forward:/success&quot; if (returnValue instanceof CharSequence) &#123; String viewName = returnValue.toString(); // 【把视图名称设置进入 ModelAndViewContainer 中】 mavContainer.setViewName(viewName); // 判断是否是重定向数据 `viewName.startsWith(&quot;redirect:&quot;)` if (isRedirectViewName(viewName)) &#123; // 如果是重定向，设置是重定向指令 mavContainer.setRedirectModelScenario(true); &#125; &#125; else if (returnValue != null) &#123; // should not happen throw new UnsupportedOperationException(); &#125;&#125; 结果派发doDispatch() 中的 processDispatchResult：处理派发结果 1234567891011121314151617private void processDispatchResult(HttpServletRequest request, HttpServletResponse response, @Nullable HandlerExecutionChain mappedHandler, @Nullable ModelAndView mv, @Nullable Exception exception) throws Exception &#123; boolean errorView = false; if (exception != null) &#123; &#125; // mv 是 ModelAndValue if (mv != null &amp;&amp; !mv.wasCleared()) &#123; // 渲染视图 render(mv, request, response); if (errorView) &#123; WebUtils.clearErrorRequestAttributes(request); &#125; &#125; else &#123;&#125; &#125; DispatcherServlet#render： Locale locale = this.localeResolver.resolveLocale(request)：国际化相关 String viewName = mv.getViewName()：视图名字，是请求转发 forward:&#x2F;success（响应数据解析并存入 ModelAndView） view = resolveViewName(viewName, mv.getModelInternal(), locale, request)：解析视图 for (ViewResolver viewResolver : this.viewResolvers)：遍历所有的视图解析器 view = viewResolver.resolveViewName(viewName, locale)：根据视图名字解析视图，调用内容协商视图处理器 ContentNegotiatingViewResolver 的方法 attrs = RequestContextHolder.getRequestAttributes()：获取请求的相关属性信息 requestedMediaTypes = getMediaTypes(((ServletRequestAttributes) attrs).getRequest())：获取最佳匹配的媒体类型，函数内进行了匹配的逻辑 candidateViews = getCandidateViews(viewName, locale, requestedMediaTypes)：获取候选的视图对象 for (ViewResolver viewResolver : this.viewResolvers)：遍历所有的视图解析器 View view = viewResolver.resolveViewName(viewName, locale)：解析视图 AbstractCachingViewResolver#resolveViewName： returnview = createView(viewName, locale)：UrlBasedViewResolver#createView 请求转发：实例为 InternalResourceView if (viewName.startsWith(FORWARD_URL_PREFIX))：视图名字是否是 forward: 的前缀 forwardUrl = viewName.substring(FORWARD_URL_PREFIX.length())：名字截取前缀 view = new InternalResourceView(forwardUrl)：新建 InternalResourceView 对象并返回 return applyLifecycleMethods(FORWARD_URL_PREFIX, view)：Spring 中的初始化操作 重定向：实例为 RedirectView if (viewName.startsWith(REDIRECT_URL_PREFIX))：视图名字是否是 redirect: 的前缀 redirectUrl = viewName.substring(REDIRECT_URL_PREFIX.length())：名字截取前缀 RedirectView view = new RedirectView()：新建 RedirectView 对象并返回 bestView = getBestView(candidateViews, requestedMediaTypes, attrs)：选出最佳匹配的视图对象 view.render(mv.getModelInternal(), request, response)：页面渲染 mergedModel = createMergedOutputModel(model, request, response)：把请求域中的数据封装到 model prepareResponse(request, response)：响应前的准备工作，设置一些响应头 renderMergedOutputModel(mergedModel, getRequestToExpose(request), response)：渲染输出的数据 getRequestToExpose(request)：获取 Servlet 原生的方式 请求转发 InternalResourceView 的逻辑：请求域中的数据不丢失 exposeModelAsRequestAttributes(model, request)：暴露 model 作为请求域的属性 model.forEach()：遍历 Model 中的数据 request.setAttribute(name, value)：设置到请求域中 exposeHelpers(request)：自定义接口 dispatcherPath = prepareForRendering(request, response)：确定调度分派的路径，此例是 &#x2F;success rd = getRequestDispatcher(request, dispatcherPath)：获取 Servlet 原生的 RequestDispatcher 实现转发 rd.forward(request, response)：实现请求转发 重定向 RedirectView 的逻辑：请求域中的数据会丢失 targetUrl = createTargetUrl(model, request)：获取目标 URL enc = request.getCharacterEncoding()：设置编码 UTF-8 appendQueryProperties(targetUrl, model, enc)：添加一些属性，比如 url + ?name=123&amp;&amp;age=324 sendRedirect(request, response, targetUrl, this.http10Compatible)：重定向 response.sendRedirect(encodedURL)：使用 Servlet 原生方法实现重定向 异步调用请求参数名称：@RequestBody 类型：形参注解 位置：处理器类中的方法形参前方 作用：将异步提交数据转换成标准请求参数格式，并赋值给形参范例： 12345678@Controller //控制层public class AjaxController &#123; @RequestMapping(&quot;/ajaxController&quot;) public String ajaxController(@RequestBody String message)&#123; System.out.println(message); return &quot;page.jsp&quot;; &#125; &#125; 注解添加到 POJO 参数前方时，封装的异步提交数据按照 POJO 的属性格式进行关系映射 POJO 中的属性如果请求数据中没有，属性值为 null POJO 中没有的属性如果请求数据中有，不进行映射 注解添加到集合参数前方时，封装的异步提交数据按照集合的存储结构进行关系映射 12345678910111213@RequestMapping(&quot;/ajaxPojoToController&quot;)//如果处理参数是POJO，且页面发送的请求数据格式与POJO中的属性对应，@RequestBody注解可以自动映射对应请求数据到POJO中public String ajaxPojoToController(@RequestBody User user)&#123; System.out.println(&quot;controller pojo :&quot;+user); return &quot;page.jsp&quot;;&#125;@RequestMapping(&quot;/ajaxListToController&quot;)//如果处理参数是List集合且封装了POJO，且页面发送的数据是JSON格式，数据将自动映射到集合参数public String ajaxListToController(@RequestBody List&lt;User&gt; userList)&#123; System.out.println(&quot;controller list :&quot;+userList); return &quot;page.jsp&quot;;&#125; ajax.jsp 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;%@page pageEncoding=&quot;UTF-8&quot; language=&quot;java&quot; contentType=&quot;text/html;UTF-8&quot; %&gt;&lt;a href=&quot;javascript:void(0);&quot; id=&quot;testAjax&quot;&gt;访问springmvc后台controller&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;javascript:void(0);&quot; id=&quot;testAjaxPojo&quot;&gt;传递Json格式POJO&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;javascript:void(0);&quot; id=&quot;testAjaxList&quot;&gt;传递Json格式List&lt;/a&gt;&lt;br/&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;$&#123;pageContext.request.contextPath&#125;/js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; //为id=&quot;testAjax&quot;的组件绑定点击事件 $(&quot;#testAjax&quot;).click(function()&#123; //发送异步调用 $.ajax(&#123; //请求方式：POST请求 type:&quot;POST&quot;, //请求的地址 url:&quot;ajaxController&quot;, //请求参数（也就是请求内容） data:&#x27;ajax message&#x27;, //响应正文类型 dataType:&quot;text&quot;, //请求正文的MIME类型 contentType:&quot;application/text&quot;, &#125;); &#125;); //为id=&quot;testAjaxPojo&quot;的组件绑定点击事件 $(&quot;#testAjaxPojo&quot;).click(function()&#123; $.ajax(&#123; type:&quot;POST&quot;, url:&quot;ajaxPojoToController&quot;, data:&#x27;&#123;&quot;name&quot;:&quot;Jock&quot;,&quot;age&quot;:39&#125;&#x27;, dataType:&quot;text&quot;, contentType:&quot;application/json&quot;, &#125;); &#125;); //为id=&quot;testAjaxList&quot;的组件绑定点击事件 $(&quot;#testAjaxList&quot;).click(function()&#123; $.ajax(&#123;//..... data:&#x27;[&#123;&quot;name&quot;:&quot;Jock&quot;,&quot;age&quot;:39&#125;,&#123;&quot;name&quot;:&quot;Jockme&quot;,&quot;age&quot;:40&#125;]&#x27;&#125;)&#125; &#125;&lt;/script&gt; web.xml配置：请求响应章节请求中的web.xml配置 1CharacterEncodingFilter + DispatcherServlet spring-mvc.xml： 123&lt;context:component-scan base-package=&quot;controller,domain&quot;/&gt;&lt;mvc:resources mapping=&quot;/js/**&quot; location=&quot;/js/&quot;/&gt;&lt;mvc:annotation-driven/&gt; 响应数据注解：@ResponseBody 作用：将 Java 对象转为 json 格式的数据 方法返回值为 POJO 时，自动封装数据成 Json 对象数据： 1234567@RequestMapping(&quot;/ajaxReturnJson&quot;)@ResponseBodypublic User ajaxReturnJson()&#123; System.out.println(&quot;controller return json pojo...&quot;); User user = new User(&quot;Jockme&quot;,40); return user;&#125; 方法返回值为 List 时，自动封装数据成 json 对象数组数据： 12345678910111213@RequestMapping(&quot;/ajaxReturnJsonList&quot;)@ResponseBody//基于jackon技术，使用@ResponseBody注解可以将返回的保存POJO对象的集合转成json数组格式数据public List ajaxReturnJsonList()&#123; System.out.println(&quot;controller return json list...&quot;); User user1 = new User(&quot;Tom&quot;,3); User user2 = new User(&quot;Jerry&quot;,5); ArrayList al = new ArrayList(); al.add(user1); al.add(user2); return al;&#125; AJAX 文件： 12345678910111213141516171819202122232425262728293031323334353637//为id=&quot;testAjaxReturnString&quot;的组件绑定点击事件$(&quot;#testAjaxReturnString&quot;).click(function()&#123; //发送异步调用 $.ajax(&#123; type:&quot;POST&quot;, url:&quot;ajaxReturnString&quot;, //回调函数 success:function(data)&#123; //打印返回结果 alert(data); &#125; &#125;);&#125;);//为id=&quot;testAjaxReturnJson&quot;的组件绑定点击事件$(&quot;#testAjaxReturnJson&quot;).click(function()&#123; $.ajax(&#123; type:&quot;POST&quot;, url:&quot;ajaxReturnJson&quot;, success:function(data)&#123; alert(data[&#x27;name&#x27;]+&quot; , &quot;+data[&#x27;age&#x27;]); &#125; &#125;);&#125;);//为id=&quot;testAjaxReturnJsonList&quot;的组件绑定点击事件$(&quot;#testAjaxReturnJsonList&quot;).click(function()&#123; $.ajax(&#123; type:&quot;POST&quot;, url:&quot;ajaxReturnJsonList&quot;, success:function(data)&#123; alert(data); alert(data[0][&quot;name&quot;]); alert(data[1][&quot;age&quot;]); &#125; &#125;);&#125;); 跨域访问跨域访问：当通过域名 A 下的操作访问域名 B 下的资源时，称为跨域访问，跨域访问时，会出现无法访问的现象 环境搭建： 为当前主机添加备用域名 修改 windows 安装目录中的 host 文件 格式： ip 域名 动态刷新 DNS 命令： ipconfig &#x2F;displaydns 命令： ipconfig &#x2F;flushdns 跨域访问支持： 名称：@CrossOrigin 类型：方法注解 、 类注解 位置：处理器类中的方法上方或类上方 作用：设置当前处理器方法 &#x2F; 处理器类中所有方法支持跨域访问 范例： 1234567891011@RequestMapping(&quot;/cross&quot;)@ResponseBody//使用@CrossOrigin开启跨域访问//标注在处理器方法上方表示该方法支持跨域访问//标注在处理器类上方表示该处理器类中的所有处理器方法均支持跨域访问@CrossOriginpublic User cross(HttpServletRequest request)&#123; System.out.println(&quot;controller cross...&quot; + request.getRequestURL()); User user = new User(&quot;Jockme&quot;,36); return user;&#125; jsp 文件 123456789101112131415161718&lt;a href=&quot;javascript:void(0);&quot; id=&quot;testCross&quot;&gt;跨域访问&lt;/a&gt;&lt;br/&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;$&#123;pageContext.request.contextPath&#125;/js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; //为id=&quot;testCross&quot;的组件绑定点击事件 $(&quot;#testCross&quot;).click(function()&#123; //发送异步调用 $.ajax(&#123; type:&quot;POST&quot;, url:&quot;http://127.0.0.1/cross&quot;, //回调函数 success:function(data)&#123; alert(&quot;跨域调用信息反馈:&quot; + data[&#x27;name&#x27;] + &quot;,&quot; + data[&#x27;age&#x27;]); &#125; &#125;); &#125;); &#125;);&lt;/script&gt; 拦截器基本介绍拦截器（Interceptor）是一种动态拦截方法调用的机制 作用： 在指定的方法调用前后执行预先设定后的的代码 阻止原始方法的执行 核心原理：AOP 思想 拦截器链：多个拦截器按照一定的顺序，对原始被调用功能进行增强 拦截器和过滤器对比： 归属不同： Filter 属于 Servlet 技术， Interceptor 属于 SpringMVC 技术 拦截内容不同： Filter 对所有访问进行增强， Interceptor 仅针对 SpringMVC 的访问进行增强 处理方法前置处理原始方法之前运行： 123456public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;preHandle&quot;); return true;&#125; 参数： request：请求对象 response：响应对象 handler：被调用的处理器对象，本质上是一个方法对象，对反射中的Method对象进行了再包装 handler：public String controller.InterceptorController.handleRun handler.getClass()：org.springframework.web.method.HandlerMethod 返回值： 返回值为 false，被拦截的处理器将不执行 后置处理原始方法运行后运行，如果原始方法被拦截，则不执行： 123456public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;postHandle&quot;);&#125; 参数： modelAndView：如果处理器执行完成具有返回结果，可以读取到对应数据与页面信息，并进行调整 异常处理拦截器最后执行的方法，无论原始方法是否执行： 123456public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; System.out.println(&quot;afterCompletion&quot;);&#125; 参数： ex：如果处理器执行过程中出现异常对象，可以针对异常情况进行单独处理 拦截配置拦截路径： /**：表示拦截所有映射 /* ：表示拦截所有&#x2F;开头的映射 /user/*：表示拦截所有 &#x2F;user&#x2F; 开头的映射 /user/add*：表示拦截所有 &#x2F;user&#x2F; 开头，且具体映射名称以 add 开头的映射 /user/*All：表示拦截所有 &#x2F;user&#x2F; 开头，且具体映射名称以 All 结尾的映射 1234567891011&lt;mvc:interceptors&gt; &lt;!--开启具体的拦截器的使用，可以配置多个--&gt; &lt;mvc:interceptor&gt; &lt;!--设置拦截器的拦截路径，支持*通配--&gt; &lt;mvc:mapping path=&quot;/handleRun*&quot;/&gt; &lt;!--设置拦截排除的路径，配置/**或/*，达到快速配置的目的--&gt; &lt;mvc:exclude-mapping path=&quot;/b*&quot;/&gt; &lt;!--指定具体的拦截器类--&gt; &lt;bean class=&quot;MyInterceptor&quot;/&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 拦截器链责任链模式：责任链模式是一种行为模式 特点：沿着一条预先设定的任务链顺序执行，每个节点具有独立的工作任务优势： 独立性：只关注当前节点的任务，对其他任务直接放行到下一节点 隔离性：具备链式传递特征，无需知晓整体链路结构，只需等待请求到达后进行处理即可 灵活性：可以任意修改链路结构动态新增或删减整体链路责任 解耦：将动态任务与原始任务解耦 缺点： 链路过长时，处理效率低下 可能存在节点上的循环引用现象，造成死循环，导致系统崩溃 源码解析DispatcherServlet#doDispatch 方法中： 123456789101112131415161718protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;\ttry &#123; // 获取映射器以及映射器的所有拦截器（运行原理部分详解了源码） mappedHandler = getHandler(processedRequest); // 前置处理，返回 false 代表条件成立 if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; //请求从这里直接结束 return; &#125; //所有拦截器都返回 true，执行目标方法 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()) // 倒序执行所有拦截器的后置处理方法 mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; //异常处理机制 triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125;&#125; HandlerExecutionChain#applyPreHandle：前置处理 12345678910111213boolean applyPreHandle(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; //遍历所有的拦截器 for (int i = 0; i &lt; this.interceptorList.size(); i++) &#123; HandlerInterceptor interceptor = this.interceptorList.get(i); //执行前置处理，如果拦截器返回 false，则条件成立，不在执行其他的拦截器，直接返回 false，请求直接结束 if (!interceptor.preHandle(request, response, this.handler)) &#123; triggerAfterCompletion(request, response, null); return false; &#125; this.interceptorIndex = i; &#125; return true;&#125; HandlerExecutionChain#applyPostHandle：后置处理 12345678void applyPostHandle(HttpServletRequest request, HttpServletResponse response, @Nullable ModelAndView mv) throws Exception &#123;\t//倒序遍历 for (int i = this.interceptorList.size() - 1; i &gt;= 0; i--) &#123; HandlerInterceptor interceptor = this.interceptorList.get(i); interceptor.postHandle(request, response, this.handler, mv); &#125;&#125; DispatcherServlet#triggerAfterCompletion 底层调用 HandlerExecutionChain#triggerAfterCompletion： 前面的步骤有任何异常都会直接倒序触发 afterCompletion 页面成功渲染有异常，也会倒序触发 afterCompletion 12345678910111213void triggerAfterCompletion(HttpServletRequest request, HttpServletResponse response, @Nullable Exception ex) &#123; //倒序遍历 for (int i = this.interceptorIndex; i &gt;= 0; i--) &#123; HandlerInterceptor interceptor = this.interceptorList.get(i); try &#123; //执行异常处理的方法 interceptor.afterCompletion(request, response, this.handler, ex); &#125; catch (Throwable ex2) &#123; logger.error(&quot;HandlerInterceptor.afterCompletion threw exception&quot;, ex2); &#125; &#125;&#125; 拦截器的执行流程： 参考文章：https://www.yuque.com/atguigu/springboot/vgzmgh#wtPLU 自定义 Contoller层 12345678@Controllerpublic class InterceptorController &#123; @RequestMapping(&quot;/handleRun&quot;) public String handleRun() &#123; System.out.println(&quot;业务处理器运行------------main&quot;); return &quot;page.jsp&quot;; &#125;&#125; 自定义拦截器需要实现 HandleInterceptor 接口 12345678910111213141516171819202122232425262728293031//自定义拦截器需要实现HandleInterceptor接口public class MyInterceptor implements HandlerInterceptor &#123; //处理器运行之前执行 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;前置运行----a1&quot;); //返回值为false将拦截原始处理器的运行 //如果配置多拦截器，返回值为false将终止当前拦截器后面配置的拦截器的运行 return true; &#125; //处理器运行之后执行 @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;后置运行----b1&quot;); &#125; //所有拦截器的后置执行全部结束后，执行该操作 @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; System.out.println(&quot;完成运行----c1&quot;); &#125;&#125; 说明：三个方法的运行顺序为 preHandle → postHandle → afterCompletion，如果 preHandle 返回值为 false，三个方法仅运行preHandle web.xml： 1CharacterEncodingFilter + DispatcherServlet 配置拦截器：spring-mvc.xml 12345678&lt;mvc:annotation-driven/&gt;&lt;context:component-scan base-package=&quot;interceptor,controller&quot;/&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/handleRun&quot;/&gt; &lt;bean class=&quot;interceptor.MyInterceptor&quot;/&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 注意：配置顺序为先配置执行位置，后配置执行类 异常处理处理器异常处理器： HandlerExceptionResolver 接口 类继承该接口的以后，当开发出现异常后会执行指定的功能 12345678910111213141516@Componentpublic class ExceptionResolver implements HandlerExceptionResolver &#123; @Override public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; System.out.println(&quot;异常处理器正在执行中&quot;); ModelAndView modelAndView = new ModelAndView(); //定义异常现象出现后，反馈给用户查看的信息 modelAndView.addObject(&quot;msg&quot;,&quot;出错啦！ &quot;); //定义异常现象出现后，反馈给用户查看的页面 modelAndView.setViewName(&quot;error.jsp&quot;); return modelAndView; &#125;&#125; 根据异常的种类不同，进行分门别类的管理，返回不同的信息： 12345678910111213141516171819public class ExceptionResolver implements HandlerExceptionResolver &#123; @Override public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; System.out.println(&quot;my exception is running ....&quot; + ex); ModelAndView modelAndView = new ModelAndView(); if( ex instanceof NullPointerException)&#123; modelAndView.addObject(&quot;msg&quot;,&quot;空指针异常&quot;); &#125;else if ( ex instanceof ArithmeticException)&#123; modelAndView.addObject(&quot;msg&quot;,&quot;算数运算异常&quot;); &#125;else&#123; modelAndView.addObject(&quot;msg&quot;,&quot;未知的异常&quot;); &#125; modelAndView.setViewName(&quot;error.jsp&quot;); return modelAndView; &#125;&#125; 模拟错误： 123456789101112@Controllerpublic class UserController &#123; @RequestMapping(&quot;/save&quot;) @ResponseBody public String save(@RequestBody String name) &#123; //模拟业务层发起调用产生了异常// int i = 1/0;// String str = null;// str.length(); return &quot;error.jsp&quot;; &#125; 注解开发使用注解实现异常分类管理，开发异常处理器 @ControllerAdvice 注解： 类型：类注解 位置：异常处理器类上方 作用：设置当前类为异常处理器类 格式： 12345@Component//声明该类是一个Controller的通知类，声明后该类就会被加载成异常处理器@ControllerAdvicepublic class ExceptionAdvice &#123;&#125; @ExceptionHandler 注解： 类型：方法注解 位置：异常处理器类中针对指定异常进行处理的方法上方 作用：设置指定异常的处理方式 说明：处理器方法可以设定多个 格式： 12345678910111213141516@Component@ControllerAdvicepublic class ExceptionAdvice &#123; //类中定义的方法携带@ExceptionHandler注解的会被作为异常处理器，后面添加实际处理的异常类型 @ExceptionHandler(NullPointerException.class) @ResponseBody public String doNullException(Exception ex)&#123; return &quot;空指针异常&quot;; &#125; @ExceptionHandler(Exception.class) @ResponseBody public String doException(Exception ex)&#123; return &quot;all Exception&quot;; &#125;&#125; @ResponseStatus 注解： 类型：类注解、方法注解 位置：异常处理器类、方法上方 参数： value：出现错误指定返回状态码 reason：出现错误返回的错误信息 解决方案 web.xml 1DispatcherServlet + CharacterEncodingFilter ajax.jsp 123456789101112131415161718192021222324252627&lt;%@page pageEncoding=&quot;UTF-8&quot; language=&quot;java&quot; contentType=&quot;text/html;UTF-8&quot; %&gt;&lt;a href=&quot;javascript:void(0);&quot; id=&quot;testException&quot;&gt;点击&lt;/a&gt;&lt;br/&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;$&#123;pageContext.request.contextPath&#125;/js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; $(&quot;#testException&quot;).click(function()&#123; $.ajax(&#123; contentType:&quot;application/json&quot;, type:&quot;POST&quot;, url:&quot;save&quot;, /*通过修改参数，激活自定义异常的出现*/ // name长度低于8位出现业务异常 // age小于0出现业务异常 // age大于100出现系统异常 // age类型如果无法匹配将转入其他类别异常 data:&#x27;&#123;&quot;name&quot;:&quot;JockSuperMan&quot;,&quot;age&quot;:&quot;-1&quot;&#125;&#x27;, dataType:&quot;text&quot;, //回调函数 success:function(data)&#123; alert(data); &#125; &#125;); &#125;); &#125;);&lt;/script&gt; spring-mvc.xml 123&lt;mvc:annotation-driven/&gt;&lt;context:component-scan base-package=&quot;com.seazean&quot;/&gt;&lt;mvc:resources mapping=&quot;/js/**&quot; location=&quot;/js/&quot;/&gt; java &#x2F; controller &#x2F; UserController 123456789101112131415161718192021222324@Controllerpublic class UserController &#123; @RequestMapping(&quot;/save&quot;) @ResponseBody public List&lt;User&gt; save(@RequestBody User user) &#123; System.out.println(&quot;user controller save is running ...&quot;); //对用户的非法操作进行判定，并包装成异常对象进行处理，便于统一管理 if(user.getName().trim().length() &lt; 8)&#123; throw new BusinessException(&quot;对不起，用户名长度不满足要求，请重新输入！&quot;); &#125; if(user.getAge() &lt; 0)&#123; throw new BusinessException(&quot;对不起，年龄必须是0到100之间的数字！&quot;); &#125; if(user.getAge() &gt; 100)&#123; throw new SystemException(&quot;服务器连接失败，请尽快检查处理！&quot;); &#125; User u1 = new User(&quot;Tom&quot;,3); User u2 = new User(&quot;Jerry&quot;,5); ArrayList&lt;User&gt; al = new ArrayList&lt;User&gt;(); al.add(u1);al.add(u2); return al; &#125;&#125; 自定义异常 12//自定义异常继承RuntimeException，覆盖父类所有的构造方法public class BusinessException extends RuntimeException &#123;覆盖父类所有的构造方法&#125; 1public class SystemException extends RuntimeException &#123;&#125; 通过自定义异常将所有的异常现象进行分类管理，以统一的格式对外呈现异常消息 1234567891011121314151617181920212223242526@Component@ControllerAdvicepublic class ProjectExceptionAdvice &#123; @ExceptionHandler(BusinessException.class) public String doBusinessException(Exception ex, Model m)&#123; //使用参数Model将要保存的数据传递到页面上，功能等同于ModelAndView //业务异常出现的消息要发送给用户查看 m.addAttribute(&quot;msg&quot;,ex.getMessage()); return &quot;error.jsp&quot;; &#125; @ExceptionHandler(SystemException.class) public String doSystemException(Exception ex, Model m)&#123; //系统异常出现的消息不要发送给用户查看，发送统一的信息给用户看 m.addAttribute(&quot;msg&quot;,&quot;服务器出现问题，请联系管理员！&quot;); return &quot;error.jsp&quot;; &#125; @ExceptionHandler(Exception.class) public String doException(Exception ex, Model m)&#123; m.addAttribute(&quot;msg&quot;,ex.getMessage()); //将ex对象保存起来 return &quot;error.jsp&quot;; &#125;&#125; 文件传输上传下载上传文件过程： MultipartResolver接口： MultipartResolver 接口定义了文件上传过程中的相关操作，并对通用性操作进行了封装 MultipartResolver 接口底层实现类 CommonsMultipartResovler CommonsMultipartResovler 并未自主实现文件上传下载对应的功能，而是调用了 apache 文件上传下载组件 文件上传下载实现： 导入坐标 12345&lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt;&lt;/dependency&gt; 页面表单 fileupload.jsp 1234&lt;form method=&quot;post&quot; action=&quot;/upload&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;input type=&quot;file&quot; name=&quot;file&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;&lt;/form&gt; web.xml 1DispatcherServlet + CharacterEncodingFilter 控制器 123456789101112@PostMapping(&quot;/upload&quot;)public String upload(@RequestParam(&quot;email&quot;) String email, @RequestParam(&quot;username&quot;) String username, @RequestPart(&quot;headerImg&quot;) MultipartFile headerImg) throws IOException &#123; if(!headerImg.isEmpty())&#123; //保存到文件服务器，OSS服务器 String originalFilename = headerImg.getOriginalFilename(); headerImg.transferTo(new File(&quot;H:\\\\cache\\\\&quot; + originalFilename)); &#125; return &quot;main&quot;;&#125; 名称问题MultipartFile 参数中封装了上传的文件的相关信息。 文件命名问题， 获取上传文件名，并解析文件名与扩展名 1file.getOriginalFilename(); 文件名过长问题 文件保存路径 1234ServletContext context = request.getServletContext();String realPath = context.getRealPath(&quot;/uploads&quot;);File file = new File(realPath + &quot;/&quot;);if(!file.exists()) file.mkdirs(); 重名问题 1String uuid = UUID.randomUUID.toString().replace(&quot;-&quot;, &quot;&quot;).toUpperCase(); 1234567891011121314151617181920212223242526272829303132333435363738@Controllerpublic class FileUploadController &#123; @RequestMapping(value = &quot;/fileupload&quot;)\t//参数中定义MultipartFile参数，用于接收页面提交的type=file类型的表单，表单名称与参数名相同 public String fileupload(MultipartFile file,MultipartFile file1,MultipartFile file2, HttpServletRequest request) throws IOException &#123; System.out.println(&quot;file upload is running ...&quot;+file);// MultipartFile参数中封装了上传的文件的相关信息// System.out.println(file.getSize());// System.out.println(file.getBytes().length);// System.out.println(file.getContentType());// System.out.println(file.getName());// System.out.println(file.getOriginalFilename());// System.out.println(file.isEmpty()); //首先判断是否是空文件，也就是存储空间占用为0的文件 if(!file.isEmpty())&#123; //如果大小在范围要求内正常处理，否则抛出自定义异常告知用户（未实现） //获取原始上传的文件名，可以作为当前文件的真实名称保存到数据库中备用 String fileName = file.getOriginalFilename(); //设置保存的路径 String realPath = request.getServletContext().getRealPath(&quot;/images&quot;); //保存文件的方法，通常文件名使用随机生成策略产生，避免文件名冲突问题 file.transferTo(new File(realPath,file.getOriginalFilename())); &#125; //测试一次性上传多个文件 if(!file1.isEmpty())&#123; String fileName = file1.getOriginalFilename(); //可以根据需要，对不同种类的文件做不同的存储路径的区分，修改对应的保存位置即可 String realPath = request.getServletContext().getRealPath(&quot;/images&quot;); file1.transferTo(new File(realPath,file1.getOriginalFilename())); &#125; if(!file2.isEmpty())&#123; String fileName = file2.getOriginalFilename(); String realPath = request.getServletContext().getRealPath(&quot;/images&quot;); file2.transferTo(new File(realPath,file2.getOriginalFilename())); &#125; return &quot;page.jsp&quot;; &#125;&#125; 源码解析StandardServletMultipartResolver 是文件上传解析器 DispatcherServlet#doDispatch： 123456protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; // 判断当前请求是不是文件上传请求 processedRequest = checkMultipart(request); // 文件上传请求会对 request 进行包装，导致两者不相等，此处赋值为 true，代表已经被解析 multipartRequestParsed = (processedRequest != request);&#125; DispatcherServlet#checkMultipart： if (this.multipartResolver != null &amp;&amp; this.multipartResolver.isMultipart(request))：判断是否是文件请求 StandardServletMultipartResolver#isMultipart：根据开头是否符合 multipart&#x2F;form-data 或者 multipart&#x2F; return this.multipartResolver.resolveMultipart(request)：把请求封装成 StandardMultipartHttpServletRequest 对象 开始执行 ha.handle() 目标方法进行数据的解析 RequestPartMethodArgumentResolver#supportsParameter：支持解析文件上传数据 123456public boolean supportsParameter(MethodParameter parameter) &#123; // 参数上有 @RequestPart 注解 if (parameter.hasParameterAnnotation(RequestPart.class)) &#123; return true; &#125;&#125; RequestPartMethodArgumentResolver#resolveArgument：解析参数数据，封装成 MultipartFile 对象 RequestPart requestPart = parameter.getParameterAnnotation(RequestPart.class)：获取注解的相关信息 String name = getPartName(parameter, requestPart)：获取上传文件的名字 Object mpArg = MultipartResolutionDelegate.resolveMultipartArgument()：解析参数 List&lt;MultipartFile&gt; files = multipartRequest.getFiles(name)：获取文件的所有数据 return doInvoke(args)：解析完成执行自定义的方法，完成上传功能 实用技术校验框架校验概述表单校验保障了数据有效性、安全性 校验分类：客户端校验和服务端校验 格式校验 客户端：使用 js 技术，利用正则表达式校验 服务端：使用校验框架 逻辑校验 客户端：使用ajax发送要校验的数据，在服务端完成逻辑校验，返回校验结果 服务端：接收到完整的请求后，在执行业务操作前，完成逻辑校验 表单校验框架： JSR（Java Specification Requests）：Java 规范提案 303：提供bean属性相关校验规则 JCP（Java Community Process）：Java社区 Hibernate框架中包含一套独立的校验框架hibernate-validator 导入坐标： 123456789101112&lt;!--导入校验的jsr303规范--&gt;&lt;dependency&gt; &lt;groupId&gt;javax.validation&lt;/groupId&gt; &lt;artifactId&gt;validation-api&lt;/artifactId&gt; &lt;version&gt;2.0.1.Final&lt;/version&gt;&lt;/dependency&gt;&lt;!--导入校验框架实现技术--&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;6.1.0.Final&lt;/version&gt;&lt;/dependency&gt; 注意： tomcat7：搭配 hibernate-validator 版本 5...Final tomcat8.5：搭配 hibernate-validator 版本 6...Final 基本使用开启校验名称：@Valid、@Validated 类型：形参注解 位置：处理器类中的实体类类型的方法形参前方 作用：设定对当前实体类类型参数进行校验 范例： 1234@RequestMapping(value = &quot;/addemployee&quot;)public String addEmployee(@Valid Employee employee) &#123; System.out.println(employee);&#125; 校验规则名称：@NotNull 类型：属性注解等 位置：实体类属性上方 作用：设定当前属性校验规则 范例：每个校验规则所携带的参数不同，根据校验规则进行相应的调整，具体的校验规则查看对应的校验框架进行获取 1234public class Employee&#123; @NotNull(message = &quot;姓名不能为空&quot;) private String name;//员工姓名&#125; 错误信息123456789101112131415@RequestMapping(value = &quot;/addemployee&quot;)//Errors对象用于封装校验结果，如果不满足校验规则，对应的校验结果封装到该对象中，包含校验的属性名和校验不通过返回的消息public String addEmployee(@Valid Employee employee, Errors errors, Model model)&#123; System.out.println(employee); //判定Errors对象中是否存在未通过校验的字段 if(errors.hasErrors())&#123; for(FieldError error : errors.getFieldErrors())&#123; //将校验结果添加到Model对象中，用于页面显示，返回json数据即可 model.addAttribute(error.getField(),error.getDefaultMessage()); &#125; //当出现未通过校验的字段时，跳转页面到原始页面，进行数据回显 return &quot;addemployee.jsp&quot;; &#125; return &quot;success.jsp&quot;;&#125; 通过形参Errors获取校验结果数据，通过Model接口将数据封装后传递到页面显示，页面获取后台封装的校验结果信息 12345&lt;form action=&quot;/addemployee&quot; method=&quot;post&quot;&gt; 员工姓名：&lt;input type=&quot;text&quot; name=&quot;name&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;$&#123;name&#125;&lt;/span&gt;&lt;br/&gt; 员工年龄：&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;$&#123;age&#125;&lt;/span&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;&lt;/form&gt; 多规则校验 同一个属性可以添加多个校验器 123456789public class Employee&#123; @NotBlank(message = &quot;姓名不能为空&quot;) private String name;//员工姓名 @NotNull(message = &quot;请输入年龄&quot;) @Max(value = 60,message = &quot;年龄最大值60&quot;) @Min(value = 18,message = &quot;年龄最小值18&quot;) private Integer age;//员工年龄&#125; 三种判定空校验器的区别 嵌套校验名称：@Valid 类型：属性注解 位置：实体类中的引用类型属性上方 作用：设定当前应用类型属性中的属性开启校验 范例： 12345public class Employee &#123; //实体类中的引用类型通过标注@Valid注解，设定开启当前引用类型字段中的属性参与校验 @Valid private Address address;&#125; 注意：开启嵌套校验后，被校验对象内部需要添加对应的校验规则 123456789//嵌套校验的实体中，对每个属性正常添加校验规则即可public class Address implements Serializable &#123; @NotBlank(message = &quot;请输入省份名称&quot;) private String provinceName;//省份名称 @NotBlank(message = &quot;请输入邮政编码&quot;) @Size(max = 6,min = 6,message = &quot;邮政编码由6位组成&quot;) private String zipCode;//邮政编码&#125; 分组校验分组校验的介绍 同一个模块，根据执行的业务不同，需要校验的属性会有不同 新增用户 修改用户 对不同种类的属性进行分组，在校验时可以指定参与校验的字段所属的组类别 定义组（通用） 为属性设置所属组，可以设置多个 开启组校验 domain： 123//用于设定分组校验中的组名，当前接口仅提供字节码，用于识别public interface GroupOne &#123;&#125; 12345678910111213public class Employee&#123; @NotBlank(message = &quot;姓名不能为空&quot;,groups = &#123;GroupA.class&#125;) private String name;//员工姓名 @NotNull(message = &quot;请输入年龄&quot;,groups = &#123;GroupA.class&#125;) @Max(value = 60,message = &quot;年龄最大值60&quot;)//不加Group的校验不生效 @Min(value = 18,message = &quot;年龄最小值18&quot;) private Integer age;//员工年龄 @Valid private Address address; //......&#125; controller： 123456789101112131415@Controllerpublic class EmployeeController &#123; @RequestMapping(value = &quot;/addemployee&quot;) public String addEmployee(@Validated(&#123;GroupA.class&#125;) Employee employee, Errors errors, Model m)&#123; if(errors.hasErrors())&#123; List&lt;FieldError&gt; fieldErrors = errors.getFieldErrors(); System.out.println(fieldErrors.size()); for(FieldError error : fieldErrors)&#123; m.addAttribute(error.getField(),error.getDefaultMessage()); &#125; return &quot;addemployee.jsp&quot;; &#125; return &quot;success.jsp&quot;; &#125;&#125; jsp： 1234567&lt;form action=&quot;/addemployee&quot; method=&quot;post&quot;&gt;&lt;%--页面使用$&#123;&#125;获取后台传递的校验信息--%&gt; 员工姓名：&lt;input type=&quot;text&quot; name=&quot;name&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;$&#123;name&#125;&lt;/span&gt;&lt;br/&gt; 员工年龄：&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;$&#123;age&#125;&lt;/span&gt;&lt;br/&gt; &lt;%--注意，引用类型的校验未通过信息不是通过对象进行封装的，直接使用对象名.属性名的格式作为整体属性字符串进行保存的，和使用者的属性传递方式有关，不具有通用性，仅适用于本案例--%&gt; 省：&lt;input type=&quot;text&quot; name=&quot;address.provinceName&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;$&#123;requestScope[&#x27;address.provinceName&#x27;]&#125;&lt;/span&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;/form&gt; LombokLombok 用标签方式代替构造器、getter&#x2F;setter、toString() 等方法 引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt; 下载插件：IDEA 中 File → Settings → Plugins，搜索安装 Lombok 插件 常用注解： 12345@NoArgsConstructor // 无参构造@AllArgsConstructor // 全参构造@Data // set + get@ToString // toString@EqualsAndHashCode // hashConde + equals 简化日志： 123456789@Slf4j@RestControllerpublic class HelloController &#123; @RequestMapping(&quot;/hello&quot;) public String handle01(@RequestParam(&quot;name&quot;) String name)&#123; log.info(&quot;请求进来了....&quot;); return &quot;Hello, Spring!&quot; + &quot;你好：&quot; + name; &#125;&#125; Boot基本介绍Boot介绍SpringBoot 提供了一种快速使用 Spring 的方式，基于约定优于配置的思想，可以让开发人员不必在配置与逻辑业务之间进行思维的切换，全身心的投入到逻辑业务的代码编写中，从而大大提高了开发的效率 SpringBoot 功能： 自动配置，自动配置是一个运行时（更准确地说，是应用程序启动时）的过程，考虑了众多因素选择使用哪个配置，该过程是SpringBoot 自动完成的 起步依赖，起步依赖本质上是一个 Maven 项目对象模型（Project Object Model，POM），定义了对其他库的传递依赖，这些东西加在一起即支持某项功能。简单的说，起步依赖就是将具备某种功能的坐标打包到一起，并提供一些默认的功能 辅助功能，提供了一些大型项目中常见的非功能性特性，如内嵌 web 服务器、安全、指标，健康检测、外部配置等 参考视频：https://www.bilibili.com/video/BV19K4y1L7MT 构建工程普通构建： 创建 Maven 项目 导入 SpringBoot 起步依赖 1234567891011121314&lt;!--springboot 工程需要继承的父工程--&gt;&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;!--web 开发的起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 定义 Controller 1234567@RestControllerpublic class HelloController &#123; @RequestMapping(&quot;/hello&quot;) public String hello()&#123; return &quot; hello Spring Boot !&quot;; &#125;&#125; 编写引导类 1234567// 引导类，SpringBoot项目的入口@SpringBootApplicationpublic class HelloApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(HelloApplication.class, args); &#125;&#125; 快速构建： 自动装配依赖管理在 spring-boot-starter-parent 中定义了各种技术的版本信息，组合了一套最优搭配的技术版本。在各种 starter 中，定义了完成该功能需要的坐标合集，其中大部分版本信息来自于父工程。工程继承 parent，引入 starter 后，通过依赖传递，就可以简单方便获得需要的 jar 包，并且不会存在版本冲突，自动版本仲裁机制 底层注解SpringBoot@SpringBootApplication：启动注解，实现 SpringBoot 的自动部署 参数 scanBasePackages：可以指定扫描范围 默认扫描当前引导类所在包及其子包 假如所在包为 com.example.springbootenable，扫描配置包 com.example.config 的信息，三种解决办法： 使用 @ComponentScan 扫描 com.example.config 包 使用 @Import 注解加载类，这些类都会被 Spring 创建并放入 ioc 容器，默认组件的名字就是全类名 对 @Import 注解进行封装 1234567891011121314//1.@ComponentScan(&quot;com.example.config&quot;)//2.@Import(UserConfig.class)@EnableUser@SpringBootApplicationpublic class SpringbootEnableApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext context = SpringApplication.run(SpringbootEnableApplication.class, args); //获取Bean Object user = context.getBean(&quot;user&quot;); System.out.println(user);\t&#125;&#125; UserConfig： 1234567@Configurationpublic class UserConfig &#123; @Bean public User user() &#123; return new User(); &#125;&#125; EnableUser 注解类： 123456@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(UserConfig.class)//@Import注解实现Bean的动态加载public @interface EnableUser &#123;&#125; Configuration@Configuration：设置当前类为 SpringBoot 的配置类 proxyBeanMethods &#x3D; true：Full 全模式，每个 @Bean 方法被调用多少次返回的组件都是单实例的，默认值，类组件之间有依赖关系，方法会被调用得到之前单实例组件 proxyBeanMethods &#x3D; false：Lite 轻量级模式，每个 @Bean 方法被调用多少次返回的组件都是新创建的，类组件之间无依赖关系用 Lite 模式加速容器启动过程 12345678@Configuration(proxyBeanMethods = true)public class MyConfig &#123; @Bean //给容器中添加组件。以方法名作为组件的 id。返回类型就是组件类型。返回的值，就是组件在容器中的实例 public User user()&#123; User user = new User(&quot;zhangsan&quot;, 18); return user; &#125;&#125; Condition条件注解Condition 是 Spring4.0 后引入的条件化配置接口，通过实现 Condition 接口可以完成有条件的加载相应的 Bean 注解：@Conditional 作用：条件装配，满足 Conditional 指定的条件则进行组件注入，加上方法或者类上，作用范围不同 使用：@Conditional 配合 Condition 的实现类（ClassCondition）进行使用 ConditionContext 类API： 方法 说明 ConfigurableListableBeanFactory getBeanFactory() 获取到 IOC 使用的 beanfactory ClassLoader getClassLoader() 获取类加载器 Environment getEnvironment() 获取当前环境信息 BeanDefinitionRegistry getRegistry() 获取到 bean 定义的注册类 ClassCondition： 12345678910111213141516171819public class ClassCondition implements Condition &#123; /** * context 上下文对象。用于获取环境，IOC容器，ClassLoader对象 * metadata 注解元对象。 可以用于获取注解定义的属性值 */ @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; //1.需求： 导入Jedis坐标后创建Bean //思路：判断redis.clients.jedis.Jedis.class文件是否存在 boolean flag = true; try &#123; Class&lt;?&gt; cls = Class.forName(&quot;redis.clients.jedis.Jedis&quot;); &#125; catch (ClassNotFoundException e) &#123; flag = false; &#125; return flag; &#125;&#125; UserConfig： 12345678@Configurationpublic class UserConfig &#123; @Bean @Conditional(ClassCondition.class) public User user()&#123; return new User(); &#125;&#125; 启动类： 12345678910@SpringBootApplicationpublic class SpringbootConditionApplication &#123; public static void main(String[] args) &#123; //启动SpringBoot应用，返回Spring的IOC容器 ConfigurableApplicationContext context = SpringApplication.run(SpringbootConditionApplication.class, args); Object user = context.getBean(&quot;user&quot;); System.out.println(user); &#125;&#125; 自定义注解将类的判断定义为动态的，判断哪个字节码文件存在可以动态指定 自定义条件注解类 1234567@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(ClassCondition.class)public @interface ConditionOnClass &#123; String[] value();&#125; ClassCondition 12345678910111213141516171819202122public class ClassCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata metadata) &#123; //需求：通过注解属性值value指定坐标后创建bean Map&lt;String, Object&gt; map = metadata.getAnnotationAttributes (ConditionOnClass.class.getName()); //map = &#123;value=&#123;属性值&#125;&#125; //获取所有的 String[] value = (String[]) map.get(&quot;value&quot;); boolean flag = true; try &#123; for (String className : value) &#123; Class&lt;?&gt; cls = Class.forName(className); &#125; &#125; catch (Exception e) &#123; flag = false; &#125; return flag; &#125;&#125; UserConfig 12345678@Configurationpublic class UserConfig &#123; @Bean @ConditionOnClass(&quot;com.alibaba.fastjson.JSON&quot;)//JSON加载了才注册 User 到容器 public User user()&#123; return new User(); &#125;&#125; 测试 User 对象的创建 常用注解SpringBoot 提供的常用条件注解： @ConditionalOnProperty：判断配置文件中是否有对应属性和值才初始化 Bean 12345678@Configurationpublic class UserConfig &#123; @Bean @ConditionalOnProperty(name = &quot;it&quot;, havingValue = &quot;seazean&quot;) public User user() &#123; return new User(); &#125;&#125; 1it=seazean @ConditionalOnClass：判断环境中是否有对应类文件才初始化 Bean @ConditionalOnMissingClass：判断环境中是否有对应类文件才初始化 Bean @ConditionalOnMissingBean：判断环境中没有对应Bean才初始化 Bean ImportRes使用 bean.xml 文件生成配置 bean，如果需要继续复用 bean.xml，@ImportResource 导入配置文件即可 1234@ImportResource(&quot;classpath:beans.xml&quot;)public class MyConfig &#123;\t//...&#125; 12345678910&lt;beans ...&gt; &lt;bean id=&quot;haha&quot; class=&quot;com.lun.boot.bean.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;zhangsan&quot;&gt;&lt;/property&gt; &lt;property name=&quot;age&quot; value=&quot;18&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;hehe&quot; class=&quot;com.lun.boot.bean.Pet&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;tomcat&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; Properties@ConfigurationProperties：读取到 properties 文件中的内容，并且封装到 JavaBean 中 配置文件： 12mycar.brand=BYDmycar.price=100000 JavaBean 类： 123456@Component\t//导入到容器内@ConfigurationProperties(prefix = &quot;mycar&quot;)//代表配置文件的前缀public class Car &#123; private String brand; private Integer price;&#125; 装配原理启动流程应用启动： 1234567@SpringBootApplicationpublic class BootApplication &#123; public static void main(String[] args) &#123; // 启动代码 SpringApplication.run(BootApplication.class, args); &#125;&#125; SpringApplication 构造方法： this.resourceLoader = resourceLoader：资源加载器，初始为 null this.webApplicationType = WebApplicationType.deduceFromClasspath()：判断当前应用的类型，是响应式还是 Web 类 this.bootstrapRegistryInitializers = getBootstrapRegistryInitializersFromSpringFactories()：获取引导器 去 META-INF/spring.factories 文件中找 org.springframework.boot.Bootstrapper 寻找的顺序：classpath → spring-beans → boot-devtools → springboot → boot-autoconfigure setInitializers(getSpringFactoriesInstances(ApplicationContextInitializer.class))：获取初始化器 去 META-INF/spring.factories 文件中找 org.springframework.context.ApplicationContextInitializer setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class))：获取监听器 去 META-INF/spring.factories 文件中找 org.springframework.context.ApplicationListener this.mainApplicationClass = deduceMainApplicationClass()：获取出 main 程序类 SpringApplication#run(String… args)：创建 IOC 容器并实现了自动装配 StopWatch stopWatch = new StopWatch()：停止监听器，监控整个应用的启停 stopWatch.start()：记录应用的启动时间 bootstrapContext = createBootstrapContext()：创建引导上下文环境 bootstrapContext = new DefaultBootstrapContext()：创建默认的引导类环境 this.bootstrapRegistryInitializers.forEach()：遍历所有的引导器调用 initialize 方法完成初始化设置 configureHeadlessProperty()：让当前应用进入 headless 模式 listeners = getRunListeners(args)：获取所有 RunListener（运行监听器） 去 META-INF/spring.factories 文件中找 org.springframework.boot.SpringApplicationRunListener listeners.starting(bootstrapContext, this.mainApplicationClass)：遍历所有的运行监听器调用 starting 方法 applicationArguments = new DefaultApplicationArguments(args)：获取所有的命令行参数 environment = prepareEnvironment(listeners, bootstrapContext, applicationArguments)：准备环境 environment = getOrCreateEnvironment()：返回或创建基础环境信息对象 switch (this.webApplicationType)：根据当前应用的类型创建环境 case SERVLET：Web 应用环境对应 ApplicationServletEnvironment case REACTIVE：响应式编程对应 ApplicationReactiveWebEnvironment default：默认为 Spring 环境 ApplicationEnvironment configureEnvironment(environment, applicationArguments.getSourceArgs())：读取所有配置源的属性值配置环境 ConfigurationPropertySources.attach(environment)：属性值绑定环境信息 sources.addFirst(ATTACHED_PROPERTY_SOURCE_NAME,..)：把 configurationProperties 放入环境的属性信息头部 listeners.environmentPrepared(bootstrapContext, environment)：运行监听器调用 environmentPrepared()，EventPublishingRunListener 发布事件通知所有的监听器当前环境准备完成 DefaultPropertiesPropertySource.moveToEnd(environment)：移动 defaultProperties 属性源到环境中的最后一个源 bindToSpringApplication(environment)：与容器绑定当前环境 ConfigurationPropertySources.attach(environment)：重新将属性值绑定环境信息 sources.remove(ATTACHED_PROPERTY_SOURCE_NAME)：从环境信息中移除 configurationProperties sources.addFirst(ATTACHED_PROPERTY_SOURCE_NAME,..)：把 configurationProperties 重新放入环境信息 configureIgnoreBeanInfo(environment)：配置忽略的 bean printedBanner = printBanner(environment)：打印 SpringBoot 标志 context = createApplicationContext()：创建 IOC 容器 switch (this.webApplicationType)：根据当前应用的类型创建 IOC 容器 case SERVLET：Web 应用环境对应 AnnotationConfigServletWebServerApplicationContext case REACTIVE：响应式编程对应 AnnotationConfigReactiveWebServerApplicationContext default：默认为 Spring 环境 AnnotationConfigApplicationContext context.setApplicationStartup(this.applicationStartup)：设置一个启动器 prepareContext()：配置 IOC 容器的基本信息 postProcessApplicationContext(context)：后置处理流程 applyInitializers(context)：获取所有的初始化器调用 initialize() 方法进行初始化 listeners.contextPrepared(context)：所有的运行监听器调用 environmentPrepared() 方法，EventPublishingRunListener 发布事件通知 IOC 容器准备完成 listeners.contextLoaded(context)：所有的运行监听器调用 contextLoaded() 方法，通知 IOC 加载完成 refreshContext(context)：刷新 IOC 容器 Spring 的容器启动流程 invokeBeanFactoryPostProcessors(beanFactory)：实现了自动装配 onRefresh()：创建 WebServer 使用该接口 afterRefresh(context, applicationArguments)：留给用户自定义容器刷新完成后的处理逻辑 stopWatch.stop()：记录应用启动完成的时间 callRunners(context, applicationArguments)：调用所有 runners listeners.started(context)：所有的运行监听器调用 started() 方法 listeners.running(context)：所有的运行监听器调用 running() 方法 获取容器中的 ApplicationRunner、CommandLineRunner AnnotationAwareOrderComparator.sort(runners)：合并所有 runner 并且按照 @Order 进行排序 callRunner()：遍历所有的 runner，调用 run 方法 handleRunFailure(context, ex, listeners)：处理异常，出现异常进入该逻辑 handleExitCode(context, exception)：处理错误代码 listeners.failed(context, exception)：运行监听器调用 failed() 方法 reportFailure(getExceptionReporters(context), exception)：通知异常 注解分析SpringBoot 定义了一套接口规范，这套规范规定 SpringBoot 在启动时会扫描外部引用 jar 包中的 META-INF/spring.factories 文件，将文件中配置的类型信息加载到 Spring 容器，并执行类中定义的各种操作，对于外部的 jar 包，直接引入一个 starter 即可 @SpringBootApplication 注解是 @SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan 注解的集合 @SpringBootApplication 注解 1234567@Inherited@SpringBootConfiguration\t//代表 @SpringBootApplication 拥有了该注解的功能@EnableAutoConfiguration\t//同理@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)// 扫描被 @Component (@Service,@Controller)注解的 bean，容器中将排除TypeExcludeFilter 和 AutoConfigurationExcludeFilterpublic @interface SpringBootApplication &#123; &#125; @SpringBootConfiguration 注解： 123456@Configuration\t// 代表是配置类@Indexedpublic @interface SpringBootConfiguration &#123;\t@AliasFor(annotation = Configuration.class)\tboolean proxyBeanMethods() default true;&#125; @AliasFor 注解：表示别名，可以注解到自定义注解的两个属性上表示这两个互为别名，两个属性其实是同一个含义相互替代 @ComponentScan 注解：默认扫描当前类所在包及其子级包下的所有文件 @EnableAutoConfiguration 注解：启用 SpringBoot 的自动配置机制 1234567@AutoConfigurationPackage\t@Import(AutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123; String ENABLED_OVERRIDE_PROPERTY = &quot;spring.boot.enableautoconfiguration&quot;; Class&lt;?&gt;[] exclude() default &#123;&#125;; String[] excludeName() default &#123;&#125;;&#125; @AutoConfigurationPackage：将添加该注解的类所在的 package 作为自动配置 package 进行管理，把启动类所在的包设置一次，为了给各种自动配置的第三方库扫描用，比如带 @Mapper 注解的类，Spring 自身是不能识别的，但自动配置的 Mybatis 需要扫描用到，而 ComponentScan 只是用来扫描注解类，并没有提供接口给三方使用 12345@Import(AutoConfigurationPackages.Registrar.class)\t// 利用 Registrar 给容器中导入组件public @interface AutoConfigurationPackage &#123; String[] basePackages() default &#123;&#125;;\t//自动配置包，指定了配置类的包 Class&lt;?&gt;[] basePackageClasses() default &#123;&#125;;&#125; register(registry, new PackageImports(metadata).getPackageNames().toArray(new String[0]))：注册 BD new PackageImports(metadata).getPackageNames()：获取添加当前注解的类的所在包 registry.registerBeanDefinition(BEAN, new BasePackagesBeanDefinition(packageNames))：存放到容器中 new BasePackagesBeanDefinition(packageNames)：把当前主类所在的包名封装到该对象中 @Import(AutoConfigurationImportSelector.class)：自动装配的核心类 容器刷新时执行：invokeBeanFactoryPostProcessors() → invokeBeanDefinitionRegistryPostProcessors() → postProcessBeanDefinitionRegistry() → processConfigBeanDefinitions() → parse() → process() → processGroupImports() → getImports() → process() → AutoConfigurationImportSelector#getAutoConfigurationEntry() 12345678910111213141516171819202122protected AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return EMPTY_ENTRY; &#125; // 获取注解属性，@SpringBootApplication 注解的 exclude 属性和 excludeName 属性 AnnotationAttributes attributes = getAttributes(annotationMetadata); // 获取所有需要自动装配的候选项 List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); // 去除重复的选项 configurations = removeDuplicates(configurations); // 获取注解配置的排除的自动装配类 Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); // 移除所有的配置的不需要自动装配的类 configurations.removeAll(exclusions); // 过滤，条件装配 configurations = getConfigurationClassFilter().filter(configurations); // 获取 AutoConfigurationImportListener 类的监听器调用 onAutoConfigurationImportEvent 方法 fireAutoConfigurationImportEvents(configurations, exclusions); // 包装成 AutoConfigurationEntry 返回 return new AutoConfigurationEntry(configurations, exclusions);&#125; AutoConfigurationImportSelector#getCandidateConfigurations：获取自动配置的候选项 List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames()：加载自动配置类 参数一：getSpringFactoriesLoaderFactoryClass()：获取 @EnableAutoConfiguration 注解类 参数二：getBeanClassLoader()：获取类加载器 factoryTypeName = factoryType.getName()：@EnableAutoConfiguration 注解的全类名 return loadSpringFactories(classLoaderToUse).getOrDefault()：加载资源 urls = classLoader.getResources(FACTORIES_RESOURCE_LOCATION)：获取资源类 FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;：加载的资源的位置 return configurations：返回所有自动装配类的候选项 从 spring-boot-autoconfigure-2.5.3.jar&#x2F;META-INF&#x2F;spring.factories 文件中寻找 EnableAutoConfiguration 字段，获取自动装配类，进行条件装配，按需装配 装配流程Spring Boot 通过 @EnableAutoConfiguration 开启自动装配，通过 SpringFactoriesLoader 加载 META-INF/spring.factories 中的自动配置类实现自动装配，自动配置类其实就是通过 @Conditional 注解按需加载的配置类，想要其生效必须引入 spring-boot-starter-xxx 包实现起步依赖 SpringBoot 先加载所有的自动配置类 xxxxxAutoConfiguration 每个自动配置类进行条件装配，默认都会绑定配置文件指定的值（xxxProperties 和配置文件进行了绑定） SpringBoot 默认会在底层配好所有的组件，如果用户自己配置了以用户的优先 定制化配置： 用户可以使用 @Bean 新建自己的组件来替换底层的组件 用户可以去看这个组件是获取的配置文件前缀值，在配置文件中修改 以 DispatcherServletAutoConfiguration 为例： 12345678910111213141516171819202122232425262728293031323334353637383940414243@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE)// 类中的 Bean 默认不是单例@Configuration(proxyBeanMethods = false)@ConditionalOnWebApplication(type = Type.SERVLET)// 条件装配，环境中有 DispatcherServlet 类才进行自动装配@ConditionalOnClass(DispatcherServlet.class)@AutoConfigureAfter(ServletWebServerFactoryAutoConfiguration.class)public class DispatcherServletAutoConfiguration &#123;\t// 注册的 DispatcherServlet 的 BeanName\tpublic static final String DEFAULT_DISPATCHER_SERVLET_BEAN_NAME = &quot;dispatcherServlet&quot;;\t@Configuration(proxyBeanMethods = false)\t@Conditional(DefaultDispatcherServletCondition.class)\t@ConditionalOnClass(ServletRegistration.class) // 绑定配置文件的属性，从配置文件中获取配置项\t@EnableConfigurationProperties(WebMvcProperties.class)\tprotected static class DispatcherServletConfiguration &#123; // 给容器注册一个 DispatcherServlet，起名字为 dispatcherServlet @Bean(name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME) public DispatcherServlet dispatcherServlet(WebMvcProperties webMvcProperties) &#123; // 新建一个 DispatcherServlet 设置相关属性 DispatcherServlet dispatcherServlet = new DispatcherServlet(); // spring.mvc 中的配置项获取注入，没有就填充默认值 dispatcherServlet.setDispatchOptionsRequest(webMvcProperties.isDispatchOptionsRequest()); // ...... // 返回该对象注册到容器内 return dispatcherServlet; &#125; @Bean // 容器中有这个类型组件才进行装配 @ConditionalOnBean(MultipartResolver.class) // 容器中没有这个名字 multipartResolver 的组件 @ConditionalOnMissingBean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME) // 方法名就是 BeanName public MultipartResolver multipartResolver(MultipartResolver resolver) &#123; // 给 @Bean 标注的方法传入了对象参数，这个参数就会从容器中找，因为用户自定义了该类型，以用户配置的优先 // 但是名字不符合规范，所以获取到该 Bean 并返回到容器一个规范的名称：multipartResolver return resolver; &#125;\t&#125;&#125; 123// 将配置文件中的 spring.mvc 前缀的属性与该类绑定@ConfigurationProperties(prefix = &quot;spring.mvc&quot;)\tpublic class WebMvcProperties &#123; &#125; 事件监听SpringBoot 在项目启动时，会对几个监听器进行回调，可以实现监听器接口，在项目启动时完成一些操作 ApplicationContextInitializer、SpringApplicationRunListener、CommandLineRunner、ApplicationRunner MyApplicationRunner 自定义监听器的启动时机：MyApplicationRunner 和 MyCommandLineRunner 都是当项目启动后执行，使用 @Component 放入容器即可使用 123456789//当项目启动后执行run方法@Componentpublic class MyApplicationRunner implements ApplicationRunner &#123; @Override public void run(ApplicationArguments args) throws Exception &#123; System.out.println(&quot;ApplicationRunner...run&quot;); System.out.println(Arrays.asList(args.getSourceArgs()));//properties配置信息 &#125;&#125; MyCommandLineRunner 12345678@Componentpublic class MyCommandLineRunner implements CommandLineRunner &#123; @Override public void run(String... args) throws Exception &#123; System.out.println(&quot;CommandLineRunner...run&quot;); System.out.println(Arrays.asList(args)); &#125;&#125; MyApplicationContextInitializer 的启用要在 resource 文件夹下添加 META-INF&#x2F;spring.factories 12org.springframework.context.ApplicationContextInitializer=\\com.example.springbootlistener.listener.MyApplicationContextInitializer 1234567@Componentpublic class MyApplicationContextInitializer implements ApplicationContextInitializer &#123; @Override public void initialize(ConfigurableApplicationContext applicationContext) &#123; System.out.println(&quot;ApplicationContextInitializer....initialize&quot;); &#125;&#125; MySpringApplicationRunListener 的使用要添加构造器 12345678910111213141516171819202122232425262728293031323334353637383940public class MySpringApplicationRunListener implements SpringApplicationRunListener &#123;\t//构造器 public MySpringApplicationRunListener(SpringApplication sa, String[] args) &#123; &#125; @Override public void starting() &#123; System.out.println(&quot;starting...项目启动中&quot;);//输出SPRING之前 &#125; @Override public void environmentPrepared(ConfigurableEnvironment environment) &#123; System.out.println(&quot;environmentPrepared...环境对象开始准备&quot;); &#125; @Override public void contextPrepared(ConfigurableApplicationContext context) &#123; System.out.println(&quot;contextPrepared...上下文对象开始准备&quot;); &#125; @Override public void contextLoaded(ConfigurableApplicationContext context) &#123; System.out.println(&quot;contextLoaded...上下文对象开始加载&quot;); &#125; @Override public void started(ConfigurableApplicationContext context) &#123; System.out.println(&quot;started...上下文对象加载完成&quot;); &#125; @Override public void running(ConfigurableApplicationContext context) &#123; System.out.println(&quot;running...项目启动完成，开始运行&quot;); &#125; @Override public void failed(ConfigurableApplicationContext context, Throwable exception) &#123; System.out.println(&quot;failed...项目启动失败&quot;); &#125;&#125; 配置文件配置方式文件类型SpringBoot 是基于约定的，很多配置都有默认值，如果想使用自己的配置替换默认配置，可以使用 application.properties 或者 application.yml（application.yaml）进行配置 默认配置文件名称：application 在同一级目录下优先级为：properties &gt; yml &gt; yaml 例如配置内置 Tomcat 的端口 properties： 1server.port=8080 yml： 1server: port: 8080 yaml： 1server: port: 8080 加载顺序所有位置的配置文件都会被加载，互补配置，高优先级配置内容会覆盖低优先级配置内容 扫描配置文件的位置按优先级从高到底： file:./config/：当前项目下的 &#x2F;config 目录下 file:./：当前项目的根目录，Project工程目录 classpath:/config/：classpath 的 &#x2F;config 目录 classpath:/：classpath 的根目录，就是 resoureces 目录 项目外部配置文件加载顺序：外部配置文件的使用是为了对内部文件的配合 命令行：在 package 打包后的 target 目录下，使用该命令 1java -jar myproject.jar --server.port=9000 指定配置文件位置 1java -jar myproject.jar --spring.config.location=e://application.properties 按优先级从高到底选择配置文件的加载命令 1java -jar myproject.jar yaml语法基本语法： 大小写敏感 数据值前边必须有空格，作为分隔符 使用缩进表示层级关系 缩进时不允许使用Tab键，只允许使用空格（各个系统 Tab对应空格数目可能不同，导致层次混乱） 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 ‘’#” 表示注释，从这个字符一直到行尾，都会被解析器忽略 123server: port: 8080 address: 127.0.0.1 数据格式： 纯量：单个的、不可再分的值 12msg1: &#x27;hello world&#x27; # 单引忽略转义字符msg2: &quot;hello world&quot; # 双引识别转义字符 对象：键值对集合，Map、Hash 12345person: name: zhangsan age: 20# 行内写法person: &#123;name: zhangsan&#125; 注意：不建议使用 JSON，应该使用 yaml 语法 数组：一组按次序排列的值，List、Array 12345address: - beijing - shanghai# 行内写法address: [beijing,shanghai] 12345allPerson\t#List&lt;Person&gt; - &#123;name:lisi, age:18&#125; - &#123;name:wangwu, age:20&#125;# 行内写法allPerson: [&#123;name:lisi, age:18&#125;, &#123;name:wangwu, age:20&#125;] 参数引用： 123name: lisi person: name: $&#123;name&#125; # 引用上边定义的name值 获取配置三种获取配置文件的方式： 注解 @Value 1234567891011121314151617181920212223@RestControllerpublic class HelloController &#123; @Value(&quot;$&#123;name&#125;&quot;) private String name; @Value(&quot;$&#123;person.name&#125;&quot;) private String name2; @Value(&quot;$&#123;address[0]&#125;&quot;) private String address1; @Value(&quot;$&#123;msg1&#125;&quot;) private String msg1; @Value(&quot;$&#123;msg2&#125;&quot;) private String msg2; @RequestMapping(&quot;/hello&quot;) public String hello()&#123; System.out.println(&quot;所有的数据&quot;); return &quot; hello Spring Boot !&quot;; &#125;&#125; Evironment 对象 123456789@Autowiredprivate Environment env;@RequestMapping(&quot;/hello&quot;)public String hello() &#123; System.out.println(env.getProperty(&quot;person.name&quot;)); System.out.println(env.getProperty(&quot;address[0]&quot;)); return &quot; hello Spring Boot !&quot;;&#125; 注解 @ConfigurationProperties 配合 @Component 使用 注意：参数 prefix 一定要指定 1234567@Component\t//不扫描该组件到容器内，无法完成自动装配@ConfigurationProperties(prefix = &quot;person&quot;)public class Person &#123; private String name; private int age; private String[] address;&#125; 123456789@Autowiredprivate Person person;@RequestMapping(&quot;/hello&quot;)public String hello() &#123; System.out.println(person); //Person&#123;name=&#x27;zhangsan&#x27;, age=20, address=[beijing, shanghai]&#125; return &quot; hello Spring Boot !&quot;;&#125; 配置提示自定义的类和配置文件绑定一般没有提示，添加如下依赖可以使用提示： 1234567891011121314151617181920212223&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;!-- 下面插件作用是工程打包时，不将spring-boot-configuration-processor打进包内，让其只在编码的时候有用 --&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; Profile@Profile：指定组件在哪个环境的情况下才能被注册到容器中，不指定，任何环境下都能注册这个组件 加了环境标识的 bean，只有这个环境被激活的时候才能注册到容器中，默认是 default 环境 写在配置类上，只有是指定的环境的时候，整个配置类里面的所有配置才能开始生效 没有标注环境标识的 bean 在，任何环境下都是加载的 Profile 的配置： profile 是用来完成不同环境下，配置动态切换功能 profile 配置方式：多 profile 文件方式，提供多个配置文件，每个代表一种环境 application-dev.properties&#x2F;yml 开发环境 application-test.properties&#x2F;yml 测试环境 sapplication-pro.properties&#x2F;yml 生产环境 yml 多文档方式：在 yml 中使用 — 分隔不同配置 12345678910111213141516---server: port: 8081spring: profiles:dev---server: port: 8082spring: profiles:test---server: port: 8083spring: profiles:pro--- profile 激活方式 配置文件：在配置文件中配置：spring.profiles.active&#x3D;dev 1spring.profiles.active=dev 虚拟机参数：在VM options 指定：-Dspring.profiles.active=dev 命令行参数：java –jar xxx.jar --spring.profiles.active=dev 在 Program arguments 里输入，也可以先 package Web开发功能支持SpringBoot 自动配置了很多约定，大多场景都无需自定义配置 内容协商视图解析器 ContentNegotiatingViewResolver 和 BeanName 视图解析器 BeanNameViewResolver 支持静态资源（包括 webjars）和静态 index.html 页支持 自动注册相关类：Converter、GenericConverter、Formatter 内容协商处理器：HttpMessageConverters 国际化：MessageCodesResolver 开发规范： 使用 @Configuration + WebMvcConfigurer 自定义规则，不使用 @EnableWebMvc 注解 声明 WebMvcRegistrations 的实现类改变默认底层组件 使用 @EnableWebMvc + @Configuration + DelegatingWebMvcConfiguration 全面接管 SpringMVC 静态资源访问规则默认的静态资源路径是 classpath 下的，优先级由高到低为：&#x2F;META-INF&#x2F;resources、&#x2F;resources、 &#x2F;static、&#x2F;public 的包内，/ 表示当前项目的根路径 静态映射 /** ，表示请求 / + 静态资源名 就直接去默认的资源路径寻找请求的资源 处理原理：静请求去寻找 Controller 处理，不能处理的请求就会交给静态资源处理器，静态资源也找不到就响应 404 页面 修改默认资源路径： 1234spring: web: resources: static-locations:: [classpath:/haha/] 修改静态资源访问前缀，默认是 /**： 123spring: mvc: static-path-pattern: /resources/** 访问 URL：http://localhost:8080/resources/ + 静态资源名，将所有资源重定位到 /resources/ webjar 访问资源： 12345&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt;&lt;/dependency&gt; 访问地址：http://localhost:8080/webjars/jquery/3.5.1/jquery.js，后面地址要按照依赖里面的包路径 欢迎页面静态资源路径下 index.html 默认作为欢迎页面，访问 http://localhost:8080 出现该页面，使用 welcome page 功能不能修改前缀 网页标签上的小图标可以自定义规则，把资源重命名为 favicon.ico 放在静态资源目录下即可 源码分析SpringMVC 功能的自动配置类 WebMvcAutoConfiguration： 1234public class WebMvcAutoConfiguration &#123; //当前项目的根路径 private static final String SERVLET_LOCATION = &quot;/&quot;;&#125; 内部类 WebMvcAutoConfigurationAdapter： 123456789101112131415161718@Import(EnableWebMvcConfiguration.class)// 绑定 spring.mvc、spring.web、spring.resources 相关的配置属性@EnableConfigurationProperties(&#123; WebMvcProperties.class,ResourceProperties.class, WebProperties.class &#125;)@Order(0)public static class WebMvcAutoConfigurationAdapter implements WebMvcConfigurer, ServletContextAware &#123;\t//有参构造器所有参数的值都会从容器中确定 public WebMvcAutoConfigurationAdapter(/*参数*/) &#123; this.resourceProperties = resourceProperties.hasBeenCustomized() ? resourceProperties : webProperties.getResources(); this.mvcProperties = mvcProperties; this.beanFactory = beanFactory; this.messageConvertersProvider = messageConvertersProvider; this.resourceHandlerRegistrationCustomizer = resourceHandlerRegistrationCustomizerProvider.getIfAvailable(); this.dispatcherServletPath = dispatcherServletPath; this.servletRegistrations = servletRegistrations; this.mvcProperties.checkConfiguration();\t&#125;&#125; ResourceProperties resourceProperties：获取和 spring.resources 绑定的所有的值的对象 WebMvcProperties mvcProperties：获取和 spring.mvc 绑定的所有的值的对象 ListableBeanFactory beanFactory：Spring 的 beanFactory HttpMessageConverters：找到所有的 HttpMessageConverters ResourceHandlerRegistrationCustomizer：找到 资源处理器的自定义器。 DispatcherServletPath：项目路径 ServletRegistrationBean：给应用注册 Servlet、Filter WebMvcAutoConfiguration.WebMvcAutoConfigurationAdapter.addResourceHandler()：两种静态资源映射规则 123456789101112131415161718public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; //配置文件设置 spring.resources.add-mappings: false，禁用所有静态资源 if (!this.resourceProperties.isAddMappings()) &#123; logger.debug(&quot;Default resource handling disabled&quot;);//被禁用 return; &#125; //注册webjars静态资源的映射规则\t映射 路径 addResourceHandler(registry, &quot;/webjars/**&quot;, &quot;classpath:/META-INF/resources/webjars/&quot;); //注册静态资源路径的映射规则 默认映射 staticPathPattern = &quot;/**&quot; addResourceHandler(registry, this.mvcProperties.getStaticPathPattern(), (registration) -&gt; &#123; //staticLocations = CLASSPATH_RESOURCE_LOCATIONS registration.addResourceLocations(this.resourceProperties.getStaticLocations()); if (this.servletContext != null) &#123; ServletContextResource resource = new ServletContextResource(this.servletContext, SERVLET_LOCATION); registration.addResourceLocations(resource); &#125; &#125;);&#125; 123456789101112131415@ConfigurationProperties(&quot;spring.web&quot;)public class WebProperties &#123; public static class Resources &#123; //默认资源路径，优先级从高到低 static final String[] CLASSPATH_RESOURCE_LOCATIONS = &#123; &quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; &#125; private String[] staticLocations = CLASSPATH_RESOURCE_LOCATIONS; //可以进行规则重写 public void setStaticLocations(String[] staticLocations) &#123; this.staticLocations = appendSlashIfNecessary(staticLocations); this.customized = true; &#125; &#125;&#125; WebMvcAutoConfiguration.EnableWebMvcConfiguration.welcomePageHandlerMapping()：欢迎页 12345678910111213141516171819202122232425//spring.web 属性@EnableConfigurationProperties(WebProperties.class)public static class EnableWebMvcConfiguration &#123; @Bean public WelcomePageHandlerMapping welcomePageHandlerMapping(/*参数*/) &#123; WelcomePageHandlerMapping welcomePageHandlerMapping = new WelcomePageHandlerMapping( new TemplateAvailabilityProviders(applicationContext), applicationContext, getWelcomePage(), //staticPathPattern = &quot;/**&quot; this.mvcProperties.getStaticPathPattern()); return welcomePageHandlerMapping; &#125;&#125;WelcomePageHandlerMapping(/*参数*/) &#123; //所以限制 staticPathPattern 必须为 /** 才能启用该功能 if (welcomePage != null &amp;&amp; &quot;/**&quot;.equals(staticPathPattern)) &#123; logger.info(&quot;Adding welcome page: &quot; + welcomePage); //重定向 setRootViewName(&quot;forward:index.html&quot;); &#125; else if (welcomeTemplateExists(templateAvailabilityProviders, applicationContext)) &#123; logger.info(&quot;Adding welcome page template: index&quot;); setRootViewName(&quot;index&quot;); &#125;&#125; WelcomePageHandlerMapping，访问 &#x2F; 能访问到 index.html Rest映射开启 Rest 功能 12345spring: mvc: hiddenmethod: filter: enabled: true #开启页面表单的Rest功能 源码分析，注入了 HiddenHttpMethodFilte 解析 Rest 风格的访问： 12345678public class WebMvcAutoConfiguration &#123; @Bean\t@ConditionalOnMissingBean(HiddenHttpMethodFilter.class)\t@ConditionalOnProperty(prefix = &quot;spring.mvc.hiddenmethod.filter&quot;, name = &quot;enabled&quot;)\tpublic OrderedHiddenHttpMethodFilter hiddenHttpMethodFilter() &#123; return new OrderedHiddenHttpMethodFilter();\t&#125;&#125; 详细源码解析：SpringMVC → 基本操作 → Restful → 识别原理 Web 部分源码详解：SpringMVC → 运行原理 内嵌容器SpringBoot 嵌入式 Servlet 容器，默认支持的 WebServe：Tomcat、Jetty、Undertow 配置方式： 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;!--必须要把内嵌的 Tomcat 容器--&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt;&lt;/dependency&gt; Web 应用启动，SpringBoot 导入 Web 场景包 tomcat，创建一个 Web 版的 IOC 容器： SpringApplication.run(BootApplication.class, args)：应用启动 ConfigurableApplicationContext.run()： context = createApplicationContext()：创建容器 applicationContextFactory = ApplicationContextFactory.DEFAULT 1234567891011121314151617ApplicationContextFactory DEFAULT = (webApplicationType) -&gt; &#123; try &#123; switch (webApplicationType) &#123; case SERVLET: // Servlet 容器，继承自 ServletWebServerApplicationContext return new AnnotationConfigServletWebServerApplicationContext(); case REACTIVE: // 响应式编程 return new AnnotationConfigReactiveWebServerApplicationContext(); default: // 普通 Spring 容器 return new AnnotationConfigApplicationContext(); &#125; &#125; catch (Exception ex) &#123; throw new IllegalStateException(); &#125;&#125; applicationContextFactory.create(this.webApplicationType)：根据应用类型创建容器 refreshContext(context)：容器启动刷新 内嵌容器工作流程： Spring 容器启动逻辑中，在实例化非懒加载的单例 Bean 之前有一个方法 **onRefresh()**，留给子类去扩展，Web 容器就是重写这个方法创建 WebServer 123456789protected void onRefresh() &#123; //省略....\tcreateWebServer();&#125;private void createWebServer() &#123; ServletWebServerFactory factory = getWebServerFactory(); this.webServer = factory.getWebServer(getSelfInitializer()); createWebServer.end();&#125; 获取 WebServer 工厂 ServletWebServerFactory，并且获取的数量不等于 1 会报错，Spring 底层有三种： TomcatServletWebServerFactory、JettyServletWebServerFactory、UndertowServletWebServerFactory 自动配置类 ServletWebServerFactoryAutoConfiguration 导入了 ServletWebServerFactoryConfiguration（配置类），根据条件装配判断系统中到底导入了哪个 Web 服务器的包，创建出服务器并启动 默认是 web-starter 导入 tomcat 包，容器中就有 TomcatServletWebServerFactory，创建出 Tomcat 服务器并启动， 1234public TomcatWebServer(Tomcat tomcat, boolean autoStart, Shutdown shutdown) &#123;\t// 初始化 initialize();&#125; 初始化方法 initialize 中有启动方法：this.tomcat.start() 自定义定制规则1234567891011@Configurationpublic class MyWebMvcConfigurer implements WebMvcConfigurer &#123; @Bean public WebMvcConfigurer webMvcConfigurer() &#123; return new WebMvcConfigurer() &#123; //进行一些方法重写，来实现自定义的规则 //比如添加一些解析器和拦截器，就是对原始容器功能的增加 &#125; &#125; //也可以不加 @Bean，直接从这里重写方法进行功能增加&#125; 定制容器@EnableWebMvc：全面接管 SpringMVC，所有规则全部自己重新配置 @EnableWebMvc + WebMvcConfigurer + @Bean 全面接管SpringMVC @Import(DelegatingWebMvcConfiguration.class)，该类继承 WebMvcConfigurationSupport，自动配置了一些非常底层的组件，只能保证 SpringMVC 最基本的使用 原理：自动配置类 WebMvcAutoConfiguration 里面的配置要能生效，WebMvcConfigurationSupport 类不能被加载，所以 @EnableWebMvc 导致配置类失效，从而接管了 SpringMVC 12@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)public class WebMvcAutoConfiguration &#123;&#125; 注意：一般不适用此注解 数据访问JDBC基本使用导入 starter： 1234567891011&lt;!--导入 JDBC 场景--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--导入 MySQL 驱动--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!--版本对应你的 MySQL 版本&lt;version&gt;5.1.49&lt;/version&gt;--&gt;&lt;/dependency&gt; 单独导入 MySQL 驱动是因为不确定用户使用的什么数据库 配置文件： 123456spring: datasource: url: jdbc:mysql://192.168.0.107:3306/db1?useSSL=false\t# 不加 useSSL 会警告 username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver 测试文件： 12345678910111213@Slf4j@SpringBootTestclass Boot05WebAdminApplicationTests &#123; @Autowired JdbcTemplate jdbcTemplate; @Test void contextLoads() &#123; Long res = jdbcTemplate.queryForObject(&quot;select count(*) from account_tbl&quot;, Long.class); log.info(&quot;记录总数：&#123;&#125;&quot;, res); &#125;&#125; 自动配置DataSourceAutoConfiguration：数据源的自动配置 1234567891011121314@Configuration(proxyBeanMethods = false)@ConditionalOnClass(&#123; DataSource.class, EmbeddedDatabaseType.class &#125;)@EnableConfigurationProperties(DataSourceProperties.class)public class DataSourceAutoConfiguration &#123; @Conditional(PooledDataSourceCondition.class) @ConditionalOnMissingBean(&#123; DataSource.class, XADataSource.class &#125;)\t@Import(&#123; DataSourceConfiguration.Hikari.class, DataSourceConfiguration.Tomcat.class, DataSourceConfiguration.Dbcp2.class, DataSourceConfiguration.OracleUcp.class&#125;)\tprotected static class PooledDataSourceConfiguration &#123;&#125;&#125;// 配置项@ConfigurationProperties(prefix = &quot;spring.datasource&quot;)public class DataSourceProperties implements BeanClassLoaderAware, InitializingBean &#123;&#125; 底层默认配置好的连接池是：HikariDataSource 数据库连接池的配置，是容器中没有 DataSource 才自动配置的 修改数据源相关的配置：spring.datasource 相关配置： DataSourceTransactionManagerAutoConfiguration： 事务管理器的自动配置 JdbcTemplateAutoConfiguration： JdbcTemplate 的自动配置 可以修改这个配置项 @ConfigurationProperties(prefix &#x3D; “spring.jdbc”) 来修改JdbcTemplate @AutoConfigureAfter(DataSourceAutoConfiguration.class)：在 DataSource 装配后装配 JndiDataSourceAutoConfiguration： jndi 的自动配置 XADataSourceAutoConfiguration： 分布式事务相关 Druid导入坐标： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.17&lt;/version&gt;&lt;/dependency&gt; 123456789@Configuration@ConditionalOnClass(DruidDataSource.class)@AutoConfigureBefore(DataSourceAutoConfiguration.class)@EnableConfigurationProperties(&#123;DruidStatProperties.class, DataSourceProperties.class&#125;)@Import(&#123;DruidSpringAopConfiguration.class, DruidStatViewServletConfiguration.class, DruidWebStatFilterConfiguration.class, DruidFilterConfiguration.class&#125;)public class DruidDataSourceAutoConfigure &#123;&#125; 自动配置： 扩展配置项 spring.datasource.druid DruidSpringAopConfiguration： 监控 SpringBean，配置项为 spring.datasource.druid.aop-patterns DruidStatViewServletConfiguration：监控页的配置项为 spring.datasource.druid.stat-view-servlet，默认开启 DruidWebStatFilterConfiguration：Web 监控配置项为 spring.datasource.druid.web-stat-filter，默认开启 DruidFilterConfiguration：所有 Druid 自己 filter 的配置 配置示例： 1234567891011121314151617181920212223242526272829303132spring: datasource: url: jdbc:mysql://localhost:3306/db_account username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver druid: aop-patterns: com.atguigu.admin.* #监控SpringBean filters: stat,wall # 底层开启功能，stat（sql监控），wall（防火墙） stat-view-servlet: # 配置监控页功能 enabled: true login-username: admin\t#项目启动访问：http://localhost:8080/druid ，账号和密码是 admin login-password: admin resetEnable: false web-stat-filter: # 监控web enabled: true urlPattern: /* exclusions: &#x27;*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*&#x27; filter: stat: # 对上面filters里面的stat的详细配置 slow-sql-millis: 1000 logSlowSql: true enabled: true wall: enabled: true config: drop-table-allow: false 配置示例：https://github.com/alibaba/druid/tree/master/druid-spring-boot-starter 配置项列表：https://github.com/alibaba/druid/wiki/DruidDataSource%E9%85%8D%E7%BD%AE%E5%B1%9E%E6%80%A7%E5%88%97%E8%A1%A8 MyBatis基本使用导入坐标： 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.4&lt;/version&gt;&lt;/dependency&gt; 编写 MyBatis 相关配置：application.yml 12345678# 配置mybatis规则mybatis:# config-location: classpath:mybatis/mybatis-config.xml 建议不写 mapper-locations: classpath:mybatis/mapper/*.xml configuration: map-underscore-to-camel-case: true #可以不写全局配置文件，所有全局配置文件的配置都放在 configuration 配置项中即可 定义表和实体类 12345public class User &#123; private int id; private String username; private String password;&#125; 编写 dao 和 mapper 文件&#x2F;纯注解开发 dao：**@Mapper 注解必须加，使用自动装配的 package，否则在启动类指定 @MapperScan() 扫描路径（不建议）** 12345@Mapper //必须加Mapper@Repositorypublic interface UserXmlMapper &#123; public List&lt;User&gt; findAll();&#125; mapper.xml 1234567&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.seazean.springbootmybatis.mapper.UserXmlMapper&quot;&gt; &lt;select id=&quot;findAll&quot; resultType=&quot;user&quot;&gt; select * from t_user &lt;/select&gt;&lt;/mapper&gt; 纯注解开发 123456@Mapper@Repositorypublic interface UserMapper &#123; @Select(&quot;select * from t_user&quot;) public List&lt;User&gt; findAll();&#125; 自动配置MybatisAutoConfiguration： 123456789101112131415161718@EnableConfigurationProperties(MybatisProperties.class)\t//MyBatis配置项绑定类。@AutoConfigureAfter(&#123; DataSourceAutoConfiguration.class, MybatisLanguageDriverAutoConfiguration.class &#125;)public class MybatisAutoConfiguration &#123; @Bean @ConditionalOnMissingBean public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean factory = new SqlSessionFactoryBean(); return factory.getObject(); &#125; @org.springframework.context.annotation.Configuration @Import(AutoConfiguredMapperScannerRegistrar.class) @ConditionalOnMissingBean(&#123; MapperFactoryBean.class, MapperScannerConfigurer.class &#125;) public static class MapperScannerRegistrarNotFoundConfiguration implements InitializingBean &#123;&#125;&#125;@ConfigurationProperties(prefix = &quot;mybatis&quot;)public class MybatisProperties &#123;&#125; 配置文件：mybatis 自动配置了 SqlSessionFactory 导入 AutoConfiguredMapperScannerRegistra 实现 @Mapper 的扫描 MyBatis-Plus12345&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.4.1&lt;/version&gt;&lt;/dependency&gt; 自动配置类：MybatisPlusAutoConfiguration 只需要 Mapper 继承 BaseMapper 就可以拥有 CRUD 功能 Redis基本使用1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 配置redis相关属性 1234spring: redis: host: 127.0.0.1 # redis的主机ip port: 6379 注入 RedisTemplate 模板 123456789101112131415161718@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringbootRedisApplicationTests &#123; @Autowired private RedisTemplate redisTemplate; @Test public void testSet() &#123; //存入数据 redisTemplate.boundValueOps(&quot;name&quot;).set(&quot;zhangsan&quot;); &#125; @Test public void testGet() &#123; //获取数据 Object name = redisTemplate.boundValueOps(&quot;name&quot;).get(); System.out.println(name); &#125;&#125; 自动配置RedisAutoConfiguration 自动配置类 123456789101112131415161718192021222324@Configuration(proxyBeanMethods = false)@ConditionalOnClass(RedisOperations.class)@EnableConfigurationProperties(RedisProperties.class)@Import(&#123; LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class &#125;)public class RedisAutoConfiguration &#123; @Bean @ConditionalOnMissingBean(name = &quot;redisTemplate&quot;) @ConditionalOnSingleCandidate(RedisConnectionFactory.class) public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; &#125; @Bean @ConditionalOnMissingBean @ConditionalOnSingleCandidate(RedisConnectionFactory.class) public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; &#125;&#125; 配置项：spring.redis 自动导入了连接工厂配置类：LettuceConnectionConfiguration、JedisConnectionConfiguration 自动注入了模板类：RedisTemplate&lt;Object, Object&gt; 、StringRedisTemplate，k v 都是 String 类型 使用 @Autowired 注入模板类就可以操作 redis 单元测试Junit5Spring Boot 2.2.0 版本开始引入 JUnit 5 作为单元测试默认库，由三个不同的子模块组成： JUnit Platform：在 JVM 上启动测试框架的基础，不仅支持 Junit 自制的测试引擎，其他测试引擎也可以接入 JUnit Jupiter：提供了 JUnit5 的新的编程模型，是 JUnit5 新特性的核心，内部包含了一个测试引擎，用于在 Junit Platform 上运行 JUnit Vintage：JUnit Vintage 提供了兼容 JUnit4.x、Junit3.x 的测试引擎 注意：SpringBoot 2.4 以上版本移除了默认对 Vintage 的依赖，如果需要兼容 Junit4 需要自行引入 12345@SpringBootTestclass Boot05WebAdminApplicationTests &#123; @Test void contextLoads() &#123; &#125;&#125; 常用注解JUnit5 的注解如下： @Test：表示方法是测试方法，但是与 JUnit4 的 @Test 不同，它的职责非常单一不能声明任何属性，拓展的测试将会由 Jupiter 提供额外测试，包是 org.junit.jupiter.api.Test @ParameterizedTest：表示方法是参数化测试 @RepeatedTest：表示方法可重复执行 @DisplayName：为测试类或者测试方法设置展示名称 @BeforeEach：表示在每个单元测试之前执行 @AfterEach：表示在每个单元测试之后执行 @BeforeAll：表示在所有单元测试之前执行 @AfterAll：表示在所有单元测试之后执行 @Tag：表示单元测试类别，类似于 JUnit4 中的 @Categories @Disabled：表示测试类或测试方法不执行，类似于 JUnit4 中的 @Ignore @Timeout：表示测试方法运行如果超过了指定时间将会返回错误 @ExtendWith：为测试类或测试方法提供扩展类引用 断言机制简单断言断言（assertions）是测试方法中的核心，用来对测试需要满足的条件进行验证，断言方法都是 org.junit.jupiter.api.Assertions 的静态方法 用来对单个值进行简单的验证： 方法 说明 assertEquals 判断两个对象或两个原始类型是否相等 assertNotEquals 判断两个对象或两个原始类型是否不相等 assertSame 判断两个对象引用是否指向同一个对象 assertNotSame 判断两个对象引用是否指向不同的对象 assertTrue 判断给定的布尔值是否为 true assertFalse 判断给定的布尔值是否为 false assertNull 判断给定的对象引用是否为 null assertNotNull 判断给定的对象引用是否不为 null 1234567@Test@DisplayName(&quot;simple assertion&quot;)public void simple() &#123; assertEquals(3, 1 + 2, &quot;simple math&quot;); assertNull(null); assertNotNull(new Object());&#125; 数组断言通过 assertArrayEquals 方法来判断两个对象或原始类型的数组是否相等 12345@Test@DisplayName(&quot;array assertion&quot;)public void array() &#123; assertArrayEquals(new int[]&#123;1, 2&#125;, new int[] &#123;1, 2&#125;);&#125; 组合断言assertAll 方法接受多个 org.junit.jupiter.api.Executable 函数式接口的实例作为验证的断言，可以通过 lambda 表达式提供这些断言 12345678@Test@DisplayName(&quot;assert all&quot;)public void all() &#123;\tassertAll(&quot;Math&quot;, () -&gt; assertEquals(2, 1 + 1), () -&gt; assertTrue(1 &gt; 0) );&#125; 异常断言Assertions.assertThrows()，配合函数式编程就可以进行使用 12345678@Test@DisplayName(&quot;异常测试&quot;)public void exceptionTest() &#123; ArithmeticException exception = Assertions.assertThrows( //扔出断言异常 ArithmeticException.class, () -&gt; System.out.println(1 / 0) );&#125; 超时断言Assertions.assertTimeout() 为测试方法设置了超时时间 123456@Test@DisplayName(&quot;超时测试&quot;)public void timeoutTest() &#123; //如果测试方法时间超过1s将会异常 Assertions.assertTimeout(Duration.ofMillis(1000), () -&gt; Thread.sleep(500));&#125; 快速失败通过 fail 方法直接使得测试失败 12345@Test@DisplayName(&quot;fail&quot;)public void shouldFail() &#123;\tfail(&quot;This should fail&quot;);&#125; 前置条件JUnit 5 中的前置条件（assumptions）类似于断言，不同之处在于不满足的断言会使得测试方法失败，而不满足的前置条件只会使得测试方法的执行终止，前置条件可以看成是测试方法执行的前提，当该前提不满足时，就没有继续执行的必要 1234567@DisplayName(&quot;测试前置条件&quot;)@Testvoid testassumptions()&#123; Assumptions.assumeTrue(false,&quot;结果不是true&quot;); System.out.println(&quot;111111&quot;);&#125; 嵌套测试JUnit 5 可以通过 Java 中的内部类和 @Nested 注解实现嵌套测试，从而可以更好的把相关的测试方法组织在一起，在内部类中可以使用 @BeforeEach 和 @AfterEach 注解，而且嵌套的层次没有限制 123456789101112131415161718192021222324252627282930313233@DisplayName(&quot;A stack&quot;)class TestingAStackDemo &#123; Stack&lt;Object&gt; stack; @Test @DisplayName(&quot;is instantiated with new Stack()&quot;) void isInstantiatedWithNew() &#123; assertNull(stack) &#125; @Nested @DisplayName(&quot;when new&quot;) class WhenNew &#123; @BeforeEach void createNewStack() &#123; stack = new Stack&lt;&gt;(); &#125; @Test @DisplayName(&quot;is empty&quot;) void isEmpty() &#123; assertTrue(stack.isEmpty()); &#125; @Test @DisplayName(&quot;throws EmptyStackException when popped&quot;) void throwsExceptionWhenPopped() &#123; assertThrows(EmptyStackException.class, stack::pop); &#125; &#125;&#125; 参数测试参数化测试是 JUnit5 很重要的一个新特性，它使得用不同的参数多次运行测试成为了可能 利用**@ValueSource**等注解，指定入参，我们将可以使用不同的参数进行多次单元测试，而不需要每新增一个参数就新增一个单元测试，省去了很多冗余代码。 @ValueSource：为参数化测试指定入参来源，支持八大基础类以及 String 类型、Class 类型 @NullSource：表示为参数化测试提供一个 null 的入参 @EnumSource：表示为参数化测试提供一个枚举入参 @CsvFileSource：表示读取指定 CSV 文件内容作为参数化测试入参 @MethodSource：表示读取指定方法的返回值作为参数化测试入参（注意方法返回需要是一个流） 指标监控Actuator每一个微服务在云上部署以后，都需要对其进行监控、追踪、审计、控制等，SpringBoot 抽取了 Actuator 场景，使得每个微服务快速引用即可获得生产级别的应用监控、审计等功能 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 暴露所有监控信息为 HTTP： 123456management: endpoints: enabled-by-default: true #暴露所有端点信息 web: exposure: include: &#x27;*&#x27; #以web方式暴露 访问 http://localhost:8080/actuator/[beans/health/metrics/] 可视化界面：https://github.com/codecentric/spring-boot-admin Endpoint默认所有的 Endpoint 除过 shutdown 都是开启的 12345678management: endpoints: enabled-by-default: false\t#禁用所有的 endpoint: #手动开启一部分 beans: enabled: true health: enabled: true 端点： ID 描述 auditevents 暴露当前应用程序的审核事件信息。需要一个 AuditEventRepository 组件 beans 显示应用程序中所有 Spring Bean 的完整列表 caches 暴露可用的缓存 conditions 显示自动配置的所有条件信息，包括匹配或不匹配的原因 configprops 显示所有 @ConfigurationProperties env 暴露 Spring 的属性 ConfigurableEnvironment flyway 显示已应用的所有 Flyway 数据库迁移。 需要一个或多个 Flyway 组件。 health 显示应用程序运行状况信息 httptrace 显示 HTTP 跟踪信息，默认情况下 100 个 HTTP 请求-响应需要一个 HttpTraceRepository 组件 info 显示应用程序信息 integrationgraph 显示 Spring integrationgraph，需要依赖 spring-integration-core loggers 显示和修改应用程序中日志的配置 liquibase 显示已应用的所有 Liquibase 数据库迁移，需要一个或多个 Liquibase 组件 metrics 显示当前应用程序的指标信息。 mappings 显示所有 @RequestMapping 路径列表 scheduledtasks 显示应用程序中的计划任务 sessions 允许从 Spring Session 支持的会话存储中检索和删除用户会话，需要使用 Spring Session 的基于 Servlet 的 Web 应用程序 shutdown 使应用程序正常关闭，默认禁用 startup 显示由 ApplicationStartup 收集的启动步骤数据。需要使用 SpringApplication 进行配置 BufferingApplicationStartup threaddump 执行线程转储 应用程序是 Web 应用程序（Spring MVC，Spring WebFlux 或 Jersey），则可以使用以下附加端点： ID 描述 heapdump 返回 hprof 堆转储文件。 jolokia 通过 HTTP 暴露 JMX bean（需要引入 Jolokia，不适用于 WebFlux），需要引入依赖 jolokia-core logfile 返回日志文件的内容（如果已设置 logging.file.name 或 logging.file.path 属性），支持使用 HTTP Range标头来检索部分日志文件的内容。 prometheus 以 Prometheus 服务器可以抓取的格式公开指标，需要依赖 micrometer-registry-prometheus 常用 Endpoint： Health：监控状况 Metrics：运行时指标 Loggers：日志记录 项目部署SpringBoot 项目开发完毕后，支持两种方式部署到服务器： jar 包 (官方推荐，默认) war 包 更改 pom 文件中的打包方式为 war 修改启动类 1234567891011@SpringBootApplicationpublic class SpringbootDeployApplication extends SpringBootServletInitializer &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootDeployApplication.class, args); &#125; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder b) &#123; return b.sources(SpringbootDeployApplication.class); &#125;&#125; 指定打包的名称 12345678910&lt;packaging&gt;war&lt;/packaging&gt;&lt;build&gt; &lt;finalName&gt;springboot&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; Cloud基本介绍SpringCloud 是分布式微服务的一站式解决方案，是多种微服务落地技术的集合体，俗称微服务全家桶 参考文档：https://www.yuque.com/mrlinxi/pxvr4g/wcwd39 服务注册Eureka基本介绍Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务治理。Eureka 采用了 CS(Client-Server) 的设计架构，Eureka Server 是服务注册中心，系统中的其他微服务使用 Eureka 的客户端连接到 Eureka Server 并维持心跳连接 Eureka Server 提供服务注册服务：各个微服务节点通过配置启动后，会在 EurekaServer 中进行注册，EurekaServer 中的服务注册表中将会存储所有可用服务节点的信息，并且具有可视化界面 Eureka Client 通过注册中心进行访问：用于简化 Eureka Server的交互，客户端也具备一个内置的、使用轮询 (round-robin) 负载算法的负载均衡器。在应用启动后将会向 Eureka Server 发送心跳（默认周期为30秒），如果 Eureka Server 在多个心跳周期内没有接收到某个节点的心跳，将会从服务注册表中把这个服务节点移除（默认 90 秒） 服务端服务器端主启动类增加 @EnableEurekaServer 注解，指定该模块作为 Eureka 注册中心的服务器 构建流程如下： 主启动类 1234567@SpringBootApplication@EnableEurekaServer // 表示当前是Eureka的服务注册中心public class EurekaMain7001 &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaMain7001.class, args); &#125;&#125; 修改 pom 文件 12345678910111213141.x: server跟client合在一起&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt;2.x： server跟client分开&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 修改 application.yml 文件 1234567891011121314server: port: 7001eureka: instance: hostname: localhost # eureka服务端的实例名称 client: # false表示不向注册中心注册自己。 register-with-eureka: false # false表示自己端就是注册中心，职责就是维护服务实例，并不需要去检索服务 fetch-registry: false service-url: # 设置与 Eureka Server 交互的地址查询服务和注册服务都需要依赖这个地址。 defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ 游览器访问 http://localhost:7001 客户端生产者服务器端主启动类需要增加 @EnableEurekaClient 注解，表示这是一个 Eureka 客户端，要注册进 EurekaServer 中 主启动类：PaymentMain8001 1234567@SpringBootApplication@EnableEurekaClientpublic class PaymentMain8001 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentMain8001.class, args); &#125;&#125; 修改 pom 文件：添加一个 Eureka-Client 依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 写 yml 文件 1234567891011121314server: port: 8001 eureka: client: # 表示将自己注册进EurekaServer默认为true register-with-eureka: true # 表示可以从Eureka抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合ribbon使用负载均衡 fetch-registry: true service-url: defaultZone: http://localhost:7001/eureka instance: instance-id: payment8001 # 只暴露服务名，不带有主机名 prefer-ip-address: true # 访问信息有 IP 信息提示(鼠标停留在服务名称上时) 游览器访问 http://localhost:7001 消费者 主启动类：PaymentMain8001 12345678@SpringBootApplication@EnableEurekaClient@EnableDiscoveryClientpublic class PaymentMain8001 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentMain8001.class, args); &#125;&#125; pom 文件同生产者 写 yml 文件 12345678910111213server: port: 80# 微服务名称spring: application: name: cloud-order-serviceeureka: client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://localhost:7001/eureka 浏览器访问 http://localhost:7001 集群构建服务端Server 端高可用集群原理：实现负载均衡和故障容错，互相注册，相互守望 多台 Eureka 服务器，每一台 Eureka 服务器需要有自己的主机名，同时各服务器需要相互注册 Eureka1： 1234567891011121314151617server: port: 7001eureka: instance: hostname: eureka7001.com client: register-with-eureka: false fetch-registry: false service-url: # 设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。 # 单机就是自己 # defaultZone: http://eureka7001.com:7001/eureka/ # 集群指向其他eureka #defaultZone: http://eureka7002.com:7002/eureka/ # 写成这样可以直接通过可视化页面跳转到7002 defaultZone: http://eureka7002.com:7002/ Eureka2： 123456789101112server: port: 7002eureka: instance: hostname: eureka7002.com client: register-with-eureka: false fetch-registry: false service-url: #写成这样可以直接通过可视化页面跳转到7001 defaultZone: http://eureka7001.com:7001/ 主启动类： 1234567@SpringBootApplication@EnableEurekaServerpublic class EurekaMain7002 &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaMain7002.class, args); &#125;&#125; 访问 http://eureka7001.com:7001 和 http://eureka7002.com:7002： RPC 调用：controller.OrderController 123456789101112131415@RestController@Slf4jpublic class OrderController &#123; public static final String PAYMENT_URL = &quot;http://localhost:8001&quot;; @Autowired private RestTemplate restTemplate; // CommonResult 是一个公共的返回类型 @GetMapping(&quot;/consumer/payment/get/&#123;id&#125;&quot;) public CommonResult&lt;Payment&gt; getPayment(@PathVariable(&quot;id&quot;) long id) &#123; // 返回对象为响应体中数据转化成的对象，基本上可以理解为JSON return restTemplate.getForObject(PAYMENT_URL + &quot;/payment/get/&quot; + id, CommonResult.class); &#125;&#125; 生产者构建 PaymentMain8001 的服务集群 主启动类 12345678@SpringBootApplication@EnableEurekaClient@EnableDiscoveryClientpublic class PaymentMain8002 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentMain8002.class, args); &#125;&#125; 写 yml 文件：端口修改，并且 spring.application.name 均为 cloud-payment-service 123456789101112131415server: port: 8002spring: application: name: cloud-payment-service eureka: client: # 表示将自己注册进EurekaServer默认为true register-with-eureka: true # 表示可以从Eureka抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合ribbon使用负载均衡 fetch-registry: true service-url: defaultZone: http://localhost:7001/eureka 负载均衡消费者端的 Controller 12// public static final String PAYMENT_URL = &quot;http://localhost:8001&quot;;public static final String PAYMENT_URL = &quot;http://localhost:8002&quot;; 由于已经建立了生产者集群，所以可以进行负载均衡的操作： Controller：只修改 PAYMENT_URL 会报错，因为 CLOUD-PAYMENT-SERVICE 对应多个微服务，需要规则来判断调用哪个端口 1public static final String PAYMENT_URL = &quot;http://CLOUD-PAYMENT-SERVICE&quot;; 使用 @LoadBlanced 注解赋予 RestTemplate 负载均衡的能力，增加 config.ApplicationContextConfig 文件： 12345678@Configurationpublic class ApplicationContextConfig &#123; @Bean @LoadBalanced public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125;&#125; 服务发现服务发现：对于注册进 Eureka 里面的微服务，可以通过服务发现来获得该服务的信息 主启动类增加注解 @EnableDiscoveryClient： 12345678@SpringBootApplication@EnableEurekaClient@EnableDiscoveryClientpublic class PaymentMain8001 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentMain8001.class, args); &#125;&#125; 修改生产者的 Controller 1234567891011121314151617181920@RestController@Slf4jpublic class PaymentController &#123; @Autowired private DiscoveryClient discoveryClient; @GetMapping(value = &quot;/payment/discovery&quot;) public Object discovery() &#123; List&lt;String&gt; services = discoveryClient.getServices(); for (String service : services) &#123; log.info(&quot;**** element:&quot; + service); &#125; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(&quot;PAYMENT-SERVICE&quot;); for (ServiceInstance instance : instances) &#123; log.info(instance.getServiceId() + &quot;\\t&quot; + instance.getHost() + &quot;\\t&quot; + instance.getPort()); &#125; return this.discoveryClient; &#125;&#125; 自我保护保护模式用于客户端和 EurekaServer 之间存在网络分区场景下的保护，一旦进入保护模式 EurekaServer 将会尝试保护其服务注册表中的信息，不在删除服务注册表中的数据，属于 CAP 里面的 AP 思想（可用性和分区容错性） 如果一定时间内丢失大量该微服务的实例，这时 Eureka 就会开启自我保护机制，不会剔除该服务。 因为这个现象可能是因为网络暂时不通，出现了 Eureka 的假死、拥堵、卡顿，客户端恢复后还能正常发送心跳 禁止自我保护： Server： 12345eureka: server: # 关闭自我保护机制，不可用的服务直接删除 enable-self-preservation: false eviction-interval-timer-in-ms: 2000 Client： 123456eureka: instance: # Eureka客户端向服务端发送心跳的时间间隔默认30秒 lease-renewal-interval-in-seconds: 1 # Eureka服务端在收到最后一次心跳后，90s没有收到心跳，剔除服务 lease-expiration-duration-in-seconds: 2 Consul基本介绍Consul 是开源的分布式服务发现和配置管理系统，采用 Go 语言开发，官网：https://developer.hashicorp.com/consul 提供了微服务系统中心的服务治理，配置中心，控制总线等功能 基于 Raft 协议，支持健康检查，同时支持 HTTP 和 DNS 协议支持跨数据中心的 WAN 集群 提供图形界面 下载 Consul 后，运行指令：consul -version 12345D:\\Program Files\\Java&gt;consul -versionConsul v1.15.1Revision 7c04b6a0Build Date 2023-03-07T20:35:33ZProtocol 2 spoken by default, understands 2 to 3 (.....) 启动命令： 1consul agent -dev 访问浏览器：http://localhost:8500/ 中文文档：https://www.springcloud.cc/spring-cloud-consul.html 基本使用无需 Server 端代码的编写 生产者： 引入 pom 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;&lt;/dependency&gt; application.yml： 1234567891011121314###consul 服务端口号server: port: 8006spring: application: name: consul-provider-payment ####consul注册中心地址 cloud: consul: host: localhost port: 8500 discovery: service-name: $&#123;spring.application.name&#125; 主启动类： 1234567@SpringBootApplication@EnableDiscoveryClientpublic class PaymentMain8006 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentMain8006.class, args); &#125;&#125; 消费者： application.yml： 123456789101112131415###consul服务端口号server: port: 80spring: application: name: cloud-consumer-order ####consul注册中心地址 cloud: consul: host: localhost port: 8500 discovery: #hostname: 127.0.0.1 service-name: $&#123;spring.application.name&#125; 主启动类：同生产者 配置类： 12345678@Configurationpublic class ApplicationContextConfig &#123; @Bean @LoadBalanced public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125;&#125; 业务类 Controller： 12345678910111213@RestController@Slf4jpublic class OrderConsulController &#123; public static final String INVOKE_URL = &quot;http://cloud-provider-pament&quot;; @Resource private RestTemplate restTemplate; @GetMapping(&quot;/consumer/payment/consul&quot;) public String paymentInfo() &#123; return restTemplate.getForObject(INVOKE_URL, String.class); &#125;&#125; 服务调用Ribbon基本介绍SpringCloud Ribbon 是基于 Netflix Ribbon 实现的一套负载均衡工具，提供客户端的软件负载均衡算法和服务调用，Ribbon 客户端组件提供一系列完善的配置项如连接超时，重试等 官网： https://github.com/Netflix/ribbon/wiki/Getting-Started （已进入维护模式，未来替换为 Load Banlancer） 负载均衡 Load Balance (LB) 就是将用户的请求平摊的分配到多个服务上，从而达到系统的 HA（高可用） 常见的负载均衡算法： 轮询：为请求选择健康池中的第一个后端服务器，然后按顺序往后依次选择 最小连接：优先选择连接数最少，即压力最小的后端服务器，在会话较长的情况下可以采取这种方式 散列：根据请求源的 IP 的散列（hash）来选择要转发的服务器，可以一定程度上保证特定用户能连接到相同的服务器，如果应用需要处理状态而要求用户能连接到和之前相同的服务器，可以采取这种方式 Ribbon 本地负载均衡客户端与 Nginx 服务端负载均衡区别： Nginx 是服务器负载均衡，客户端所有请求都会交给 Nginx，然后由 Nginx 实现转发请求，即负载均衡是由服务端实现的 Ribbon 本地负载均衡，在调用微服务接口时会在注册中心上获取注册信息服务列表，然后缓存到 JVM 本地，从而在本地实现 RPC 远程服务调用技术 集中式 LB 和进程内 LB 的对比： 集中式 LB：在服务的消费方和提供方之间使用独立的 LB 设施（如 Nginx），由该设施把访问请求通过某种策略转发至服务的提供方 进程内 LB：将 LB 逻辑集成到消费方，消费方从服务注册中心获知有哪些服务可用，然后从中选择出一个服务器，Ribbon 属于该类 工作流程Ribbon 是一个软负载均衡的客户端组件 第一步先选择 EurekaServer，优先选择在同一个区域内负载较少的 Server 第二步根据用户指定的策略，再从 Server 取到的服务注册列表中选择一个地址 核心组件Ribbon 核心组件 IRule 接口，主要实现类： RoundRobinRule：轮询 RandomRule：随机 RetryRule：先按照 RoundRobinRule 的策略获取服务，如果获取服务失败则在指定时间内会进行重试 WeightedResponseTimeRule：对 RoundRobinRule 的扩展，响应速度越快的实例选择权重越大，越容易被选择 BestAvailableRule：会先过滤掉由于多次访问故障而处于断路器跳闸状态的服务，然后选择一个并发量最小的服务 AvailabilityFilteringRule：先过滤掉故障实例，再选择并发较小的实例 ZoneAvoidanceRule：默认规则，复合判断 Server 所在区域的性能和 Server 的可用性选择服务器 注意：官方文档明确给出了警告，自定义负载均衡配置类不能放在 @ComponentScan 所扫描的当前包下以及子包下 更换负载均衡算法方式： 自定义负载均衡配置类 MySelfRule： 1234567@Configurationpublic class MySelfRule &#123; @Bean public IRule myRule() &#123; return new RandomRule();//定义为随机负载均衡算法 &#125;&#125; 主启动类添加 @RibbonCilent 注解 123456789@SpringBootApplication@EnableEurekaClient// 指明访问的服务CLOUD-PAYMENT-SERVICE，以及指定负载均衡策略@RibbonClient(name = &quot;CLOUD-PAYMENT-SERVICE&quot;, configuration= MySelfRule.class)public class OrderMain80 &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderMain80.class, args); &#125;&#125; OpenFeign基本介绍Feign 是一个声明式 WebService 客户端，能让编写 Web 客户端更加简单，只要创建一个接口并添加注解 @Feign 即可，可以与 Eureka 和 Ribbon 组合使用支持负载均衡，所以一般用在消费者端 OpenFeign 在 Feign 的基础上支持了 SpringMVC 注解，并且 @FeignClient 注解可以解析 @RequestMapping 注解下的接口，并通过动态代理的方式产生实现类，在实现类中做负载均衡和服务调用 优点：利用 RestTemplate 对 HTTP 请求的封装处理，形成了一套模版化的调用方法。但是对服务依赖的调用可能不止一处，往往一个接口会被多处调用，所以一个微服务接口上面标注一个 @Feign 注解，就可以完成包装依赖服务的调用 基本使用@FeignClient(“provider name”) 注解使用规则： 声明的方法签名必须和 provider 微服务中的 controller 中的方法签名一致 如果需要传递参数，那么 @RequestParam 、@RequestBody 、@PathVariable 也需要加上 改造消费者服务 引入 pom 依赖：OpenFeign 整合了 Ribbon，具有负载均衡的功能 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; application.yml：不将其注册到 Eureka 作为微服务 12345678910server: port: 80eureka: client: # 表示不将其注入Eureka作为微服务，不作为Eureak客户端了，而是作为Feign客户端 register-with-eureka: false service-url: # 集群版 defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka 主启动类：开启 Feign 12345678@SpringBootApplication@EnableFeignClients //不作为Eureak客户端了，而是作为Feign客户端public class OrderOpenFeignMain80 &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderOpenFeignMain80.class, args); &#125;&#125; 新建 Service 接口：PaymentFeignService 接口和 @FeignClient 注解，完成 Feign 的包装调用 123456789@Component@FeignClient(value = &quot;CLOUD-PAYMENT-SERVICE&quot;) // 作为一个Feign功能绑定的的接口public interface PaymentFeignService &#123; @GetMapping(value = &quot;/payment/get/&#123;id&#125;&quot;) public CommonResult&lt;Payment&gt; getPaymentById(@PathVariable(&quot;id&quot;) long id); @GetMapping(&quot;/payment/feign/timeout&quot;) public String paymentFeignTimeout();&#125; Controller： 12345678910111213141516171819@RestController@Slf4jpublic class OrderFeignController &#123; @Autowired private PaymentFeignService paymentFeignService; @GetMapping(&quot;/consumer/payment/get/&#123;id&#125;&quot;) public CommonResult&lt;Payment&gt; getPayment(@PathVariable(&quot;id&quot;) long id) &#123; // 返回对象为响应体中数据转化成的对象，基本上可以理解为JSON return paymentFeignService.getPaymentById(id); &#125; @GetMapping(&quot;/consumer/payment/feign/timeout&quot;) public String paymentFeignTimeout() &#123; // openfeign-ribbon，客户端一般默认等待1s return paymentFeignService.paymentFeignTimeout(); &#125;&#125; 超时问题Feign 默认是支持 Ribbon，Feign 客户端的负载均衡和超时控制都由 Ribbon 控制 设置 Feign 客户端的超时等待时间： 12345ribbon: #指的是建立连接后从服务器读取到可用资源所用的时间 ReadTimeout: 5000 #指的是建立连接所用的时间，适用于网络状况正常的情况下,两端连接所用的时间 ConnectTimeout: 5000 演示超时现象：OpenFeign 默认等待时间为 1 秒钟，超过后会报错 服务提供方 Controller： 123456789@GetMapping(&quot;/payment/feign/timeout&quot;)public String paymentFeignTimeout() &#123; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return serverPort;&#125; 消费者 PaymentFeignService 和 OrderFeignController 参考上一小节代码 测试报错： !](C:\\Users\\Seazean\\Desktop\\123\\Cloud-OpenFeign超时错误.png) 日志级别Feign 提供了日志打印功能，可以通过配置来调整日志级别，从而了解 Feign 中 HTTP 请求的细节 NONE 默认的，不显示任何日志 BASIC 仅记录请求方法、URL、响应状态码及执行时间 HEADERS 除了 BASIC 中定义的信息之外，还有请求和响应的头信息 FULL 除了 HEADERS 中定义的信息外，还有请求和响应的正文及元数据 配置在消费者端 新建 config.FeignConfig 文件：配置日志 Bean 1234567@Configurationpublic class FeignConfig &#123; @Bean Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125;&#125; application.yml： 1234logging: level: # feign 日志以什么级别监控哪个接口 com.atguigu.springcloud.service.PaymentFeignService: debug Debug 后查看后台日志 服务熔断Hystrix基本介绍Hystrix 是一个用于处理分布式系统的延迟和容错的开源库，在分布式系统里，许多依赖会出现调用失败，比如超时、异常等，Hystrix 能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性 断路器本身是一种开关装置，当某个服务单元发生故障之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个符合预期的、可处理的备选响应（FallBack），而不是长时间的等待或者抛出调用方无法处理的异常，这样就保证了服务调用方的线程不会被长时间地占用，避免了故障在分布式系统中的蔓延，乃至雪崩 服务降级 Fallback：系统不可用时需要一个兜底的解决方案或备选响应，向调用方返回一个可处理的响应 服务熔断 Break：达到最大服务访问后，直接拒绝访问 服务限流 Flowlimit：高并发操作时严禁所有请求一次性过来拥挤，一秒钟 N 个，有序排队进行 官方文档：https://github.com/Netflix/Hystrix/wiki/How-To-Use 服务降级案例构建生产者模块： 引入 pom 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 主启动类：开启 Feign 12345678@SpringBootApplication@EnableEurekaClient@EnableCircuitBreaker // 降级使用public class PaymentHystrixMain8001 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentHystrixMain8001.class, args); &#125;&#125; Controller： 1234567891011121314151617181920@RestController@Slf4jpublic class PaymentController &#123; @Resource private PaymentService paymentService; @Value(&quot;$&#123;server.port&#125;&quot;) private String serverPort; // 正常访问 @GetMapping(&quot;/payment/hystrix/ok/&#123;id&#125;&quot;) private String paymentInfo_Ok(@PathVariable(&quot;id&quot;) Integer id) &#123; return paymentService.paymentInfo_Ok(id); &#125;\t// 超时 @GetMapping(&quot;/payment/hystrix/timeout/&#123;id&#125;&quot;) private String paymentInfo_Timeout(@PathVariable(&quot;id&quot;) Integer id) &#123; // service 层有 Thread.sleep() 操作，保证超时 return paymentService.paymentInfo_Timeout(id); &#125;&#125; Service： 12345678910111213141516@Servicepublic class PaymentService &#123; public String paymentInfo_Ok(Integer id) &#123; return &quot;线程池: &quot; + Thread.currentThread().getName() + &quot;paymentInfo_OK, id: &quot; + id&quot;; &#125; public String paymentInfo_Timeout(Integer id) &#123; int timeNumber = 3; try &#123; TimeUnit.SECONDS.sleep(timeNumber); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;线程池: &quot; + Thread.currentThread().getName() + &quot; payment_Timeout, id: &quot; + id; &#125;&#125; jmeter 压测两个接口，发现接口 paymentInfo_Ok 也变的卡顿 消费者模块： Service 接口： 123456789@Component@FeignClient(value = &quot;CLOUD-PROVIDER-HYSTRIX-PAYMENT&quot;)public interface PaymentHystrixService &#123; @GetMapping(&quot;/payment/hystrix/ok/&#123;id&#125;&quot;) public String paymentInfo_Ok(@PathVariable(&quot;id&quot;) Integer id); @GetMapping(&quot;/payment/hystrix/timeout/&#123;id&#125;&quot;) public String paymentInfo_Timeout(@PathVariable(&quot;id&quot;) Integer id);&#125; Controller： 12345678910111213141516@RestController@Slf4jpublic class OrderHystirxController &#123; @Resource PaymentHystrixService paymentHystrixService; @GetMapping(&quot;/consumer/payment/hystrix/ok/&#123;id&#125;&quot;) public String paymentInfo_Ok(@PathVariable(&quot;id&quot;) Integer id) &#123; return paymentHystrixService.paymentInfo_Ok(id); &#125; @GetMapping(&quot;/consumer/payment/hystrix/timeout/&#123;id&#125;&quot;) public String paymentInfo_Timeout(@PathVariable(&quot;id&quot;) Integer id) &#123; return paymentHystrixService.paymentInfo_Timeout(id); &#125;&#125; 测试：使用的是 Feign 作为客户端，默认 1s 没有得到响应就会报超时错误，进行并发压测 解决： 超时导致服务器变慢（转圈）：超时不再等待 出错（宕机或程序运行出错）：出错要有兜底 降级操作生产者端和消费者端都可以进行服务降级，使用 @HystrixCommand 注解指定降级后的方法 生产者端：主启动类添加新注解 @EnableCircuitBreaker，业务类（Service）方法进行如下修改， 123456789101112// 模拟拥堵的情况@HystrixCommand(fallbackMethod = &quot;paymentInfo_TimeoutHandler&quot;, commandProperties = &#123; //规定这个线程的超时时间是3s，3s后就由fallbackMethod指定的方法“兜底”（服务降级） @HystrixProperty(name=&quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value = &quot;3000&quot;)&#125;)public String paymentInfo_Timeout(Integer id) &#123; // 超时或者出错&#125;public String paymentInfo_TimeoutHandler(Integer id) &#123; return &quot;线程池：&quot; + Thread.currentThread().getName() + &quot; paymentInfo_TimeoutHandler, id: &quot; + id&quot;;&#125; 服务降级的方法和业务处理的方法混杂在了一块，耦合度很高，并且每个方法配置一个服务降级方法 在业务类Controller上加 @DefaultProperties(defaultFallback &#x3D; “method_name”) 注解 在需要服务降级的方法上标注 @HystrixCommand 注解，如果 @HystrixCommand 里没有指明 fallbackMethod，就默认使用 @DefaultProperties 中指明的降级服务 1234567891011121314151617181920212223242526@RestController@Slf4j@DefaultProperties(defaultFallback = &quot;payment_Global_FallbackMethod&quot;)public class OrderHystrixController &#123; @Resource PaymentHystrixService paymentHystrixService; @GetMapping(&quot;/consumer/payment/hystrix/ok/&#123;id&#125;&quot;) public String paymentInfo_Ok(@PathVariable(&quot;id&quot;) Integer id) &#123; return paymentHystrixService.paymentInfo_OK(id); &#125; @HystrixCommand public String paymentInfo_Timeout(@PathVariable(&quot;id&quot;) Integer id) &#123; return paymentHystrixService.paymentInfo_Timeout(id); &#125; public String paymentTimeOutFallbackMethod(@PathVariable(&quot;id&quot;) Integer id) &#123; return &quot;fallback&quot;; &#125; // 下面是全局fallback方法 public String payment_Global_FallbackMethod() &#123; return &quot;Global fallback&quot;; &#125;&#125; 客户端调用服务端，遇到服务端宕机或关闭等极端情况，为 Feign 客户端定义的接口添加一个服务降级实现类即可实现解耦 application.yml：配置文件中开启了 Hystrix 1234# 用于服务降级 在注解 @FeignClient中添加fallbackFactory属性值feign: hystrix: enabled: true #在Feign中开启Hystrix Service：统一为接口里面的方法进行异常处理，服务异常找 PaymentFallbackService，来统一进行服务降级的处理 12345678910@Component@FeignClient(value = &quot;PROVIDER-HYSTRIX-PAYMENT&quot;, fallback = PaymentFallbackService.class)public interface PaymentHystrixService &#123; @GetMapping(&quot;/payment/hystrix/ok/&#123;id&#125;&quot;) public String paymentInfo_OK(@PathVariable(&quot;id&quot;) Integer id); @GetMapping(&quot;/payment/hystrix/timeout/&#123;id&#125;&quot;) public String paymentInfo_Timeout(@PathVariable(&quot;id&quot;) Integer id);&#125; PaymentFallbackService： 123456789101112@Componentpublic class PaymentFallbackService implements PaymentHystrixService &#123; @Override public String paymentInfo_OK(Integer id) &#123; return &quot;------PaymentFallbackService-paymentInfo_Ok, fallback&quot;; &#125; @Override public String paymentInfo_Timeout(Integer id) &#123; return &quot;------PaymentFallbackService-paymentInfo_Timeout, fallback&quot;; &#125;&#125; 服务熔断熔断类型熔断机制是应对雪崩效应的一种微服务链路保护机制，当扇出链路的某个微服务出错不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回错误的响应信息 Hystrix 会监控微服务间调用的状况，当失败的调用到一定阈值，缺省时 5 秒内 20 次调用失败，就会启动熔断机制；当检测到该节点微服务调用响应正常后（检测方式是尝试性放开请求），自动恢复调用链路 熔断打开：请求不再进行调用当前服务，再有请求调用时将不会调用主逻辑，而是直接调用降级 fallback。实现了自动的发现错误并将降级逻辑切换为主逻辑，减少响应延迟效果。内部设置时钟一般为 MTTR（Mean time to repair，平均故障处理时间），当打开时长达到所设时钟则进入半熔断状态 熔断关闭：熔断关闭不会对服务进行熔断，服务正常调用 熔断半开：部分请求根据规则调用当前服务，如果请求成功且符合规则则认为当前服务恢复正常，关闭熔断，反之继续熔断 熔断操作涉及到断路器的四个重要参数：快照时间窗、请求总数阀值、窗口睡眠时间、错误百分比阀值 circuitBreaker.enabled：是否开启断路器 metrics.rollingStats.timeInMilliseconds：快照时间窗口，断路器确定是否打开需要统计一些请求和错误数据，而统计的时间范围就是快照时间窗，默认为最近的 10 秒 circuitBreaker.requestVolumeThreshold：请求总数阀值，该属性设置在快照时间窗内（默认 10s）使断路器跳闸的最小请求数量（默认是 20），如果 10s 内请求数小于设定值，就算请求全部失败也不会触发断路器 circuitBreaker.sleepWindowInMilliseconds：窗口睡眠时间，短路多久以后开始尝试是否恢复进入半开状态，默认 5s circuitBreaker.errorThresholdPercentage：错误百分比阀值，失败率达到多少后将断路器打开 123456789101112131415 //总的意思就是在n(10)毫秒内的时间窗口期内，m次请求中有p% (60%)的请求失败了，那么断路器启动@HystrixCommand(fallbackMethod = &quot;paymentCircuitBreaker_fallback&quot;, commandProperties = &#123; @HystrixProperty(name = &quot;circuitBreaker.enabled&quot;, value = &quot;true&quot;), @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;), @HystrixProperty(name = &quot;circuitBreaker.sleepWindowInMilliseconds&quot;, value = &quot;10000&quot;), @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;, value = &quot;60&quot;) &#125;)public String paymentCircuitBreaker(@PathVariable(&quot;id&quot;) Integer id) &#123; if (id &lt; 0) &#123; throw new RuntimeException(&quot;******id 不能负数&quot;); &#125; String serialNumber = IdUtil.simpleUUID(); // 等价于UUID.randomUUID().toString() return Thread.currentThread().getName() + &quot;\\t&quot; + &quot;调用成功，流水号: &quot; + serialNumber;&#125; 开启：满足一定的阈值（默认 10 秒内超过 20 个请求次数）、失败率达到阈值（默认 10 秒内超过 50% 的请求失败） 关闭：一段时间之后（默认是 5 秒），断路器是半开状态，会让其中一个请求进行转发，如果成功断路器会关闭，反之继续开启 工作流程具体工作流程： 创建 HystrixCommand（用在依赖的服务返回单个操作结果的时候） 或 HystrixObserableCommand（用在依赖的服务返回多个操作结果的时候） 对象 命令执行，其中 HystrixComand 实现了下面前两种执行方式，而 HystrixObservableCommand 实现了后两种执行方式 execute()：同步执行，从依赖的服务返回一个单一的结果对象， 或是在发生错误的时候抛出异常 queue()：异步执行， 直接返回 一个 Future 对象， 其中包含了服务执行结束时要返回的单一结果对象 observe()：返回 Observable 对象，代表了操作的多个结果，它是一个 Hot Obserable（不论事件源是否有订阅者，都会在创建后对事件进行发布，所以对于 Hot Observable 的每个订阅者都有可能是从事件源的中途开始的，并可能只是看到了整个操作的局部过程） toObservable()：同样会返回 Observable 对象，也代表了操作的多个结果，但它返回的是一个 Cold Observable（没有订阅者的时候并不会发布事件，而是进行等待，直到有订阅者之后才发布事件，所以对于 Cold Observable 的订阅者，它可以保证从一开始看到整个操作的全部过程） 若当前命令的请求缓存功能是被启用的，并且该命令缓存命中，那么缓存的结果会立即以 Observable 对象的形式返回 检查断路器是否为打开状态，如果断路器是打开的，那么 Hystrix 不会执行命令，而是转接到 fallback 处理逻辑（第 8 步）；如果断路器是关闭的，检查是否有可用资源来执行命令（第 5 步） 线程池&#x2F;请求队列&#x2F;信号量是否占满，如果命令依赖服务的专有线程池和请求队列，或者信号量（不使用线程池时）已经被占满， 那么 Hystrix 也不会执行命令， 而是转接到 fallback 处理逻辑（第 8 步） Hystrix 会根据我们编写的方法来决定采取什么样的方式去请求依赖服务 HystrixCommand.run()：返回一个单一的结果，或者抛出异常 HystrixObservableCommand.construct()：返回一个Observable 对象来发射多个结果，或通过 onError 发送错误通知 Hystrix会将”成功”、”失败”、”拒绝”、”超时”等信息报告给断路器，而断路器会维护一组计数器来统计这些数据。断路器会使用这些统计数据来决定是否要将断路器打开，来对某个依赖服务的请求进行”熔断&#x2F;短路” 当命令执行失败的时候，Hystrix 会进入 fallback 尝试回退处理，通常也称该操作为”服务降级”，而能够引起服务降级情况： 第 4 步：当前命令处于”熔断&#x2F;短路”状态，断路器是打开的时候 第 5 步：当前命令的线程池、请求队列或 者信号量被占满的时候 第 6 步：HystrixObservableCommand.construct() 或 HystrixCommand.run() 抛出异常的时候 当 Hystrix 命令执行成功之后， 它会将处理结果直接返回或是以 Observable 的形式返回 注意：如果、没有为命令实现降级逻辑或者在降级处理逻辑中抛出了异常， Hystrix 依然会返回一个 Observable 对象， 但是它不会发射任何结果数据，而是通过 onError 方法通知命令立即中断请求，并通过 onError() 方法将引起命令失败的异常发送给调用者 官方文档：https://github.com/Netflix/Hystrix/wiki/How-it-Works 服务监控Hystrix 提供了准实时的调用监控（Hystrix Dashboard），Hystrix 会持续的记录所有通过 Hystrix 发起的请求的执行信息，并以统计报表和图形的形式展示给用户，包括每秒执行多少请求多少成功，多少失败等，Netflix 通过 hystrix-metrics-event-stream 项目实现了对以上指标的监控，Spring Cloud 提供了 Hystrix Dashboard 的整合，对监控内容转化成可视化页面 引入 pom 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt; application.yml：只需要端口即可 12server: port: 9001 主启动类： 1234567@SpringBootApplication@EnableHystrixDashboard // 开启Hystrix仪表盘public class HystrixDashboardMain9001 &#123; public static void main(String[] args) &#123; SpringApplication.run(HystrixDashboardMain9001.class, args); &#125;&#125; 所有微服务（生产者）提供类 8001&#x2F;8002&#x2F;8003 都需要监控依赖配置 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 启动测试：http://localhost:9001/hystrix 新版本 Hystrix 需要在需要监控的微服务端的主启动类中指定监控路径，不然会报错 1234567891011121314151617181920212223@SpringBootApplication@EnableEurekaClient // 本服务启动后会自动注册进eureka服务中@EnableCircuitBreaker // 对hystrixR熔断机制的支持public class PaymentHystrixMain8001 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentHystrixMain8001.class, args); &#125; /** ======================================需要添加的代码================== *此配置是为了服务监控而配置，与服务容错本身无关，springcloud升级后的坑 *ServletRegistrationBean因为springboot的默认路径不是&quot;/hystrix.stream&quot;， *只要在自己的项目里配置上下面的servlet就可以了 */ @Bean public ServletRegistrationBean getServlet() &#123; HystrixMetricsStreamServlet streamServlet = new HystrixMetricsStreamServlet(); ServletRegistrationBean registrationBean = new ServletRegistrationBean(streamServlet); registrationBean.setLoadOnStartup(1); registrationBean.addUrlMappings(&quot;/hystrix.stream&quot;); registrationBean.setName(&quot;HystrixMetricsStreamServlet&quot;); return registrationBean; &#125;&#125; 指标说明： 服务网关ZuulSpringCloud 中所集成的 Zuul 版本，采用的是 Tomcat 容器，基于 Servlet 之上的一个阻塞式处理模型，不支持任何长连接，用 Java 实现，而 JVM 本身会有第一次加载较慢的情况，使得 Zuul 的性能相对较差 官网： https://github.com/Netflix/zuul/wiki Gateway基本介绍SpringCloud Gateway 是 Spring Cloud 的一个全新项目，基于 Spring 5.0+Spring Boot 2.0 和 Project Reactor 等技术开发的网关，旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。 基于 WebFlux 框架实现，而 WebFlux 框架底层则使用了高性能的 Reactor 模式通信框架 Netty（异步非阻塞响应式的框架） 基于 Filter 链的方式提供了网关基本的功能，例如：安全、监控&#x2F;指标、限流等 Gateway 的三个核心组件： Route：路由是构建网关的基本模块，由 ID、目标 URI、一系列的断言和过滤器组成，如果断言为 true 则匹配该路由 Predicate：断言，可以匹配 HTTP 请求中的所有内容（例如请求头或请求参数），如果请求参数与断言相匹配则进行路由 Filter：指 Spring 框架中的 GatewayFilter实例，使用过滤器可以在请求被路由前或之后（拦截）对请求进行修改 核心逻辑：路由转发 + 执行过滤器链 客户端向 Spring Cloud Gateway 发出请求，然后在 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler Handler 通过指定的过滤器链来将请求发送到际的服务执行业务逻辑，然后返回 过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（pre）或之后（post）执行业务逻辑 Filter 在 pre 类型的过滤器可以做参数校验、权限校验、流量监控、日志输出、协议转换等，在 post 类型的过滤器中可以做响应内容、响应头的修改、日志的输出、流量监控等 网关使用配置方式Gateway 网关路由有两种配置方式，分别为通过 yml 配置和注入 Bean 引入 pom 依赖：Gateway 不需要 spring-boot-starter-web 依赖，否在会报错，原因是底层使用的是 WebFlux 与 Web 冲突 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt; application.yml： 123456789101112131415server: port: 9527spring: application: name: cloud-gatewayeureka: instance: hostname: cloud-gateway-service client: #服务提供者provider注册进eureka服务列表内 service-url: register-with-eureka: true fetch-registry: true defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka #集群版 主启动类（网关不需要业务类）： 1234567@SpringBootApplication@EnableEurekaClientpublic class GateWayMain9527 &#123; public static void main(String[] args) &#123; SpringApplication.run(GateWayMain9527.class, args); &#125;&#125; 以前访问 provider-payment8001 中的 Controller 方法，通过 localhost:8001&#x2F;payment&#x2F;get&#x2F;id 和 localhost:8001&#x2F;payment&#x2F;lb，项目不想暴露 8001 端口号，希望在 8001 外面套一层 9527 端口： 12345678910111213141516171819server: port: 9527spring: application: name: cloud-gateway## =====================新增==================== cloud: gateway: routes: - id: payment_routh # payment_route\t#路由的ID，没有固定规则但要求【唯一】，建议配合服务名 uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/get/** # 断言，路径相匹配的进行路由 - id: payment_routh2 # payment_route#路由的ID，没有固定规则但要求【唯一】，建议配合服务名 uri: http://localhost:8001 #匹配后提供服务的路由地址 predicates: - Path=/payment/lb/** # 断言，路径相匹配的进行路由 uri + predicate 拼接就是具体的接口请求路径，通过 localhost:9527 映射的地址 predicate 断言 http://localhost:8001下面有一个 &#x2F;payment&#x2F;get&#x2F;** 的地址，如果找到了该地址就返回 true，可以用 9527 端口访问，进行端口的适配 ** 表示通配符，因为这是一个不确定的参数 注入Bean通过 9527 网关访问到百度的网址 https://www.baidu.com/，在 config 包下创建一个配置类，路由规则是访问 &#x2F;baidu 跳转到百度 12345678910111213@Configurationpublic class GatewayConfig &#123; // 配置了一个 id 为 path_route_cloud 的路由规则 @Bean public RouteLocator customRouteLocator(RouteLocatorBuilder routeLocatorBuilder)&#123; // 构建一个路由器，这个routes相当于yml配置文件中的routes RouteLocatorBuilder.Builder routes = routeLocatorBuilder.routes(); // 路由器的id是：path_route_cloud，规则是访问/baidu ，将会转发到 https://www.baidu.com/ routes.route(&quot;path_route_cloud&quot;, r -&gt; r.path(&quot;/baidu&quot;).uri(&quot; https://www.baidu.com&quot;)).build(); return routes.build(); &#125;&#125; 动态路由Gateway 会根据注册中心注册的服务列表，以注册中心上微服务名为路径创建动态路由进行转发，从而实现动态路由和负载均衡，避免出现一个路由规则仅对应一个接口方法，当请求地址很多时需要很大的配置文件 application.yml 开启动态路由功能 12345678910111213141516171819spring: application: name: cloud-gateway cloud: gateway: discovery: locator: enabled: true # 开启从注册中心动态创建路由的功能，利用微服务名进行路由 routes: - id: payment_routh1 # 路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://cloud-payment-service # 匹配后提供服务的路由地址 predicates: - Path=/payment/get/** # 断言，路径相匹配的进行路由 - id: payment_routh2 #路由的ID，没有固定规则但要求唯一，建议配合服务名 uri: lb://cloud-payment-service #匹配后提供服务的路由地址 predicates: - Path=/payment/lb/** # 断言，路径相匹配的进行路由 - After=2021-09-28T19:14:51.514+08:00[Asia/Shanghai] lb:&#x2F;&#x2F; 开头代表从注册中心中获取服务，后面是需要转发到的服务名称 断言类型After Route Predicate：匹配该断言时间之后的 URI 请求 获取时间： 123456public class TimeTest &#123; public static void main(String[] args) &#123; ZonedDateTime zbj = ZonedDateTime.now(); // 默认时区 System.out.println(zbj); //2023-01-10T16:31:44.106+08:00[Asia/Shanghai] &#125;&#125; 配置 yml：动态路由小结有配置 测试：正常访问成功，将时间修改到 2023-01-10T16:31:44.106+08:00[Asia&#x2F;Shanghai] 之后访问失败 常见断言类型： Before Route Predicate：匹配该断言时间之前的 URI 请求 Between Route Predicate：匹配该断言时间之间的 URI 请求 1- Between=2022-02-02T17:45:06.206+08:00[Asia/Shanghai],2022-03-25T18:59:06.206+08:00[Asia/Shanghai] Cookie Route Predicate：Cookie 断言，两个参数分别是 Cookie name 和正则表达式，路由规则会通过获取对应的 Cookie name 值和正则表达式去匹配，如果匹配上就会执行路由 1- Cookie=username, seazean # 只有发送的请求有 cookie，而且有username=seazean这个数据才能访问，反之404 Header Route Predicate：请求头断言 1- Header=X-Request-Id, \\d+ # 请求头要有 X-Request-Id 属性，并且值为整数的正则表达式 Host Route Predicate：指定主机可以访问，可以指定多个用 , 分隔开 1- Host=**.seazean.com Method Route Predicate：请求类型断言 1- Method=GET\t# 只有 Get 请求才能访问 Path Route Predicate：路径匹配断言 Query Route Predicate：请求参数断言 1- Query=username, \\d+ # 要有参数名 username 并且值还要是整数才能路由 Filter使用Filter 链是同时满足一系列的过滤链，路由过滤器可用于修改进入的 HTTP 请求和返回的 HTTP 响应，路由过滤器只能指定路由进行使用，Spring Cloud Gateway 内置了多种路由过滤器，都由 GatewayFilter 的工厂类来产生 配置文件：https://cloud.spring.io/spring-cloud-static/spring-cloud-gateway/2.2.1.RELEASE/reference/html/#gatewayfilter-factories 自定义全局过滤器：实现两个主要接口 GlobalFilter, Ordered 1234567891011121314151617181920212223242526272829@Component@Slf4jpublic class MyLogGateWayFilter implements GlobalFilter, Ordered &#123; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; log.info(&quot;*********************come in MyLogGateWayFilter: &quot;+ new Date()); // 取出请求参数的uname对应的值 String uname = exchange.getRequest().getQueryParams().getFirst(&quot;uname&quot;); // 如果 uname 为空，就直接过滤掉，不走路由 if(uname == null)&#123; log.info(&quot;************* 用户名为 NULL 非法用户 o(╥﹏╥)o&quot;); // 判断该请求不通过时：给一个回应，返回 exchange.getResponse().setStatusCode(HttpStatus.NOT_ACCEPTABLE); return exchange.getResponse().setComplete(); &#125; // 反之，调用下一个过滤器，也就是放行：在该环节判断通过的 exchange 放行，交给下一个 filter 判断 return chain.filter(exchange); &#125; // 设置这个过滤器在Filter链中的加载顺序，数字越小，优先级越高 @Override public int getOrder() &#123; return 0; &#125;&#125; 服务配置config基本介绍SpringCloud Config 为微服务架构中的微服务提供集中化的外部配置支持（Git&#x2F;GitHub），为各个不同微服务应用的所有环境提供了一个中心化的外部配置（Config Server） SpringCloud Config 分为服务端和客户端两部分 服务端也称为分布式配置中心，是一个独立的微服务应用，连接配置服务器并为客户端提供获取配置信息，加密&#x2F;解密等信息访问接口 客户端通过指定的配置中心来管理应用资源，以及与业务相关的配置内容，并在启动时从配置中心获取和加载配置信息，配置服务器默认采用 Git 来存储配置信息，这样既有助于对环境配置进行版本管理，也可以通过 Git 客户端来方便的管理和访问配置内容 优点： 集中管理配置文件 不同环境不同配置，动态化的配置更新，分环境部署比如 dev&#x2F;test&#x2F;prod&#x2F;beta&#x2F;release 运行期间动态调整配置，服务向配置中心统一拉取配置的信息，服务不需要重启即可感知到配置的变化并应用新的配置 将配置信息以 Rest 接口的形式暴露 官网： https://cloud.spring.io/spring-cloud-static/spring-cloud-config/2.2.1.RELEASE/reference/html/ 服务端构建 Config Server 模块 引入 pom 依赖： 12345&lt;!--springCloud Config Server--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt; application.yml： 123456789101112131415161718192021222324server: port: 3344spring: application: name: cloud-config-center #注册进Eureka服务器的微服务名 cloud: config: server: git: # GitHub上面的git仓库名字 这里可以写https地址跟ssh地址，https地址需要配置username和 password uri: git@github.com:seazean/springcloud-config.git default-label: main search-paths: - springcloud-config\t# 搜索目录 # username: # password: label: main # 读取分支,以前是master#服务注册到eureka地址eureka: client: service-url: defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka #集群版 search-paths 表示远程仓库下有一个叫做 springcloud-config 的，label 则表示读取 main分支里面的内容 主启动类： 12345678@SpringBootApplication@EnableEurekaClient@EnableConfigServer //开启SpringCloud的public class ConfigCenterMain3344 &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigCenterMain3344.class, args); &#125;&#125; 配置读取规则： 12345/&#123;application&#125;/&#123;profile&#125;[/&#123;label&#125;]/&#123;application&#125;-&#123;profile&#125;.yml/&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.yml/&#123;application&#125;-&#123;profile&#125;.properties/&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.properties label：分支 name：服务名 profile：环境（dev&#x2F;test&#x2F;prod） 比如：http://localhost:3344/master/config-dev.yaml 客户端基本配置配置客户端 Config Client，客户端从配置中心（Config Server）获取配置信息 引入 pom 依赖： 12345&lt;!--这里就是客户端的SpringCloud config 因为是客户端所以没有server--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt; bootstrap.yml：系统级配置，优先级更高，application.yml 是用户级的资源配置项 Spring Cloud 会创建一个 Bootstrap Context 作为 Spring 应用的 Application Context 的父上下文，初始化的时候 Bootstrap Context 负责从外部源加载配置属性并解析配置，这两个上下文共享一个从外部获取的 Environment，为了配置文件的加载顺序和分级管理，这里使用 bootstrap.yml 1234567891011121314151617181920server: port: 3355\t# 构建多个微服务，3366 3377 等spring: application: name: config-client cloud: #Config客户端配置 config: label: main #分支名称 以前是master name: config #配置文件名称 profile: dev #读取后缀名称 # main分支上config-dev.yml的配置文件被读取 http://config-3344.com:3344/master/config-dev.yml uri: http://localhost:3344 # 配置中心地址k#服务注册到eureka地址eureka: client: service-url: defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka 主启动类： 1234567@SpringBootApplication@EnableEurekaClientpublic class ConfigClientMain3355 &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigClientMain3355.class, args); &#125;&#125; 业务类：将配置信息以 REST 窗口的形式暴露 12345678910@RestControllerpublic class ConfigClientController &#123; @Value(&quot;$&#123;config.info&#125;&quot;) private String configInfo; @GetMapping(&quot;/configInfo&quot;) public String getConfigInfo() &#123; return configInfo; &#125;&#125; 动态刷新分布式配置的动态刷新问题，修改 GitHub 上的配置文件，Config Server 配置中心立刻响应，但是 Config Client 客户端没有任何响应，需要重启客户端 引入 pom 依赖： 123456789&lt;!--web/actuator这两个一般一起使用，写在一起--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 修改 yml，暴露监控端口：SpringBoot 的 actuator 启动端点监控 Web 端默认加载默认只有两个 info，health 可见的页面节点 123456management: endpoints: web: exposure: include: &quot;*&quot; # 表示包含所有节点页面 exclude: env,beans\t# 表示排除env、beans 业务类：加 @RefreshScope 注解 123456789101112@RestController@RefreshScopepublic class ConfigClientController &#123; // 从配置文件中取前缀为server.port的值 @Value(&quot;$&#123;config.info&#125;&quot;) private String configInfo;\t// config-&#123;profile&#125;.yml @GetMapping(&quot;/configInfo&quot;) public String getConfigInfo() &#123; return configInfo; &#125;&#125; 此时客户端还是没有刷新，需要发送 POST 请求刷新 3355：curl -X POST &quot;http://localhost:3355/actuator/refresh 引出问题： 在微服务多的情况下，每个微服务都需要执行一个 POST 请求，手动刷新成本太大 可否广播，一次通知，处处生效，大范围的实现自动刷新 解决方法：Bus 总线 服务消息Bus基本介绍Spring Cloud Bus 能管理和传播分布式系统间的消息，就像分布式执行器，可用于广播状态更改、事件推送、微服务间的通信通道等 消息总线：在微服务架构的系统中，通常会使用轻量级的消息代理来构建一个共用的消息主题，并让系统中所有微服务实例都连接上来。由于该主题中产生的消息会被所有实例监听和消费，所以称为消息总线 基本原理：ConfigClient 实例都监听 MQ 中同一个 Topic（默认 springCloudBus)，当一个服务刷新数据时，会把信息放入 Topic 中，这样其它监听同一 Topic 的服务就能得到通知，然后去更新自身的配置 全局广播利用消息总线接触一个服务端 ConfigServer 的 /bus/refresh 断点，从而刷新所有客户端的配置 改造 ConfigClient： 引入 MQ 的依赖： 12345&lt;!--添加消息总线RabbitMQ支持--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;&lt;/dependency&gt; yml 文件添加 MQ 信息： 1234567891011121314151617181920server: port: 3344spring: application: name: config-client #注册进Eureka服务器的微服务名 cloud: # rabbitmq相关配置 rabbitmq: host: localhost port: 5672 username: guest password: guest# rabbitmq相关配置,暴露bus刷新配置的端点management: endpoints: # 暴露bus刷新配置的端点 web: exposure: include: &#x27;bus-refresh&#x27; 只需要调用一次 curl -X POST &quot;http://localhost:3344/actuator/bus-refresh，可以实现全局广播 定点通知动态刷新情况下，只通知指定的微服务，比如只通知 3355 服务，不通知 3366，指定具体某一个实例生效，而不是全部 公式：http://localhost:port/actuator/bus-refresh/&#123;destination&#125; &#x2F;bus&#x2F;refresh 请求不再发送到具体的服务实例上，而是发给 Config Server 并通过 destination 参数类指定需要更新配置的服务或实例 Stream基本介绍Spring Cloud Stream 是一个构建消息驱动微服务的框架，通过定义绑定器 Binder 作为中间层，实现了应用程序与消息中间件细节之间的隔离，屏蔽底层消息中间件的差异，降低切换成本，统一消息的编程模型 Stream 中的消息通信方式遵循了发布订阅模式，Binder 可以生成 Binding 用来绑定消息容器的生产者和消费者，Binding 有两种类型 Input 和 Output，Input 对应于消费者（消费者从 Stream 接收消息），Output 对应于生产者（生产者从 Stream 发布消息） Binder：连接中间件 Channel：通道，是队列 Queue 的一种抽象，在消息通讯系统中实现存储和转发的媒介，通过 Channel 对队列进行配置 Source、Sink：生产者和消费者 中文手册：https://m.wang1314.com/doc/webapp/topic/20971999.html 基本使用Binder 是应用与消息中间件之间的封装，目前实现了 Kafka 和 RabbitMQ 的 Binder，可以动态的改变消息类型（Kafka 的 Topic 和 RabbitMQ 的 Exchange），可以通过配置文件实现，常用注解如下： @Input：标识输入通道，接收的消息通过该通道进入应用程序 @Output：标识输出通道，发布的消息通过该通道离开应用程序 @StreamListener：监听队列，用于消费者队列的消息接收 @EnableBinding：信道 Channel 和 Exchange 绑定 生产者发消息模块： 引入 pom 依赖：RabbitMQ 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; application.yml： 123456789101112131415161718192021222324252627282930313233server: port: 8801spring: application: name: cloud-stream-provider cloud: stream: binders: # 在此处配置要绑定的rabbitmq的服务信息； defaultRabbit: # 表示定义的名称，用于于binding整合 type: rabbit # 消息组件类型 environment: # 设置rabbitmq的相关的环境配置 spring: rabbitmq: host: localhost port: 5672 username: guest password: guest bindings: # 服务的整合处理 output: # 这个名字是一个通道的名称 destination: studyExchange # 表示要使用的Exchange名称定义 content-type: application/json\t# 设置消息类型，本次为json，文本则设置“text/plain” binder: defaultRabbit # 设置要绑定的消息服务的具体设置eureka: client: # 客户端进行Eureka注册的配置 service-url: defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka instance: lease-renewal-interval-in-seconds: 2 # 设置心跳的时间间隔（默认是30秒） lease-expiration-duration-in-seconds: 5 # 如果现在超过了5秒的间隔（默认是90秒） instance-id: send-8801.com # 在信息列表时显示主机名称 prefer-ip-address: true # 访问的路径变为IP地址 主启动类： 1234567@SpringBootApplication@EnableEurekaClientpublic class StreamMQMain8801 &#123; public static void main(String[] args) &#123; SpringApplication.run(StreamMQMain8801.class, args); &#125;&#125; 业务类：MessageChannel 的实例名必须是 output，否则无法启动 123456789101112131415161718// 可以理解为定义消息的发送管道Source对应output(生产者)，Sink对应input(消费者)@EnableBinding(Source.class)// @Service：这里不需要，不是传统的controller调用service。这个service是和rabbitMQ打交道的// IMessageProvider 只有一个 send 方法的接口public class MessageProviderImpl implements IMessageProvider &#123; @Resource private MessageChannel output; // 消息的发送管道 @Override public String send() &#123; String serial = UUID.randomUUID().toString(); //创建消息，通过output这个管道向消息中间件发消息 this.output.send(MessageBuilder.withPayload(serial).build()); System.out.println(&quot;***serial: &quot; + serial); return serial; &#125;&#125; Controller： 12345678910@RestControllerpublic class SendMessageController &#123; @Resource private IMessageProvider messageProvider; @GetMapping(value = &quot;/sendMessage&quot;) public String sendMessage() &#123; return messageProvider.send(); &#125;&#125; 消费者模块：8802 和 8803 两个消费者 application.yml：只标注出与生产者不同的地方 12345678910111213141516171819server: port: 8802spring: application: name: cloud-stream-consumer cloud: stream: # ... bindings: # 服务的整合处理 input: # 这个名字是一个通道的名称 # ... binder: &#123; defaultRabbit &#125; # 设置要绑定的消息服务的具体设置eureka: # ... instance: # ... instance-id: receive-8802.com # 在信息列表时显示主机名称 Controller： 12345678910111213@Component@EnableBinding(Sink.class) // 理解为定义一个消息消费者的接收管道public class ReceiveMessageListener &#123; @Value(&quot;$&#123;server.port&#125;&quot;) private String serverPort; @StreamListener(Sink.INPUT) //输入源：作为一个消息监听者 public void input(Message&lt;String&gt; message) &#123; // 获取到消息 String messageStr = message.getPayload(); System.out.println(&quot;消费者1号，-------&gt;接收到的消息：&quot; + messageStr + &quot;\\t port: &quot; + serverPort); &#125;&#125; 高级特性重复消费问题：生产者 8801 发送一条消息后，8802 和 8803 会同时收到 8801 的消息 解决方法：微服务应用放置于同一个 group 中，能够保证消息只会被其中一个应用消费一次。不同的组是可以全面消费的（重复消费），同一个组内的多个消费者会发生竞争关系，只有其中一个可以消费 123456bindings: input: destination: studyExchange content-type: application/json binder: &#123; defaultRabbit &#125; group: seazean\t# 设置分组 消息持久化问题： 停止 8802&#x2F;8803 并去除掉 8802 的分组 group: seazean，8801 先发送 4 条消息到 MQ 先启动 8802，无分组属性配置，后台没有打出来消息，消息丢失 再启动 8803，有分组属性配置，后台打印出来了 MQ 上的消息 Sleuth基本介绍Spring Cloud Sleuth 提供了一套完整的分布式请求链路跟踪的解决方案，并且兼容支持了 zipkin 在微服务框架中，一个客户端发起的请求在后端系统中会经过多次不同的服务节点调用来协同产生最后的请求结果，形成一条复杂的分布式服务调用链路，链路中的任何一环出现高延时或错误都会引起整个请求最后的失败，所以需要链路追踪 Sleuth 官网：https://github.com/spring-cloud/spring-cloud-sleuth zipkin 下载地址：https://repo1.maven.org/maven2/io/zipkin/java/zipkin-server/ 链路监控Sleuth 负责跟踪整理，zipkin 负责可视化展示 1java -jar zipkin-server-2.12.9-exec.jar # 启动 zipkin 访问 http://localhost:9411/zipkin/ 展示交互界面 一条请求链路通过 Trace ID 唯一标识，Span 标识发起的请求信息 Trace：类似于树结构的 Span 集合，表示一条调用链路，存在唯一 ID 标识 Span：表示调用链路来源，通俗的理解 Span 就是一次请求信息，各个 Span 通过 ParentID 关联起来 服务生产者模块： 引入 pom 依赖： 12345&lt;!--包含了sleuth+zipkin--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;&lt;/dependency&gt; application.yml： 123456789101112server: port: 8001spring: application: name: cloud-payment-service zipkin: base-url: http://localhost:9411 sleuth: sampler: #采样率值介于 0 到 1 之间，1 则表示全部采集 probability: 1 业务类： 1234@GetMapping(&quot;/payment/zipkin&quot;)public String paymentZipkin() &#123; return &quot;hi ,i&#x27;am paymentzipkin server fall back，welcome to seazean&quot;;&#125; 服务消费者模块： application.yml： 123456789101112server: port: 80# 微服务名称spring: application: name: cloud-order-service zipkin: base-url: http://localhost:9411 sleuth: sampler: probability: 1 业务类： 12345@GetMapping(&quot;/comsumer/payment/zipkin&quot;)public String paymentZipKin() &#123; String result = restTemplate.getForObject(&quot;http://localhost:8001&quot; + &quot;/payment/zipkin/&quot;, String.class); return result;&#125; AlibabaSpring Cloud Alibaba 致力于提供微服务开发的一站式解决方案，此项目包含开发分布式应用微服务的必需组件，方便开发者通过 Spring Cloud 编程模型轻松使用这些组件来开发分布式应用服务 服务限流降级：默认支持 WebServlet、WebFlux、OpenFeign、RestTemplate、Spring Cloud Gateway、Zuul、Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控。 服务注册与发现：适配 Spring Cloud 服务注册与发现标准，默认集成了 Ribbon 的支持 分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新 消息驱动能力：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力 分布式事务：使用 @GlobalTransactional 注解， 高效并且对业务零侵入地解决分布式事务问题 阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务 分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 Worker（schedulerx-client）上执行 官方文档：https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md 官方手册：https://spring-cloud-alibaba-group.github.io/github-pages/greenwich/spring-cloud-alibaba.html Nacos基本介绍Nacos 全称 Dynamic Naming and Configuration Service，一个更易于构建云原生应用的动态服务发现、配置管理和服务的管理平台，Nacos &#x3D; Eureka + Config + Bus 下载地址：https://github.com/alibaba/nacos/releases 启动命令：命令运行成功后直接访问 http://localhost:8848/nacos，默认账号密码都是 nacos 1startup.cmd -m standalone # standalone 代表着单机模式运行，非集群模式 关闭命令： 1shutdown.cmd 注册中心对比：C 一致性，A 可用性，P 分区容错性 注册中心 CAP 模型 控制台管理 Eureka AP 支持 Zookeeper CP 不支持 Consul CP 支持 Nacos AP 支持 切换模式：curl -X PUT &#39;$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&amp;value=CP 官网：https://nacos.io 注册中心Nacos 作为服务注册中心 服务提供者： 引入 pom 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; application.yml： 12345678910111213141516server: port: 9001spring: application: name: nacos-payment-provider cloud: nacos: discovery: server-addr: localhost:8848 #配置Nacos地址，注册到Nacos# 做监控需要把这个全部暴露出来management: endpoints: web: exposure: include: &#x27;*&#x27; 主启动类：注解是 EnableDiscoveryClient 1234567@EnableDiscoveryClient@SpringBootApplicationpublic class PaymentMain9001 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentMain9001.class, args); &#125;&#125; Controller： 12345678910@RestControllerpublic class PaymentController &#123; @Value(&quot;$&#123;server.port&#125;&quot;) private String serverPort; @GetMapping(value = &quot;/payment/nacos/&#123;id&#125;&quot;) public String getPayment(@PathVariable(&quot;id&quot;) Integer id) &#123; return &quot;nacos registry, serverPort: &quot; + serverPort + &quot;\\t id&quot; + id; &#125;&#125; 管理后台服务： 新建一个模块端口是 9002，其他与 9001 服务一样，nacos-payment-provider 的实例数就变为 2 服务消费者： application.yml： 1234567891011121314server: port: 83spring: application: name: nacos-order-consumer cloud: nacos: discovery: server-addr: localhost:8848# 消费者将要去访问的微服务名称(注册成功进nacos的微服务提供者)service-url: nacos-user-service: http://nacos-payment-provider 主启动类： 1234567@SpringBootApplication@EnableDiscoveryClientpublic class OrderNacosMain83 &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderNacosMain83.class, args); &#125;&#125; 业务类： 12345678@Configurationpublic class ApplicationContextBean &#123; @Bean @LoadBalanced // 生产者集群状态下，必须添加，防止找不到实例 public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125;&#125; 123456789101112131415@RestController@Slf4jpublic class OrderNacosController &#123; @Resource private RestTemplate restTemplate;\t// 从配置文件中读取 URL @Value(&quot;$&#123;service-url.nacos-user-service&#125;&quot;) private String serverURL; @GetMapping(&quot;/consumer/payment/nacos/&#123;id&#125;&quot;) public String paymentInfo(@PathVariable(&quot;id&quot;) Long id) &#123; String result = restTemplate.getForObject(serverURL + &quot;/payment/nacos/&quot; + id, String.class); return result; &#125;&#125; 配置中心基础配置把配置文件写进 Nacos，然后再用 Nacos 做 config 这样的功能，直接从 Nacos 上抓取服务的配置信息 在 Nacos 中，dataId 的完整格式如下 $&#123;prefix&#125;-$&#123;spring.profiles.active&#125;.$&#123;file-extension&#125; prefix：默认为 spring.application.name 的值，也可以通过配置项 spring.cloud.nacos.config.prefix 来配置 spring.profiles.active：当前环境对应的 profile，当该值为空时，dataId 的拼接格式变成 $&#123;prefix&#125;.$&#123;file-extension&#125; file-exetension：配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置，目前只支持 properties 和 yaml 类型（不是 yml） 构建流程： 引入 pom 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; 配置两个 yml 文件：配置文件的加载是存在优先级顺序的，bootstrap 优先级高于 application bootstrap.yml：全局配置 12345678910111213141516# nacos配置server: port: 3377spring: application: name: nacos-config-client cloud: nacos: discovery: server-addr: localhost:8848 #Nacos服务注册中心地址 config: server-addr: localhost:8848 #Nacos作为配置中心地址 file-extension: yaml #指定yaml格式的配置# $&#123;spring.application.name&#125;-$&#123;spring.profile.active&#125;.$&#123;spring.cloud.nacos.config.file-extension&#125; application.yml：服务独立配置，表示服务要去配置中心找名为 nacos-config-client-dev.yaml 的文件 123spring: profiles: active: dev # 表示开发环境 主启动类： 1234567@SpringBootApplication@EnableDiscoveryClientpublic class NacosConfigClientMain3377 &#123; public static void main(String[] args) &#123; SpringApplication.run(NacosConfigClientMain3377.class, args); &#125;&#125; 业务类：@RefreshScope 注解使当前类下的配置支持 Nacos 的动态刷新功能 1234567891011@RestController@RefreshScopepublic class ConfigClientController &#123; @Value(&quot;$&#123;config.info&#125;&quot;) private String configInfo; @GetMapping(&quot;/config/info&quot;) public String getConfigInfo() &#123; return configInfo; &#125;&#125; 新增配置，然后访问 http://localhost:3377/config/info 分类配置分布式开发中的多环境多项目管理问题，Namespace 用于区分部署环境，Group 和 DataID 逻辑上区分两个目标对象 Namespace 默认 public，主要用来实现隔离，图示三个开发环境 Group 默认是 DEFAULT_GROUP，Group 可以把不同的微服务划分到同一个分组里面去 加载配置DataID 方案：指定 spring.profile.active 和配置文件的 DataID 来使不同环境下读取不同的配置 Group 方案：通过 Group 实现环境分区，在 config 下增加一条 Group 的配置即可 Namespace 方案： 123456789101112131415server: port: 3377spring: application: name: nacos-config-client cloud: nacos: discovery: server-addr: localhost:8848 #Nacos服务注册中心地址 config: server-addr: localhost:8848 #Nacos作为配置中心地址 file-extension: yaml #指定yaml格式的配置 group: DEV_GROUP namespace: 95d44530-a4a6-4ead-98c6-23d192cee298 集群架构集群部署参考官方文档，Nacos 支持的三种部署模式： 单机模式：用于测试和单机使用 集群模式：用于生产环境，确保高可用 多集群模式：用于多数据中心场景 集群部署文档：https://nacos.io/zh-cn/docs/v2/guide/admin/cluster-mode-quick-start.html 默认 Nacos 使用嵌入式数据库 derby 实现数据的存储，重启 Nacos 后配置文件不会消失，但是多个 Nacos 节点数据存储存在一致性问题，每个 Nacos 都有独立的嵌入式数据库，所以 Nacos 采用了集中式存储的方式来支持集群化部署，目前只支持 MySQL 的存储 Windows 下 Nacos 切换 MySQL 存储： 在 Nacos 安装目录的 conf 目录下找到一个名为 nacos-mysql.sql 的脚本并执行 在 conf 目录下找到 application.properties，增加如下数据 123456spring.datasource.platform=mysql db.num=1db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_config?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=truedb.user=usernamedb.password=password 重新启动 Nacos，可以看到是个全新的空记录界面 Linux 参考：https://www.yuque.com/mrlinxi/pxvr4g/rnahsn#dPvMy Sentinel基本介绍Sentinel 是面向分布式、多语言异构化服务架构的流量治理组件 Sentinel 分为两个部分： 核心库（Java 客户端）不依赖任何框架&#x2F;库，能够运行于 Java 8 及以上的版本的运行时环境 控制台（Dashboard）主要负责管理推送规则、监控、管理机器信息等 下载到本地，运行命令：java -jar sentinel-dashboard-1.8.2.jar （要求 Java8，且 8080 端口不能被占用），访问 http://localhost:8080/，账号密码均为 sentinel 官网：https://sentinelguard.io 下载地址：https://github.com/alibaba/Sentinel/releases 基本使用构建演示工程： 引入 pom 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; application.yml：sentinel.transport.port 端口配置会在应用对应的机器上启动一个 HTTP Server，该 Server 与 Sentinel 控制台做交互。比如 Sentinel 控制台添加了 1 个限流规则，会把规则数据 Push 给 Server 接收，Server 再将规则注册到 Sentinel 中 12345678910111213141516171819202122server: port: 8401spring: application: name: cloudalibaba-sentinel-service cloud: nacos: discovery: server-addr: localhost:8848 # Nacos 服务注册中心地址【需要启动Nacos8848】 sentinel: transport: # 配置Sentinel dashboard地址 dashboard: localhost:8080 # 默认8719端口，假如被占用会自动从8719开始依次+1扫描,直至找到未被占用的端口 port: 8719management: endpoints: web: exposure: include: &#x27;*&#x27; 主启动类： 1234567@EnableDiscoveryClient@SpringBootApplicationpublic class SentinelMainApp8401 &#123; public static void main(String[] args) &#123; SpringApplication.run(SentinelMainApp8401.class, args); &#125;&#125; 流量控制 Controller： 12345678910111213@RestController@Slf4jpublic class FlowLimitController &#123; @GetMapping(&quot;/testA&quot;) public String testA() &#123; return &quot;------testA&quot;; &#125; @GetMapping(&quot;/testB&quot;) public String testB() &#123; return &quot;------testB&quot;; &#125;&#125; Sentinel 采用懒加载机制，需要先访问 http://localhost:8401/testA，控制台才能看到 流控规则流量控制规则 FlowRule：同一个资源可以同时有多个限流规则 资源名 resource：限流规则的作用对象，Demo 中为 testA 针对资源 limitApp：针对调用者进行限流，默认为 default 代表不区分调用来源 阈值类型 grade：QPS 或线程数模式 单机阈值 count：限流阈值 流控模式 strategy：调用关系限流策略 直接：资源本身达到限流条件直接限流 关联：当关联的资源达到阈值时，限流自身 链路：只记录指定链路上的流量，从入口资源进来的流量 流控效果 controlBehavior： 快速失败：直接失败，抛出异常 Warm Up：冷启动，根据 codeFactory（冷加载因子，默认 3）的值，从 count&#x2F;codeFactory 开始缓慢增加，给系统预热时间 排队等待：匀速排队，让请求以匀速的方式通过，阈值类型必须设置为 QPS，否则无效 通过调用 SystemRuleManager.loadRules() 方法来用硬编码的方式定义流量控制规则： 1234567private void initSystemProtectionRule() &#123; List&lt;SystemRule&gt; rules = new ArrayList&lt;&gt;(); SystemRule rule = new SystemRule(); rule.setHighestSystemLoad(10); rules.add(rule); SystemRuleManager.loadRules(rules);&#125; 详细内容参考文档：https://sentinelguard.io/zh-cn/docs/flow-control.html 降级熔断Sentinel 熔断降级会在调用链路中某个资源出现不稳定状态时，对这个资源的调用进行限制，让请求快速失败，避免影响到其它的资源而导致级联错误。当资源被降级后，在接下来的降级时间窗口之内，对该资源的调用都自动熔断（默认行为是抛出 DegradeException） Sentinel 提供以下几种熔断策略： 资源名 resource：限流规则的作用对象，Demo 中为 testA 熔断策略 grade： 慢调用比例（SLOW_REQUEST_RATIO）：以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大的响应时间），请求的响应时间大于该值则统计为慢调用。当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且慢调用的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断 异常比例（ERROR_RATIO）：当单位统计时长内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态，若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100% 异常数 （ERROR_COUNT）：当单位统计时长内的异常数目超过阈值之后会自动进行熔断。经过熔断时长后熔断器会进入探测恢复状态，若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断 单机阈值 count：慢调用比例模式下为慢调用临界 RT；异常比例&#x2F;异常数模式下为对应的阈值 熔断时长 timeWindow：单位为 s 最小请求数 minRequestAmount：熔断触发的最小请求数，请求数小于该值时即使异常比率超出阈值也不会熔断，默认 5 统计时长 statIntervalMs：单位统计时长 慢调用比例阈值 slowRatioThreshold：仅慢调用比例模式有效 注意异常降级仅针对业务异常，对 Sentinel 限流降级本身的异常 BlockException 不生效，为了统计异常比例或异常数，需要通过 Tracer.trace(ex) 记录业务异常或者通过@SentinelResource 注解会自动统计业务异常 123456789101112131415Entry entry = null;try &#123; entry = SphU.entry(resource); // Write your biz code here. // &lt;&lt;BIZ CODE&gt;&gt;&#125; catch (Throwable t) &#123; if (!BlockException.isBlockException(t)) &#123; Tracer.trace(t); &#125;&#125; finally &#123; if (entry != null) &#123; entry.exit(); &#125;&#125; 详细内容参考文档：https://sentinelguard.io/zh-cn/docs/circuit-breaking.html 热点限流热点参数限流会统计传入参数中的热点参数，并根据配置的限流阈值与模式，对包含热点参数的资源调用进行限流，Sentinel 利用 LRU 策略统计最近最常访问的热点参数，结合令牌桶算法来进行参数级别的流控 引入 @SentinelResource 注解：https://sentinelguard.io/zh-cn/docs/annotation-support.html value：Sentinel 资源名，默认为请求路径，这里 value 的值可以任意写，但是约定与 Restful 地址一致 blockHandler：表示触发了 Sentinel 中配置的流控规则，就会调用兜底方法 del_testHotKey blockHandlerClass：如果设置了该值，就会去该类中寻找 blockHandler 方法 fallback：用于在抛出异常的时候提供 fallback 处理逻辑 说明：fallback 对应服务降级（服务出错了需要有个兜底方法），blockHandler 对应服务熔断（服务不可用的兜底方法） 1234567891011@GetMapping(&quot;/testHotKey&quot;)@SentinelResource(value = &quot;testHotKey&quot;, blockHandler = &quot;del_testHotKey&quot;)public String testHotkey(@RequestParam(value = &quot;p1&quot;, required = false) String p1, @RequestParam(value = &quot;p1&quot;, required = false) String p2) &#123; return &quot;------testHotkey&quot;;&#125;// 自定义的兜底方法，必须是 BlockExceptionpublic String del_testHotKey(String p1, String p2, BlockException e) &#123; return &quot;不用默认的兜底提示 Blocked by Sentinel(flow limiting)，自定义提示：del_testHotKeyo.&quot;;&#125; 图示设置 p1 参数限流，规则是 1s 访问 1 次，当 p1&#x3D;5 时 QPS &gt; 100，只访问 p2 不会出现限流 http://localhost:8401/testHotKey?p2=b 参数索引 paramIdx：热点参数的索引，图中索引 0 对应方法中的 p1 参数 参数例外项 paramFlowItemList：针对指定的参数值单独设置限流阈值，不受 count 阈值的限制，仅支持基本类型和字符串类型 说明：@SentinelResource 只管控制台配置规则问题，出现运行时异常依然会报错 详细内容参考文档：https://sentinelguard.io/zh-cn/docs/parameter-flow-control.html 系统规则Sentinel 系统自适应保护从整体维度对应用入口流量进行控制，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性 系统规则支持以下的阈值类型： Load（仅对 Linux&#x2F;Unix-like 机器生效）：当系统 load1 超过阈值，且系统当前的并发线程数超过系统容量时才会触发系统保护，系统容量由系统的 maxQps * minRt 计算得出，设定参考值一般是 CPU cores * 2.5 RT：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒 线程数：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护 入口 QPS：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护 CPU usage：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0） 详细内容参考文档：https://sentinelguard.io/zh-cn/docs/system-adaptive-protection.html 服务调用消费者需要进行服务调用 引入 pom 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; application.yml：激活 Sentinel 对 Feign 的支持 123feign: sentinel: enabled: true 主启动类：加上 @EnableFeignClient 注解开启 OpenFeign 业务类： 123456789// 指明调用失败的兜底方法在PaymentFallbackService，使用 fallback 方式是无法获取异常信息的，// 如果想要获取异常信息，可以使用 fallbackFactory 参数@FeignClient(value = &quot;nacos-payment-provider&quot;, fallback = PaymentFallbackService.class)public interface PaymentFeignService &#123; // 去生产则服务中找相应的接口，方法签名一定要和生产者中 controller 的一致 @GetMapping(value = &quot;/paymentSQL/&#123;id&#125;&quot;) public CommonResult&lt;Payment&gt; paymentSQL(@PathVariable(&quot;id&quot;) Long id);&#125; 12345@Component //不要忘记注解，降级方法public class PaymentFallbackService implements PaymentFeignService &#123; @Override public CommonResult&lt;Payment&gt; paymentSQL(Long id) &#123; return new CommonResult&lt;&gt;(444,&quot;服务降级返回,没有该流水信息-------PaymentFallbackSe 持久化配置持久化： 引入 pom 依赖： 12345&lt;!--SpringCloud ailibaba sentinel-datasource-nacos 后续做持久化用到--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt;&lt;/dependency&gt; 添加 Nacos 数据源配置： 12345678910111213141516171819202122232425262728server: port: 8401spring: application: name: cloudalibaba-sentinel-service cloud: nacos: discovery: server-addr: localhost:8848 #Nacos服务注册中心地址 sentinel: transport: dashboard: localhost:8080 port: 8719 # 关闭默认收敛所有URL的入口context，不然链路限流不生效 web-context-unify: false # filter: # enabled: false # 关闭自动收敛 #持久化配置 datasource: ds1: nacos: server-addr: localhost:8848 dataId: cloudalibaba-sentinel-service groupId: DEFAULT_GROUP data-type: json rule-type: flow Seata分布事物一个分布式事务过程，可以用分布式处理过程的一 ID + 三组件模型来描述： XID (Transaction ID)：全局唯一的事务 ID，在这个事务ID下的所有事务会被统一控制 TC (Transaction Coordinator)：事务协调者，维护全局和分支事务的状态，驱动全局事务提交或回滚 TM (Transaction Manager)：事务管理器，定义全局事务的范围，开始全局事务、提交或回滚全局事务 RM (Resource Manager)：资源管理器，管理分支事务处理的资源，与 TC 交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚 典型的分布式事务流程： TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的 XID XID 在微服务调用链路的上下文中传播（也就是在多个 TM，RM 中传播） RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖 TM 向 TC 发起针对 XID 的全局提交或回滚决议 TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求 基本配置Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案 下载 seata-server 文件修改 conf 目录下的配置文件 file.conf：自定义事务组名称、事务日志存储模式为 db、数据库连接信息 事务分组：seata 的资源逻辑，可以按微服务的需要，在应用程序（客户端）对自行定义事务分组，每组取一个名字 数据库新建库 seata，建表 db_store.sql 在 https://github.com/seata/seata/tree/2.x/script/server/db 目录里面 registry.conf：指明注册中心为 Nacos，及修改 Nacos 连接信息 启动 Nacos 和 Seata，如果 DB 报错，需要把将 lib 文件夹下 mysql-connector-java-5.1.30.jar 删除，替换为自己 MySQL 连接器版本 官网：https://seata.io 下载地址：https://github.com/seata/seata/releases 基本介绍：https://seata.io/zh-cn/docs/overview/what-is-seata.html 基本使用两个注解： Spring 提供的本地事务：@Transactional Seata 提供的全局事务：**@GlobalTransactional** 搭建简单 Demo： 创建 UNDO_LOG 表：SEATA AT 模式需要 UNDO_LOG 表，如果一个模块的事务提交了，Seata 会把提交了哪些数据记录到 undo_log 表中，如果这时 TC 通知全局事务回滚，那么 RM 就从 undo_log 表中获取之前修改了哪些资源，并根据这个表回滚 1234567891011121314-- 注意此处0.3.0+ 增加唯一索引 ux_undo_logCREATE TABLE `undo_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20) NOT NULL, `xid` varchar(100) NOT NULL, `context` varchar(128) NOT NULL, `rollback_info` longblob NOT NULL, `log_status` int(11) NOT NULL, `log_created` datetime NOT NULL, `log_modified` datetime NOT NULL, `ext` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 引入 pom 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-seata&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt;&lt;/dependency&gt; application.yml： 1234567891011121314151617spring: application: name: seata-order-service cloud: alibaba: seata: # 自定义事务组名称需要与seata-server中file.conf中配置的事务组ID对应 # vgroup_mapping.my_test_tx_group = &quot;my_group&quot; tx-service-group: my_group nacos: discovery: server-addr: localhost:8848 datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/seata_order?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=UTC username: root password: 123456 构建三个服务： 1234567891011121314151617// 仓储服务public interface StorageService &#123; // 扣除存储数量 void deduct(String commodityCode, int count);&#125;// 订单服务public interface OrderService &#123;\t// 创建订单 Order create(String userId, String commodityCode, int orderCount);&#125;// 帐户服务public interface AccountService &#123; // 从用户账户中借出 void debit(String userId, int money);&#125; 业务逻辑：增加 @GlobalTransactional 注解 123456789101112131415161718192021public class OrderServiceImpl implements OrderService &#123;\t@Resource private OrderDAO orderDAO;\t@Resource private AccountService accountService; @Transactional(rollbackFor = Exception.class) public Order create(String userId, String commodityCode, int orderCount) &#123; int orderMoney = calculate(commodityCode, orderCount); // 账户扣钱 accountService.debit(userId, orderMoney); Order order = new Order(); order.userId = userId; order.commodityCode = commodityCode; order.count = orderCount; order.money = orderMoney; return orderDAO.insert(order); &#125;&#125; 1234567891011121314public class BusinessServiceImpl implements BusinessService &#123;\t@Resource private StorageService storageService;\t@Resource private OrderService orderService; // 采购，涉及多服务的分布式事务问题 @GlobalTransactional @Transactional(rollbackFor = Exception.class) public void purchase(String userId, String commodityCode, int orderCount) &#123; storageService.deduct(commodityCode, orderCount); orderService.create(userId, commodityCode, orderCount); &#125;&#125; 详细示例参考：https://github.com/seata/seata-samples/tree/master/springcloud-nacos-seata"}]